<?xml version="1.0" encoding="iso-8859-1"?>
<ipm_job_profile>
<calltable nsections="1" >
<section module="MPI" nentries="69" >
<entry name="MPI_Init" />
<entry name="MPI_Init_thread" />
<entry name="MPI_Finalize" />
<entry name="MPI_Comm_rank" />
<entry name="MPI_Comm_size" />
<entry name="MPI_Send" />
<entry name="MPI_Ssend" />
<entry name="MPI_Rsend" />
<entry name="MPI_Bsend" />
<entry name="MPI_Isend" />
<entry name="MPI_Issend" />
<entry name="MPI_Irsend" />
<entry name="MPI_Ibsend" />
<entry name="MPI_Recv" />
<entry name="MPI_Irecv" />
<entry name="MPI_Sendrecv" />
<entry name="MPI_Sendrecv_replace" />
<entry name="MPI_Wait" />
<entry name="MPI_Waitany" />
<entry name="MPI_Waitall" />
<entry name="MPI_Waitsome" />
<entry name="MPI_Probe" />
<entry name="MPI_Iprobe" />
<entry name="MPI_Send_init" />
<entry name="MPI_Ssend_init" />
<entry name="MPI_Rsend_init" />
<entry name="MPI_Bsend_init" />
<entry name="MPI_Recv_init" />
<entry name="MPI_Buffer_attach" />
<entry name="MPI_Buffer_detach" />
<entry name="MPI_Test" />
<entry name="MPI_Testany" />
<entry name="MPI_Testall" />
<entry name="MPI_Testsome" />
<entry name="MPI_Start" />
<entry name="MPI_Startall" />
<entry name="MPI_Bcast" />
<entry name="MPI_Reduce" />
<entry name="MPI_Reduce_scatter" />
<entry name="MPI_Barrier" />
<entry name="MPI_Gather" />
<entry name="MPI_Gatherv" />
<entry name="MPI_Scatter" />
<entry name="MPI_Scatterv" />
<entry name="MPI_Scan" />
<entry name="MPI_Allgather" />
<entry name="MPI_Allgatherv" />
<entry name="MPI_Allreduce" />
<entry name="MPI_Alltoall" />
<entry name="MPI_Alltoallv" />
<entry name="MPI_Comm_group" />
<entry name="MPI_Comm_compare" />
<entry name="MPI_Comm_dup" />
<entry name="MPI_Comm_create" />
<entry name="MPI_Comm_split" />
<entry name="MPI_Comm_free" />
<entry name="MPI_Ibcast" />
<entry name="MPI_Ireduce" />
<entry name="MPI_Ireduce_scatter" />
<entry name="MPI_Igather" />
<entry name="MPI_Igatherv" />
<entry name="MPI_Iscatter" />
<entry name="MPI_Iscatterv" />
<entry name="MPI_Iscan" />
<entry name="MPI_Iallgather" />
<entry name="MPI_Iallgatherv" />
<entry name="MPI_Iallreduce" />
<entry name="MPI_Ialltoall" />
<entry name="MPI_Ialltoallv" />
</section>
</calltable>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="0" mpi_size="696" stamp_init="1724765564.465366" stamp_final="1724765674.545955" username="apac4" allocationname="unknown" flags="0" pid="1612117" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10081e+02" utime="7.64556e+01" stime="2.54115e+01" mtime="6.93802e+01" gflop="0.00000e+00" gbyte="9.29726e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.93802e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09939e+02" utime="7.64231e+01" stime="2.54009e+01" mtime="6.93802e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.93802e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="43" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2152e+09" > 7.3292e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2098e+09" > 3.2550e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1121e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.3658e-03 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8428e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1457e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 7.9870e-04 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.1468e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0574e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3125e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3751e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383411" nkey="207" >
<hent key="02400100000000000000080000000004" call="MPI_Isend" bytes="2048" orank="4" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="02400100000000000000080000000014" call="MPI_Isend" bytes="2048" orank="20" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="02400100000000000000080000000018" call="MPI_Isend" bytes="2048" orank="24" region="0" commid="0" count="150" tid="0" op="" dtype="" >1.0090e-03 4.7684e-06 1.7166e-05</hent>
<hent key="024001000000000000000800000002A0" call="MPI_Isend" bytes="2048" orank="672" region="0" commid="0" count="157" tid="0" op="" dtype="" >9.2125e-04 3.8147e-06 3.2902e-05</hent>
<hent key="038001000000000000000E0000000001" call="MPI_Irecv" bytes="3584" orank="1" region="0" commid="0" count="90" tid="0" op="" dtype="" >1.9550e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000003" call="MPI_Irecv" bytes="3584" orank="3" region="0" commid="0" count="203" tid="0" op="" dtype="" >6.5804e-05 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000E0000000018" call="MPI_Irecv" bytes="3584" orank="24" region="0" commid="0" count="126" tid="0" op="" dtype="" >6.9857e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000E00000002A0" call="MPI_Irecv" bytes="3584" orank="672" region="0" commid="0" count="136" tid="0" op="" dtype="" >2.6703e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >3.8147e-06 3.8147e-06 3.8147e-06</hent>
<hent key="024001000000000000000E0000000001" call="MPI_Isend" bytes="3584" orank="1" region="0" commid="0" count="182" tid="0" op="" dtype="" >5.5552e-04 9.5367e-07 7.1526e-06</hent>
<hent key="024001000000000000000E0000000003" call="MPI_Isend" bytes="3584" orank="3" region="0" commid="0" count="737" tid="0" op="" dtype="" >1.1954e-03 0.0000e+00 3.0994e-06</hent>
<hent key="024001000000000000000E0000000018" call="MPI_Isend" bytes="3584" orank="24" region="0" commid="0" count="140" tid="0" op="" dtype="" >1.0672e-03 5.0068e-06 1.6928e-05</hent>
<hent key="024001000000000000000E00000002A0" call="MPI_Isend" bytes="3584" orank="672" region="0" commid="0" count="140" tid="0" op="" dtype="" >8.3566e-04 3.8147e-06 1.4067e-05</hent>
<hent key="03800100000000000000028000000001" call="MPI_Irecv" bytes="640" orank="1" region="0" commid="0" count="397" tid="0" op="" dtype="" >9.6798e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000003" call="MPI_Irecv" bytes="640" orank="3" region="0" commid="0" count="393" tid="0" op="" dtype="" >1.5545e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000004" call="MPI_Irecv" bytes="640" orank="4" region="0" commid="0" count="398" tid="0" op="" dtype="" >1.9956e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000014" call="MPI_Irecv" bytes="640" orank="20" region="0" commid="0" count="436" tid="0" op="" dtype="" >1.0872e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000018" call="MPI_Irecv" bytes="640" orank="24" region="0" commid="0" count="19" tid="0" op="" dtype="" >1.5259e-05 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000140000000018" call="MPI_Irecv" bytes="5120" orank="24" region="0" commid="0" count="719" tid="0" op="" dtype="" >2.3389e-04 0.0000e+00 2.4080e-05</hent>
<hent key="038001000000000000000280000002A0" call="MPI_Irecv" bytes="640" orank="672" region="0" commid="0" count="28" tid="0" op="" dtype="" >9.5367e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.0081e-05 6.0081e-05 6.0081e-05</hent>
<hent key="038001000000000000001400000002A0" call="MPI_Irecv" bytes="5120" orank="672" region="0" commid="0" count="872" tid="0" op="" dtype="" >2.2864e-04 0.0000e+00 2.8849e-05</hent>
<hent key="03800100000000000000A00000000018" call="MPI_Irecv" bytes="40960" orank="24" region="0" commid="0" count="2430" tid="0" op="" dtype="" >1.3382e-03 0.0000e+00 4.9114e-05</hent>
<hent key="02400100000000000000028000000001" call="MPI_Isend" bytes="640" orank="1" region="0" commid="0" count="348" tid="0" op="" dtype="" >1.1308e-03 1.9073e-06 8.1062e-06</hent>
<hent key="02400100000000000000028000000003" call="MPI_Isend" bytes="640" orank="3" region="0" commid="0" count="391" tid="0" op="" dtype="" >6.4707e-04 9.5367e-07 6.1989e-06</hent>
<hent key="02400100000000000000028000000004" call="MPI_Isend" bytes="640" orank="4" region="0" commid="0" count="413" tid="0" op="" dtype="" >5.3144e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000028000000014" call="MPI_Isend" bytes="640" orank="20" region="0" commid="0" count="405" tid="0" op="" dtype="" >5.0092e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000028000000018" call="MPI_Isend" bytes="640" orank="24" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.7500e-04 3.8147e-06 1.9073e-05</hent>
<hent key="03800100000000000000A000000002A0" call="MPI_Irecv" bytes="40960" orank="672" region="0" commid="0" count="3041" tid="0" op="" dtype="" >9.1147e-04 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000140000000018" call="MPI_Isend" bytes="5120" orank="24" region="0" commid="0" count="989" tid="0" op="" dtype="" >5.3573e-03 1.9073e-06 3.6001e-05</hent>
<hent key="024001000000000000000280000002A0" call="MPI_Isend" bytes="640" orank="672" region="0" commid="0" count="15" tid="0" op="" dtype="" >8.0347e-05 3.8147e-06 1.1921e-05</hent>
<hent key="03800100000000000000800000000018" call="MPI_Irecv" bytes="32768" orank="24" region="0" commid="0" count="10270" tid="0" op="" dtype="" >5.2993e-03 0.0000e+00 6.6996e-05</hent>
<hent key="024001000000000000001400000002A0" call="MPI_Isend" bytes="5120" orank="672" region="0" commid="0" count="856" tid="0" op="" dtype="" >3.8764e-03 9.5367e-07 4.8876e-05</hent>
<hent key="038001000000000000008000000002A0" call="MPI_Irecv" bytes="32768" orank="672" region="0" commid="0" count="9659" tid="0" op="" dtype="" >2.9547e-03 0.0000e+00 2.5034e-05</hent>
<hent key="03800100000000000000014000000001" call="MPI_Irecv" bytes="320" orank="1" region="0" commid="0" count="583" tid="0" op="" dtype="" >1.3828e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000003" call="MPI_Irecv" bytes="320" orank="3" region="0" commid="0" count="553" tid="0" op="" dtype="" >2.6083e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000004" call="MPI_Irecv" bytes="320" orank="4" region="0" commid="0" count="477" tid="0" op="" dtype="" >2.1338e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000014" call="MPI_Irecv" bytes="320" orank="20" region="0" commid="0" count="430" tid="0" op="" dtype="" >1.0872e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000018" call="MPI_Irecv" bytes="320" orank="24" region="0" commid="0" count="23" tid="0" op="" dtype="" >1.0252e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A00000000018" call="MPI_Isend" bytes="40960" orank="24" region="0" commid="0" count="3395" tid="0" op="" dtype="" >7.2891e-02 7.8678e-06 7.2956e-05</hent>
<hent key="038001000000000000000140000002A0" call="MPI_Irecv" bytes="320" orank="672" region="0" commid="0" count="21" tid="0" op="" dtype="" >1.6451e-05 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000A000000002A0" call="MPI_Isend" bytes="40960" orank="672" region="0" commid="0" count="2929" tid="0" op="" dtype="" >5.0235e-02 6.9141e-06 9.2030e-05</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >7.4842e-03 1.7040e-03 2.3000e-03</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >6.1121e+00 0.0000e+00 1.1059e-01</hent>
<hent key="02400100000000000000800000000018" call="MPI_Isend" bytes="32768" orank="24" region="0" commid="0" count="9305" tid="0" op="" dtype="" >1.8894e-01 9.0599e-06 8.4162e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >6.6757e-06 1.9073e-06 2.8610e-06</hent>
<hent key="024001000000000000008000000002A0" call="MPI_Isend" bytes="32768" orank="672" region="0" commid="0" count="9771" tid="0" op="" dtype="" >1.7176e-01 5.9605e-06 1.2708e-04</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.2388e-04 5.5075e-05 1.0490e-04</hent>
<hent key="02400100000000000000014000000001" call="MPI_Isend" bytes="320" orank="1" region="0" commid="0" count="586" tid="0" op="" dtype="" >1.7257e-03 1.9073e-06 1.0014e-05</hent>
<hent key="02400100000000000000014000000003" call="MPI_Isend" bytes="320" orank="3" region="0" commid="0" count="535" tid="0" op="" dtype="" >7.3671e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000014000000004" call="MPI_Isend" bytes="320" orank="4" region="0" commid="0" count="448" tid="0" op="" dtype="" >4.9329e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000014000000014" call="MPI_Isend" bytes="320" orank="20" region="0" commid="0" count="464" tid="0" op="" dtype="" >4.6158e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000018" call="MPI_Isend" bytes="320" orank="24" region="0" commid="0" count="35" tid="0" op="" dtype="" >2.0909e-04 4.0531e-06 1.5974e-05</hent>
<hent key="024001000000000000000140000002A0" call="MPI_Isend" bytes="320" orank="672" region="0" commid="0" count="20" tid="0" op="" dtype="" >1.0777e-04 3.8147e-06 1.0967e-05</hent>
<hent key="03800100000000000000000000000001" call="MPI_Irecv" bytes="0" orank="1" region="0" commid="0" count="515" tid="0" op="" dtype="" >1.0300e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000003" call="MPI_Irecv" bytes="0" orank="3" region="0" commid="0" count="515" tid="0" op="" dtype="" >1.5807e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000004" call="MPI_Irecv" bytes="0" orank="4" region="0" commid="0" count="343" tid="0" op="" dtype="" >1.0610e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000014" call="MPI_Irecv" bytes="0" orank="20" region="0" commid="0" count="353" tid="0" op="" dtype="" >8.7500e-05 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000000000018" call="MPI_Irecv" bytes="0" orank="24" region="0" commid="0" count="88" tid="0" op="" dtype="" >2.5749e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002A0" call="MPI_Irecv" bytes="0" orank="672" region="0" commid="0" count="87" tid="0" op="" dtype="" >2.4796e-05 0.0000e+00 4.0531e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >6.9380e-05 1.9073e-06 5.0068e-06</hent>
<hent key="03800100000000000000030000000004" call="MPI_Irecv" bytes="768" orank="4" region="0" commid="0" count="532" tid="0" op="" dtype="" >1.2088e-04 0.0000e+00 1.3828e-05</hent>
<hent key="03800100000000000000030000000014" call="MPI_Irecv" bytes="768" orank="20" region="0" commid="0" count="638" tid="0" op="" dtype="" >1.0586e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000000000001" call="MPI_Isend" bytes="0" orank="1" region="0" commid="0" count="509" tid="0" op="" dtype="" >1.1358e-03 9.5367e-07 2.5988e-05</hent>
<hent key="02400100000000000000000000000003" call="MPI_Isend" bytes="0" orank="3" region="0" commid="0" count="542" tid="0" op="" dtype="" >4.8256e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000000000000004" call="MPI_Isend" bytes="0" orank="4" region="0" commid="0" count="328" tid="0" op="" dtype="" >2.3317e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000000000014" call="MPI_Isend" bytes="0" orank="20" region="0" commid="0" count="347" tid="0" op="" dtype="" >2.3556e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000000000018" call="MPI_Isend" bytes="0" orank="24" region="0" commid="0" count="80" tid="0" op="" dtype="" >3.8290e-04 1.9073e-06 1.0014e-05</hent>
<hent key="024001000000000000000000000002A0" call="MPI_Isend" bytes="0" orank="672" region="0" commid="0" count="91" tid="0" op="" dtype="" >3.8409e-04 1.9073e-06 1.0967e-05</hent>
<hent key="03800100000000000000060000000001" call="MPI_Irecv" bytes="1536" orank="1" region="0" commid="0" count="11" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000060000000003" call="MPI_Irecv" bytes="1536" orank="3" region="0" commid="0" count="18" tid="0" op="" dtype="" >7.3910e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000004" call="MPI_Irecv" bytes="1536" orank="4" region="0" commid="0" count="49" tid="0" op="" dtype="" >2.6941e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000060000000014" call="MPI_Irecv" bytes="1536" orank="20" region="0" commid="0" count="52" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000018" call="MPI_Irecv" bytes="1536" orank="24" region="0" commid="0" count="83" tid="0" op="" dtype="" >5.4598e-05 0.0000e+00 8.8215e-06</hent>
<hent key="038001000000000000000600000002A0" call="MPI_Irecv" bytes="1536" orank="672" region="0" commid="0" count="83" tid="0" op="" dtype="" >2.8610e-05 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000030000000004" call="MPI_Isend" bytes="768" orank="4" region="0" commid="0" count="680" tid="0" op="" dtype="" >3.6335e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000030000000014" call="MPI_Isend" bytes="768" orank="20" region="0" commid="0" count="816" tid="0" op="" dtype="" >4.2152e-04 0.0000e+00 1.0014e-05</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="43" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 2.8610e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000001" call="MPI_Irecv" bytes="448" orank="1" region="0" commid="0" count="26" tid="0" op="" dtype="" >8.8215e-06 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000001C000000003" call="MPI_Irecv" bytes="448" orank="3" region="0" commid="0" count="56" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000001" call="MPI_Isend" bytes="1536" orank="1" region="0" commid="0" count="17" tid="0" op="" dtype="" >7.1764e-05 2.8610e-06 1.5974e-05</hent>
<hent key="02400100000000000000060000000003" call="MPI_Isend" bytes="1536" orank="3" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.5259e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000060000000004" call="MPI_Isend" bytes="1536" orank="4" region="0" commid="0" count="46" tid="0" op="" dtype="" >8.7261e-05 9.5367e-07 8.8215e-06</hent>
<hent key="02400100000000000000060000000014" call="MPI_Isend" bytes="1536" orank="20" region="0" commid="0" count="67" tid="0" op="" dtype="" >1.1373e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000018" call="MPI_Isend" bytes="1536" orank="24" region="0" commid="0" count="89" tid="0" op="" dtype="" >5.7840e-04 4.7684e-06 1.7881e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002A0" call="MPI_Isend" bytes="1536" orank="672" region="0" commid="0" count="75" tid="0" op="" dtype="" >3.8862e-04 3.8147e-06 1.0967e-05</hent>
<hent key="038001000000000000000C0000000018" call="MPI_Irecv" bytes="3072" orank="24" region="0" commid="0" count="359" tid="0" op="" dtype="" >2.3460e-04 0.0000e+00 1.9073e-05</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002A0" call="MPI_Irecv" bytes="3072" orank="672" region="0" commid="0" count="348" tid="0" op="" dtype="" >1.1802e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000001C000000001" call="MPI_Isend" bytes="448" orank="1" region="0" commid="0" count="50" tid="0" op="" dtype="" >4.8876e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000001C000000003" call="MPI_Isend" bytes="448" orank="3" region="0" commid="0" count="192" tid="0" op="" dtype="" >9.4175e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000018" call="MPI_Isend" bytes="3072" orank="24" region="0" commid="0" count="350" tid="0" op="" dtype="" >2.4672e-03 5.0068e-06 2.0981e-05</hent>
<hent key="024001000000000000000C00000002A0" call="MPI_Isend" bytes="3072" orank="672" region="0" commid="0" count="359" tid="0" op="" dtype="" >2.1348e-03 3.8147e-06 3.0994e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.1656e-02 7.0820e-03 7.3540e-03</hent>
<hent key="03800100000000000000038000000001" call="MPI_Irecv" bytes="896" orank="1" region="0" commid="0" count="139" tid="0" op="" dtype="" >3.1948e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000003" call="MPI_Irecv" bytes="896" orank="3" region="0" commid="0" count="165" tid="0" op="" dtype="" >7.4625e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000004" call="MPI_Irecv" bytes="896" orank="4" region="0" commid="0" count="3140" tid="0" op="" dtype="" >7.0453e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000038000000014" call="MPI_Irecv" bytes="896" orank="20" region="0" commid="0" count="3031" tid="0" op="" dtype="" >5.3287e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000018" call="MPI_Irecv" bytes="896" orank="24" region="0" commid="0" count="31" tid="0" op="" dtype="" >1.8835e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002A0" call="MPI_Irecv" bytes="896" orank="672" region="0" commid="0" count="29" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.0599e-06 9.0599e-06 9.0599e-06</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000180000000004" call="MPI_Irecv" bytes="6144" orank="4" region="0" commid="0" count="2019" tid="0" op="" dtype="" >2.2650e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000180000000014" call="MPI_Irecv" bytes="6144" orank="20" region="0" commid="0" count="2276" tid="0" op="" dtype="" >2.4128e-04 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000180000000018" call="MPI_Irecv" bytes="6144" orank="24" region="0" commid="0" count="5" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000001800000002A0" call="MPI_Irecv" bytes="6144" orank="672" region="0" commid="0" count="10" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000038000000001" call="MPI_Isend" bytes="896" orank="1" region="0" commid="0" count="181" tid="0" op="" dtype="" >6.4564e-04 2.8610e-06 9.0599e-06</hent>
<hent key="02400100000000000000038000000003" call="MPI_Isend" bytes="896" orank="3" region="0" commid="0" count="159" tid="0" op="" dtype="" >3.2067e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000038000000004" call="MPI_Isend" bytes="896" orank="4" region="0" commid="0" count="3023" tid="0" op="" dtype="" >1.7376e-03 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000038000000014" call="MPI_Isend" bytes="896" orank="20" region="0" commid="0" count="2829" tid="0" op="" dtype="" >1.8542e-03 0.0000e+00 1.7881e-05</hent>
<hent key="02400100000000000000038000000018" call="MPI_Isend" bytes="896" orank="24" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.8477e-04 5.0068e-06 1.4782e-05</hent>
<hent key="024001000000000000000380000002A0" call="MPI_Isend" bytes="896" orank="672" region="0" commid="0" count="39" tid="0" op="" dtype="" >2.1482e-04 3.8147e-06 1.4067e-05</hent>
<hent key="02400100000000000000180000000004" call="MPI_Isend" bytes="6144" orank="4" region="0" commid="0" count="2582" tid="0" op="" dtype="" >4.3974e-03 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000180000000014" call="MPI_Isend" bytes="6144" orank="20" region="0" commid="0" count="3126" tid="0" op="" dtype="" >5.6424e-03 9.5367e-07 2.3842e-05</hent>
<hent key="02400100000000000000180000000018" call="MPI_Isend" bytes="6144" orank="24" region="0" commid="0" count="7" tid="0" op="" dtype="" >8.5592e-05 7.1526e-06 1.8120e-05</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0967e-05 1.0967e-05 1.0967e-05</hent>
<hent key="024001000000000000001800000002A0" call="MPI_Isend" bytes="6144" orank="672" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.9114e-05 6.1989e-06 7.8678e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >9.0623e+00 1.1921e-05 1.2671e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >9.1219e-04 9.1219e-04 9.1219e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0574e-02 1.0574e-02 1.0574e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >6.9191e-03 6.9191e-03 6.9191e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.4860e-01 3.3309e-03 2.4084e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.9509e-04 5.9509e-04 5.9509e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >4.8940e-03 1.6751e-03 3.2189e-03</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >7.9870e-04 1.5020e-05 8.1062e-05</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >3.8428e+01 1.9073e-06 3.8405e+01</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="03800100000000000000070000000001" call="MPI_Irecv" bytes="1792" orank="1" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000003" call="MPI_Irecv" bytes="1792" orank="3" region="0" commid="0" count="4" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000004" call="MPI_Irecv" bytes="1792" orank="4" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.5020e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000014" call="MPI_Irecv" bytes="1792" orank="20" region="0" commid="0" count="21" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000018" call="MPI_Irecv" bytes="1792" orank="24" region="0" commid="0" count="110" tid="0" op="" dtype="" >8.3447e-05 0.0000e+00 7.1526e-06</hent>
<hent key="038001000000000000000700000002A0" call="MPI_Irecv" bytes="1792" orank="672" region="0" commid="0" count="111" tid="0" op="" dtype="" >2.8610e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >2.5988e-05 1.9073e-06 5.0068e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.2227e-03 1.9073e-06 2.6240e-03</hent>
<hent key="038001000000000000000A0000000001" call="MPI_Irecv" bytes="2560" orank="1" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000A0000000004" call="MPI_Irecv" bytes="2560" orank="4" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000014" call="MPI_Irecv" bytes="2560" orank="20" region="0" commid="0" count="5" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000018" call="MPI_Irecv" bytes="2560" orank="24" region="0" commid="0" count="382" tid="0" op="" dtype="" >2.3794e-04 0.0000e+00 8.1062e-06</hent>
<hent key="038001000000000000000A00000002A0" call="MPI_Irecv" bytes="2560" orank="672" region="0" commid="0" count="374" tid="0" op="" dtype="" >1.1182e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000070000000001" call="MPI_Isend" bytes="1792" orank="1" region="0" commid="0" count="2" tid="0" op="" dtype="" >7.8678e-06 3.8147e-06 4.0531e-06</hent>
<hent key="02400100000000000000070000000003" call="MPI_Isend" bytes="1792" orank="3" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.8597e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000004" call="MPI_Isend" bytes="1792" orank="4" region="0" commid="0" count="18" tid="0" op="" dtype="" >3.3379e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000014" call="MPI_Isend" bytes="1792" orank="20" region="0" commid="0" count="20" tid="0" op="" dtype="" >3.6716e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000018" call="MPI_Isend" bytes="1792" orank="24" region="0" commid="0" count="114" tid="0" op="" dtype="" >7.3743e-04 4.7684e-06 1.9073e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3125e-02 8.3125e-02 8.3125e-02</hent>
<hent key="024001000000000000000700000002A0" call="MPI_Isend" bytes="1792" orank="672" region="0" commid="0" count="108" tid="0" op="" dtype="" >5.8722e-04 3.8147e-06 1.1206e-05</hent>
<hent key="024001000000000000000A0000000014" call="MPI_Isend" bytes="2560" orank="20" region="0" commid="0" count="2" tid="0" op="" dtype="" >3.8147e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000A0000000018" call="MPI_Isend" bytes="2560" orank="24" region="0" commid="0" count="382" tid="0" op="" dtype="" >2.6896e-03 5.0068e-06 2.4080e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.3607e-02 4.2551e-03 4.9610e-03</hent>
<hent key="024001000000000000000A00000002A0" call="MPI_Isend" bytes="2560" orank="672" region="0" commid="0" count="374" tid="0" op="" dtype="" >2.2082e-03 3.8147e-06 1.6928e-05</hent>
<hent key="03800100000000000000100000000001" call="MPI_Irecv" bytes="4096" orank="1" region="0" commid="0" count="12610" tid="0" op="" dtype="" >2.7466e-03 0.0000e+00 1.5974e-05</hent>
<hent key="03800100000000000000100000000003" call="MPI_Irecv" bytes="4096" orank="3" region="0" commid="0" count="12497" tid="0" op="" dtype="" >3.9165e-03 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000100000000018" call="MPI_Irecv" bytes="4096" orank="24" region="0" commid="0" count="2934" tid="0" op="" dtype="" >8.2517e-04 0.0000e+00 5.2214e-05</hent>
<hent key="038001000000000000001000000002A0" call="MPI_Irecv" bytes="4096" orank="672" region="0" commid="0" count="2807" tid="0" op="" dtype="" >7.9036e-04 0.0000e+00 1.7881e-05</hent>
<hent key="02400100000000000000100000000001" call="MPI_Isend" bytes="4096" orank="1" region="0" commid="0" count="12518" tid="0" op="" dtype="" >3.9159e-02 0.0000e+00 2.1935e-05</hent>
<hent key="02400100000000000000100000000003" call="MPI_Isend" bytes="4096" orank="3" region="0" commid="0" count="11963" tid="0" op="" dtype="" >2.0586e-02 0.0000e+00 3.2902e-05</hent>
<hent key="02400100000000000000100000000018" call="MPI_Isend" bytes="4096" orank="24" region="0" commid="0" count="2679" tid="0" op="" dtype="" >1.4546e-02 1.9073e-06 3.8147e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.1833e-02 2.5498e-02 2.6335e-02</hent>
<hent key="024001000000000000001000000002A0" call="MPI_Isend" bytes="4096" orank="672" region="0" commid="0" count="2802" tid="0" op="" dtype="" >1.3303e-02 9.5367e-07 6.1035e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1173e-02 1.1173e-02 1.1173e-02</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.0240e-01 1.9427e-02 4.3069e-02</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 1.9073e-06 4.0531e-06</hent>
<hent key="03800100000000000000000400000001" call="MPI_Irecv" bytes="4" orank="1" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.0892e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000003" call="MPI_Irecv" bytes="4" orank="3" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.0870e-03 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000000400000004" call="MPI_Irecv" bytes="4" orank="4" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.1355e-04 0.0000e+00 2.1935e-05</hent>
<hent key="03800100000000000000000400000014" call="MPI_Irecv" bytes="4" orank="20" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.6015e-04 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000000400000018" call="MPI_Irecv" bytes="4" orank="24" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.0345e-03 0.0000e+00 2.3842e-05</hent>
<hent key="038001000000000000001C0000000004" call="MPI_Irecv" bytes="7168" orank="4" region="0" commid="0" count="10681" tid="0" op="" dtype="" >1.2510e-03 0.0000e+00 2.1935e-05</hent>
<hent key="038001000000000000001C0000000014" call="MPI_Irecv" bytes="7168" orank="20" region="0" commid="0" count="10424" tid="0" op="" dtype="" >1.3504e-03 0.0000e+00 1.9073e-05</hent>
<hent key="038001000000000000001C0000000018" call="MPI_Irecv" bytes="7168" orank="24" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="038001000000000000000004000002A0" call="MPI_Irecv" bytes="4" orank="672" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.6328e-04 0.0000e+00 1.5974e-05</hent>
<hent key="02400100000000000000000400000001" call="MPI_Isend" bytes="4" orank="1" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.5905e-03 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000000400000003" call="MPI_Isend" bytes="4" orank="3" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.2092e-03 0.0000e+00 2.0981e-05</hent>
<hent key="02400100000000000000000400000004" call="MPI_Isend" bytes="4" orank="4" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.9803e-03 0.0000e+00 3.2496e-04</hent>
<hent key="02400100000000000000000400000014" call="MPI_Isend" bytes="4" orank="20" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.9910e-03 0.0000e+00 3.0994e-05</hent>
<hent key="02400100000000000000000400000018" call="MPI_Isend" bytes="4" orank="24" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.3173e-02 4.7684e-06 5.5075e-05</hent>
<hent key="024001000000000000001C0000000004" call="MPI_Isend" bytes="7168" orank="4" region="0" commid="0" count="10118" tid="0" op="" dtype="" >1.7426e-02 9.5367e-07 1.7166e-05</hent>
<hent key="024001000000000000001C0000000014" call="MPI_Isend" bytes="7168" orank="20" region="0" commid="0" count="9574" tid="0" op="" dtype="" >1.7853e-02 9.5367e-07 3.6001e-05</hent>
<hent key="024001000000000000000004000002A0" call="MPI_Isend" bytes="4" orank="672" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.3999e-02 3.8147e-06 5.6982e-05</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.2806e-02 1.2806e-02 1.2806e-02</hent>
<hent key="03800100000000000000020000000001" call="MPI_Irecv" bytes="512" orank="1" region="0" commid="0" count="3374" tid="0" op="" dtype="" >5.1665e-04 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000020000000003" call="MPI_Irecv" bytes="512" orank="3" region="0" commid="0" count="3344" tid="0" op="" dtype="" >4.3249e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000050000000001" call="MPI_Irecv" bytes="1280" orank="1" region="0" commid="0" count="50" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000003" call="MPI_Irecv" bytes="1280" orank="3" region="0" commid="0" count="51" tid="0" op="" dtype="" >2.9325e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000050000000004" call="MPI_Irecv" bytes="1280" orank="4" region="0" commid="0" count="131" tid="0" op="" dtype="" >7.2241e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000050000000014" call="MPI_Irecv" bytes="1280" orank="20" region="0" commid="0" count="127" tid="0" op="" dtype="" >2.2173e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000018" call="MPI_Irecv" bytes="1280" orank="24" region="0" commid="0" count="49" tid="0" op="" dtype="" >2.5511e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4431e+01 1.1921e-05 1.8408e-01</hent>
<hent key="038001000000000000000500000002A0" call="MPI_Irecv" bytes="1280" orank="672" region="0" commid="0" count="43" tid="0" op="" dtype="" >1.0014e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000020000000001" call="MPI_Isend" bytes="512" orank="1" region="0" commid="0" count="3350" tid="0" op="" dtype="" >2.7046e-03 0.0000e+00 1.2875e-05</hent>
<hent key="02400100000000000000020000000003" call="MPI_Isend" bytes="512" orank="3" region="0" commid="0" count="3208" tid="0" op="" dtype="" >1.5860e-03 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000080000000003" call="MPI_Irecv" bytes="2048" orank="3" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000080000000004" call="MPI_Irecv" bytes="2048" orank="4" region="0" commid="0" count="5" tid="0" op="" dtype="" >5.0068e-06 9.5367e-07 1.1921e-06</hent>
<hent key="03800100000000000000080000000014" call="MPI_Irecv" bytes="2048" orank="20" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000018" call="MPI_Irecv" bytes="2048" orank="24" region="0" commid="0" count="171" tid="0" op="" dtype="" >9.1314e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000800000002A0" call="MPI_Irecv" bytes="2048" orank="672" region="0" commid="0" count="151" tid="0" op="" dtype="" >3.8147e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000050000000001" call="MPI_Isend" bytes="1280" orank="1" region="0" commid="0" count="57" tid="0" op="" dtype="" >2.0742e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000050000000003" call="MPI_Isend" bytes="1280" orank="3" region="0" commid="0" count="59" tid="0" op="" dtype="" >1.2636e-04 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000050000000004" call="MPI_Isend" bytes="1280" orank="4" region="0" commid="0" count="143" tid="0" op="" dtype="" >2.2388e-04 9.5367e-07 2.8610e-06</hent>
<hent key="02400100000000000000050000000014" call="MPI_Isend" bytes="1280" orank="20" region="0" commid="0" count="149" tid="0" op="" dtype="" >2.3389e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000050000000018" call="MPI_Isend" bytes="1280" orank="24" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.9360e-04 4.0531e-06 1.3828e-05</hent>
<hent key="024001000000000000000500000002A0" call="MPI_Isend" bytes="1280" orank="672" region="0" commid="0" count="57" tid="0" op="" dtype="" >2.8181e-04 3.8147e-06 8.8215e-06</hent>
</hash>
<internal rank="0" log_i="1724765674.545955" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="1" mpi_size="696" stamp_init="1724765564.465669" stamp_final="1724765674.532665" username="apac4" allocationname="unknown" flags="0" pid="1612118" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10067e+02" utime="8.96073e+01" stime="1.38737e+01" mtime="7.28004e+01" gflop="0.00000e+00" gbyte="3.78361e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28004e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf477149014a2146056a2149d14ce" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09928e+02" utime="8.95781e+01" stime="1.38590e+01" mtime="7.28004e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28004e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1870e+09" > 6.5858e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1981e+09" > 2.9032e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3027e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9739e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.4809e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8147e-06 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5808e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.8341e-04 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0567e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3096e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4396e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="208" >
<hent key="02400100000000000000080000000005" call="MPI_Isend" bytes="2048" orank="5" region="0" commid="0" count="6" tid="0" op="" dtype="" >9.7752e-06 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000080000000015" call="MPI_Isend" bytes="2048" orank="21" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.1921e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000080000000019" call="MPI_Isend" bytes="2048" orank="25" region="0" commid="0" count="161" tid="0" op="" dtype="" >9.7108e-04 4.7684e-06 1.1921e-05</hent>
<hent key="024001000000000000000800000002A1" call="MPI_Isend" bytes="2048" orank="673" region="0" commid="0" count="175" tid="0" op="" dtype="" >9.1076e-04 3.8147e-06 1.0967e-05</hent>
<hent key="038001000000000000000E0000000000" call="MPI_Irecv" bytes="3584" orank="0" region="0" commid="0" count="182" tid="0" op="" dtype="" >8.2970e-05 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000E0000000002" call="MPI_Irecv" bytes="3584" orank="2" region="0" commid="0" count="133" tid="0" op="" dtype="" >2.9325e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000019" call="MPI_Irecv" bytes="3584" orank="25" region="0" commid="0" count="143" tid="0" op="" dtype="" >5.3167e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E00000002A1" call="MPI_Irecv" bytes="3584" orank="673" region="0" commid="0" count="137" tid="0" op="" dtype="" >5.5313e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="024001000000000000000E0000000000" call="MPI_Isend" bytes="3584" orank="0" region="0" commid="0" count="90" tid="0" op="" dtype="" >1.7619e-04 0.0000e+00 8.8215e-06</hent>
<hent key="024001000000000000000E0000000002" call="MPI_Isend" bytes="3584" orank="2" region="0" commid="0" count="52" tid="0" op="" dtype="" >1.4496e-04 9.5367e-07 5.0068e-06</hent>
<hent key="024001000000000000000E0000000019" call="MPI_Isend" bytes="3584" orank="25" region="0" commid="0" count="132" tid="0" op="" dtype="" >8.5044e-04 5.9605e-06 7.8678e-06</hent>
<hent key="024001000000000000000E00000002A1" call="MPI_Isend" bytes="3584" orank="673" region="0" commid="0" count="157" tid="0" op="" dtype="" >8.7619e-04 3.8147e-06 1.0014e-05</hent>
<hent key="03800100000000000000028000000000" call="MPI_Irecv" bytes="640" orank="0" region="0" commid="0" count="348" tid="0" op="" dtype="" >1.2589e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000002" call="MPI_Irecv" bytes="640" orank="2" region="0" commid="0" count="355" tid="0" op="" dtype="" >8.2254e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000005" call="MPI_Irecv" bytes="640" orank="5" region="0" commid="0" count="449" tid="0" op="" dtype="" >6.9141e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000015" call="MPI_Irecv" bytes="640" orank="21" region="0" commid="0" count="417" tid="0" op="" dtype="" >8.8930e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000019" call="MPI_Irecv" bytes="640" orank="25" region="0" commid="0" count="12" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000140000000019" call="MPI_Irecv" bytes="5120" orank="25" region="0" commid="0" count="609" tid="0" op="" dtype="" >1.0061e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000280000002A1" call="MPI_Irecv" bytes="640" orank="673" region="0" commid="0" count="34" tid="0" op="" dtype="" >1.5259e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000001400000002A1" call="MPI_Irecv" bytes="5120" orank="673" region="0" commid="0" count="623" tid="0" op="" dtype="" >1.4162e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000A00000000019" call="MPI_Irecv" bytes="40960" orank="25" region="0" commid="0" count="2017" tid="0" op="" dtype="" >5.6291e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000028000000000" call="MPI_Isend" bytes="640" orank="0" region="0" commid="0" count="397" tid="0" op="" dtype="" >6.6328e-04 9.5367e-07 1.5020e-05</hent>
<hent key="02400100000000000000028000000002" call="MPI_Isend" bytes="640" orank="2" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.5202e-03 2.8610e-06 1.1921e-05</hent>
<hent key="02400100000000000000028000000005" call="MPI_Isend" bytes="640" orank="5" region="0" commid="0" count="401" tid="0" op="" dtype="" >5.3096e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000028000000015" call="MPI_Isend" bytes="640" orank="21" region="0" commid="0" count="388" tid="0" op="" dtype="" >4.6873e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000028000000019" call="MPI_Isend" bytes="640" orank="25" region="0" commid="0" count="19" tid="0" op="" dtype="" >9.8944e-05 3.8147e-06 6.9141e-06</hent>
<hent key="03800100000000000000A000000002A1" call="MPI_Irecv" bytes="40960" orank="673" region="0" commid="0" count="2092" tid="0" op="" dtype="" >4.8280e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000140000000019" call="MPI_Isend" bytes="5120" orank="25" region="0" commid="0" count="444" tid="0" op="" dtype="" >1.9505e-03 1.9073e-06 1.5974e-05</hent>
<hent key="024001000000000000000280000002A1" call="MPI_Isend" bytes="640" orank="673" region="0" commid="0" count="19" tid="0" op="" dtype="" >8.5115e-05 3.8147e-06 5.0068e-06</hent>
<hent key="03800100000000000000800000000019" call="MPI_Irecv" bytes="32768" orank="25" region="0" commid="0" count="10683" tid="0" op="" dtype="" >2.7955e-03 0.0000e+00 3.2187e-05</hent>
<hent key="024001000000000000001400000002A1" call="MPI_Isend" bytes="5120" orank="673" region="0" commid="0" count="463" tid="0" op="" dtype="" >1.7171e-03 9.5367e-07 1.0014e-05</hent>
<hent key="038001000000000000008000000002A1" call="MPI_Irecv" bytes="32768" orank="673" region="0" commid="0" count="10608" tid="0" op="" dtype="" >2.5887e-03 0.0000e+00 2.2888e-05</hent>
<hent key="03800100000000000000014000000000" call="MPI_Irecv" bytes="320" orank="0" region="0" commid="0" count="586" tid="0" op="" dtype="" >2.5940e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000002" call="MPI_Irecv" bytes="320" orank="2" region="0" commid="0" count="580" tid="0" op="" dtype="" >1.6284e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000005" call="MPI_Irecv" bytes="320" orank="5" region="0" commid="0" count="397" tid="0" op="" dtype="" >6.7472e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000015" call="MPI_Irecv" bytes="320" orank="21" region="0" commid="0" count="411" tid="0" op="" dtype="" >7.7486e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000019" call="MPI_Irecv" bytes="320" orank="25" region="0" commid="0" count="16" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A00000000019" call="MPI_Isend" bytes="40960" orank="25" region="0" commid="0" count="1348" tid="0" op="" dtype="" >3.0612e-02 1.0967e-05 4.1962e-05</hent>
<hent key="038001000000000000000140000002A1" call="MPI_Irecv" bytes="320" orank="673" region="0" commid="0" count="17" tid="0" op="" dtype="" >6.4373e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A000000002A1" call="MPI_Isend" bytes="40960" orank="673" region="0" commid="0" count="1575" tid="0" op="" dtype="" >1.7715e-02 7.8678e-06 9.5129e-05</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >4.7684e-06 9.5367e-07 1.9073e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >6.3027e+00 0.0000e+00 1.1102e-01</hent>
<hent key="02400100000000000000800000000019" call="MPI_Isend" bytes="32768" orank="25" region="0" commid="0" count="11352" tid="0" op="" dtype="" >2.5889e-01 9.0599e-06 9.5844e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-05 0.0000e+00 1.9073e-05</hent>
<hent key="024001000000000000008000000002A1" call="MPI_Isend" bytes="32768" orank="673" region="0" commid="0" count="11125" tid="0" op="" dtype="" >1.2336e-01 6.9141e-06 5.3883e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000000" call="MPI_Isend" bytes="320" orank="0" region="0" commid="0" count="583" tid="0" op="" dtype="" >7.2527e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000014000000002" call="MPI_Isend" bytes="320" orank="2" region="0" commid="0" count="570" tid="0" op="" dtype="" >2.2006e-03 2.8610e-06 1.0014e-05</hent>
<hent key="02400100000000000000014000000005" call="MPI_Isend" bytes="320" orank="5" region="0" commid="0" count="449" tid="0" op="" dtype="" >4.8399e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000015" call="MPI_Isend" bytes="320" orank="21" region="0" commid="0" count="446" tid="0" op="" dtype="" >4.4823e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000014000000019" call="MPI_Isend" bytes="320" orank="25" region="0" commid="0" count="23" tid="0" op="" dtype="" >1.1134e-04 4.0531e-06 5.9605e-06</hent>
<hent key="024001000000000000000140000002A1" call="MPI_Isend" bytes="320" orank="673" region="0" commid="0" count="23" tid="0" op="" dtype="" >1.0180e-04 3.8147e-06 5.0068e-06</hent>
<hent key="03800100000000000000000000000000" call="MPI_Irecv" bytes="0" orank="0" region="0" commid="0" count="509" tid="0" op="" dtype="" >1.7452e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000002" call="MPI_Irecv" bytes="0" orank="2" region="0" commid="0" count="533" tid="0" op="" dtype="" >1.4782e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000000000005" call="MPI_Irecv" bytes="0" orank="5" region="0" commid="0" count="360" tid="0" op="" dtype="" >6.8188e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000015" call="MPI_Irecv" bytes="0" orank="21" region="0" commid="0" count="337" tid="0" op="" dtype="" >6.7472e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000000000000019" call="MPI_Irecv" bytes="0" orank="25" region="0" commid="0" count="94" tid="0" op="" dtype="" >2.1935e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002A1" call="MPI_Irecv" bytes="0" orank="673" region="0" commid="0" count="87" tid="0" op="" dtype="" >1.7643e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.2674e-04 0.0000e+00 8.3923e-05</hent>
<hent key="03800100000000000000030000000005" call="MPI_Irecv" bytes="768" orank="5" region="0" commid="0" count="514" tid="0" op="" dtype="" >9.7275e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000030000000015" call="MPI_Irecv" bytes="768" orank="21" region="0" commid="0" count="694" tid="0" op="" dtype="" >1.2374e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000000" call="MPI_Isend" bytes="0" orank="0" region="0" commid="0" count="515" tid="0" op="" dtype="" >4.4560e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000002" call="MPI_Isend" bytes="0" orank="2" region="0" commid="0" count="533" tid="0" op="" dtype="" >1.4324e-03 9.5367e-07 7.9155e-05</hent>
<hent key="02400100000000000000000000000005" call="MPI_Isend" bytes="0" orank="5" region="0" commid="0" count="338" tid="0" op="" dtype="" >2.5129e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000015" call="MPI_Isend" bytes="0" orank="21" region="0" commid="0" count="338" tid="0" op="" dtype="" >2.2888e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000000000019" call="MPI_Isend" bytes="0" orank="25" region="0" commid="0" count="89" tid="0" op="" dtype="" >3.7241e-04 1.9073e-06 9.0599e-06</hent>
<hent key="024001000000000000000000000002A1" call="MPI_Isend" bytes="0" orank="673" region="0" commid="0" count="80" tid="0" op="" dtype="" >3.0804e-04 1.9073e-06 7.1526e-06</hent>
<hent key="03800100000000000000060000000000" call="MPI_Irecv" bytes="1536" orank="0" region="0" commid="0" count="17" tid="0" op="" dtype="" >1.0490e-05 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000060000000002" call="MPI_Irecv" bytes="1536" orank="2" region="0" commid="0" count="18" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000005" call="MPI_Irecv" bytes="1536" orank="5" region="0" commid="0" count="50" tid="0" op="" dtype="" >9.5367e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000060000000015" call="MPI_Irecv" bytes="1536" orank="21" region="0" commid="0" count="56" tid="0" op="" dtype="" >1.1206e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000019" call="MPI_Irecv" bytes="1536" orank="25" region="0" commid="0" count="74" tid="0" op="" dtype="" >2.3127e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002A1" call="MPI_Irecv" bytes="1536" orank="673" region="0" commid="0" count="98" tid="0" op="" dtype="" >2.6464e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000030000000005" call="MPI_Isend" bytes="768" orank="5" region="0" commid="0" count="694" tid="0" op="" dtype="" >3.9244e-04 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000030000000015" call="MPI_Isend" bytes="768" orank="21" region="0" commid="0" count="818" tid="0" op="" dtype="" >3.5262e-04 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.9073e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C000000000" call="MPI_Irecv" bytes="448" orank="0" region="0" commid="0" count="50" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000002" call="MPI_Irecv" bytes="448" orank="2" region="0" commid="0" count="36" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000000" call="MPI_Isend" bytes="1536" orank="0" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.3842e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000060000000002" call="MPI_Isend" bytes="1536" orank="2" region="0" commid="0" count="15" tid="0" op="" dtype="" >8.8215e-05 3.8147e-06 2.0027e-05</hent>
<hent key="02400100000000000000060000000005" call="MPI_Isend" bytes="1536" orank="5" region="0" commid="0" count="63" tid="0" op="" dtype="" >1.0419e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000015" call="MPI_Isend" bytes="1536" orank="21" region="0" commid="0" count="64" tid="0" op="" dtype="" >1.0538e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000019" call="MPI_Isend" bytes="1536" orank="25" region="0" commid="0" count="75" tid="0" op="" dtype="" >4.3082e-04 4.7684e-06 6.9141e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002A1" call="MPI_Isend" bytes="1536" orank="673" region="0" commid="0" count="83" tid="0" op="" dtype="" >4.2510e-04 4.0531e-06 1.0014e-05</hent>
<hent key="038001000000000000000C0000000019" call="MPI_Irecv" bytes="3072" orank="25" region="0" commid="0" count="355" tid="0" op="" dtype="" >9.2030e-05 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002A1" call="MPI_Irecv" bytes="3072" orank="673" region="0" commid="0" count="343" tid="0" op="" dtype="" >1.0681e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C000000000" call="MPI_Isend" bytes="448" orank="0" region="0" commid="0" count="26" tid="0" op="" dtype="" >2.5272e-05 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000001C000000002" call="MPI_Isend" bytes="448" orank="2" region="0" commid="0" count="14" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000019" call="MPI_Isend" bytes="3072" orank="25" region="0" commid="0" count="370" tid="0" op="" dtype="" >2.3580e-03 5.0068e-06 1.5020e-05</hent>
<hent key="024001000000000000000C00000002A1" call="MPI_Isend" bytes="3072" orank="673" region="0" commid="0" count="350" tid="0" op="" dtype="" >1.9317e-03 4.0531e-06 1.2159e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.0085e-04 3.2902e-05 3.4094e-05</hent>
<hent key="03800100000000000000038000000000" call="MPI_Irecv" bytes="896" orank="0" region="0" commid="0" count="181" tid="0" op="" dtype="" >7.3910e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000002" call="MPI_Irecv" bytes="896" orank="2" region="0" commid="0" count="153" tid="0" op="" dtype="" >4.3392e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000005" call="MPI_Irecv" bytes="896" orank="5" region="0" commid="0" count="3159" tid="0" op="" dtype="" >6.1011e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000015" call="MPI_Irecv" bytes="896" orank="21" region="0" commid="0" count="2997" tid="0" op="" dtype="" >5.1975e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000019" call="MPI_Irecv" bytes="896" orank="25" region="0" commid="0" count="29" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002A1" call="MPI_Irecv" bytes="896" orank="673" region="0" commid="0" count="28" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.5020e-05 1.5020e-05 1.5020e-05</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000180000000005" call="MPI_Irecv" bytes="6144" orank="5" region="0" commid="0" count="1892" tid="0" op="" dtype="" >3.2806e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000180000000015" call="MPI_Irecv" bytes="6144" orank="21" region="0" commid="0" count="2597" tid="0" op="" dtype="" >4.9424e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000180000000019" call="MPI_Irecv" bytes="6144" orank="25" region="0" commid="0" count="10" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000001800000002A1" call="MPI_Irecv" bytes="6144" orank="673" region="0" commid="0" count="8" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000038000000000" call="MPI_Isend" bytes="896" orank="0" region="0" commid="0" count="139" tid="0" op="" dtype="" >2.6226e-04 9.5367e-07 7.8678e-06</hent>
<hent key="02400100000000000000038000000002" call="MPI_Isend" bytes="896" orank="2" region="0" commid="0" count="171" tid="0" op="" dtype="" >7.9560e-04 2.8610e-06 1.8120e-05</hent>
<hent key="02400100000000000000038000000005" call="MPI_Isend" bytes="896" orank="5" region="0" commid="0" count="2987" tid="0" op="" dtype="" >1.8263e-03 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000038000000015" call="MPI_Isend" bytes="896" orank="21" region="0" commid="0" count="2864" tid="0" op="" dtype="" >1.5974e-03 0.0000e+00 8.8215e-06</hent>
<hent key="02400100000000000000038000000019" call="MPI_Isend" bytes="896" orank="25" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.6165e-04 4.7684e-06 6.1989e-06</hent>
<hent key="024001000000000000000380000002A1" call="MPI_Isend" bytes="896" orank="673" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.7834e-04 3.8147e-06 6.1989e-06</hent>
<hent key="02400100000000000000180000000005" call="MPI_Isend" bytes="6144" orank="5" region="0" commid="0" count="2554" tid="0" op="" dtype="" >5.2080e-03 9.5367e-07 2.9087e-05</hent>
<hent key="02400100000000000000180000000015" call="MPI_Isend" bytes="6144" orank="21" region="0" commid="0" count="3025" tid="0" op="" dtype="" >5.6398e-03 9.5367e-07 1.5974e-05</hent>
<hent key="02400100000000000000180000000019" call="MPI_Isend" bytes="6144" orank="25" region="0" commid="0" count="8" tid="0" op="" dtype="" >6.2943e-05 6.9141e-06 9.0599e-06</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.5020e-05 1.5020e-05 1.5020e-05</hent>
<hent key="024001000000000000001800000002A1" call="MPI_Isend" bytes="6144" orank="673" region="0" commid="0" count="9" tid="0" op="" dtype="" >5.7220e-05 5.9605e-06 7.1526e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >9.3526e+00 1.9073e-05 1.2676e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.8310e-04 8.8310e-04 8.8310e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0567e-02 1.0567e-02 1.0567e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >5.2240e-03 5.2240e-03 5.2240e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.8617e-01 3.3722e-03 1.7785e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >2.9802e-04 2.9802e-04 2.9802e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 1.1921e-06 1.9073e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5808e+00 4.2701e-04 2.4827e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >3.4809e-05 0.0000e+00 1.8835e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="03800100000000000000070000000000" call="MPI_Irecv" bytes="1792" orank="0" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000002" call="MPI_Irecv" bytes="1792" orank="2" region="0" commid="0" count="5" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000070000000005" call="MPI_Irecv" bytes="1792" orank="5" region="0" commid="0" count="16" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000015" call="MPI_Irecv" bytes="1792" orank="21" region="0" commid="0" count="14" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000019" call="MPI_Irecv" bytes="1792" orank="25" region="0" commid="0" count="104" tid="0" op="" dtype="" >3.0279e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002A1" call="MPI_Irecv" bytes="1792" orank="673" region="0" commid="0" count="133" tid="0" op="" dtype="" >3.4809e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >9.0361e-05 9.5367e-07 6.6042e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9739e+01 0.0000e+00 3.8413e+01</hent>
<hent key="038001000000000000000A0000000005" call="MPI_Irecv" bytes="2560" orank="5" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000015" call="MPI_Irecv" bytes="2560" orank="21" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000019" call="MPI_Irecv" bytes="2560" orank="25" region="0" commid="0" count="378" tid="0" op="" dtype="" >1.0633e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002A1" call="MPI_Irecv" bytes="2560" orank="673" region="0" commid="0" count="370" tid="0" op="" dtype="" >1.0324e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000000" call="MPI_Isend" bytes="1792" orank="0" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.0014e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000002" call="MPI_Isend" bytes="1792" orank="2" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.0027e-05 4.0531e-06 6.1989e-06</hent>
<hent key="02400100000000000000070000000005" call="MPI_Isend" bytes="1792" orank="5" region="0" commid="0" count="16" tid="0" op="" dtype="" >3.0994e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000015" call="MPI_Isend" bytes="1792" orank="21" region="0" commid="0" count="21" tid="0" op="" dtype="" >3.8385e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000019" call="MPI_Isend" bytes="1792" orank="25" region="0" commid="0" count="119" tid="0" op="" dtype="" >6.8951e-04 5.0068e-06 1.0014e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3096e-02 8.3096e-02 8.3096e-02</hent>
<hent key="024001000000000000000700000002A1" call="MPI_Isend" bytes="1792" orank="673" region="0" commid="0" count="125" tid="0" op="" dtype="" >6.2370e-04 3.8147e-06 6.1989e-06</hent>
<hent key="024001000000000000000A0000000000" call="MPI_Isend" bytes="2560" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000A0000000005" call="MPI_Isend" bytes="2560" orank="5" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000A0000000015" call="MPI_Isend" bytes="2560" orank="21" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000A0000000019" call="MPI_Isend" bytes="2560" orank="25" region="0" commid="0" count="378" tid="0" op="" dtype="" >2.3181e-03 4.7684e-06 7.1526e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >5.1975e-05 9.0599e-06 2.3842e-05</hent>
<hent key="024001000000000000000A00000002A1" call="MPI_Isend" bytes="2560" orank="673" region="0" commid="0" count="354" tid="0" op="" dtype="" >1.9279e-03 3.8147e-06 4.0054e-05</hent>
<hent key="03800100000000000000100000000000" call="MPI_Irecv" bytes="4096" orank="0" region="0" commid="0" count="12518" tid="0" op="" dtype="" >5.6431e-03 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000100000000002" call="MPI_Irecv" bytes="4096" orank="2" region="0" commid="0" count="12567" tid="0" op="" dtype="" >2.1682e-03 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000100000000019" call="MPI_Irecv" bytes="4096" orank="25" region="0" commid="0" count="3069" tid="0" op="" dtype="" >5.1236e-04 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000001000000002A1" call="MPI_Irecv" bytes="4096" orank="673" region="0" commid="0" count="3024" tid="0" op="" dtype="" >7.4625e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000100000000000" call="MPI_Isend" bytes="4096" orank="0" region="0" commid="0" count="12610" tid="0" op="" dtype="" >2.2329e-02 0.0000e+00 1.8120e-05</hent>
<hent key="02400100000000000000100000000002" call="MPI_Isend" bytes="4096" orank="2" region="0" commid="0" count="12648" tid="0" op="" dtype="" >3.7163e-02 0.0000e+00 1.0681e-04</hent>
<hent key="02400100000000000000100000000019" call="MPI_Isend" bytes="4096" orank="25" region="0" commid="0" count="3218" tid="0" op="" dtype="" >1.2943e-02 9.5367e-07 2.2888e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.4305e-04 6.1035e-05 8.2016e-05</hent>
<hent key="024001000000000000001000000002A1" call="MPI_Isend" bytes="4096" orank="673" region="0" commid="0" count="3185" tid="0" op="" dtype="" >1.1446e-02 9.5367e-07 2.4080e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.3365e-04 3.6955e-05 7.3910e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.0014e-05 2.1458e-06 7.8678e-06</hent>
<hent key="03800100000000000000000400000000" call="MPI_Irecv" bytes="4" orank="0" region="0" commid="0" count="3400" tid="0" op="" dtype="" >8.3804e-04 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000000400000002" call="MPI_Irecv" bytes="4" orank="2" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.5981e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000000400000005" call="MPI_Irecv" bytes="4" orank="5" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.8018e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000015" call="MPI_Irecv" bytes="4" orank="21" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.0378e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000019" call="MPI_Irecv" bytes="4" orank="25" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.6733e-04 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000001C0000000005" call="MPI_Irecv" bytes="7168" orank="5" region="0" commid="0" count="10808" tid="0" op="" dtype="" >1.7538e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000001C0000000015" call="MPI_Irecv" bytes="7168" orank="21" region="0" commid="0" count="10103" tid="0" op="" dtype="" >1.9255e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000004000002A1" call="MPI_Irecv" bytes="4" orank="673" region="0" commid="0" count="3400" tid="0" op="" dtype="" >8.2445e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000400000000" call="MPI_Isend" bytes="4" orank="0" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.5182e-03 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000400000002" call="MPI_Isend" bytes="4" orank="2" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.4928e-03 0.0000e+00 8.2970e-05</hent>
<hent key="02400100000000000000000400000005" call="MPI_Isend" bytes="4" orank="5" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.4455e-03 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000000400000015" call="MPI_Isend" bytes="4" orank="21" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.5492e-03 0.0000e+00 2.9087e-05</hent>
<hent key="02400100000000000000000400000019" call="MPI_Isend" bytes="4" orank="25" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.5748e-02 4.7684e-06 6.1035e-05</hent>
<hent key="024001000000000000001C0000000005" call="MPI_Isend" bytes="7168" orank="5" region="0" commid="0" count="10146" tid="0" op="" dtype="" >2.0354e-02 9.5367e-07 1.4067e-05</hent>
<hent key="024001000000000000001C0000000015" call="MPI_Isend" bytes="7168" orank="21" region="0" commid="0" count="9675" tid="0" op="" dtype="" >1.8705e-02 9.5367e-07 2.5034e-05</hent>
<hent key="024001000000000000001C0000000019" call="MPI_Isend" bytes="7168" orank="25" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.5736e-05 7.8678e-06 7.8678e-06</hent>
<hent key="024001000000000000000004000002A1" call="MPI_Isend" bytes="4" orank="673" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.0165e-02 3.8147e-06 1.4210e-04</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.6015e-05 4.6015e-05 4.6015e-05</hent>
<hent key="03800100000000000000020000000000" call="MPI_Irecv" bytes="512" orank="0" region="0" commid="0" count="3350" tid="0" op="" dtype="" >4.2892e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000020000000002" call="MPI_Irecv" bytes="512" orank="2" region="0" commid="0" count="3364" tid="0" op="" dtype="" >5.1689e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000000" call="MPI_Irecv" bytes="1280" orank="0" region="0" commid="0" count="57" tid="0" op="" dtype="" >1.4782e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000002" call="MPI_Irecv" bytes="1280" orank="2" region="0" commid="0" count="55" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000005" call="MPI_Irecv" bytes="1280" orank="5" region="0" commid="0" count="142" tid="0" op="" dtype="" >2.5034e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000015" call="MPI_Irecv" bytes="1280" orank="21" region="0" commid="0" count="168" tid="0" op="" dtype="" >3.6478e-05 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000050000000019" call="MPI_Irecv" bytes="1280" orank="25" region="0" commid="0" count="46" tid="0" op="" dtype="" >1.1921e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4850e+01 1.0014e-05 1.8461e-01</hent>
<hent key="038001000000000000000500000002A1" call="MPI_Irecv" bytes="1280" orank="673" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.3590e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000020000000000" call="MPI_Isend" bytes="512" orank="0" region="0" commid="0" count="3374" tid="0" op="" dtype="" >1.7543e-03 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000020000000002" call="MPI_Isend" bytes="512" orank="2" region="0" commid="0" count="3386" tid="0" op="" dtype="" >2.4014e-03 0.0000e+00 1.0967e-05</hent>
<hent key="03800100000000000000080000000002" call="MPI_Irecv" bytes="2048" orank="2" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000080000000005" call="MPI_Irecv" bytes="2048" orank="5" region="0" commid="0" count="10" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000015" call="MPI_Irecv" bytes="2048" orank="21" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000019" call="MPI_Irecv" bytes="2048" orank="25" region="0" commid="0" count="161" tid="0" op="" dtype="" >5.3167e-05 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000800000002A1" call="MPI_Irecv" bytes="2048" orank="673" region="0" commid="0" count="166" tid="0" op="" dtype="" >5.3406e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000050000000000" call="MPI_Isend" bytes="1280" orank="0" region="0" commid="0" count="50" tid="0" op="" dtype="" >1.0991e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000002" call="MPI_Isend" bytes="1280" orank="2" region="0" commid="0" count="42" tid="0" op="" dtype="" >2.0456e-04 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000050000000005" call="MPI_Isend" bytes="1280" orank="5" region="0" commid="0" count="145" tid="0" op="" dtype="" >2.3818e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000015" call="MPI_Isend" bytes="1280" orank="21" region="0" commid="0" count="153" tid="0" op="" dtype="" >2.4414e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000050000000019" call="MPI_Isend" bytes="1280" orank="25" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.7595e-04 3.8147e-06 6.1989e-06</hent>
<hent key="024001000000000000000500000002A1" call="MPI_Isend" bytes="1280" orank="673" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.9813e-04 3.8147e-06 6.1989e-06</hent>
</hash>
<internal rank="1" log_i="1724765674.532665" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="2" mpi_size="696" stamp_init="1724765564.465351" stamp_final="1724765674.523155" username="apac4" allocationname="unknown" flags="0" pid="1612119" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10058e+02" utime="8.61084e+01" stime="1.48031e+01" mtime="7.12609e+01" gflop="0.00000e+00" gbyte="3.76507e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.12609e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007315721509" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09918e+02" utime="8.60711e+01" stime="1.47972e+01" mtime="7.12609e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.12609e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1834e+09" > 9.0243e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1873e+09" > 5.2828e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.4321e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9791e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.2588e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8406e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5802e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.9918e-04 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0613e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3125e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3406e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="210" >
<hent key="02400100000000000000080000000001" call="MPI_Isend" bytes="2048" orank="1" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="02400100000000000000080000000006" call="MPI_Isend" bytes="2048" orank="6" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.3113e-05 1.9073e-06 9.0599e-06</hent>
<hent key="02400100000000000000080000000016" call="MPI_Isend" bytes="2048" orank="22" region="0" commid="0" count="10" tid="0" op="" dtype="" >1.9312e-05 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000008000000001A" call="MPI_Isend" bytes="2048" orank="26" region="0" commid="0" count="174" tid="0" op="" dtype="" >1.1656e-03 3.8147e-06 2.5988e-05</hent>
<hent key="024001000000000000000800000002A2" call="MPI_Isend" bytes="2048" orank="674" region="0" commid="0" count="145" tid="0" op="" dtype="" >8.0156e-04 3.8147e-06 1.5020e-05</hent>
<hent key="038001000000000000000E0000000001" call="MPI_Irecv" bytes="3584" orank="1" region="0" commid="0" count="52" tid="0" op="" dtype="" >2.5272e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000003" call="MPI_Irecv" bytes="3584" orank="3" region="0" commid="0" count="138" tid="0" op="" dtype="" >3.9339e-05 0.0000e+00 1.0014e-05</hent>
<hent key="038001000000000000000E000000001A" call="MPI_Irecv" bytes="3584" orank="26" region="0" commid="0" count="115" tid="0" op="" dtype="" >5.8413e-05 0.0000e+00 1.0967e-05</hent>
<hent key="038001000000000000000E00000002A2" call="MPI_Irecv" bytes="3584" orank="674" region="0" commid="0" count="145" tid="0" op="" dtype="" >1.4138e-04 0.0000e+00 7.1526e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000E0000000001" call="MPI_Isend" bytes="3584" orank="1" region="0" commid="0" count="133" tid="0" op="" dtype="" >2.1243e-04 0.0000e+00 8.1062e-06</hent>
<hent key="024001000000000000000E0000000003" call="MPI_Isend" bytes="3584" orank="3" region="0" commid="0" count="92" tid="0" op="" dtype="" >2.0742e-04 9.5367e-07 4.0531e-06</hent>
<hent key="024001000000000000000E000000001A" call="MPI_Isend" bytes="3584" orank="26" region="0" commid="0" count="134" tid="0" op="" dtype="" >9.5582e-04 5.0068e-06 2.0027e-05</hent>
<hent key="024001000000000000000E00000002A2" call="MPI_Isend" bytes="3584" orank="674" region="0" commid="0" count="129" tid="0" op="" dtype="" >7.7653e-04 3.8147e-06 1.2159e-05</hent>
<hent key="03800100000000000000028000000001" call="MPI_Irecv" bytes="640" orank="1" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.7023e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000028000000003" call="MPI_Irecv" bytes="640" orank="3" region="0" commid="0" count="363" tid="0" op="" dtype="" >9.7752e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000006" call="MPI_Irecv" bytes="640" orank="6" region="0" commid="0" count="433" tid="0" op="" dtype="" >9.4891e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000016" call="MPI_Irecv" bytes="640" orank="22" region="0" commid="0" count="415" tid="0" op="" dtype="" >2.2388e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000002800000001A" call="MPI_Irecv" bytes="640" orank="26" region="0" commid="0" count="27" tid="0" op="" dtype="" >8.8215e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000014000000001A" call="MPI_Irecv" bytes="5120" orank="26" region="0" commid="0" count="610" tid="0" op="" dtype="" >3.8338e-04 0.0000e+00 2.0981e-05</hent>
<hent key="038001000000000000000280000002A2" call="MPI_Irecv" bytes="640" orank="674" region="0" commid="0" count="18" tid="0" op="" dtype="" >1.1921e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.8311e-04 1.8311e-04 1.8311e-04</hent>
<hent key="038001000000000000001400000002A2" call="MPI_Irecv" bytes="5120" orank="674" region="0" commid="0" count="328" tid="0" op="" dtype="" >2.6298e-04 0.0000e+00 6.6996e-05</hent>
<hent key="03800100000000000000A0000000001A" call="MPI_Irecv" bytes="40960" orank="26" region="0" commid="0" count="2029" tid="0" op="" dtype="" >3.0134e-03 0.0000e+00 7.1049e-05</hent>
<hent key="02400100000000000000028000000001" call="MPI_Isend" bytes="640" orank="1" region="0" commid="0" count="355" tid="0" op="" dtype="" >4.8590e-04 9.5367e-07 7.8678e-06</hent>
<hent key="02400100000000000000028000000003" call="MPI_Isend" bytes="640" orank="3" region="0" commid="0" count="341" tid="0" op="" dtype="" >1.0970e-03 1.9073e-06 1.5974e-05</hent>
<hent key="02400100000000000000028000000006" call="MPI_Isend" bytes="640" orank="6" region="0" commid="0" count="431" tid="0" op="" dtype="" >5.0402e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000028000000016" call="MPI_Isend" bytes="640" orank="22" region="0" commid="0" count="444" tid="0" op="" dtype="" >4.3273e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000002800000001A" call="MPI_Isend" bytes="640" orank="26" region="0" commid="0" count="20" tid="0" op="" dtype="" >1.0514e-04 3.8147e-06 1.0967e-05</hent>
<hent key="03800100000000000000A000000002A2" call="MPI_Irecv" bytes="40960" orank="674" region="0" commid="0" count="1080" tid="0" op="" dtype="" >4.8876e-04 0.0000e+00 3.8862e-05</hent>
<hent key="0240010000000000000014000000001A" call="MPI_Isend" bytes="5120" orank="26" region="0" commid="0" count="369" tid="0" op="" dtype="" >3.3784e-03 1.9073e-06 1.5807e-04</hent>
<hent key="024001000000000000000280000002A2" call="MPI_Isend" bytes="640" orank="674" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.5116e-04 3.8147e-06 1.0014e-05</hent>
<hent key="0380010000000000000080000000001A" call="MPI_Irecv" bytes="32768" orank="26" region="0" commid="0" count="10671" tid="0" op="" dtype="" >1.1731e-02 0.0000e+00 7.4148e-05</hent>
<hent key="024001000000000000001400000002A2" call="MPI_Isend" bytes="5120" orank="674" region="0" commid="0" count="405" tid="0" op="" dtype="" >2.1880e-03 9.5367e-07 3.3855e-05</hent>
<hent key="038001000000000000008000000002A2" call="MPI_Irecv" bytes="32768" orank="674" region="0" commid="0" count="11620" tid="0" op="" dtype="" >7.8270e-03 0.0000e+00 9.4891e-05</hent>
<hent key="03800100000000000000014000000001" call="MPI_Irecv" bytes="320" orank="1" region="0" commid="0" count="570" tid="0" op="" dtype="" >2.7680e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000014000000003" call="MPI_Irecv" bytes="320" orank="3" region="0" commid="0" count="566" tid="0" op="" dtype="" >1.6737e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000006" call="MPI_Irecv" bytes="320" orank="6" region="0" commid="0" count="438" tid="0" op="" dtype="" >9.6083e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000016" call="MPI_Irecv" bytes="320" orank="22" region="0" commid="0" count="454" tid="0" op="" dtype="" >2.3460e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000001400000001A" call="MPI_Irecv" bytes="320" orank="26" region="0" commid="0" count="21" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000A0000000001A" call="MPI_Isend" bytes="40960" orank="26" region="0" commid="0" count="1177" tid="0" op="" dtype="" >3.1945e-02 1.0014e-05 1.8191e-04</hent>
<hent key="038001000000000000000140000002A2" call="MPI_Irecv" bytes="320" orank="674" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.7405e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A000000002A2" call="MPI_Isend" bytes="40960" orank="674" region="0" commid="0" count="1302" tid="0" op="" dtype="" >2.9513e-02 7.8678e-06 1.3018e-04</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 9.5367e-07 1.9073e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >5.4321e+00 0.0000e+00 1.1029e-01</hent>
<hent key="0240010000000000000080000000001A" call="MPI_Isend" bytes="32768" orank="26" region="0" commid="0" count="11523" tid="0" op="" dtype="" >3.1344e-01 8.8215e-06 2.2411e-04</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6928e-05 0.0000e+00 1.5974e-05</hent>
<hent key="024001000000000000008000000002A2" call="MPI_Isend" bytes="32768" orank="674" region="0" commid="0" count="11398" tid="0" op="" dtype="" >2.5298e-01 7.8678e-06 1.4186e-04</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="02400100000000000000014000000001" call="MPI_Isend" bytes="320" orank="1" region="0" commid="0" count="580" tid="0" op="" dtype="" >6.4468e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000014000000003" call="MPI_Isend" bytes="320" orank="3" region="0" commid="0" count="538" tid="0" op="" dtype="" >1.5347e-03 1.9073e-06 1.2159e-05</hent>
<hent key="02400100000000000000014000000006" call="MPI_Isend" bytes="320" orank="6" region="0" commid="0" count="444" tid="0" op="" dtype="" >4.0579e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000014000000016" call="MPI_Isend" bytes="320" orank="22" region="0" commid="0" count="435" tid="0" op="" dtype="" >3.5024e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000001400000001A" call="MPI_Isend" bytes="320" orank="26" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.4830e-04 3.0994e-06 1.3113e-05</hent>
<hent key="0380010000000000000020000000001A" call="MPI_Irecv" bytes="8192" orank="26" region="0" commid="0" count="2" tid="0" op="" dtype="" >4.2915e-06 1.1921e-06 3.0994e-06</hent>
<hent key="024001000000000000000140000002A2" call="MPI_Isend" bytes="320" orank="674" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.4710e-04 3.8147e-06 1.3113e-05</hent>
<hent key="03800100000000000000000000000001" call="MPI_Irecv" bytes="0" orank="1" region="0" commid="0" count="533" tid="0" op="" dtype="" >1.6975e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000003" call="MPI_Irecv" bytes="0" orank="3" region="0" commid="0" count="546" tid="0" op="" dtype="" >1.6236e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000006" call="MPI_Irecv" bytes="0" orank="6" region="0" commid="0" count="353" tid="0" op="" dtype="" >8.2254e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000016" call="MPI_Irecv" bytes="0" orank="22" region="0" commid="0" count="314" tid="0" op="" dtype="" >1.2875e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000001A" call="MPI_Irecv" bytes="0" orank="26" region="0" commid="0" count="87" tid="0" op="" dtype="" >2.7895e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002A2" call="MPI_Irecv" bytes="0" orank="674" region="0" commid="0" count="82" tid="0" op="" dtype="" >2.9087e-05 0.0000e+00 5.0068e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.0385e-04 0.0000e+00 7.8917e-05</hent>
<hent key="03800100000000000000030000000006" call="MPI_Irecv" bytes="768" orank="6" region="0" commid="0" count="1036" tid="0" op="" dtype="" >2.0289e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000030000000016" call="MPI_Irecv" bytes="768" orank="22" region="0" commid="0" count="790" tid="0" op="" dtype="" >2.0885e-04 0.0000e+00 8.0109e-05</hent>
<hent key="02400100000000000000000000000001" call="MPI_Isend" bytes="0" orank="1" region="0" commid="0" count="533" tid="0" op="" dtype="" >3.7074e-04 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000000000000003" call="MPI_Isend" bytes="0" orank="3" region="0" commid="0" count="562" tid="0" op="" dtype="" >1.1311e-03 9.5367e-07 3.0994e-05</hent>
<hent key="02400100000000000000000000000006" call="MPI_Isend" bytes="0" orank="6" region="0" commid="0" count="331" tid="0" op="" dtype="" >2.1315e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000016" call="MPI_Isend" bytes="0" orank="22" region="0" commid="0" count="347" tid="0" op="" dtype="" >1.9073e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000000000000001A" call="MPI_Isend" bytes="0" orank="26" region="0" commid="0" count="87" tid="0" op="" dtype="" >4.3869e-04 1.9073e-06 1.5020e-05</hent>
<hent key="024001000000000000000000000002A2" call="MPI_Isend" bytes="0" orank="674" region="0" commid="0" count="81" tid="0" op="" dtype="" >3.4308e-04 2.8610e-06 1.3828e-05</hent>
<hent key="03800100000000000000060000000001" call="MPI_Irecv" bytes="1536" orank="1" region="0" commid="0" count="15" tid="0" op="" dtype="" >7.6294e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000060000000003" call="MPI_Irecv" bytes="1536" orank="3" region="0" commid="0" count="18" tid="0" op="" dtype="" >9.7752e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000006" call="MPI_Irecv" bytes="1536" orank="6" region="0" commid="0" count="55" tid="0" op="" dtype="" >7.3910e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000016" call="MPI_Irecv" bytes="1536" orank="22" region="0" commid="0" count="50" tid="0" op="" dtype="" >3.5286e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000006000000001A" call="MPI_Irecv" bytes="1536" orank="26" region="0" commid="0" count="77" tid="0" op="" dtype="" >4.7684e-05 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000600000002A2" call="MPI_Irecv" bytes="1536" orank="674" region="0" commid="0" count="67" tid="0" op="" dtype="" >4.6730e-05 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000030000000006" call="MPI_Isend" bytes="768" orank="6" region="0" commid="0" count="542" tid="0" op="" dtype="" >3.1590e-04 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000030000000016" call="MPI_Isend" bytes="768" orank="22" region="0" commid="0" count="934" tid="0" op="" dtype="" >5.5480e-04 0.0000e+00 1.5974e-05</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 1.9073e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C000000001" call="MPI_Irecv" bytes="448" orank="1" region="0" commid="0" count="14" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000003" call="MPI_Irecv" bytes="448" orank="3" region="0" commid="0" count="30" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000001" call="MPI_Isend" bytes="1536" orank="1" region="0" commid="0" count="18" tid="0" op="" dtype="" >3.8385e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000003" call="MPI_Isend" bytes="1536" orank="3" region="0" commid="0" count="17" tid="0" op="" dtype="" >6.3419e-05 2.8610e-06 4.0531e-06</hent>
<hent key="02400100000000000000060000000006" call="MPI_Isend" bytes="1536" orank="6" region="0" commid="0" count="39" tid="0" op="" dtype="" >5.8889e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000016" call="MPI_Isend" bytes="1536" orank="22" region="0" commid="0" count="76" tid="0" op="" dtype="" >1.0562e-04 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000006000000001A" call="MPI_Isend" bytes="1536" orank="26" region="0" commid="0" count="85" tid="0" op="" dtype="" >5.6577e-04 3.8147e-06 3.1948e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002A2" call="MPI_Isend" bytes="1536" orank="674" region="0" commid="0" count="72" tid="0" op="" dtype="" >3.8242e-04 2.8610e-06 1.1206e-05</hent>
<hent key="038001000000000000000C000000001A" call="MPI_Irecv" bytes="3072" orank="26" region="0" commid="0" count="365" tid="0" op="" dtype="" >2.1720e-04 0.0000e+00 8.1062e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002A2" call="MPI_Irecv" bytes="3072" orank="674" region="0" commid="0" count="371" tid="0" op="" dtype="" >3.4833e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000001C000000001" call="MPI_Isend" bytes="448" orank="1" region="0" commid="0" count="36" tid="0" op="" dtype="" >1.7643e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C000000003" call="MPI_Isend" bytes="448" orank="3" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.2636e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C000000001A" call="MPI_Isend" bytes="3072" orank="26" region="0" commid="0" count="336" tid="0" op="" dtype="" >2.5430e-03 4.7684e-06 3.5048e-05</hent>
<hent key="024001000000000000000C00000002A2" call="MPI_Isend" bytes="3072" orank="674" region="0" commid="0" count="328" tid="0" op="" dtype="" >1.9817e-03 3.8147e-06 1.5974e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.3185e-04 3.9816e-05 4.6015e-05</hent>
<hent key="03800100000000000000038000000001" call="MPI_Irecv" bytes="896" orank="1" region="0" commid="0" count="171" tid="0" op="" dtype="" >7.0810e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000003" call="MPI_Irecv" bytes="896" orank="3" region="0" commid="0" count="156" tid="0" op="" dtype="" >4.2915e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000006" call="MPI_Irecv" bytes="896" orank="6" region="0" commid="0" count="2621" tid="0" op="" dtype="" >5.8937e-04 0.0000e+00 2.5034e-05</hent>
<hent key="03800100000000000000038000000016" call="MPI_Irecv" bytes="896" orank="22" region="0" commid="0" count="2906" tid="0" op="" dtype="" >5.9581e-04 0.0000e+00 1.2875e-05</hent>
<hent key="0380010000000000000003800000001A" call="MPI_Irecv" bytes="896" orank="26" region="0" commid="0" count="20" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002A2" call="MPI_Irecv" bytes="896" orank="674" region="0" commid="0" count="26" tid="0" op="" dtype="" >2.0742e-05 0.0000e+00 4.0531e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.7108e-04 2.7108e-04 2.7108e-04</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="03800100000000000000180000000006" call="MPI_Irecv" bytes="6144" orank="6" region="0" commid="0" count="3837" tid="0" op="" dtype="" >5.6648e-04 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000180000000016" call="MPI_Irecv" bytes="6144" orank="22" region="0" commid="0" count="2922" tid="0" op="" dtype="" >4.1866e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000018000000001A" call="MPI_Irecv" bytes="6144" orank="26" region="0" commid="0" count="10" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001800000002A2" call="MPI_Irecv" bytes="6144" orank="674" region="0" commid="0" count="11" tid="0" op="" dtype="" >1.2875e-05 9.5367e-07 1.9073e-06</hent>
<hent key="02400100000000000000038000000001" call="MPI_Isend" bytes="896" orank="1" region="0" commid="0" count="153" tid="0" op="" dtype="" >2.7156e-04 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000038000000003" call="MPI_Isend" bytes="896" orank="3" region="0" commid="0" count="175" tid="0" op="" dtype="" >6.1917e-04 2.8610e-06 1.5020e-05</hent>
<hent key="02400100000000000000038000000006" call="MPI_Isend" bytes="896" orank="6" region="0" commid="0" count="3154" tid="0" op="" dtype="" >2.0382e-03 0.0000e+00 7.7009e-05</hent>
<hent key="02400100000000000000038000000016" call="MPI_Isend" bytes="896" orank="22" region="0" commid="0" count="2703" tid="0" op="" dtype="" >2.4755e-03 0.0000e+00 1.0705e-04</hent>
<hent key="0240010000000000000003800000001A" call="MPI_Isend" bytes="896" orank="26" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.4353e-04 3.8147e-06 2.1935e-05</hent>
<hent key="024001000000000000000380000002A2" call="MPI_Isend" bytes="896" orank="674" region="0" commid="0" count="23" tid="0" op="" dtype="" >1.1802e-04 3.8147e-06 1.1206e-05</hent>
<hent key="02400100000000000000180000000006" call="MPI_Isend" bytes="6144" orank="6" region="0" commid="0" count="2011" tid="0" op="" dtype="" >4.0236e-03 9.5367e-07 2.2888e-05</hent>
<hent key="02400100000000000000180000000016" call="MPI_Isend" bytes="6144" orank="22" region="0" commid="0" count="3432" tid="0" op="" dtype="" >7.0789e-03 9.5367e-07 1.1992e-04</hent>
<hent key="0240010000000000000018000000001A" call="MPI_Isend" bytes="6144" orank="26" region="0" commid="0" count="8" tid="0" op="" dtype="" >6.3181e-05 5.9605e-06 1.1921e-05</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.0981e-05 2.0981e-05 2.0981e-05</hent>
<hent key="024001000000000000001800000002A2" call="MPI_Isend" bytes="6144" orank="674" region="0" commid="0" count="12" tid="0" op="" dtype="" >9.0122e-05 5.0068e-06 1.2875e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.7126e+00 1.0014e-05 1.2665e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >9.1696e-04 9.1696e-04 9.1696e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0613e-02 1.0613e-02 1.0613e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >4.1270e-03 4.1270e-03 4.1270e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.0290e-01 3.3181e-03 1.9499e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >6.0296e-04 6.0296e-04 6.0296e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 9.5367e-07 9.5367e-07</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5802e+00 4.1199e-04 2.4822e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >8.2588e-04 1.9073e-06 6.4898e-04</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.1989e-06 6.1989e-06 6.1989e-06</hent>
<hent key="03800100000000000000070000000001" call="MPI_Irecv" bytes="1792" orank="1" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000003" call="MPI_Irecv" bytes="1792" orank="3" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000006" call="MPI_Irecv" bytes="1792" orank="6" region="0" commid="0" count="22" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000016" call="MPI_Irecv" bytes="1792" orank="22" region="0" commid="0" count="21" tid="0" op="" dtype="" >1.4067e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000007000000001A" call="MPI_Irecv" bytes="1792" orank="26" region="0" commid="0" count="119" tid="0" op="" dtype="" >4.7922e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002A2" call="MPI_Irecv" bytes="1792" orank="674" region="0" commid="0" count="91" tid="0" op="" dtype="" >6.3419e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >8.3685e-05 0.0000e+00 6.2943e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9791e+01 0.0000e+00 3.8412e+01</hent>
<hent key="038001000000000000000A0000000006" call="MPI_Irecv" bytes="2560" orank="6" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000016" call="MPI_Irecv" bytes="2560" orank="22" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000001A" call="MPI_Irecv" bytes="2560" orank="26" region="0" commid="0" count="400" tid="0" op="" dtype="" >1.8716e-04 0.0000e+00 6.9141e-06</hent>
<hent key="038001000000000000000A00000002A2" call="MPI_Irecv" bytes="2560" orank="674" region="0" commid="0" count="386" tid="0" op="" dtype="" >3.1924e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000070000000001" call="MPI_Isend" bytes="1792" orank="1" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.1444e-05 1.9073e-06 2.8610e-06</hent>
<hent key="02400100000000000000070000000003" call="MPI_Isend" bytes="1792" orank="3" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.3113e-05 4.0531e-06 5.0068e-06</hent>
<hent key="02400100000000000000070000000006" call="MPI_Isend" bytes="1792" orank="6" region="0" commid="0" count="21" tid="0" op="" dtype="" >3.6478e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000016" call="MPI_Isend" bytes="1792" orank="22" region="0" commid="0" count="21" tid="0" op="" dtype="" >2.8372e-05 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000007000000001A" call="MPI_Isend" bytes="1792" orank="26" region="0" commid="0" count="120" tid="0" op="" dtype="" >7.7367e-04 3.8147e-06 3.5048e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3125e-02 8.3125e-02 8.3125e-02</hent>
<hent key="024001000000000000000700000002A2" call="MPI_Isend" bytes="1792" orank="674" region="0" commid="0" count="113" tid="0" op="" dtype="" >6.4278e-04 3.8147e-06 2.0027e-05</hent>
<hent key="024001000000000000000A0000000006" call="MPI_Isend" bytes="2560" orank="6" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000A0000000016" call="MPI_Isend" bytes="2560" orank="22" region="0" commid="0" count="3" tid="0" op="" dtype="" >4.2915e-06 9.5367e-07 2.1458e-06</hent>
<hent key="024001000000000000000A000000001A" call="MPI_Isend" bytes="2560" orank="26" region="0" commid="0" count="341" tid="0" op="" dtype="" >2.4221e-03 4.7684e-06 3.4094e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >5.6982e-05 9.0599e-06 2.5988e-05</hent>
<hent key="024001000000000000000A00000002A2" call="MPI_Isend" bytes="2560" orank="674" region="0" commid="0" count="394" tid="0" op="" dtype="" >2.2347e-03 3.8147e-06 1.8835e-05</hent>
<hent key="03800100000000000000100000000001" call="MPI_Irecv" bytes="4096" orank="1" region="0" commid="0" count="12648" tid="0" op="" dtype="" >6.0747e-03 0.0000e+00 1.7881e-05</hent>
<hent key="03800100000000000000100000000003" call="MPI_Irecv" bytes="4096" orank="3" region="0" commid="0" count="12562" tid="0" op="" dtype="" >2.1422e-03 0.0000e+00 1.5020e-05</hent>
<hent key="0380010000000000000010000000001A" call="MPI_Irecv" bytes="4096" orank="26" region="0" commid="0" count="3044" tid="0" op="" dtype="" >1.5514e-03 0.0000e+00 6.2943e-05</hent>
<hent key="038001000000000000001000000002A2" call="MPI_Irecv" bytes="4096" orank="674" region="0" commid="0" count="3355" tid="0" op="" dtype="" >3.0208e-03 0.0000e+00 9.8944e-05</hent>
<hent key="02400100000000000000100000000001" call="MPI_Isend" bytes="4096" orank="1" region="0" commid="0" count="12567" tid="0" op="" dtype="" >2.1488e-02 0.0000e+00 8.7023e-05</hent>
<hent key="02400100000000000000100000000003" call="MPI_Isend" bytes="4096" orank="3" region="0" commid="0" count="12608" tid="0" op="" dtype="" >3.0542e-02 9.5367e-07 3.3855e-05</hent>
<hent key="0240010000000000000010000000001A" call="MPI_Isend" bytes="4096" orank="26" region="0" commid="0" count="3317" tid="0" op="" dtype="" >3.5415e-02 1.9073e-06 1.3208e-04</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.7810e-04 7.1049e-05 1.0705e-04</hent>
<hent key="024001000000000000001000000002A2" call="MPI_Isend" bytes="4096" orank="674" region="0" commid="0" count="3306" tid="0" op="" dtype="" >1.9442e-02 9.5367e-07 8.1062e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.5644e-04 5.8174e-05 1.1110e-04</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >6.9141e-06 1.9073e-06 5.0068e-06</hent>
<hent key="03800100000000000000000400000001" call="MPI_Irecv" bytes="4" orank="1" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.1778e-03 0.0000e+00 2.0981e-05</hent>
<hent key="03800100000000000000000400000003" call="MPI_Irecv" bytes="4" orank="3" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.7827e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000400000006" call="MPI_Irecv" bytes="4" orank="6" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.6042e-04 0.0000e+00 8.3923e-05</hent>
<hent key="03800100000000000000000400000016" call="MPI_Irecv" bytes="4" orank="22" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.3692e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0380010000000000000000040000001A" call="MPI_Irecv" bytes="4" orank="26" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.1780e-03 0.0000e+00 9.5129e-05</hent>
<hent key="038001000000000000001C0000000006" call="MPI_Irecv" bytes="7168" orank="6" region="0" commid="0" count="8863" tid="0" op="" dtype="" >1.4305e-03 0.0000e+00 3.5048e-05</hent>
<hent key="038001000000000000001C0000000016" call="MPI_Irecv" bytes="7168" orank="22" region="0" commid="0" count="9778" tid="0" op="" dtype="" >1.4193e-03 0.0000e+00 3.1233e-05</hent>
<hent key="038001000000000000000004000002A2" call="MPI_Irecv" bytes="4" orank="674" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.2109e-03 0.0000e+00 9.4891e-05</hent>
<hent key="038001000000000000001C00000002A2" call="MPI_Irecv" bytes="7168" orank="674" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="02400100000000000000000400000001" call="MPI_Isend" bytes="4" orank="1" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.4789e-03 0.0000e+00 3.8862e-05</hent>
<hent key="02400100000000000000000400000003" call="MPI_Isend" bytes="4" orank="3" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.3522e-03 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000000400000006" call="MPI_Isend" bytes="4" orank="6" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.5078e-03 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000400000016" call="MPI_Isend" bytes="4" orank="22" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.8337e-03 0.0000e+00 2.5034e-05</hent>
<hent key="0240010000000000000000040000001A" call="MPI_Isend" bytes="4" orank="26" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.8337e-02 4.7684e-06 3.5715e-04</hent>
<hent key="024001000000000000001C0000000006" call="MPI_Isend" bytes="7168" orank="6" region="0" commid="0" count="10689" tid="0" op="" dtype="" >2.1926e-02 9.5367e-07 4.0054e-05</hent>
<hent key="024001000000000000001C0000000016" call="MPI_Isend" bytes="7168" orank="22" region="0" commid="0" count="9268" tid="0" op="" dtype="" >1.9361e-02 9.5367e-07 1.0109e-04</hent>
<hent key="024001000000000000000004000002A2" call="MPI_Isend" bytes="4" orank="674" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.6434e-02 3.8147e-06 1.4496e-04</hent>
<hent key="024001000000000000001C00000002A2" call="MPI_Isend" bytes="7168" orank="674" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.3113e-05 5.9605e-06 7.1526e-06</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.8903e-05 6.8903e-05 6.8903e-05</hent>
<hent key="03800100000000000000020000000001" call="MPI_Irecv" bytes="512" orank="1" region="0" commid="0" count="3386" tid="0" op="" dtype="" >8.0776e-04 0.0000e+00 1.3828e-05</hent>
<hent key="03800100000000000000020000000003" call="MPI_Irecv" bytes="512" orank="3" region="0" commid="0" count="3370" tid="0" op="" dtype="" >6.4135e-04 0.0000e+00 3.6001e-05</hent>
<hent key="03800100000000000000050000000001" call="MPI_Irecv" bytes="1280" orank="1" region="0" commid="0" count="42" tid="0" op="" dtype="" >1.8120e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000003" call="MPI_Irecv" bytes="1280" orank="3" region="0" commid="0" count="48" tid="0" op="" dtype="" >1.5736e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000006" call="MPI_Irecv" bytes="1280" orank="6" region="0" commid="0" count="135" tid="0" op="" dtype="" >2.6464e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000016" call="MPI_Irecv" bytes="1280" orank="22" region="0" commid="0" count="140" tid="0" op="" dtype="" >7.7963e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000005000000001A" call="MPI_Irecv" bytes="1280" orank="26" region="0" commid="0" count="46" tid="0" op="" dtype="" >1.6212e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4485e+01 9.0599e-06 1.8444e-01</hent>
<hent key="038001000000000000000500000002A2" call="MPI_Irecv" bytes="1280" orank="674" region="0" commid="0" count="50" tid="0" op="" dtype="" >3.9101e-05 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000020000000001" call="MPI_Isend" bytes="512" orank="1" region="0" commid="0" count="3364" tid="0" op="" dtype="" >1.7512e-03 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000020000000003" call="MPI_Isend" bytes="512" orank="3" region="0" commid="0" count="3372" tid="0" op="" dtype="" >2.3215e-03 0.0000e+00 1.5020e-05</hent>
<hent key="03800100000000000000080000000003" call="MPI_Irecv" bytes="2048" orank="3" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.1921e-06 1.1921e-06 1.1921e-06</hent>
<hent key="03800100000000000000080000000006" call="MPI_Irecv" bytes="2048" orank="6" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000016" call="MPI_Irecv" bytes="2048" orank="22" region="0" commid="0" count="8" tid="0" op="" dtype="" >6.6757e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000001A" call="MPI_Irecv" bytes="2048" orank="26" region="0" commid="0" count="157" tid="0" op="" dtype="" >7.9870e-05 0.0000e+00 7.1526e-06</hent>
<hent key="038001000000000000000800000002A2" call="MPI_Irecv" bytes="2048" orank="674" region="0" commid="0" count="141" tid="0" op="" dtype="" >1.2016e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000050000000001" call="MPI_Isend" bytes="1280" orank="1" region="0" commid="0" count="55" tid="0" op="" dtype="" >1.0204e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000050000000003" call="MPI_Isend" bytes="1280" orank="3" region="0" commid="0" count="64" tid="0" op="" dtype="" >2.2674e-04 2.8610e-06 4.0531e-06</hent>
<hent key="02400100000000000000050000000006" call="MPI_Isend" bytes="1280" orank="6" region="0" commid="0" count="134" tid="0" op="" dtype="" >2.0194e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000050000000016" call="MPI_Isend" bytes="1280" orank="22" region="0" commid="0" count="127" tid="0" op="" dtype="" >1.7285e-04 9.5367e-07 2.8610e-06</hent>
<hent key="0240010000000000000005000000001A" call="MPI_Isend" bytes="1280" orank="26" region="0" commid="0" count="56" tid="0" op="" dtype="" >3.3712e-04 4.0531e-06 1.2875e-05</hent>
<hent key="024001000000000000000500000002A2" call="MPI_Isend" bytes="1280" orank="674" region="0" commid="0" count="35" tid="0" op="" dtype="" >1.6212e-04 2.8610e-06 6.9141e-06</hent>
</hash>
<internal rank="2" log_i="1724765674.523155" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="3" mpi_size="696" stamp_init="1724765564.465542" stamp_final="1724765674.530390" username="apac4" allocationname="unknown" flags="0" pid="1612120" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10065e+02" utime="8.94297e+01" stime="1.40923e+01" mtime="7.24701e+01" gflop="0.00000e+00" gbyte="3.77239e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24701e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf45715581559159e56591559151d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09928e+02" utime="8.93969e+01" stime="1.40817e+01" mtime="7.24701e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24701e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2178e+09" > 5.9208e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2149e+09" > 2.6101e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8833e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9770e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6464e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.8876e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5802e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2136e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0608e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3146e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4523e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="207" >
<hent key="02400100000000000000080000000000" call="MPI_Isend" bytes="2048" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="02400100000000000000080000000002" call="MPI_Isend" bytes="2048" orank="2" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="02400100000000000000080000000007" call="MPI_Isend" bytes="2048" orank="7" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.0252e-05 1.9073e-06 2.1458e-06</hent>
<hent key="02400100000000000000080000000017" call="MPI_Isend" bytes="2048" orank="23" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.1206e-05 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000008000000001B" call="MPI_Isend" bytes="2048" orank="27" region="0" commid="0" count="144" tid="0" op="" dtype="" >7.9536e-04 5.0068e-06 6.1989e-06</hent>
<hent key="024001000000000000000800000002A3" call="MPI_Isend" bytes="2048" orank="675" region="0" commid="0" count="138" tid="0" op="" dtype="" >6.4874e-04 3.8147e-06 5.9605e-06</hent>
<hent key="038001000000000000000E0000000000" call="MPI_Irecv" bytes="3584" orank="0" region="0" commid="0" count="737" tid="0" op="" dtype="" >1.0633e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000002" call="MPI_Irecv" bytes="3584" orank="2" region="0" commid="0" count="92" tid="0" op="" dtype="" >4.2439e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E000000001B" call="MPI_Irecv" bytes="3584" orank="27" region="0" commid="0" count="148" tid="0" op="" dtype="" >5.6982e-05 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000000E00000002A3" call="MPI_Irecv" bytes="3584" orank="675" region="0" commid="0" count="106" tid="0" op="" dtype="" >4.0054e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000E0000000000" call="MPI_Isend" bytes="3584" orank="0" region="0" commid="0" count="203" tid="0" op="" dtype="" >4.8900e-04 9.5367e-07 5.0068e-06</hent>
<hent key="024001000000000000000E0000000002" call="MPI_Isend" bytes="3584" orank="2" region="0" commid="0" count="138" tid="0" op="" dtype="" >2.2459e-04 0.0000e+00 3.0994e-06</hent>
<hent key="024001000000000000000E000000001B" call="MPI_Isend" bytes="3584" orank="27" region="0" commid="0" count="142" tid="0" op="" dtype="" >8.3661e-04 5.0068e-06 1.2875e-05</hent>
<hent key="024001000000000000000E00000002A3" call="MPI_Isend" bytes="3584" orank="675" region="0" commid="0" count="141" tid="0" op="" dtype="" >7.2479e-04 4.0531e-06 1.0967e-05</hent>
<hent key="03800100000000000000028000000000" call="MPI_Irecv" bytes="640" orank="0" region="0" commid="0" count="391" tid="0" op="" dtype="" >1.1897e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000028000000002" call="MPI_Irecv" bytes="640" orank="2" region="0" commid="0" count="341" tid="0" op="" dtype="" >1.5807e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000007" call="MPI_Irecv" bytes="640" orank="7" region="0" commid="0" count="441" tid="0" op="" dtype="" >2.8729e-04 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000028000000017" call="MPI_Irecv" bytes="640" orank="23" region="0" commid="0" count="413" tid="0" op="" dtype="" >9.6560e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000001B" call="MPI_Irecv" bytes="640" orank="27" region="0" commid="0" count="26" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000014000000001B" call="MPI_Irecv" bytes="5120" orank="27" region="0" commid="0" count="839" tid="0" op="" dtype="" >1.5473e-04 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000280000002A3" call="MPI_Irecv" bytes="640" orank="675" region="0" commid="0" count="27" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000001400000002A3" call="MPI_Irecv" bytes="5120" orank="675" region="0" commid="0" count="955" tid="0" op="" dtype="" >2.3389e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000A0000000001B" call="MPI_Irecv" bytes="40960" orank="27" region="0" commid="0" count="2941" tid="0" op="" dtype="" >7.5650e-04 0.0000e+00 8.8215e-06</hent>
<hent key="02400100000000000000028000000000" call="MPI_Isend" bytes="640" orank="0" region="0" commid="0" count="393" tid="0" op="" dtype="" >1.1649e-03 1.9073e-06 1.0014e-05</hent>
<hent key="02400100000000000000028000000002" call="MPI_Isend" bytes="640" orank="2" region="0" commid="0" count="363" tid="0" op="" dtype="" >5.1069e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000028000000007" call="MPI_Isend" bytes="640" orank="7" region="0" commid="0" count="411" tid="0" op="" dtype="" >5.1117e-04 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000028000000017" call="MPI_Isend" bytes="640" orank="23" region="0" commid="0" count="419" tid="0" op="" dtype="" >4.3368e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000002800000001B" call="MPI_Isend" bytes="640" orank="27" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.2898e-04 3.8147e-06 5.9605e-06</hent>
<hent key="03800100000000000000A000000002A3" call="MPI_Irecv" bytes="40960" orank="675" region="0" commid="0" count="3299" tid="0" op="" dtype="" >7.1836e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000014000000001B" call="MPI_Isend" bytes="5120" orank="27" region="0" commid="0" count="1022" tid="0" op="" dtype="" >4.0338e-03 9.5367e-07 1.3113e-05</hent>
<hent key="024001000000000000000280000002A3" call="MPI_Isend" bytes="640" orank="675" region="0" commid="0" count="26" tid="0" op="" dtype="" >1.1468e-04 3.8147e-06 5.0068e-06</hent>
<hent key="0380010000000000000080000000001B" call="MPI_Irecv" bytes="32768" orank="27" region="0" commid="0" count="9759" tid="0" op="" dtype="" >2.7411e-03 0.0000e+00 1.1206e-05</hent>
<hent key="024001000000000000001400000002A3" call="MPI_Isend" bytes="5120" orank="675" region="0" commid="0" count="917" tid="0" op="" dtype="" >3.2814e-03 9.5367e-07 1.0014e-05</hent>
<hent key="038001000000000000008000000002A3" call="MPI_Irecv" bytes="32768" orank="675" region="0" commid="0" count="9401" tid="0" op="" dtype="" >1.7743e-03 0.0000e+00 1.5020e-05</hent>
<hent key="03800100000000000000014000000000" call="MPI_Irecv" bytes="320" orank="0" region="0" commid="0" count="535" tid="0" op="" dtype="" >1.5521e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000014000000002" call="MPI_Irecv" bytes="320" orank="2" region="0" commid="0" count="538" tid="0" op="" dtype="" >2.4128e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000007" call="MPI_Irecv" bytes="320" orank="7" region="0" commid="0" count="440" tid="0" op="" dtype="" >2.3222e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000017" call="MPI_Irecv" bytes="320" orank="23" region="0" commid="0" count="452" tid="0" op="" dtype="" >1.0324e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000001B" call="MPI_Irecv" bytes="320" orank="27" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.0490e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A0000000001B" call="MPI_Isend" bytes="40960" orank="27" region="0" commid="0" count="3453" tid="0" op="" dtype="" >6.8061e-02 8.8215e-06 3.8147e-05</hent>
<hent key="038001000000000000000140000002A3" call="MPI_Irecv" bytes="320" orank="675" region="0" commid="0" count="15" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A000000002A3" call="MPI_Isend" bytes="40960" orank="675" region="0" commid="0" count="3168" tid="0" op="" dtype="" >3.4691e-02 6.9141e-06 2.0027e-05</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >5.8833e+00 0.0000e+00 1.1072e-01</hent>
<hent key="0240010000000000000080000000001B" call="MPI_Isend" bytes="32768" orank="27" region="0" commid="0" count="9247" tid="0" op="" dtype="" >1.7529e-01 8.1062e-06 7.6056e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.0981e-05 0.0000e+00 2.0027e-05</hent>
<hent key="024001000000000000008000000002A3" call="MPI_Isend" bytes="32768" orank="675" region="0" commid="0" count="9532" tid="0" op="" dtype="" >1.0289e-01 6.9141e-06 3.5048e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000000" call="MPI_Isend" bytes="320" orank="0" region="0" commid="0" count="553" tid="0" op="" dtype="" >1.4794e-03 9.5367e-07 1.5974e-05</hent>
<hent key="02400100000000000000014000000002" call="MPI_Isend" bytes="320" orank="2" region="0" commid="0" count="566" tid="0" op="" dtype="" >6.2656e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000014000000007" call="MPI_Isend" bytes="320" orank="7" region="0" commid="0" count="442" tid="0" op="" dtype="" >4.5323e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000014000000017" call="MPI_Isend" bytes="320" orank="23" region="0" commid="0" count="440" tid="0" op="" dtype="" >3.6073e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0240010000000000000001400000001B" call="MPI_Isend" bytes="320" orank="27" region="0" commid="0" count="21" tid="0" op="" dtype="" >9.4891e-05 3.8147e-06 5.0068e-06</hent>
<hent key="024001000000000000000140000002A3" call="MPI_Isend" bytes="320" orank="675" region="0" commid="0" count="18" tid="0" op="" dtype="" >7.7963e-05 3.8147e-06 5.0068e-06</hent>
<hent key="03800100000000000000000000000000" call="MPI_Irecv" bytes="0" orank="0" region="0" commid="0" count="542" tid="0" op="" dtype="" >1.3733e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000002" call="MPI_Irecv" bytes="0" orank="2" region="0" commid="0" count="562" tid="0" op="" dtype="" >1.8930e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000000000007" call="MPI_Irecv" bytes="0" orank="7" region="0" commid="0" count="331" tid="0" op="" dtype="" >1.1873e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000017" call="MPI_Irecv" bytes="0" orank="23" region="0" commid="0" count="336" tid="0" op="" dtype="" >6.6519e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000001B" call="MPI_Irecv" bytes="0" orank="27" region="0" commid="0" count="82" tid="0" op="" dtype="" >1.7643e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002A3" call="MPI_Irecv" bytes="0" orank="675" region="0" commid="0" count="91" tid="0" op="" dtype="" >2.3365e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.4033e-04 0.0000e+00 8.4162e-05</hent>
<hent key="03800100000000000000030000000007" call="MPI_Irecv" bytes="768" orank="7" region="0" commid="0" count="954" tid="0" op="" dtype="" >1.5569e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000030000000017" call="MPI_Irecv" bytes="768" orank="23" region="0" commid="0" count="508" tid="0" op="" dtype="" >6.3419e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000000000000" call="MPI_Isend" bytes="0" orank="0" region="0" commid="0" count="515" tid="0" op="" dtype="" >1.0297e-03 9.5367e-07 6.7949e-05</hent>
<hent key="02400100000000000000000000000002" call="MPI_Isend" bytes="0" orank="2" region="0" commid="0" count="546" tid="0" op="" dtype="" >3.6550e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000007" call="MPI_Isend" bytes="0" orank="7" region="0" commid="0" count="344" tid="0" op="" dtype="" >2.5892e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000000000000017" call="MPI_Isend" bytes="0" orank="23" region="0" commid="0" count="340" tid="0" op="" dtype="" >1.8597e-04 0.0000e+00 3.0994e-06</hent>
<hent key="0240010000000000000000000000001B" call="MPI_Isend" bytes="0" orank="27" region="0" commid="0" count="82" tid="0" op="" dtype="" >3.1948e-04 9.5367e-07 8.1062e-06</hent>
<hent key="024001000000000000000000000002A3" call="MPI_Isend" bytes="0" orank="675" region="0" commid="0" count="86" tid="0" op="" dtype="" >3.1590e-04 9.5367e-07 5.0068e-06</hent>
<hent key="03800100000000000000060000000000" call="MPI_Irecv" bytes="1536" orank="0" region="0" commid="0" count="6" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000060000000002" call="MPI_Irecv" bytes="1536" orank="2" region="0" commid="0" count="17" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000007" call="MPI_Irecv" bytes="1536" orank="7" region="0" commid="0" count="58" tid="0" op="" dtype="" >4.6015e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000060000000017" call="MPI_Irecv" bytes="1536" orank="23" region="0" commid="0" count="62" tid="0" op="" dtype="" >1.7643e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000001B" call="MPI_Irecv" bytes="1536" orank="27" region="0" commid="0" count="81" tid="0" op="" dtype="" >2.1458e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002A3" call="MPI_Irecv" bytes="1536" orank="675" region="0" commid="0" count="68" tid="0" op="" dtype="" >2.0266e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000030000000007" call="MPI_Isend" bytes="768" orank="7" region="0" commid="0" count="890" tid="0" op="" dtype="" >4.4727e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000030000000017" call="MPI_Isend" bytes="768" orank="23" region="0" commid="0" count="738" tid="0" op="" dtype="" >3.7408e-04 0.0000e+00 5.9605e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.9073e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000000" call="MPI_Irecv" bytes="448" orank="0" region="0" commid="0" count="192" tid="0" op="" dtype="" >3.0994e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000002" call="MPI_Irecv" bytes="448" orank="2" region="0" commid="0" count="28" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000000" call="MPI_Isend" bytes="1536" orank="0" region="0" commid="0" count="18" tid="0" op="" dtype="" >6.6519e-05 2.8610e-06 4.0531e-06</hent>
<hent key="02400100000000000000060000000002" call="MPI_Isend" bytes="1536" orank="2" region="0" commid="0" count="18" tid="0" op="" dtype="" >3.7432e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000007" call="MPI_Isend" bytes="1536" orank="7" region="0" commid="0" count="70" tid="0" op="" dtype="" >1.1182e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000017" call="MPI_Isend" bytes="1536" orank="23" region="0" commid="0" count="52" tid="0" op="" dtype="" >8.1062e-05 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000006000000001B" call="MPI_Isend" bytes="1536" orank="27" region="0" commid="0" count="83" tid="0" op="" dtype="" >4.3821e-04 4.0531e-06 6.1989e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002A3" call="MPI_Isend" bytes="1536" orank="675" region="0" commid="0" count="79" tid="0" op="" dtype="" >3.5715e-04 3.8147e-06 5.9605e-06</hent>
<hent key="038001000000000000000C000000001B" call="MPI_Irecv" bytes="3072" orank="27" region="0" commid="0" count="329" tid="0" op="" dtype="" >9.8944e-05 0.0000e+00 2.1458e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002A3" call="MPI_Irecv" bytes="3072" orank="675" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.2493e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C000000000" call="MPI_Isend" bytes="448" orank="0" region="0" commid="0" count="56" tid="0" op="" dtype="" >3.8624e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C000000002" call="MPI_Isend" bytes="448" orank="2" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.5020e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C000000001B" call="MPI_Isend" bytes="3072" orank="27" region="0" commid="0" count="321" tid="0" op="" dtype="" >1.8799e-03 4.7684e-06 1.4067e-05</hent>
<hent key="024001000000000000000C00000002A3" call="MPI_Isend" bytes="3072" orank="675" region="0" commid="0" count="357" tid="0" op="" dtype="" >1.7762e-03 3.8147e-06 1.1206e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.8692e-04 5.9128e-05 6.3896e-05</hent>
<hent key="03800100000000000000038000000000" call="MPI_Irecv" bytes="896" orank="0" region="0" commid="0" count="159" tid="0" op="" dtype="" >6.3658e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000038000000002" call="MPI_Irecv" bytes="896" orank="2" region="0" commid="0" count="175" tid="0" op="" dtype="" >7.9393e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000007" call="MPI_Irecv" bytes="896" orank="7" region="0" commid="0" count="2704" tid="0" op="" dtype="" >5.2524e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000038000000017" call="MPI_Irecv" bytes="896" orank="23" region="0" commid="0" count="3169" tid="0" op="" dtype="" >5.0473e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000003800000001B" call="MPI_Irecv" bytes="896" orank="27" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.3828e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002A3" call="MPI_Irecv" bytes="896" orank="675" region="0" commid="0" count="27" tid="0" op="" dtype="" >8.5831e-06 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.5020e-05 1.5020e-05 1.5020e-05</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000180000000007" call="MPI_Irecv" bytes="6144" orank="7" region="0" commid="0" count="3642" tid="0" op="" dtype="" >3.9029e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000180000000017" call="MPI_Irecv" bytes="6144" orank="23" region="0" commid="0" count="1854" tid="0" op="" dtype="" >3.6716e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000018000000001B" call="MPI_Irecv" bytes="6144" orank="27" region="0" commid="0" count="14" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000001800000002A3" call="MPI_Irecv" bytes="6144" orank="675" region="0" commid="0" count="11" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000038000000000" call="MPI_Isend" bytes="896" orank="0" region="0" commid="0" count="165" tid="0" op="" dtype="" >5.3859e-04 1.9073e-06 1.7166e-05</hent>
<hent key="02400100000000000000038000000002" call="MPI_Isend" bytes="896" orank="2" region="0" commid="0" count="156" tid="0" op="" dtype="" >2.5892e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000038000000007" call="MPI_Isend" bytes="896" orank="7" region="0" commid="0" count="2792" tid="0" op="" dtype="" >1.6654e-03 0.0000e+00 1.3828e-05</hent>
<hent key="02400100000000000000038000000017" call="MPI_Isend" bytes="896" orank="23" region="0" commid="0" count="2928" tid="0" op="" dtype="" >1.5800e-03 0.0000e+00 1.4067e-05</hent>
<hent key="0240010000000000000003800000001B" call="MPI_Isend" bytes="896" orank="27" region="0" commid="0" count="38" tid="0" op="" dtype="" >1.8859e-04 3.8147e-06 5.9605e-06</hent>
<hent key="024001000000000000000380000002A3" call="MPI_Isend" bytes="896" orank="675" region="0" commid="0" count="38" tid="0" op="" dtype="" >1.6594e-04 3.0994e-06 5.0068e-06</hent>
<hent key="02400100000000000000180000000007" call="MPI_Isend" bytes="6144" orank="7" region="0" commid="0" count="3227" tid="0" op="" dtype="" >5.7907e-03 9.5367e-07 1.3113e-05</hent>
<hent key="02400100000000000000180000000017" call="MPI_Isend" bytes="6144" orank="23" region="0" commid="0" count="2771" tid="0" op="" dtype="" >4.9582e-03 9.5367e-07 1.3113e-05</hent>
<hent key="0240010000000000000018000000001B" call="MPI_Isend" bytes="6144" orank="27" region="0" commid="0" count="13" tid="0" op="" dtype="" >9.4652e-05 5.9605e-06 1.2875e-05</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.1975e-05 5.1975e-05 5.1975e-05</hent>
<hent key="024001000000000000001800000002A3" call="MPI_Isend" bytes="6144" orank="675" region="0" commid="0" count="16" tid="0" op="" dtype="" >9.6083e-05 5.0068e-06 6.9141e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >9.3908e+00 2.9087e-05 1.2675e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >9.0003e-04 9.0003e-04 9.0003e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0608e-02 1.0608e-02 1.0608e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >4.5741e-03 4.5741e-03 4.5741e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.9928e-01 3.3500e-03 1.9106e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.8913e-04 5.8913e-04 5.8913e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5802e+00 3.9196e-04 2.4819e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.6464e-05 0.0000e+00 1.0014e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000070000000000" call="MPI_Irecv" bytes="1792" orank="0" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000002" call="MPI_Irecv" bytes="1792" orank="2" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000070000000007" call="MPI_Irecv" bytes="1792" orank="7" region="0" commid="0" count="29" tid="0" op="" dtype="" >2.1935e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000017" call="MPI_Irecv" bytes="1792" orank="23" region="0" commid="0" count="17" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000001B" call="MPI_Irecv" bytes="1792" orank="27" region="0" commid="0" count="107" tid="0" op="" dtype="" >2.4796e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002A3" call="MPI_Irecv" bytes="1792" orank="675" region="0" commid="0" count="132" tid="0" op="" dtype="" >3.7670e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >1.1849e-04 0.0000e+00 6.4850e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9770e+01 0.0000e+00 3.8415e+01</hent>
<hent key="038001000000000000000A0000000007" call="MPI_Irecv" bytes="2560" orank="7" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000A0000000017" call="MPI_Irecv" bytes="2560" orank="23" region="0" commid="0" count="4" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A000000001B" call="MPI_Irecv" bytes="2560" orank="27" region="0" commid="0" count="371" tid="0" op="" dtype="" >1.1492e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002A3" call="MPI_Irecv" bytes="2560" orank="675" region="0" commid="0" count="370" tid="0" op="" dtype="" >1.1849e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000000" call="MPI_Isend" bytes="1792" orank="0" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.4544e-05 2.8610e-06 4.0531e-06</hent>
<hent key="02400100000000000000070000000002" call="MPI_Isend" bytes="1792" orank="2" region="0" commid="0" count="2" tid="0" op="" dtype="" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="02400100000000000000070000000007" call="MPI_Isend" bytes="1792" orank="7" region="0" commid="0" count="16" tid="0" op="" dtype="" >2.5988e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000017" call="MPI_Isend" bytes="1792" orank="23" region="0" commid="0" count="29" tid="0" op="" dtype="" >4.1962e-05 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000007000000001B" call="MPI_Isend" bytes="1792" orank="27" region="0" commid="0" count="114" tid="0" op="" dtype="" >6.1440e-04 4.0531e-06 6.1989e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3146e-02 8.3146e-02 8.3146e-02</hent>
<hent key="024001000000000000000700000002A3" call="MPI_Isend" bytes="1792" orank="675" region="0" commid="0" count="121" tid="0" op="" dtype="" >5.7340e-04 3.8147e-06 1.0967e-05</hent>
<hent key="024001000000000000000A0000000017" call="MPI_Isend" bytes="2560" orank="23" region="0" commid="0" count="2" tid="0" op="" dtype="" >3.0994e-06 9.5367e-07 2.1458e-06</hent>
<hent key="024001000000000000000A000000001B" call="MPI_Isend" bytes="2560" orank="27" region="0" commid="0" count="360" tid="0" op="" dtype="" >2.0342e-03 4.7684e-06 1.2159e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >8.3685e-05 1.0967e-05 4.2915e-05</hent>
<hent key="024001000000000000000A00000002A3" call="MPI_Isend" bytes="2560" orank="675" region="0" commid="0" count="348" tid="0" op="" dtype="" >1.6780e-03 3.8147e-06 1.0014e-05</hent>
<hent key="03800100000000000000100000000000" call="MPI_Irecv" bytes="4096" orank="0" region="0" commid="0" count="11963" tid="0" op="" dtype="" >1.4727e-03 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000100000000002" call="MPI_Irecv" bytes="4096" orank="2" region="0" commid="0" count="12608" tid="0" op="" dtype="" >4.7388e-03 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000010000000001B" call="MPI_Irecv" bytes="4096" orank="27" region="0" commid="0" count="2846" tid="0" op="" dtype="" >4.9591e-04 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000001000000002A3" call="MPI_Irecv" bytes="4096" orank="675" region="0" commid="0" count="2729" tid="0" op="" dtype="" >6.2275e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000100000000000" call="MPI_Isend" bytes="4096" orank="0" region="0" commid="0" count="12497" tid="0" op="" dtype="" >3.2364e-02 0.0000e+00 6.0081e-05</hent>
<hent key="02400100000000000000100000000002" call="MPI_Isend" bytes="4096" orank="2" region="0" commid="0" count="12562" tid="0" op="" dtype="" >2.1231e-02 0.0000e+00 1.8120e-05</hent>
<hent key="0240010000000000000010000000001B" call="MPI_Isend" bytes="4096" orank="27" region="0" commid="0" count="2674" tid="0" op="" dtype="" >1.0587e-02 9.5367e-07 2.0981e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >2.5988e-04 9.8944e-05 1.6093e-04</hent>
<hent key="024001000000000000001000000002A3" call="MPI_Isend" bytes="4096" orank="675" region="0" commid="0" count="2760" tid="0" op="" dtype="" >9.7835e-03 9.5367e-07 1.7881e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.6015e-05 4.6015e-05 4.6015e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >5.8508e-04 8.1062e-05 2.1601e-04</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.7752e-06 1.9073e-06 7.8678e-06</hent>
<hent key="03800100000000000000000400000000" call="MPI_Irecv" bytes="4" orank="0" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.4751e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000002" call="MPI_Irecv" bytes="4" orank="2" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.9652e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000000400000007" call="MPI_Irecv" bytes="4" orank="7" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.2534e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000017" call="MPI_Irecv" bytes="4" orank="23" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.6039e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0380010000000000000000040000001B" call="MPI_Irecv" bytes="4" orank="27" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.8961e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001C0000000007" call="MPI_Irecv" bytes="7168" orank="7" region="0" commid="0" count="9058" tid="0" op="" dtype="" >9.3007e-04 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000001C0000000017" call="MPI_Irecv" bytes="7168" orank="23" region="0" commid="0" count="10846" tid="0" op="" dtype="" >2.0976e-03 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000001C000000001B" call="MPI_Irecv" bytes="7168" orank="27" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000004000002A3" call="MPI_Irecv" bytes="4" orank="675" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.0783e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000400000000" call="MPI_Isend" bytes="4" orank="0" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.2633e-03 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000000400000002" call="MPI_Isend" bytes="4" orank="2" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.2527e-03 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000000400000007" call="MPI_Isend" bytes="4" orank="7" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.4167e-03 0.0000e+00 1.2708e-04</hent>
<hent key="02400100000000000000000400000017" call="MPI_Isend" bytes="4" orank="23" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.1926e-03 0.0000e+00 4.3869e-05</hent>
<hent key="0240010000000000000000040000001B" call="MPI_Isend" bytes="4" orank="27" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.5213e-02 3.8147e-06 2.9778e-04</hent>
<hent key="024001000000000000001C0000000007" call="MPI_Isend" bytes="7168" orank="7" region="0" commid="0" count="9473" tid="0" op="" dtype="" >1.7934e-02 9.5367e-07 2.1935e-05</hent>
<hent key="024001000000000000001C0000000017" call="MPI_Isend" bytes="7168" orank="23" region="0" commid="0" count="9929" tid="0" op="" dtype="" >1.8116e-02 9.5367e-07 2.2888e-05</hent>
<hent key="024001000000000000000004000002A3" call="MPI_Isend" bytes="4" orank="675" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.9277e-02 3.8147e-06 6.1989e-05</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.7036e-05 9.7036e-05 9.7036e-05</hent>
<hent key="03800100000000000000020000000000" call="MPI_Irecv" bytes="512" orank="0" region="0" commid="0" count="3208" tid="0" op="" dtype="" >4.6158e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000020000000002" call="MPI_Irecv" bytes="512" orank="2" region="0" commid="0" count="3372" tid="0" op="" dtype="" >4.6945e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000000" call="MPI_Irecv" bytes="1280" orank="0" region="0" commid="0" count="59" tid="0" op="" dtype="" >2.5272e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000050000000002" call="MPI_Irecv" bytes="1280" orank="2" region="0" commid="0" count="64" tid="0" op="" dtype="" >2.3842e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000007" call="MPI_Irecv" bytes="1280" orank="7" region="0" commid="0" count="135" tid="0" op="" dtype="" >1.0800e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000050000000017" call="MPI_Irecv" bytes="1280" orank="23" region="0" commid="0" count="132" tid="0" op="" dtype="" >2.6703e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000001B" call="MPI_Irecv" bytes="1280" orank="27" region="0" commid="0" count="46" tid="0" op="" dtype="" >1.2875e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4927e+01 1.3113e-05 1.8434e-01</hent>
<hent key="038001000000000000000500000002A3" call="MPI_Irecv" bytes="1280" orank="675" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000020000000000" call="MPI_Isend" bytes="512" orank="0" region="0" commid="0" count="3344" tid="0" op="" dtype="" >2.1544e-03 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000020000000002" call="MPI_Isend" bytes="512" orank="2" region="0" commid="0" count="3370" tid="0" op="" dtype="" >1.6732e-03 0.0000e+00 1.3828e-05</hent>
<hent key="03800100000000000000080000000007" call="MPI_Irecv" bytes="2048" orank="7" region="0" commid="0" count="7" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000080000000017" call="MPI_Irecv" bytes="2048" orank="23" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000001B" call="MPI_Irecv" bytes="2048" orank="27" region="0" commid="0" count="156" tid="0" op="" dtype="" >4.4346e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002A3" call="MPI_Irecv" bytes="2048" orank="675" region="0" commid="0" count="157" tid="0" op="" dtype="" >5.1260e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000050000000000" call="MPI_Isend" bytes="1280" orank="0" region="0" commid="0" count="51" tid="0" op="" dtype="" >1.8764e-04 2.8610e-06 1.3113e-05</hent>
<hent key="02400100000000000000050000000002" call="MPI_Isend" bytes="1280" orank="2" region="0" commid="0" count="48" tid="0" op="" dtype="" >9.5367e-05 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000050000000007" call="MPI_Isend" bytes="1280" orank="7" region="0" commid="0" count="130" tid="0" op="" dtype="" >2.0003e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000050000000017" call="MPI_Isend" bytes="1280" orank="23" region="0" commid="0" count="145" tid="0" op="" dtype="" >2.0361e-04 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000005000000001B" call="MPI_Isend" bytes="1280" orank="27" region="0" commid="0" count="59" tid="0" op="" dtype="" >3.0065e-04 3.8147e-06 1.1921e-05</hent>
<hent key="024001000000000000000500000002A3" call="MPI_Isend" bytes="1280" orank="675" region="0" commid="0" count="55" tid="0" op="" dtype="" >2.5249e-04 3.8147e-06 5.0068e-06</hent>
</hash>
<internal rank="3" log_i="1724765674.530390" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="4" mpi_size="696" stamp_init="1724765564.465358" stamp_final="1724765674.529077" username="apac4" allocationname="unknown" flags="0" pid="1612121" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10064e+02" utime="8.59716e+01" stime="1.49772e+01" mtime="7.24041e+01" gflop="0.00000e+00" gbyte="3.75961e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24041e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c815ac55c815c7154b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09925e+02" utime="8.59414e+01" stime="1.49644e+01" mtime="7.24041e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24041e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1942e+09" > 8.0414e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2067e+09" > 3.9770e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.1617e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9781e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.2002e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8147e-06 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5794e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6658e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0599e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3104e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2942e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="207" >
<hent key="02400100000000000000080000000000" call="MPI_Isend" bytes="2048" orank="0" region="0" commid="0" count="5" tid="0" op="" dtype="" >9.0599e-06 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000080000000005" call="MPI_Isend" bytes="2048" orank="5" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="02400100000000000000080000000008" call="MPI_Isend" bytes="2048" orank="8" region="0" commid="0" count="5" tid="0" op="" dtype="" >8.5831e-06 9.5367e-07 1.9073e-06</hent>
<hent key="0240010000000000000008000000001C" call="MPI_Isend" bytes="2048" orank="28" region="0" commid="0" count="163" tid="0" op="" dtype="" >1.2789e-03 5.0068e-06 2.5034e-05</hent>
<hent key="024001000000000000000800000002A4" call="MPI_Isend" bytes="2048" orank="676" region="0" commid="0" count="166" tid="0" op="" dtype="" >1.0557e-03 3.8147e-06 2.0981e-05</hent>
<hent key="038001000000000000000E0000000005" call="MPI_Irecv" bytes="3584" orank="5" region="0" commid="0" count="78" tid="0" op="" dtype="" >1.1206e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000007" call="MPI_Irecv" bytes="3584" orank="7" region="0" commid="0" count="153" tid="0" op="" dtype="" >7.6056e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E000000001C" call="MPI_Irecv" bytes="3584" orank="28" region="0" commid="0" count="135" tid="0" op="" dtype="" >6.3419e-05 0.0000e+00 6.9141e-06</hent>
<hent key="038001000000000000000E00000002A4" call="MPI_Irecv" bytes="3584" orank="676" region="0" commid="0" count="127" tid="0" op="" dtype="" >6.4611e-05 0.0000e+00 6.9141e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000E0000000005" call="MPI_Isend" bytes="3584" orank="5" region="0" commid="0" count="15" tid="0" op="" dtype="" >3.6955e-05 9.5367e-07 4.0531e-06</hent>
<hent key="024001000000000000000E0000000007" call="MPI_Isend" bytes="3584" orank="7" region="0" commid="0" count="123" tid="0" op="" dtype="" >1.9121e-04 9.5367e-07 3.0994e-06</hent>
<hent key="024001000000000000000E000000001C" call="MPI_Isend" bytes="3584" orank="28" region="0" commid="0" count="139" tid="0" op="" dtype="" >1.2052e-03 5.9605e-06 2.2888e-05</hent>
<hent key="024001000000000000000E00000002A4" call="MPI_Isend" bytes="3584" orank="676" region="0" commid="0" count="131" tid="0" op="" dtype="" >9.3746e-04 4.0531e-06 4.4823e-05</hent>
<hent key="03800100000000000000028000000000" call="MPI_Irecv" bytes="640" orank="0" region="0" commid="0" count="413" tid="0" op="" dtype="" >1.0180e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000005" call="MPI_Irecv" bytes="640" orank="5" region="0" commid="0" count="381" tid="0" op="" dtype="" >1.0347e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000007" call="MPI_Irecv" bytes="640" orank="7" region="0" commid="0" count="393" tid="0" op="" dtype="" >1.8573e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000028000000008" call="MPI_Irecv" bytes="640" orank="8" region="0" commid="0" count="417" tid="0" op="" dtype="" >9.6560e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000001C" call="MPI_Irecv" bytes="640" orank="28" region="0" commid="0" count="25" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000014000000001C" call="MPI_Irecv" bytes="5120" orank="28" region="0" commid="0" count="759" tid="0" op="" dtype="" >2.0742e-04 0.0000e+00 1.3113e-05</hent>
<hent key="038001000000000000000280000002A4" call="MPI_Irecv" bytes="640" orank="676" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.5736e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000001400000002A4" call="MPI_Irecv" bytes="5120" orank="676" region="0" commid="0" count="772" tid="0" op="" dtype="" >6.1750e-04 0.0000e+00 4.9114e-05</hent>
<hent key="03800100000000000000A0000000001C" call="MPI_Irecv" bytes="40960" orank="28" region="0" commid="0" count="2550" tid="0" op="" dtype="" >1.1942e-03 0.0000e+00 2.2173e-05</hent>
<hent key="02400100000000000000028000000000" call="MPI_Isend" bytes="640" orank="0" region="0" commid="0" count="398" tid="0" op="" dtype="" >4.8709e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000028000000005" call="MPI_Isend" bytes="640" orank="5" region="0" commid="0" count="369" tid="0" op="" dtype="" >1.2569e-03 1.9073e-06 1.3828e-05</hent>
<hent key="02400100000000000000028000000007" call="MPI_Isend" bytes="640" orank="7" region="0" commid="0" count="381" tid="0" op="" dtype="" >5.8413e-04 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000028000000008" call="MPI_Isend" bytes="640" orank="8" region="0" commid="0" count="410" tid="0" op="" dtype="" >5.2786e-04 9.5367e-07 5.9605e-06</hent>
<hent key="0240010000000000000002800000001C" call="MPI_Isend" bytes="640" orank="28" region="0" commid="0" count="21" tid="0" op="" dtype="" >1.6260e-04 5.9605e-06 2.1935e-05</hent>
<hent key="03800100000000000000A000000002A4" call="MPI_Irecv" bytes="40960" orank="676" region="0" commid="0" count="2599" tid="0" op="" dtype="" >1.0514e-03 0.0000e+00 5.1022e-05</hent>
<hent key="0240010000000000000014000000001C" call="MPI_Isend" bytes="5120" orank="28" region="0" commid="0" count="570" tid="0" op="" dtype="" >5.1918e-03 1.9073e-06 8.8930e-05</hent>
<hent key="024001000000000000000280000002A4" call="MPI_Isend" bytes="640" orank="676" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.5211e-04 3.8147e-06 1.0967e-05</hent>
<hent key="0380010000000000000080000000001C" call="MPI_Irecv" bytes="32768" orank="28" region="0" commid="0" count="10150" tid="0" op="" dtype="" >5.3575e-03 0.0000e+00 6.1035e-05</hent>
<hent key="024001000000000000001400000002A4" call="MPI_Isend" bytes="5120" orank="676" region="0" commid="0" count="556" tid="0" op="" dtype="" >3.1998e-03 9.5367e-07 6.5088e-05</hent>
<hent key="038001000000000000008000000002A4" call="MPI_Irecv" bytes="32768" orank="676" region="0" commid="0" count="10101" tid="0" op="" dtype="" >4.1130e-03 0.0000e+00 5.9128e-05</hent>
<hent key="03800100000000000000014000000000" call="MPI_Irecv" bytes="320" orank="0" region="0" commid="0" count="448" tid="0" op="" dtype="" >9.2506e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000005" call="MPI_Irecv" bytes="320" orank="5" region="0" commid="0" count="544" tid="0" op="" dtype="" >1.2088e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000007" call="MPI_Irecv" bytes="320" orank="7" region="0" commid="0" count="575" tid="0" op="" dtype="" >2.5988e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000008" call="MPI_Irecv" bytes="320" orank="8" region="0" commid="0" count="416" tid="0" op="" dtype="" >1.0133e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000001C" call="MPI_Irecv" bytes="320" orank="28" region="0" commid="0" count="21" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A0000000001C" call="MPI_Isend" bytes="40960" orank="28" region="0" commid="0" count="1853" tid="0" op="" dtype="" >4.3520e-02 9.7752e-06 1.7881e-04</hent>
<hent key="038001000000000000000140000002A4" call="MPI_Irecv" bytes="320" orank="676" region="0" commid="0" count="23" tid="0" op="" dtype="" >9.7752e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A000000002A4" call="MPI_Isend" bytes="40960" orank="676" region="0" commid="0" count="1815" tid="0" op="" dtype="" >3.6328e-02 5.9605e-06 1.1706e-04</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >4.7684e-06 9.5367e-07 1.9073e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >7.1617e+00 0.0000e+00 1.1072e-01</hent>
<hent key="0240010000000000000080000000001C" call="MPI_Isend" bytes="32768" orank="28" region="0" commid="0" count="10847" tid="0" op="" dtype="" >2.3911e-01 9.0599e-06 1.3900e-04</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.1935e-05 9.5367e-07 1.9073e-05</hent>
<hent key="024001000000000000008000000002A4" call="MPI_Isend" bytes="32768" orank="676" region="0" commid="0" count="10885" tid="0" op="" dtype="" >2.1873e-01 5.9605e-06 1.4615e-04</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000000" call="MPI_Isend" bytes="320" orank="0" region="0" commid="0" count="477" tid="0" op="" dtype="" >5.0497e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000014000000005" call="MPI_Isend" bytes="320" orank="5" region="0" commid="0" count="573" tid="0" op="" dtype="" >1.7991e-03 1.9073e-06 2.0981e-05</hent>
<hent key="02400100000000000000014000000007" call="MPI_Isend" bytes="320" orank="7" region="0" commid="0" count="539" tid="0" op="" dtype="" >6.5351e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000008" call="MPI_Isend" bytes="320" orank="8" region="0" commid="0" count="454" tid="0" op="" dtype="" >5.1522e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000001400000001C" call="MPI_Isend" bytes="320" orank="28" region="0" commid="0" count="26" tid="0" op="" dtype="" >1.4305e-04 3.8147e-06 9.0599e-06</hent>
<hent key="024001000000000000000140000002A4" call="MPI_Isend" bytes="320" orank="676" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.6904e-04 3.8147e-06 1.3828e-05</hent>
<hent key="03800100000000000000000000000000" call="MPI_Irecv" bytes="0" orank="0" region="0" commid="0" count="328" tid="0" op="" dtype="" >8.0585e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000005" call="MPI_Irecv" bytes="0" orank="5" region="0" commid="0" count="516" tid="0" op="" dtype="" >1.2183e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000007" call="MPI_Irecv" bytes="0" orank="7" region="0" commid="0" count="531" tid="0" op="" dtype="" >1.9097e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000008" call="MPI_Irecv" bytes="0" orank="8" region="0" commid="0" count="341" tid="0" op="" dtype="" >6.6519e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000001C" call="MPI_Irecv" bytes="0" orank="28" region="0" commid="0" count="85" tid="0" op="" dtype="" >2.8610e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002A4" call="MPI_Irecv" bytes="0" orank="676" region="0" commid="0" count="83" tid="0" op="" dtype="" >1.8835e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.3484e-04 0.0000e+00 7.9870e-05</hent>
<hent key="03800100000000000000030000000000" call="MPI_Irecv" bytes="768" orank="0" region="0" commid="0" count="680" tid="0" op="" dtype="" >1.5330e-04 0.0000e+00 2.1935e-05</hent>
<hent key="03800100000000000000030000000008" call="MPI_Irecv" bytes="768" orank="8" region="0" commid="0" count="574" tid="0" op="" dtype="" >1.1659e-04 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000000000000000" call="MPI_Isend" bytes="0" orank="0" region="0" commid="0" count="343" tid="0" op="" dtype="" >2.3961e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000000000005" call="MPI_Isend" bytes="0" orank="5" region="0" commid="0" count="523" tid="0" op="" dtype="" >1.1559e-03 9.5367e-07 6.6996e-05</hent>
<hent key="02400100000000000000000000000007" call="MPI_Isend" bytes="0" orank="7" region="0" commid="0" count="561" tid="0" op="" dtype="" >4.2701e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000000000000008" call="MPI_Isend" bytes="0" orank="8" region="0" commid="0" count="317" tid="0" op="" dtype="" >2.2554e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000000000001C" call="MPI_Isend" bytes="0" orank="28" region="0" commid="0" count="87" tid="0" op="" dtype="" >4.6539e-04 1.9073e-06 1.2159e-05</hent>
<hent key="024001000000000000000000000002A4" call="MPI_Isend" bytes="0" orank="676" region="0" commid="0" count="78" tid="0" op="" dtype="" >3.9506e-04 2.8610e-06 1.5974e-05</hent>
<hent key="03800100000000000000060000000000" call="MPI_Irecv" bytes="1536" orank="0" region="0" commid="0" count="46" tid="0" op="" dtype="" >1.0490e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000005" call="MPI_Irecv" bytes="1536" orank="5" region="0" commid="0" count="26" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000007" call="MPI_Irecv" bytes="1536" orank="7" region="0" commid="0" count="12" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000008" call="MPI_Irecv" bytes="1536" orank="8" region="0" commid="0" count="56" tid="0" op="" dtype="" >1.9312e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000001C" call="MPI_Irecv" bytes="1536" orank="28" region="0" commid="0" count="78" tid="0" op="" dtype="" >2.6941e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002A4" call="MPI_Irecv" bytes="1536" orank="676" region="0" commid="0" count="84" tid="0" op="" dtype="" >5.3167e-05 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000030000000000" call="MPI_Isend" bytes="768" orank="0" region="0" commid="0" count="532" tid="0" op="" dtype="" >3.9768e-04 0.0000e+00 8.7976e-05</hent>
<hent key="02400100000000000000030000000008" call="MPI_Isend" bytes="768" orank="8" region="0" commid="0" count="716" tid="0" op="" dtype="" >4.1032e-04 0.0000e+00 1.8120e-05</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000005" call="MPI_Irecv" bytes="448" orank="5" region="0" commid="0" count="20" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000007" call="MPI_Irecv" bytes="448" orank="7" region="0" commid="0" count="46" tid="0" op="" dtype="" >2.3842e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000000" call="MPI_Isend" bytes="1536" orank="0" region="0" commid="0" count="49" tid="0" op="" dtype="" >8.1062e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000005" call="MPI_Isend" bytes="1536" orank="5" region="0" commid="0" count="11" tid="0" op="" dtype="" >4.4823e-05 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000060000000007" call="MPI_Isend" bytes="1536" orank="7" region="0" commid="0" count="16" tid="0" op="" dtype="" >3.7670e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000060000000008" call="MPI_Isend" bytes="1536" orank="8" region="0" commid="0" count="47" tid="0" op="" dtype="" >7.9393e-05 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000006000000001C" call="MPI_Isend" bytes="1536" orank="28" region="0" commid="0" count="85" tid="0" op="" dtype="" >6.5684e-04 5.0068e-06 2.3127e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002A4" call="MPI_Isend" bytes="1536" orank="676" region="0" commid="0" count="88" tid="0" op="" dtype="" >5.7149e-04 3.8147e-06 2.1935e-05</hent>
<hent key="038001000000000000000C000000001C" call="MPI_Irecv" bytes="3072" orank="28" region="0" commid="0" count="349" tid="0" op="" dtype="" >1.4019e-04 0.0000e+00 7.1526e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002A4" call="MPI_Irecv" bytes="3072" orank="676" region="0" commid="0" count="322" tid="0" op="" dtype="" >1.5879e-04 0.0000e+00 1.2875e-05</hent>
<hent key="0240010000000000000001C000000005" call="MPI_Isend" bytes="448" orank="5" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000001C000000007" call="MPI_Isend" bytes="448" orank="7" region="0" commid="0" count="34" tid="0" op="" dtype="" >2.0981e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C000000001C" call="MPI_Isend" bytes="3072" orank="28" region="0" commid="0" count="349" tid="0" op="" dtype="" >2.9809e-03 5.0068e-06 2.5988e-05</hent>
<hent key="024001000000000000000C00000002A4" call="MPI_Isend" bytes="3072" orank="676" region="0" commid="0" count="351" tid="0" op="" dtype="" >2.4259e-03 3.8147e-06 3.5048e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.5272e-04 7.9870e-05 9.2983e-05</hent>
<hent key="03800100000000000000038000000000" call="MPI_Irecv" bytes="896" orank="0" region="0" commid="0" count="3023" tid="0" op="" dtype="" >5.2857e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000005" call="MPI_Irecv" bytes="896" orank="5" region="0" commid="0" count="182" tid="0" op="" dtype="" >5.2214e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000007" call="MPI_Irecv" bytes="896" orank="7" region="0" commid="0" count="141" tid="0" op="" dtype="" >5.5552e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000008" call="MPI_Irecv" bytes="896" orank="8" region="0" commid="0" count="3124" tid="0" op="" dtype="" >9.2030e-04 0.0000e+00 6.6996e-05</hent>
<hent key="0380010000000000000003800000001C" call="MPI_Irecv" bytes="896" orank="28" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002A4" call="MPI_Irecv" bytes="896" orank="676" region="0" commid="0" count="34" tid="0" op="" dtype="" >1.8358e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.2101e-04 2.2101e-04 2.2101e-04</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="03800100000000000000180000000000" call="MPI_Irecv" bytes="6144" orank="0" region="0" commid="0" count="2582" tid="0" op="" dtype="" >5.0426e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000180000000008" call="MPI_Irecv" bytes="6144" orank="8" region="0" commid="0" count="2183" tid="0" op="" dtype="" >4.1318e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000018000000001C" call="MPI_Irecv" bytes="6144" orank="28" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000001800000002A4" call="MPI_Irecv" bytes="6144" orank="676" region="0" commid="0" count="19" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000038000000000" call="MPI_Isend" bytes="896" orank="0" region="0" commid="0" count="3140" tid="0" op="" dtype="" >2.4245e-03 0.0000e+00 5.8889e-05</hent>
<hent key="02400100000000000000038000000005" call="MPI_Isend" bytes="896" orank="5" region="0" commid="0" count="171" tid="0" op="" dtype="" >6.4778e-04 2.8610e-06 1.0014e-05</hent>
<hent key="02400100000000000000038000000007" call="MPI_Isend" bytes="896" orank="7" region="0" commid="0" count="145" tid="0" op="" dtype="" >2.7156e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000038000000008" call="MPI_Isend" bytes="896" orank="8" region="0" commid="0" count="2980" tid="0" op="" dtype="" >1.8511e-03 0.0000e+00 4.1962e-05</hent>
<hent key="0240010000000000000003800000001C" call="MPI_Isend" bytes="896" orank="28" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.5998e-04 5.0068e-06 1.7881e-05</hent>
<hent key="024001000000000000000380000002A4" call="MPI_Isend" bytes="896" orank="676" region="0" commid="0" count="37" tid="0" op="" dtype="" >2.3723e-04 3.8147e-06 1.4067e-05</hent>
<hent key="02400100000000000000180000000000" call="MPI_Isend" bytes="6144" orank="0" region="0" commid="0" count="2019" tid="0" op="" dtype="" >3.8517e-03 9.5367e-07 5.3883e-05</hent>
<hent key="02400100000000000000180000000008" call="MPI_Isend" bytes="6144" orank="8" region="0" commid="0" count="2709" tid="0" op="" dtype="" >5.2569e-03 9.5367e-07 1.0967e-05</hent>
<hent key="0240010000000000000018000000001C" call="MPI_Isend" bytes="6144" orank="28" region="0" commid="0" count="11" tid="0" op="" dtype="" >1.2183e-04 6.9141e-06 2.7895e-05</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.5034e-05 2.5034e-05 2.5034e-05</hent>
<hent key="024001000000000000001800000002A4" call="MPI_Isend" bytes="6144" orank="676" region="0" commid="0" count="10" tid="0" op="" dtype="" >8.6784e-05 5.0068e-06 2.0027e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.4012e+00 2.0027e-05 1.2663e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >9.1410e-04 9.1410e-04 9.1410e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0599e-02 1.0599e-02 1.0599e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >4.1358e-03 4.1358e-03 4.1358e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.8923e-01 3.2871e-03 1.7996e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.2401e-04 3.2401e-04 3.2401e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 1.1921e-06 1.9073e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5794e+00 4.2796e-04 2.4812e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >7.2002e-04 0.0000e+00 6.3920e-04</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000070000000000" call="MPI_Irecv" bytes="1792" orank="0" region="0" commid="0" count="18" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000005" call="MPI_Irecv" bytes="1792" orank="5" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000070000000007" call="MPI_Irecv" bytes="1792" orank="7" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="03800100000000000000070000000008" call="MPI_Irecv" bytes="1792" orank="8" region="0" commid="0" count="10" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000007000000001C" call="MPI_Irecv" bytes="1792" orank="28" region="0" commid="0" count="117" tid="0" op="" dtype="" >4.0293e-05 0.0000e+00 2.8610e-06</hent>
<hent key="038001000000000000000700000002A4" call="MPI_Irecv" bytes="1792" orank="676" region="0" commid="0" count="129" tid="0" op="" dtype="" >6.3181e-05 0.0000e+00 8.8215e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >8.4400e-05 0.0000e+00 6.4850e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9780e+01 0.0000e+00 3.8413e+01</hent>
<hent key="038001000000000000000A000000001C" call="MPI_Irecv" bytes="2560" orank="28" region="0" commid="0" count="357" tid="0" op="" dtype="" >3.7861e-04 0.0000e+00 2.2101e-04</hent>
<hent key="038001000000000000000A00000002A4" call="MPI_Irecv" bytes="2560" orank="676" region="0" commid="0" count="355" tid="0" op="" dtype="" >1.4186e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000070000000000" call="MPI_Isend" bytes="1792" orank="0" region="0" commid="0" count="22" tid="0" op="" dtype="" >4.3154e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000005" call="MPI_Isend" bytes="1792" orank="5" region="0" commid="0" count="2" tid="0" op="" dtype="" >8.1062e-06 4.0531e-06 4.0531e-06</hent>
<hent key="02400100000000000000070000000007" call="MPI_Isend" bytes="1792" orank="7" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.4305e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000008" call="MPI_Isend" bytes="1792" orank="8" region="0" commid="0" count="25" tid="0" op="" dtype="" >4.8399e-05 9.5367e-07 2.8610e-06</hent>
<hent key="0240010000000000000007000000001C" call="MPI_Isend" bytes="1792" orank="28" region="0" commid="0" count="101" tid="0" op="" dtype="" >7.9393e-04 5.0068e-06 2.4080e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3104e-02 8.3104e-02 8.3104e-02</hent>
<hent key="024001000000000000000700000002A4" call="MPI_Isend" bytes="1792" orank="676" region="0" commid="0" count="118" tid="0" op="" dtype="" >7.3195e-04 3.8147e-06 2.1935e-05</hent>
<hent key="024001000000000000000A0000000000" call="MPI_Isend" bytes="2560" orank="0" region="0" commid="0" count="3" tid="0" op="" dtype="" >5.7220e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000A000000001C" call="MPI_Isend" bytes="2560" orank="28" region="0" commid="0" count="346" tid="0" op="" dtype="" >2.8849e-03 5.0068e-06 2.3127e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.1587e-04 1.4067e-05 5.6982e-05</hent>
<hent key="024001000000000000000A00000002A4" call="MPI_Isend" bytes="2560" orank="676" region="0" commid="0" count="345" tid="0" op="" dtype="" >2.3155e-03 4.0531e-06 2.0981e-05</hent>
<hent key="03800100000000000000100000000005" call="MPI_Irecv" bytes="4096" orank="5" region="0" commid="0" count="12622" tid="0" op="" dtype="" >1.8992e-03 0.0000e+00 2.0981e-05</hent>
<hent key="03800100000000000000100000000007" call="MPI_Irecv" bytes="4096" orank="7" region="0" commid="0" count="12547" tid="0" op="" dtype="" >6.2115e-03 0.0000e+00 6.1989e-06</hent>
<hent key="0380010000000000000010000000001C" call="MPI_Irecv" bytes="4096" orank="28" region="0" commid="0" count="2933" tid="0" op="" dtype="" >8.9955e-04 0.0000e+00 3.1948e-05</hent>
<hent key="038001000000000000001000000002A4" call="MPI_Irecv" bytes="4096" orank="676" region="0" commid="0" count="2913" tid="0" op="" dtype="" >1.9603e-03 0.0000e+00 9.0837e-05</hent>
<hent key="02400100000000000000100000000005" call="MPI_Isend" bytes="4096" orank="5" region="0" commid="0" count="12685" tid="0" op="" dtype="" >3.2254e-02 0.0000e+00 2.5988e-05</hent>
<hent key="02400100000000000000100000000007" call="MPI_Isend" bytes="4096" orank="7" region="0" commid="0" count="12577" tid="0" op="" dtype="" >2.0171e-02 0.0000e+00 4.1008e-05</hent>
<hent key="0240010000000000000010000000001C" call="MPI_Isend" bytes="4096" orank="28" region="0" commid="0" count="3128" tid="0" op="" dtype="" >2.7219e-02 1.9073e-06 1.1802e-04</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >3.5715e-04 1.3494e-04 2.2221e-04</hent>
<hent key="024001000000000000001000000002A4" call="MPI_Isend" bytes="4096" orank="676" region="0" commid="0" count="3117" tid="0" op="" dtype="" >1.6523e-02 9.5367e-07 6.2943e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >8.0919e-04 1.1587e-04 2.8706e-04</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >7.6294e-06 3.8147e-06 3.8147e-06</hent>
<hent key="03800100000000000000000400000000" call="MPI_Irecv" bytes="4" orank="0" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.1379e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000005" call="MPI_Irecv" bytes="4" orank="5" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.7101e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000400000007" call="MPI_Irecv" bytes="4" orank="7" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.0900e-03 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000008" call="MPI_Irecv" bytes="4" orank="8" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.1202e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000000040000001C" call="MPI_Irecv" bytes="4" orank="28" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.1072e-03 0.0000e+00 4.6968e-05</hent>
<hent key="038001000000000000001C0000000000" call="MPI_Irecv" bytes="7168" orank="0" region="0" commid="0" count="10118" tid="0" op="" dtype="" >1.9412e-03 0.0000e+00 1.9073e-05</hent>
<hent key="038001000000000000001C0000000008" call="MPI_Irecv" bytes="7168" orank="8" region="0" commid="0" count="10517" tid="0" op="" dtype="" >2.2058e-03 0.0000e+00 4.4823e-05</hent>
<hent key="038001000000000000001C000000001C" call="MPI_Irecv" bytes="7168" orank="28" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000004000002A4" call="MPI_Irecv" bytes="4" orank="676" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.1380e-03 0.0000e+00 6.6042e-05</hent>
<hent key="038001000000000000001C00000002A4" call="MPI_Irecv" bytes="7168" orank="676" region="0" commid="0" count="3" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000400000000" call="MPI_Isend" bytes="4" orank="0" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.5240e-03 0.0000e+00 1.8835e-05</hent>
<hent key="02400100000000000000000400000005" call="MPI_Isend" bytes="4" orank="5" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.1841e-03 0.0000e+00 2.5034e-05</hent>
<hent key="02400100000000000000000400000007" call="MPI_Isend" bytes="4" orank="7" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.4174e-03 0.0000e+00 5.0068e-05</hent>
<hent key="02400100000000000000000400000008" call="MPI_Isend" bytes="4" orank="8" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.4462e-03 0.0000e+00 6.5088e-05</hent>
<hent key="0240010000000000000000040000001C" call="MPI_Isend" bytes="4" orank="28" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.8537e-02 4.7684e-06 3.7909e-04</hent>
<hent key="024001000000000000001C0000000000" call="MPI_Isend" bytes="7168" orank="0" region="0" commid="0" count="10681" tid="0" op="" dtype="" >2.0895e-02 9.5367e-07 6.8903e-05</hent>
<hent key="024001000000000000001C0000000008" call="MPI_Isend" bytes="7168" orank="8" region="0" commid="0" count="9991" tid="0" op="" dtype="" >2.0097e-02 9.5367e-07 4.2915e-05</hent>
<hent key="024001000000000000001C000000001C" call="MPI_Isend" bytes="7168" orank="28" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.8120e-05 9.0599e-06 9.0599e-06</hent>
<hent key="024001000000000000000004000002A4" call="MPI_Isend" bytes="4" orank="676" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.4790e-02 3.8147e-06 9.9897e-05</hent>
<hent key="024001000000000000001C00000002A4" call="MPI_Isend" bytes="7168" orank="676" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.4067e-05 6.9141e-06 7.1526e-06</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.2302e-04 1.2302e-04 1.2302e-04</hent>
<hent key="03800100000000000000020000000005" call="MPI_Irecv" bytes="512" orank="5" region="0" commid="0" count="3380" tid="0" op="" dtype="" >7.0405e-04 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000020000000007" call="MPI_Irecv" bytes="512" orank="7" region="0" commid="0" count="3354" tid="0" op="" dtype="" >4.4060e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000050000000000" call="MPI_Irecv" bytes="1280" orank="0" region="0" commid="0" count="143" tid="0" op="" dtype="" >2.2888e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000005" call="MPI_Irecv" bytes="1280" orank="5" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.0729e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000007" call="MPI_Irecv" bytes="1280" orank="7" region="0" commid="0" count="47" tid="0" op="" dtype="" >2.2173e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000008" call="MPI_Irecv" bytes="1280" orank="8" region="0" commid="0" count="161" tid="0" op="" dtype="" >3.0279e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000005000000001C" call="MPI_Irecv" bytes="1280" orank="28" region="0" commid="0" count="56" tid="0" op="" dtype="" >1.6928e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4346e+01 8.8215e-06 1.8434e-01</hent>
<hent key="038001000000000000000500000002A4" call="MPI_Irecv" bytes="1280" orank="676" region="0" commid="0" count="51" tid="0" op="" dtype="" >1.7643e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000020000000005" call="MPI_Isend" bytes="512" orank="5" region="0" commid="0" count="3396" tid="0" op="" dtype="" >2.6450e-03 0.0000e+00 3.1948e-05</hent>
<hent key="02400100000000000000020000000007" call="MPI_Isend" bytes="512" orank="7" region="0" commid="0" count="3366" tid="0" op="" dtype="" >1.8151e-03 0.0000e+00 2.3127e-05</hent>
<hent key="03800100000000000000080000000000" call="MPI_Irecv" bytes="2048" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000080000000008" call="MPI_Irecv" bytes="2048" orank="8" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000008000000001C" call="MPI_Irecv" bytes="2048" orank="28" region="0" commid="0" count="144" tid="0" op="" dtype="" >5.4598e-05 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000800000002A4" call="MPI_Irecv" bytes="2048" orank="676" region="0" commid="0" count="155" tid="0" op="" dtype="" >8.6069e-05 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000050000000000" call="MPI_Isend" bytes="1280" orank="0" region="0" commid="0" count="131" tid="0" op="" dtype="" >2.0218e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000050000000005" call="MPI_Isend" bytes="1280" orank="5" region="0" commid="0" count="50" tid="0" op="" dtype="" >2.0123e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000050000000007" call="MPI_Isend" bytes="1280" orank="7" region="0" commid="0" count="52" tid="0" op="" dtype="" >1.1301e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000050000000008" call="MPI_Isend" bytes="1280" orank="8" region="0" commid="0" count="146" tid="0" op="" dtype="" >2.6393e-04 9.5367e-07 6.9141e-06</hent>
<hent key="0240010000000000000005000000001C" call="MPI_Isend" bytes="1280" orank="28" region="0" commid="0" count="50" tid="0" op="" dtype="" >3.7217e-04 5.0068e-06 1.3828e-05</hent>
<hent key="024001000000000000000500000002A4" call="MPI_Isend" bytes="1280" orank="676" region="0" commid="0" count="47" tid="0" op="" dtype="" >2.9540e-04 3.8147e-06 1.4067e-05</hent>
</hash>
<internal rank="4" log_i="1724765674.529077" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="5" mpi_size="696" stamp_init="1724765564.467108" stamp_final="1724765674.534995" username="apac4" allocationname="unknown" flags="0" pid="1612122" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10068e+02" utime="8.94128e+01" stime="1.41417e+01" mtime="7.28324e+01" gflop="0.00000e+00" gbyte="3.77090e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28324e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c314c414c5141a55c514c514b1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09932e+02" utime="8.93783e+01" stime="1.41337e+01" mtime="7.28324e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28324e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2099e+09" > 6.3984e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2090e+09" > 3.0394e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2837e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9781e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6001e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4121e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5794e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9040e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0572e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3061e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4422e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="205" >
<hent key="02400100000000000000080000000001" call="MPI_Isend" bytes="2048" orank="1" region="0" commid="0" count="10" tid="0" op="" dtype="" >1.5974e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000080000000009" call="MPI_Isend" bytes="2048" orank="9" region="0" commid="0" count="3" tid="0" op="" dtype="" >4.7684e-06 9.5367e-07 1.9073e-06</hent>
<hent key="0240010000000000000008000000001D" call="MPI_Isend" bytes="2048" orank="29" region="0" commid="0" count="164" tid="0" op="" dtype="" >8.9550e-04 4.7684e-06 7.1526e-06</hent>
<hent key="024001000000000000000800000002A5" call="MPI_Isend" bytes="2048" orank="677" region="0" commid="0" count="153" tid="0" op="" dtype="" >6.9141e-04 3.8147e-06 5.9605e-06</hent>
<hent key="038001000000000000000E0000000004" call="MPI_Irecv" bytes="3584" orank="4" region="0" commid="0" count="15" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000E0000000006" call="MPI_Irecv" bytes="3584" orank="6" region="0" commid="0" count="218" tid="0" op="" dtype="" >3.9816e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E000000001D" call="MPI_Irecv" bytes="3584" orank="29" region="0" commid="0" count="171" tid="0" op="" dtype="" >5.7936e-05 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000E00000002A5" call="MPI_Irecv" bytes="3584" orank="677" region="0" commid="0" count="119" tid="0" op="" dtype="" >5.4598e-05 0.0000e+00 2.1458e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000E0000000004" call="MPI_Isend" bytes="3584" orank="4" region="0" commid="0" count="78" tid="0" op="" dtype="" >1.2565e-04 9.5367e-07 3.0994e-06</hent>
<hent key="024001000000000000000E0000000006" call="MPI_Isend" bytes="3584" orank="6" region="0" commid="0" count="102" tid="0" op="" dtype="" >2.4509e-04 9.5367e-07 1.0967e-05</hent>
<hent key="024001000000000000000E000000001D" call="MPI_Isend" bytes="3584" orank="29" region="0" commid="0" count="138" tid="0" op="" dtype="" >8.0895e-04 4.7684e-06 7.1526e-06</hent>
<hent key="024001000000000000000E00000002A5" call="MPI_Isend" bytes="3584" orank="677" region="0" commid="0" count="151" tid="0" op="" dtype="" >7.8678e-04 3.8147e-06 1.9073e-05</hent>
<hent key="03800100000000000000028000000001" call="MPI_Irecv" bytes="640" orank="1" region="0" commid="0" count="401" tid="0" op="" dtype="" >1.2589e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000004" call="MPI_Irecv" bytes="640" orank="4" region="0" commid="0" count="369" tid="0" op="" dtype="" >2.5558e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000028000000006" call="MPI_Irecv" bytes="640" orank="6" region="0" commid="0" count="386" tid="0" op="" dtype="" >8.0824e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000009" call="MPI_Irecv" bytes="640" orank="9" region="0" commid="0" count="455" tid="0" op="" dtype="" >1.7285e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0380010000000000000002800000001D" call="MPI_Irecv" bytes="640" orank="29" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.1206e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000014000000001D" call="MPI_Irecv" bytes="5120" orank="29" region="0" commid="0" count="828" tid="0" op="" dtype="" >1.3494e-04 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000280000002A5" call="MPI_Irecv" bytes="640" orank="677" region="0" commid="0" count="26" tid="0" op="" dtype="" >1.3590e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="038001000000000000001400000002A5" call="MPI_Irecv" bytes="5120" orank="677" region="0" commid="0" count="738" tid="0" op="" dtype="" >1.8215e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000A0000000001D" call="MPI_Irecv" bytes="40960" orank="29" region="0" commid="0" count="2930" tid="0" op="" dtype="" >8.1635e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000028000000001" call="MPI_Isend" bytes="640" orank="1" region="0" commid="0" count="449" tid="0" op="" dtype="" >5.0020e-04 0.0000e+00 3.8147e-06</hent>
<hent key="02400100000000000000028000000004" call="MPI_Isend" bytes="640" orank="4" region="0" commid="0" count="381" tid="0" op="" dtype="" >5.7578e-04 9.5367e-07 7.8678e-06</hent>
<hent key="02400100000000000000028000000006" call="MPI_Isend" bytes="640" orank="6" region="0" commid="0" count="336" tid="0" op="" dtype="" >1.4088e-03 2.8610e-06 6.2943e-05</hent>
<hent key="02400100000000000000028000000009" call="MPI_Isend" bytes="640" orank="9" region="0" commid="0" count="372" tid="0" op="" dtype="" >4.3607e-04 9.5367e-07 7.1526e-06</hent>
<hent key="0240010000000000000002800000001D" call="MPI_Isend" bytes="640" orank="29" region="0" commid="0" count="20" tid="0" op="" dtype="" >9.7275e-05 3.8147e-06 6.1989e-06</hent>
<hent key="03800100000000000000A000000002A5" call="MPI_Irecv" bytes="40960" orank="677" region="0" commid="0" count="2548" tid="0" op="" dtype="" >6.4802e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000014000000001D" call="MPI_Isend" bytes="5120" orank="29" region="0" commid="0" count="844" tid="0" op="" dtype="" >3.3586e-03 1.9073e-06 1.0014e-05</hent>
<hent key="024001000000000000000280000002A5" call="MPI_Isend" bytes="640" orank="677" region="0" commid="0" count="24" tid="0" op="" dtype="" >9.8467e-05 2.8610e-06 5.0068e-06</hent>
<hent key="0380010000000000000080000000001D" call="MPI_Irecv" bytes="32768" orank="29" region="0" commid="0" count="9770" tid="0" op="" dtype="" >2.6445e-03 0.0000e+00 1.2159e-05</hent>
<hent key="024001000000000000001400000002A5" call="MPI_Isend" bytes="5120" orank="677" region="0" commid="0" count="728" tid="0" op="" dtype="" >2.6019e-03 9.5367e-07 8.1062e-06</hent>
<hent key="038001000000000000008000000002A5" call="MPI_Irecv" bytes="32768" orank="677" region="0" commid="0" count="10152" tid="0" op="" dtype="" >2.4798e-03 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000014000000001" call="MPI_Irecv" bytes="320" orank="1" region="0" commid="0" count="449" tid="0" op="" dtype="" >1.3971e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000004" call="MPI_Irecv" bytes="320" orank="4" region="0" commid="0" count="573" tid="0" op="" dtype="" >3.9196e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000014000000006" call="MPI_Irecv" bytes="320" orank="6" region="0" commid="0" count="552" tid="0" op="" dtype="" >1.3828e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000009" call="MPI_Irecv" bytes="320" orank="9" region="0" commid="0" count="431" tid="0" op="" dtype="" >1.6451e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000001D" call="MPI_Irecv" bytes="320" orank="29" region="0" commid="0" count="25" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A0000000001D" call="MPI_Isend" bytes="40960" orank="29" region="0" commid="0" count="2862" tid="0" op="" dtype="" >6.0116e-02 1.0967e-05 4.1962e-05</hent>
<hent key="038001000000000000000140000002A5" call="MPI_Irecv" bytes="320" orank="677" region="0" commid="0" count="25" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A000000002A5" call="MPI_Isend" bytes="40960" orank="677" region="0" commid="0" count="2576" tid="0" op="" dtype="" >3.3912e-02 8.8215e-06 5.6028e-05</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >6.2837e+00 0.0000e+00 1.1142e-01</hent>
<hent key="0240010000000000000080000000001D" call="MPI_Isend" bytes="32768" orank="29" region="0" commid="0" count="9838" tid="0" op="" dtype="" >1.9772e-01 9.7752e-06 5.1022e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.8120e-05 0.0000e+00 1.8120e-05</hent>
<hent key="024001000000000000008000000002A5" call="MPI_Isend" bytes="32768" orank="677" region="0" commid="0" count="10124" tid="0" op="" dtype="" >1.3251e-01 8.8215e-06 8.9169e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000014000000001" call="MPI_Isend" bytes="320" orank="1" region="0" commid="0" count="397" tid="0" op="" dtype="" >3.6764e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000004" call="MPI_Isend" bytes="320" orank="4" region="0" commid="0" count="544" tid="0" op="" dtype="" >6.8736e-04 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000014000000006" call="MPI_Isend" bytes="320" orank="6" region="0" commid="0" count="586" tid="0" op="" dtype="" >2.1181e-03 1.9073e-06 1.5974e-05</hent>
<hent key="02400100000000000000014000000009" call="MPI_Isend" bytes="320" orank="9" region="0" commid="0" count="473" tid="0" op="" dtype="" >4.5729e-04 0.0000e+00 6.1989e-06</hent>
<hent key="0240010000000000000001400000001D" call="MPI_Isend" bytes="320" orank="29" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.2803e-04 3.8147e-06 5.0068e-06</hent>
<hent key="024001000000000000000140000002A5" call="MPI_Isend" bytes="320" orank="677" region="0" commid="0" count="24" tid="0" op="" dtype="" >9.7752e-05 2.8610e-06 5.0068e-06</hent>
<hent key="03800100000000000000000000000001" call="MPI_Irecv" bytes="0" orank="1" region="0" commid="0" count="338" tid="0" op="" dtype="" >1.0443e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000004" call="MPI_Irecv" bytes="0" orank="4" region="0" commid="0" count="523" tid="0" op="" dtype="" >2.9969e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000000000000006" call="MPI_Irecv" bytes="0" orank="6" region="0" commid="0" count="527" tid="0" op="" dtype="" >1.3185e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000009" call="MPI_Irecv" bytes="0" orank="9" region="0" commid="0" count="342" tid="0" op="" dtype="" >1.0943e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000001D" call="MPI_Irecv" bytes="0" orank="29" region="0" commid="0" count="80" tid="0" op="" dtype="" >2.2888e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002A5" call="MPI_Irecv" bytes="0" orank="677" region="0" commid="0" count="82" tid="0" op="" dtype="" >2.1219e-05 0.0000e+00 2.1458e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.2554e-04 0.0000e+00 8.2970e-05</hent>
<hent key="03800100000000000000030000000001" call="MPI_Irecv" bytes="768" orank="1" region="0" commid="0" count="694" tid="0" op="" dtype="" >1.0467e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000030000000009" call="MPI_Irecv" bytes="768" orank="9" region="0" commid="0" count="688" tid="0" op="" dtype="" >1.1253e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000000000001" call="MPI_Isend" bytes="0" orank="1" region="0" commid="0" count="360" tid="0" op="" dtype="" >2.0361e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000000000004" call="MPI_Isend" bytes="0" orank="4" region="0" commid="0" count="516" tid="0" op="" dtype="" >3.7956e-04 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000000000000006" call="MPI_Isend" bytes="0" orank="6" region="0" commid="0" count="548" tid="0" op="" dtype="" >1.3392e-03 9.5367e-07 5.0068e-05</hent>
<hent key="02400100000000000000000000000009" call="MPI_Isend" bytes="0" orank="9" region="0" commid="0" count="362" tid="0" op="" dtype="" >2.3460e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000000000000001D" call="MPI_Isend" bytes="0" orank="29" region="0" commid="0" count="84" tid="0" op="" dtype="" >3.2163e-04 9.5367e-07 9.0599e-06</hent>
<hent key="024001000000000000000000000002A5" call="MPI_Isend" bytes="0" orank="677" region="0" commid="0" count="81" tid="0" op="" dtype="" >3.0947e-04 2.1458e-06 8.1062e-06</hent>
<hent key="03800100000000000000060000000001" call="MPI_Irecv" bytes="1536" orank="1" region="0" commid="0" count="63" tid="0" op="" dtype="" >2.0027e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000004" call="MPI_Irecv" bytes="1536" orank="4" region="0" commid="0" count="11" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000060000000006" call="MPI_Irecv" bytes="1536" orank="6" region="0" commid="0" count="17" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000060000000009" call="MPI_Irecv" bytes="1536" orank="9" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.1683e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000001D" call="MPI_Irecv" bytes="1536" orank="29" region="0" commid="0" count="72" tid="0" op="" dtype="" >1.9312e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002A5" call="MPI_Irecv" bytes="1536" orank="677" region="0" commid="0" count="81" tid="0" op="" dtype="" >2.8372e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000030000000001" call="MPI_Isend" bytes="768" orank="1" region="0" commid="0" count="514" tid="0" op="" dtype="" >2.7537e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000030000000009" call="MPI_Isend" bytes="768" orank="9" region="0" commid="0" count="550" tid="0" op="" dtype="" >2.5916e-04 0.0000e+00 1.5020e-05</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 1.9073e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C000000004" call="MPI_Irecv" bytes="448" orank="4" region="0" commid="0" count="4" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000001C000000006" call="MPI_Irecv" bytes="448" orank="6" region="0" commid="0" count="58" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000001" call="MPI_Isend" bytes="1536" orank="1" region="0" commid="0" count="50" tid="0" op="" dtype="" >7.7963e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000004" call="MPI_Isend" bytes="1536" orank="4" region="0" commid="0" count="26" tid="0" op="" dtype="" >5.0068e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000006" call="MPI_Isend" bytes="1536" orank="6" region="0" commid="0" count="14" tid="0" op="" dtype="" >6.6757e-05 3.8147e-06 5.0068e-06</hent>
<hent key="02400100000000000000060000000009" call="MPI_Isend" bytes="1536" orank="9" region="0" commid="0" count="61" tid="0" op="" dtype="" >1.2493e-04 9.5367e-07 1.4067e-05</hent>
<hent key="0240010000000000000006000000001D" call="MPI_Isend" bytes="1536" orank="29" region="0" commid="0" count="69" tid="0" op="" dtype="" >3.5310e-04 3.8147e-06 6.1989e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002A5" call="MPI_Isend" bytes="1536" orank="677" region="0" commid="0" count="89" tid="0" op="" dtype="" >3.9792e-04 3.8147e-06 1.0014e-05</hent>
<hent key="038001000000000000000C000000001D" call="MPI_Irecv" bytes="3072" orank="29" region="0" commid="0" count="335" tid="0" op="" dtype="" >1.2326e-04 0.0000e+00 2.1458e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002A5" call="MPI_Irecv" bytes="3072" orank="677" region="0" commid="0" count="387" tid="0" op="" dtype="" >1.6284e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000001C000000004" call="MPI_Isend" bytes="448" orank="4" region="0" commid="0" count="20" tid="0" op="" dtype="" >1.3351e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C000000006" call="MPI_Isend" bytes="448" orank="6" region="0" commid="0" count="26" tid="0" op="" dtype="" >2.9564e-05 0.0000e+00 7.1526e-06</hent>
<hent key="024001000000000000000C000000001D" call="MPI_Isend" bytes="3072" orank="29" region="0" commid="0" count="361" tid="0" op="" dtype="" >2.1305e-03 4.7684e-06 3.5048e-05</hent>
<hent key="024001000000000000000C00000002A5" call="MPI_Isend" bytes="3072" orank="677" region="0" commid="0" count="356" tid="0" op="" dtype="" >1.7524e-03 3.8147e-06 1.0967e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.9588e-04 9.7036e-05 1.0085e-04</hent>
<hent key="03800100000000000000038000000001" call="MPI_Irecv" bytes="896" orank="1" region="0" commid="0" count="2987" tid="0" op="" dtype="" >4.8590e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000004" call="MPI_Irecv" bytes="896" orank="4" region="0" commid="0" count="171" tid="0" op="" dtype="" >1.1230e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000038000000006" call="MPI_Irecv" bytes="896" orank="6" region="0" commid="0" count="155" tid="0" op="" dtype="" >3.7193e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000009" call="MPI_Irecv" bytes="896" orank="9" region="0" commid="0" count="2980" tid="0" op="" dtype="" >6.0821e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000003800000001D" call="MPI_Irecv" bytes="896" orank="29" region="0" commid="0" count="38" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002A5" call="MPI_Irecv" bytes="896" orank="677" region="0" commid="0" count="24" tid="0" op="" dtype="" >9.7752e-06 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.2207e-04 1.2207e-04 1.2207e-04</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000180000000001" call="MPI_Irecv" bytes="6144" orank="1" region="0" commid="0" count="2554" tid="0" op="" dtype="" >5.2857e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000180000000009" call="MPI_Irecv" bytes="6144" orank="9" region="0" commid="0" count="2555" tid="0" op="" dtype="" >5.1332e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000018000000001D" call="MPI_Irecv" bytes="6144" orank="29" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001800000002A5" call="MPI_Irecv" bytes="6144" orank="677" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000038000000001" call="MPI_Isend" bytes="896" orank="1" region="0" commid="0" count="3159" tid="0" op="" dtype="" >1.8654e-03 0.0000e+00 1.7881e-05</hent>
<hent key="02400100000000000000038000000004" call="MPI_Isend" bytes="896" orank="4" region="0" commid="0" count="182" tid="0" op="" dtype="" >3.3808e-04 9.5367e-07 1.1921e-05</hent>
<hent key="02400100000000000000038000000006" call="MPI_Isend" bytes="896" orank="6" region="0" commid="0" count="159" tid="0" op="" dtype="" >7.0167e-04 2.8610e-06 1.5974e-05</hent>
<hent key="02400100000000000000038000000009" call="MPI_Isend" bytes="896" orank="9" region="0" commid="0" count="3120" tid="0" op="" dtype="" >1.7972e-03 0.0000e+00 1.9073e-05</hent>
<hent key="0240010000000000000003800000001D" call="MPI_Isend" bytes="896" orank="29" region="0" commid="0" count="20" tid="0" op="" dtype="" >1.0157e-04 4.0531e-06 5.9605e-06</hent>
<hent key="024001000000000000000380000002A5" call="MPI_Isend" bytes="896" orank="677" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.1587e-04 3.8147e-06 5.0068e-06</hent>
<hent key="02400100000000000000180000000001" call="MPI_Isend" bytes="6144" orank="1" region="0" commid="0" count="1892" tid="0" op="" dtype="" >3.7532e-03 9.5367e-07 1.5020e-05</hent>
<hent key="02400100000000000000180000000009" call="MPI_Isend" bytes="6144" orank="9" region="0" commid="0" count="2043" tid="0" op="" dtype="" >3.8207e-03 9.5367e-07 1.5020e-05</hent>
<hent key="0240010000000000000018000000001D" call="MPI_Isend" bytes="6144" orank="29" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.9591e-05 6.9141e-06 7.8678e-06</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.9114e-05 4.9114e-05 4.9114e-05</hent>
<hent key="024001000000000000001800000002A5" call="MPI_Isend" bytes="6144" orank="677" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.7193e-05 5.9605e-06 6.9141e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >9.4302e+00 1.1921e-05 1.2662e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.7690e-04 8.7690e-04 8.7690e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.1989e-06 6.1989e-06 6.1989e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0572e-02 1.0572e-02 1.0572e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >3.9001e-03 3.9001e-03 3.9001e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.0163e-01 3.3751e-03 1.9201e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.7817e-04 5.7817e-04 5.7817e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5794e+00 3.7789e-04 2.4808e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >3.6001e-05 0.0000e+00 1.5974e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="03800100000000000000070000000001" call="MPI_Irecv" bytes="1792" orank="1" region="0" commid="0" count="16" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000004" call="MPI_Irecv" bytes="1792" orank="4" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.9073e-06 9.5367e-07 9.5367e-07</hent>
<hent key="03800100000000000000070000000006" call="MPI_Irecv" bytes="1792" orank="6" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000070000000009" call="MPI_Irecv" bytes="1792" orank="9" region="0" commid="0" count="13" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000001D" call="MPI_Irecv" bytes="1792" orank="29" region="0" commid="0" count="106" tid="0" op="" dtype="" >4.2200e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002A5" call="MPI_Irecv" bytes="1792" orank="677" region="0" commid="0" count="123" tid="0" op="" dtype="" >4.3154e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >8.6308e-05 0.0000e+00 6.3896e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9781e+01 0.0000e+00 3.8413e+01</hent>
<hent key="038001000000000000000A0000000001" call="MPI_Irecv" bytes="2560" orank="1" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000009" call="MPI_Irecv" bytes="2560" orank="9" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000A000000001D" call="MPI_Irecv" bytes="2560" orank="29" region="0" commid="0" count="364" tid="0" op="" dtype="" >1.1897e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002A5" call="MPI_Irecv" bytes="2560" orank="677" region="0" commid="0" count="389" tid="0" op="" dtype="" >1.5736e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000001" call="MPI_Isend" bytes="1792" orank="1" region="0" commid="0" count="16" tid="0" op="" dtype="" >2.9564e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000004" call="MPI_Isend" bytes="1792" orank="4" region="0" commid="0" count="2" tid="0" op="" dtype="" >5.2452e-06 2.1458e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000006" call="MPI_Isend" bytes="1792" orank="6" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.2200e-05 4.0531e-06 1.1921e-05</hent>
<hent key="02400100000000000000070000000009" call="MPI_Isend" bytes="1792" orank="9" region="0" commid="0" count="15" tid="0" op="" dtype="" >2.2650e-05 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000007000000001D" call="MPI_Isend" bytes="1792" orank="29" region="0" commid="0" count="102" tid="0" op="" dtype="" >5.6624e-04 4.7684e-06 1.0967e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3061e-02 8.3061e-02 8.3061e-02</hent>
<hent key="024001000000000000000700000002A5" call="MPI_Isend" bytes="1792" orank="677" region="0" commid="0" count="124" tid="0" op="" dtype="" >5.6052e-04 3.8147e-06 6.1989e-06</hent>
<hent key="024001000000000000000A0000000001" call="MPI_Isend" bytes="2560" orank="1" region="0" commid="0" count="3" tid="0" op="" dtype="" >5.9605e-06 1.9073e-06 2.1458e-06</hent>
<hent key="024001000000000000000A000000001D" call="MPI_Isend" bytes="2560" orank="29" region="0" commid="0" count="408" tid="0" op="" dtype="" >2.3043e-03 4.7684e-06 1.4067e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.3876e-04 1.6928e-05 6.7949e-05</hent>
<hent key="024001000000000000000A00000002A5" call="MPI_Isend" bytes="2560" orank="677" region="0" commid="0" count="390" tid="0" op="" dtype="" >1.8427e-03 3.8147e-06 1.0014e-05</hent>
<hent key="03800100000000000000100000000004" call="MPI_Irecv" bytes="4096" orank="4" region="0" commid="0" count="12685" tid="0" op="" dtype="" >5.2843e-03 0.0000e+00 1.7166e-05</hent>
<hent key="03800100000000000000100000000006" call="MPI_Irecv" bytes="4096" orank="6" region="0" commid="0" count="12482" tid="0" op="" dtype="" >2.2366e-03 0.0000e+00 7.1526e-06</hent>
<hent key="0380010000000000000010000000001D" call="MPI_Irecv" bytes="4096" orank="29" region="0" commid="0" count="2838" tid="0" op="" dtype="" >5.0473e-04 0.0000e+00 3.0994e-06</hent>
<hent key="038001000000000000001000000002A5" call="MPI_Irecv" bytes="4096" orank="677" region="0" commid="0" count="2917" tid="0" op="" dtype="" >6.2633e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000100000000004" call="MPI_Isend" bytes="4096" orank="4" region="0" commid="0" count="12622" tid="0" op="" dtype="" >2.1572e-02 0.0000e+00 1.8120e-05</hent>
<hent key="02400100000000000000100000000006" call="MPI_Isend" bytes="4096" orank="6" region="0" commid="0" count="12598" tid="0" op="" dtype="" >3.0049e-02 0.0000e+00 2.5034e-05</hent>
<hent key="0240010000000000000010000000001D" call="MPI_Isend" bytes="4096" orank="29" region="0" commid="0" count="2807" tid="0" op="" dtype="" >1.1061e-02 9.5367e-07 1.1921e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >4.3821e-04 1.6212e-04 2.7609e-04</hent>
<hent key="024001000000000000001000000002A5" call="MPI_Isend" bytes="4096" orank="677" region="0" commid="0" count="2910" tid="0" op="" dtype="" >1.0484e-02 9.5367e-07 3.8147e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.9114e-05 4.9114e-05 4.9114e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >8.8120e-04 1.3399e-04 2.6894e-04</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.0967e-05 3.0994e-06 7.8678e-06</hent>
<hent key="03800100000000000000000400000001" call="MPI_Irecv" bytes="4" orank="1" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.8327e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000004" call="MPI_Irecv" bytes="4" orank="4" region="0" commid="0" count="3400" tid="0" op="" dtype="" >8.1491e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000400000006" call="MPI_Irecv" bytes="4" orank="6" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.6171e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000009" call="MPI_Irecv" bytes="4" orank="9" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.8614e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000001D" call="MPI_Irecv" bytes="4" orank="29" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.1059e-04 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000001C0000000001" call="MPI_Irecv" bytes="7168" orank="1" region="0" commid="0" count="10146" tid="0" op="" dtype="" >2.1610e-03 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000001C0000000009" call="MPI_Irecv" bytes="7168" orank="9" region="0" commid="0" count="10145" tid="0" op="" dtype="" >2.0728e-03 0.0000e+00 6.1989e-06</hent>
<hent key="038001000000000000000004000002A5" call="MPI_Irecv" bytes="4" orank="677" region="0" commid="0" count="3400" tid="0" op="" dtype="" >7.3600e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000400000001" call="MPI_Isend" bytes="4" orank="1" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.5807e-03 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000000400000004" call="MPI_Isend" bytes="4" orank="4" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.4639e-03 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000000400000006" call="MPI_Isend" bytes="4" orank="6" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.6095e-03 0.0000e+00 5.8174e-05</hent>
<hent key="02400100000000000000000400000009" call="MPI_Isend" bytes="4" orank="9" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.3802e-03 0.0000e+00 3.0994e-05</hent>
<hent key="0240010000000000000000040000001D" call="MPI_Isend" bytes="4" orank="29" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.5841e-02 4.7684e-06 1.1249e-03</hent>
<hent key="024001000000000000001C0000000001" call="MPI_Isend" bytes="7168" orank="1" region="0" commid="0" count="10808" tid="0" op="" dtype="" >2.1791e-02 9.5367e-07 2.4080e-05</hent>
<hent key="024001000000000000001C0000000009" call="MPI_Isend" bytes="7168" orank="9" region="0" commid="0" count="10657" tid="0" op="" dtype="" >2.0342e-02 9.5367e-07 1.9073e-05</hent>
<hent key="024001000000000000000004000002A5" call="MPI_Isend" bytes="4" orank="677" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.9240e-02 3.8147e-06 1.1706e-04</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.4901e-04 1.4901e-04 1.4901e-04</hent>
<hent key="03800100000000000000020000000004" call="MPI_Irecv" bytes="512" orank="4" region="0" commid="0" count="3396" tid="0" op="" dtype="" >4.5657e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000020000000006" call="MPI_Irecv" bytes="512" orank="6" region="0" commid="0" count="3342" tid="0" op="" dtype="" >5.1212e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000001" call="MPI_Irecv" bytes="1280" orank="1" region="0" commid="0" count="145" tid="0" op="" dtype="" >4.4823e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000004" call="MPI_Irecv" bytes="1280" orank="4" region="0" commid="0" count="50" tid="0" op="" dtype="" >3.4809e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000050000000006" call="MPI_Irecv" bytes="1280" orank="6" region="0" commid="0" count="61" tid="0" op="" dtype="" >1.9312e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000009" call="MPI_Irecv" bytes="1280" orank="9" region="0" commid="0" count="147" tid="0" op="" dtype="" >6.7472e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000005000000001D" call="MPI_Irecv" bytes="1280" orank="29" region="0" commid="0" count="65" tid="0" op="" dtype="" >1.9312e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4785e+01 2.1935e-05 1.8445e-01</hent>
<hent key="038001000000000000000500000002A5" call="MPI_Irecv" bytes="1280" orank="677" region="0" commid="0" count="45" tid="0" op="" dtype="" >1.7405e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000020000000004" call="MPI_Isend" bytes="512" orank="4" region="0" commid="0" count="3380" tid="0" op="" dtype="" >1.7748e-03 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000020000000006" call="MPI_Isend" bytes="512" orank="6" region="0" commid="0" count="3374" tid="0" op="" dtype="" >2.4176e-03 0.0000e+00 1.9789e-05</hent>
<hent key="03800100000000000000080000000001" call="MPI_Irecv" bytes="2048" orank="1" region="0" commid="0" count="6" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000004" call="MPI_Irecv" bytes="2048" orank="4" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="03800100000000000000080000000009" call="MPI_Irecv" bytes="2048" orank="9" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000001D" call="MPI_Irecv" bytes="2048" orank="29" region="0" commid="0" count="147" tid="0" op="" dtype="" >5.3406e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002A5" call="MPI_Irecv" bytes="2048" orank="677" region="0" commid="0" count="137" tid="0" op="" dtype="" >5.1498e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000050000000001" call="MPI_Isend" bytes="1280" orank="1" region="0" commid="0" count="142" tid="0" op="" dtype="" >2.0361e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000050000000004" call="MPI_Isend" bytes="1280" orank="4" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.0133e-04 9.5367e-07 4.0531e-06</hent>
<hent key="02400100000000000000050000000006" call="MPI_Isend" bytes="1280" orank="6" region="0" commid="0" count="50" tid="0" op="" dtype="" >2.3866e-04 3.0994e-06 1.0014e-05</hent>
<hent key="02400100000000000000050000000009" call="MPI_Isend" bytes="1280" orank="9" region="0" commid="0" count="144" tid="0" op="" dtype="" >2.3293e-04 9.5367e-07 1.4067e-05</hent>
<hent key="0240010000000000000005000000001D" call="MPI_Isend" bytes="1280" orank="29" region="0" commid="0" count="47" tid="0" op="" dtype="" >2.4056e-04 3.8147e-06 1.1921e-05</hent>
<hent key="024001000000000000000500000002A5" call="MPI_Isend" bytes="1280" orank="677" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.6499e-04 3.8147e-06 5.0068e-06</hent>
</hash>
<internal rank="5" log_i="1724765674.534995" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="6" mpi_size="696" stamp_init="1724765564.465691" stamp_final="1724765674.541643" username="apac4" allocationname="unknown" flags="0" pid="1612123" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10076e+02" utime="8.66223e+01" stime="1.45413e+01" mtime="7.16256e+01" gflop="0.00000e+00" gbyte="3.77251e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16256e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09940e+02" utime="8.65857e+01" stime="1.45335e+01" mtime="7.16256e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16256e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1948e+09" > 8.4015e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1939e+09" > 4.2759e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9468e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9780e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.8174e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1008e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5787e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.2471e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0591e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3090e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3341e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="210" >
<hent key="02400100000000000000080000000002" call="MPI_Isend" bytes="2048" orank="2" region="0" commid="0" count="4" tid="0" op="" dtype="" >6.6757e-06 9.5367e-07 1.9073e-06</hent>
<hent key="02400100000000000000080000000007" call="MPI_Isend" bytes="2048" orank="7" region="0" commid="0" count="1" tid="0" op="" dtype="" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="0240010000000000000008000000000A" call="MPI_Isend" bytes="2048" orank="10" region="0" commid="0" count="3" tid="0" op="" dtype="" >6.1989e-06 1.9073e-06 2.1458e-06</hent>
<hent key="0240010000000000000008000000001E" call="MPI_Isend" bytes="2048" orank="30" region="0" commid="0" count="149" tid="0" op="" dtype="" >9.3246e-04 4.7684e-06 1.4067e-05</hent>
<hent key="024001000000000000000800000002A6" call="MPI_Isend" bytes="2048" orank="678" region="0" commid="0" count="162" tid="0" op="" dtype="" >8.5592e-04 3.8147e-06 1.5974e-05</hent>
<hent key="038001000000000000000E0000000005" call="MPI_Irecv" bytes="3584" orank="5" region="0" commid="0" count="102" tid="0" op="" dtype="" >5.1975e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000007" call="MPI_Irecv" bytes="3584" orank="7" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E000000001E" call="MPI_Irecv" bytes="3584" orank="30" region="0" commid="0" count="143" tid="0" op="" dtype="" >5.6744e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E00000002A6" call="MPI_Irecv" bytes="3584" orank="678" region="0" commid="0" count="125" tid="0" op="" dtype="" >1.0681e-04 0.0000e+00 6.9141e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000E0000000005" call="MPI_Isend" bytes="3584" orank="5" region="0" commid="0" count="218" tid="0" op="" dtype="" >3.7217e-04 0.0000e+00 3.0994e-06</hent>
<hent key="024001000000000000000E0000000007" call="MPI_Isend" bytes="3584" orank="7" region="0" commid="0" count="51" tid="0" op="" dtype="" >1.6546e-04 9.5367e-07 1.8835e-05</hent>
<hent key="024001000000000000000E000000001E" call="MPI_Isend" bytes="3584" orank="30" region="0" commid="0" count="154" tid="0" op="" dtype="" >1.0378e-03 4.7684e-06 1.5020e-05</hent>
<hent key="024001000000000000000E00000002A6" call="MPI_Isend" bytes="3584" orank="678" region="0" commid="0" count="151" tid="0" op="" dtype="" >8.4400e-04 4.0531e-06 1.1921e-05</hent>
<hent key="03800100000000000000028000000002" call="MPI_Irecv" bytes="640" orank="2" region="0" commid="0" count="431" tid="0" op="" dtype="" >8.4400e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000005" call="MPI_Irecv" bytes="640" orank="5" region="0" commid="0" count="336" tid="0" op="" dtype="" >1.3351e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000007" call="MPI_Irecv" bytes="640" orank="7" region="0" commid="0" count="412" tid="0" op="" dtype="" >9.9421e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000002800000000A" call="MPI_Irecv" bytes="640" orank="10" region="0" commid="0" count="409" tid="0" op="" dtype="" >1.5330e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000001E" call="MPI_Irecv" bytes="640" orank="30" region="0" commid="0" count="23" tid="0" op="" dtype="" >8.8215e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000014000000001E" call="MPI_Irecv" bytes="5120" orank="30" region="0" commid="0" count="639" tid="0" op="" dtype="" >1.5354e-04 0.0000e+00 1.4067e-05</hent>
<hent key="038001000000000000000280000002A6" call="MPI_Irecv" bytes="640" orank="678" region="0" commid="0" count="20" tid="0" op="" dtype="" >7.6294e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000001400000002A6" call="MPI_Irecv" bytes="5120" orank="678" region="0" commid="0" count="494" tid="0" op="" dtype="" >2.5368e-04 0.0000e+00 2.5034e-05</hent>
<hent key="03800100000000000000A0000000001E" call="MPI_Irecv" bytes="40960" orank="30" region="0" commid="0" count="2086" tid="0" op="" dtype="" >1.4038e-03 0.0000e+00 4.1962e-05</hent>
<hent key="02400100000000000000028000000002" call="MPI_Isend" bytes="640" orank="2" region="0" commid="0" count="433" tid="0" op="" dtype="" >5.2905e-04 9.5367e-07 7.1526e-06</hent>
<hent key="02400100000000000000028000000005" call="MPI_Isend" bytes="640" orank="5" region="0" commid="0" count="386" tid="0" op="" dtype="" >5.7554e-04 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000028000000007" call="MPI_Isend" bytes="640" orank="7" region="0" commid="0" count="372" tid="0" op="" dtype="" >1.1585e-03 1.9073e-06 1.6928e-05</hent>
<hent key="0240010000000000000002800000000A" call="MPI_Isend" bytes="640" orank="10" region="0" commid="0" count="408" tid="0" op="" dtype="" >5.0592e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000002800000001E" call="MPI_Isend" bytes="640" orank="30" region="0" commid="0" count="24" tid="0" op="" dtype="" >1.4710e-04 4.0531e-06 1.8835e-05</hent>
<hent key="03800100000000000000A000000002A6" call="MPI_Irecv" bytes="40960" orank="678" region="0" commid="0" count="1562" tid="0" op="" dtype="" >6.0606e-04 0.0000e+00 5.7936e-05</hent>
<hent key="0240010000000000000014000000001E" call="MPI_Isend" bytes="5120" orank="30" region="0" commid="0" count="631" tid="0" op="" dtype="" >3.9601e-03 1.9073e-06 7.5102e-05</hent>
<hent key="024001000000000000000280000002A6" call="MPI_Isend" bytes="640" orank="678" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.0777e-04 3.8147e-06 8.1062e-06</hent>
<hent key="0380010000000000000080000000001E" call="MPI_Irecv" bytes="32768" orank="30" region="0" commid="0" count="10614" tid="0" op="" dtype="" >7.3729e-03 0.0000e+00 4.5061e-05</hent>
<hent key="024001000000000000001400000002A6" call="MPI_Isend" bytes="5120" orank="678" region="0" commid="0" count="550" tid="0" op="" dtype="" >2.6894e-03 9.5367e-07 3.6955e-05</hent>
<hent key="038001000000000000008000000002A6" call="MPI_Irecv" bytes="32768" orank="678" region="0" commid="0" count="11138" tid="0" op="" dtype="" >5.4278e-03 0.0000e+00 9.1076e-05</hent>
<hent key="03800100000000000000014000000002" call="MPI_Irecv" bytes="320" orank="2" region="0" commid="0" count="444" tid="0" op="" dtype="" >1.0681e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000005" call="MPI_Irecv" bytes="320" orank="5" region="0" commid="0" count="586" tid="0" op="" dtype="" >2.2912e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000007" call="MPI_Irecv" bytes="320" orank="7" region="0" commid="0" count="556" tid="0" op="" dtype="" >1.4353e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000A" call="MPI_Irecv" bytes="320" orank="10" region="0" commid="0" count="416" tid="0" op="" dtype="" >1.4877e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000001400000001E" call="MPI_Irecv" bytes="320" orank="30" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.9073e-05 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000A0000000001E" call="MPI_Isend" bytes="40960" orank="30" region="0" commid="0" count="2128" tid="0" op="" dtype="" >6.0732e-02 1.0014e-05 1.9813e-04</hent>
<hent key="038001000000000000000140000002A6" call="MPI_Irecv" bytes="320" orank="678" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.9312e-05 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000A000000002A6" call="MPI_Isend" bytes="40960" orank="678" region="0" commid="0" count="1914" tid="0" op="" dtype="" >3.1319e-02 6.9141e-06 1.0395e-04</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 0.0000e+00 1.9073e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >5.9468e+00 0.0000e+00 1.1038e-01</hent>
<hent key="0240010000000000000080000000001E" call="MPI_Isend" bytes="32768" orank="30" region="0" commid="0" count="10572" tid="0" op="" dtype="" >3.0097e-01 8.8215e-06 2.5105e-04</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6928e-05 0.0000e+00 1.5974e-05</hent>
<hent key="024001000000000000008000000002A6" call="MPI_Isend" bytes="32768" orank="678" region="0" commid="0" count="10786" tid="0" op="" dtype="" >1.8424e-01 6.9141e-06 3.2091e-04</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000002" call="MPI_Isend" bytes="320" orank="2" region="0" commid="0" count="438" tid="0" op="" dtype="" >4.4680e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000014000000005" call="MPI_Isend" bytes="320" orank="5" region="0" commid="0" count="552" tid="0" op="" dtype="" >6.9475e-04 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000014000000007" call="MPI_Isend" bytes="320" orank="7" region="0" commid="0" count="553" tid="0" op="" dtype="" >1.5554e-03 1.9073e-06 8.8215e-06</hent>
<hent key="0240010000000000000001400000000A" call="MPI_Isend" bytes="320" orank="10" region="0" commid="0" count="446" tid="0" op="" dtype="" >4.4012e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000001400000001E" call="MPI_Isend" bytes="320" orank="30" region="0" commid="0" count="20" tid="0" op="" dtype="" >1.0180e-04 3.8147e-06 8.1062e-06</hent>
<hent key="024001000000000000000140000002A6" call="MPI_Isend" bytes="320" orank="678" region="0" commid="0" count="18" tid="0" op="" dtype="" >8.7976e-05 2.8610e-06 9.0599e-06</hent>
<hent key="03800100000000000000000000000002" call="MPI_Irecv" bytes="0" orank="2" region="0" commid="0" count="331" tid="0" op="" dtype="" >8.7976e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000005" call="MPI_Irecv" bytes="0" orank="5" region="0" commid="0" count="548" tid="0" op="" dtype="" >1.9550e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000000000007" call="MPI_Irecv" bytes="0" orank="7" region="0" commid="0" count="507" tid="0" op="" dtype="" >1.2279e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000A" call="MPI_Irecv" bytes="0" orank="10" region="0" commid="0" count="345" tid="0" op="" dtype="" >1.1945e-04 0.0000e+00 1.3113e-05</hent>
<hent key="0380010000000000000000000000001E" call="MPI_Irecv" bytes="0" orank="30" region="0" commid="0" count="86" tid="0" op="" dtype="" >1.8597e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002A6" call="MPI_Irecv" bytes="0" orank="678" region="0" commid="0" count="81" tid="0" op="" dtype="" >4.4346e-05 0.0000e+00 7.8678e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.5129e-04 0.0000e+00 8.7023e-05</hent>
<hent key="03800100000000000000030000000002" call="MPI_Irecv" bytes="768" orank="2" region="0" commid="0" count="542" tid="0" op="" dtype="" >8.7023e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003000000000A" call="MPI_Irecv" bytes="768" orank="10" region="0" commid="0" count="768" tid="0" op="" dtype="" >2.1625e-04 0.0000e+00 2.2888e-05</hent>
<hent key="02400100000000000000000000000002" call="MPI_Isend" bytes="0" orank="2" region="0" commid="0" count="353" tid="0" op="" dtype="" >2.3556e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000000000000005" call="MPI_Isend" bytes="0" orank="5" region="0" commid="0" count="527" tid="0" op="" dtype="" >4.0865e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000007" call="MPI_Isend" bytes="0" orank="7" region="0" commid="0" count="538" tid="0" op="" dtype="" >1.1034e-03 9.5367e-07 3.0994e-05</hent>
<hent key="0240010000000000000000000000000A" call="MPI_Isend" bytes="0" orank="10" region="0" commid="0" count="338" tid="0" op="" dtype="" >2.2149e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000000000000001E" call="MPI_Isend" bytes="0" orank="30" region="0" commid="0" count="87" tid="0" op="" dtype="" >4.3440e-04 1.9073e-06 3.4094e-05</hent>
<hent key="024001000000000000000000000002A6" call="MPI_Isend" bytes="0" orank="678" region="0" commid="0" count="90" tid="0" op="" dtype="" >3.8123e-04 1.9073e-06 1.1921e-05</hent>
<hent key="03800100000000000000060000000002" call="MPI_Irecv" bytes="1536" orank="2" region="0" commid="0" count="39" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000005" call="MPI_Irecv" bytes="1536" orank="5" region="0" commid="0" count="14" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000007" call="MPI_Irecv" bytes="1536" orank="7" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000A" call="MPI_Irecv" bytes="1536" orank="10" region="0" commid="0" count="58" tid="0" op="" dtype="" >1.7405e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000001E" call="MPI_Irecv" bytes="1536" orank="30" region="0" commid="0" count="83" tid="0" op="" dtype="" >3.7432e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002A6" call="MPI_Irecv" bytes="1536" orank="678" region="0" commid="0" count="68" tid="0" op="" dtype="" >4.3154e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000030000000002" call="MPI_Isend" bytes="768" orank="2" region="0" commid="0" count="1036" tid="0" op="" dtype="" >8.7428e-04 0.0000e+00 5.5075e-05</hent>
<hent key="0240010000000000000003000000000A" call="MPI_Isend" bytes="768" orank="10" region="0" commid="0" count="812" tid="0" op="" dtype="" >6.2490e-04 0.0000e+00 1.6689e-04</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000005" call="MPI_Irecv" bytes="448" orank="5" region="0" commid="0" count="26" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C000000007" call="MPI_Irecv" bytes="448" orank="7" region="0" commid="0" count="10" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000002" call="MPI_Isend" bytes="1536" orank="2" region="0" commid="0" count="55" tid="0" op="" dtype="" >9.5129e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000005" call="MPI_Isend" bytes="1536" orank="5" region="0" commid="0" count="17" tid="0" op="" dtype="" >4.1962e-05 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000060000000007" call="MPI_Isend" bytes="1536" orank="7" region="0" commid="0" count="17" tid="0" op="" dtype="" >7.2479e-05 2.8610e-06 1.1921e-05</hent>
<hent key="0240010000000000000006000000000A" call="MPI_Isend" bytes="1536" orank="10" region="0" commid="0" count="60" tid="0" op="" dtype="" >1.0777e-04 9.5367e-07 6.9141e-06</hent>
<hent key="0240010000000000000006000000001E" call="MPI_Isend" bytes="1536" orank="30" region="0" commid="0" count="70" tid="0" op="" dtype="" >4.2462e-04 4.7684e-06 1.6928e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002A6" call="MPI_Isend" bytes="1536" orank="678" region="0" commid="0" count="94" tid="0" op="" dtype="" >4.6611e-04 3.8147e-06 1.0014e-05</hent>
<hent key="038001000000000000000C000000001E" call="MPI_Irecv" bytes="3072" orank="30" region="0" commid="0" count="328" tid="0" op="" dtype="" >1.3447e-04 0.0000e+00 6.9141e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002A6" call="MPI_Irecv" bytes="3072" orank="678" region="0" commid="0" count="381" tid="0" op="" dtype="" >3.2330e-04 0.0000e+00 1.8835e-05</hent>
<hent key="0240010000000000000001C000000005" call="MPI_Isend" bytes="448" orank="5" region="0" commid="0" count="58" tid="0" op="" dtype="" >3.0518e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C000000007" call="MPI_Isend" bytes="448" orank="7" region="0" commid="0" count="14" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C000000001E" call="MPI_Isend" bytes="3072" orank="30" region="0" commid="0" count="334" tid="0" op="" dtype="" >2.1572e-03 5.0068e-06 2.5988e-05</hent>
<hent key="024001000000000000000C00000002A6" call="MPI_Isend" bytes="3072" orank="678" region="0" commid="0" count="316" tid="0" op="" dtype="" >1.7972e-03 3.8147e-06 2.7180e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.4213e-04 1.1086e-04 1.1706e-04</hent>
<hent key="03800100000000000000038000000002" call="MPI_Irecv" bytes="896" orank="2" region="0" commid="0" count="3154" tid="0" op="" dtype="" >6.7139e-04 0.0000e+00 4.3154e-05</hent>
<hent key="03800100000000000000038000000005" call="MPI_Irecv" bytes="896" orank="5" region="0" commid="0" count="159" tid="0" op="" dtype="" >6.5327e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000007" call="MPI_Irecv" bytes="896" orank="7" region="0" commid="0" count="160" tid="0" op="" dtype="" >3.4332e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000003800000000A" call="MPI_Irecv" bytes="896" orank="10" region="0" commid="0" count="2930" tid="0" op="" dtype="" >8.4996e-04 0.0000e+00 1.4067e-05</hent>
<hent key="0380010000000000000003800000001E" call="MPI_Irecv" bytes="896" orank="30" region="0" commid="0" count="28" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002A6" call="MPI_Irecv" bytes="896" orank="678" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.7166e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0967e-05 1.0967e-05 1.0967e-05</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000180000000002" call="MPI_Irecv" bytes="6144" orank="2" region="0" commid="0" count="2011" tid="0" op="" dtype="" >2.5392e-04 0.0000e+00 3.8147e-06</hent>
<hent key="0380010000000000000018000000000A" call="MPI_Irecv" bytes="6144" orank="10" region="0" commid="0" count="2858" tid="0" op="" dtype="" >6.4349e-04 0.0000e+00 2.2173e-05</hent>
<hent key="0380010000000000000018000000001E" call="MPI_Irecv" bytes="6144" orank="30" region="0" commid="0" count="5" tid="0" op="" dtype="" >4.5300e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001800000002A6" call="MPI_Irecv" bytes="6144" orank="678" region="0" commid="0" count="12" tid="0" op="" dtype="" >1.6689e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000038000000002" call="MPI_Isend" bytes="896" orank="2" region="0" commid="0" count="2621" tid="0" op="" dtype="" >2.1608e-03 0.0000e+00 6.6996e-05</hent>
<hent key="02400100000000000000038000000005" call="MPI_Isend" bytes="896" orank="5" region="0" commid="0" count="155" tid="0" op="" dtype="" >2.9182e-04 9.5367e-07 1.1921e-05</hent>
<hent key="02400100000000000000038000000007" call="MPI_Isend" bytes="896" orank="7" region="0" commid="0" count="157" tid="0" op="" dtype="" >5.5528e-04 1.9073e-06 1.3113e-05</hent>
<hent key="0240010000000000000003800000000A" call="MPI_Isend" bytes="896" orank="10" region="0" commid="0" count="2858" tid="0" op="" dtype="" >1.8497e-03 0.0000e+00 4.6968e-05</hent>
<hent key="0240010000000000000003800000001E" call="MPI_Isend" bytes="896" orank="30" region="0" commid="0" count="24" tid="0" op="" dtype="" >1.2755e-04 4.0531e-06 9.0599e-06</hent>
<hent key="024001000000000000000380000002A6" call="MPI_Isend" bytes="896" orank="678" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.4091e-04 3.8147e-06 7.8678e-06</hent>
<hent key="02400100000000000000180000000002" call="MPI_Isend" bytes="6144" orank="2" region="0" commid="0" count="3837" tid="0" op="" dtype="" >8.1069e-03 9.5367e-07 9.0837e-05</hent>
<hent key="0240010000000000000018000000000A" call="MPI_Isend" bytes="6144" orank="10" region="0" commid="0" count="3186" tid="0" op="" dtype="" >6.8657e-03 9.5367e-07 5.0068e-05</hent>
<hent key="0240010000000000000018000000001E" call="MPI_Isend" bytes="6144" orank="30" region="0" commid="0" count="8" tid="0" op="" dtype="" >7.6771e-05 6.9141e-06 2.0027e-05</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.2943e-05 6.2943e-05 6.2943e-05</hent>
<hent key="024001000000000000001800000002A6" call="MPI_Isend" bytes="6144" orank="678" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.8120e-05 5.9605e-06 6.1989e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.8860e+00 1.1921e-05 1.2665e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >9.1100e-04 9.1100e-04 9.1100e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0591e-02 1.0591e-02 1.0591e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >4.7560e-03 4.7560e-03 4.7560e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.8986e-01 3.2852e-03 1.8061e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.9605e-04 5.9605e-04 5.9605e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 0.0000e+00 2.1458e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5787e+00 3.8099e-04 2.4803e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >5.8174e-05 9.5367e-07 3.2187e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="03800100000000000000070000000002" call="MPI_Irecv" bytes="1792" orank="2" region="0" commid="0" count="21" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000005" call="MPI_Irecv" bytes="1792" orank="5" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000007" call="MPI_Irecv" bytes="1792" orank="7" region="0" commid="0" count="5" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000007000000000A" call="MPI_Irecv" bytes="1792" orank="10" region="0" commid="0" count="28" tid="0" op="" dtype="" >9.2983e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000001E" call="MPI_Irecv" bytes="1792" orank="30" region="0" commid="0" count="108" tid="0" op="" dtype="" >4.1962e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002A6" call="MPI_Irecv" bytes="1792" orank="678" region="0" commid="0" count="113" tid="0" op="" dtype="" >7.0095e-05 0.0000e+00 7.8678e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >1.0014e-04 0.0000e+00 6.8903e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9779e+01 0.0000e+00 3.8414e+01</hent>
<hent key="038001000000000000000A0000000002" call="MPI_Irecv" bytes="2560" orank="2" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A000000000A" call="MPI_Irecv" bytes="2560" orank="10" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A000000001E" call="MPI_Irecv" bytes="2560" orank="30" region="0" commid="0" count="390" tid="0" op="" dtype="" >1.6952e-04 0.0000e+00 6.9141e-06</hent>
<hent key="038001000000000000000A00000002A6" call="MPI_Irecv" bytes="2560" orank="678" region="0" commid="0" count="362" tid="0" op="" dtype="" >2.6608e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000070000000002" call="MPI_Isend" bytes="1792" orank="2" region="0" commid="0" count="22" tid="0" op="" dtype="" >4.1485e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000005" call="MPI_Isend" bytes="1792" orank="5" region="0" commid="0" count="2" tid="0" op="" dtype="" >5.0068e-06 2.1458e-06 2.8610e-06</hent>
<hent key="02400100000000000000070000000007" call="MPI_Isend" bytes="1792" orank="7" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.1696e-05 2.8610e-06 4.0531e-06</hent>
<hent key="0240010000000000000007000000000A" call="MPI_Isend" bytes="1792" orank="10" region="0" commid="0" count="22" tid="0" op="" dtype="" >5.1737e-05 9.5367e-07 1.0967e-05</hent>
<hent key="0240010000000000000007000000001E" call="MPI_Isend" bytes="1792" orank="30" region="0" commid="0" count="113" tid="0" op="" dtype="" >6.8164e-04 5.0068e-06 1.4067e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3090e-02 8.3090e-02 8.3090e-02</hent>
<hent key="024001000000000000000700000002A6" call="MPI_Isend" bytes="1792" orank="678" region="0" commid="0" count="132" tid="0" op="" dtype="" >6.9141e-04 3.8147e-06 1.5974e-05</hent>
<hent key="024001000000000000000A0000000002" call="MPI_Isend" bytes="2560" orank="2" region="0" commid="0" count="3" tid="0" op="" dtype="" >5.7220e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000A000000000A" call="MPI_Isend" bytes="2560" orank="10" region="0" commid="0" count="2" tid="0" op="" dtype="" >4.7684e-06 1.9073e-06 2.8610e-06</hent>
<hent key="024001000000000000000A000000001E" call="MPI_Isend" bytes="2560" orank="30" region="0" commid="0" count="387" tid="0" op="" dtype="" >2.4719e-03 5.0068e-06 2.5034e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.4496e-04 1.5020e-05 6.8903e-05</hent>
<hent key="024001000000000000000A00000002A6" call="MPI_Isend" bytes="2560" orank="678" region="0" commid="0" count="378" tid="0" op="" dtype="" >2.0516e-03 3.8147e-06 2.0027e-05</hent>
<hent key="03800100000000000000100000000005" call="MPI_Irecv" bytes="4096" orank="5" region="0" commid="0" count="12598" tid="0" op="" dtype="" >6.3050e-03 0.0000e+00 2.0027e-05</hent>
<hent key="03800100000000000000100000000007" call="MPI_Irecv" bytes="4096" orank="7" region="0" commid="0" count="12660" tid="0" op="" dtype="" >2.5544e-03 0.0000e+00 4.0054e-05</hent>
<hent key="0380010000000000000010000000001E" call="MPI_Irecv" bytes="4096" orank="30" region="0" commid="0" count="3024" tid="0" op="" dtype="" >9.3889e-04 0.0000e+00 4.6015e-05</hent>
<hent key="038001000000000000001000000002A6" call="MPI_Irecv" bytes="4096" orank="678" region="0" commid="0" count="3174" tid="0" op="" dtype="" >1.8189e-03 0.0000e+00 3.0994e-05</hent>
<hent key="02400100000000000000100000000005" call="MPI_Isend" bytes="4096" orank="5" region="0" commid="0" count="12482" tid="0" op="" dtype="" >2.2688e-02 0.0000e+00 6.1035e-05</hent>
<hent key="02400100000000000000100000000007" call="MPI_Isend" bytes="4096" orank="7" region="0" commid="0" count="12649" tid="0" op="" dtype="" >3.5592e-02 0.0000e+00 2.5034e-05</hent>
<hent key="0240010000000000000010000000001E" call="MPI_Isend" bytes="4096" orank="30" region="0" commid="0" count="3055" tid="0" op="" dtype="" >2.0807e-02 1.9073e-06 1.1516e-04</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >4.9591e-04 1.8382e-04 3.1209e-04</hent>
<hent key="024001000000000000001000000002A6" call="MPI_Isend" bytes="4096" orank="678" region="0" commid="0" count="3104" tid="0" op="" dtype="" >1.5307e-02 9.5367e-07 6.8188e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.9101e-05 3.9101e-05 3.9101e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.0891e-03 1.4901e-04 3.5977e-04</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >4.7684e-06 9.5367e-07 3.8147e-06</hent>
<hent key="03800100000000000000000400000002" call="MPI_Irecv" bytes="4" orank="2" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.5003e-04 0.0000e+00 2.5988e-05</hent>
<hent key="03800100000000000000000400000005" call="MPI_Irecv" bytes="4" orank="5" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.0617e-03 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000400000007" call="MPI_Irecv" bytes="4" orank="7" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.5027e-04 0.0000e+00 1.5974e-05</hent>
<hent key="0380010000000000000000040000000A" call="MPI_Irecv" bytes="4" orank="10" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.4002e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0380010000000000000000040000001E" call="MPI_Irecv" bytes="4" orank="30" region="0" commid="0" count="3400" tid="0" op="" dtype="" >9.2077e-04 0.0000e+00 5.5075e-05</hent>
<hent key="038001000000000000001C0000000002" call="MPI_Irecv" bytes="7168" orank="2" region="0" commid="0" count="10689" tid="0" op="" dtype="" >1.7481e-03 0.0000e+00 5.9128e-05</hent>
<hent key="038001000000000000001C000000000A" call="MPI_Irecv" bytes="7168" orank="10" region="0" commid="0" count="9842" tid="0" op="" dtype="" >2.0733e-03 0.0000e+00 3.8862e-05</hent>
<hent key="038001000000000000001C000000001E" call="MPI_Irecv" bytes="7168" orank="30" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000004000002A6" call="MPI_Irecv" bytes="4" orank="678" region="0" commid="0" count="3400" tid="0" op="" dtype="" >9.6178e-04 0.0000e+00 1.9073e-05</hent>
<hent key="038001000000000000001C00000002A6" call="MPI_Irecv" bytes="7168" orank="678" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="02400100000000000000000400000002" call="MPI_Isend" bytes="4" orank="2" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.2271e-03 0.0000e+00 7.6056e-05</hent>
<hent key="02400100000000000000000400000005" call="MPI_Isend" bytes="4" orank="5" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.9629e-03 0.0000e+00 4.1008e-05</hent>
<hent key="02400100000000000000000400000007" call="MPI_Isend" bytes="4" orank="7" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.8102e-03 0.0000e+00 4.8876e-05</hent>
<hent key="0240010000000000000000040000000A" call="MPI_Isend" bytes="4" orank="10" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.0809e-03 0.0000e+00 2.5034e-05</hent>
<hent key="0240010000000000000000040000001E" call="MPI_Isend" bytes="4" orank="30" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.5573e-02 4.0531e-06 4.1509e-04</hent>
<hent key="024001000000000000001C0000000002" call="MPI_Isend" bytes="7168" orank="2" region="0" commid="0" count="8863" tid="0" op="" dtype="" >1.8955e-02 9.5367e-07 4.3154e-05</hent>
<hent key="024001000000000000001C000000000A" call="MPI_Isend" bytes="7168" orank="10" region="0" commid="0" count="9514" tid="0" op="" dtype="" >2.1193e-02 9.5367e-07 1.8001e-04</hent>
<hent key="024001000000000000001C000000001E" call="MPI_Isend" bytes="7168" orank="30" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.5020e-05 1.5020e-05 1.5020e-05</hent>
<hent key="024001000000000000000004000002A6" call="MPI_Isend" bytes="4" orank="678" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.3782e-02 3.8147e-06 7.4148e-05</hent>
<hent key="024001000000000000001C00000002A6" call="MPI_Isend" bytes="7168" orank="678" region="0" commid="0" count="1" tid="0" op="" dtype="" >7.8678e-06 7.8678e-06 7.8678e-06</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.6999e-04 1.6999e-04 1.6999e-04</hent>
<hent key="03800100000000000000020000000005" call="MPI_Irecv" bytes="512" orank="5" region="0" commid="0" count="3374" tid="0" op="" dtype="" >6.2871e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000020000000007" call="MPI_Irecv" bytes="512" orank="7" region="0" commid="0" count="3390" tid="0" op="" dtype="" >5.6291e-04 0.0000e+00 1.5020e-05</hent>
<hent key="03800100000000000000050000000002" call="MPI_Irecv" bytes="1280" orank="2" region="0" commid="0" count="134" tid="0" op="" dtype="" >2.6226e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000005" call="MPI_Irecv" bytes="1280" orank="5" region="0" commid="0" count="50" tid="0" op="" dtype="" >2.0266e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000007" call="MPI_Irecv" bytes="1280" orank="7" region="0" commid="0" count="51" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000005000000000A" call="MPI_Irecv" bytes="1280" orank="10" region="0" commid="0" count="139" tid="0" op="" dtype="" >5.2452e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000001E" call="MPI_Irecv" bytes="1280" orank="30" region="0" commid="0" count="57" tid="0" op="" dtype="" >1.5974e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4259e+01 9.0599e-06 1.8428e-01</hent>
<hent key="038001000000000000000500000002A6" call="MPI_Irecv" bytes="1280" orank="678" region="0" commid="0" count="45" tid="0" op="" dtype="" >3.2425e-05 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000020000000005" call="MPI_Isend" bytes="512" orank="5" region="0" commid="0" count="3342" tid="0" op="" dtype="" >2.3491e-03 0.0000e+00 1.2898e-04</hent>
<hent key="02400100000000000000020000000007" call="MPI_Isend" bytes="512" orank="7" region="0" commid="0" count="3386" tid="0" op="" dtype="" >2.3444e-03 0.0000e+00 2.5988e-05</hent>
<hent key="03800100000000000000080000000002" call="MPI_Irecv" bytes="2048" orank="2" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000A" call="MPI_Irecv" bytes="2048" orank="10" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000001E" call="MPI_Irecv" bytes="2048" orank="30" region="0" commid="0" count="158" tid="0" op="" dtype="" >6.8903e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000800000002A6" call="MPI_Irecv" bytes="2048" orank="678" region="0" commid="0" count="167" tid="0" op="" dtype="" >1.1039e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000050000000002" call="MPI_Isend" bytes="1280" orank="2" region="0" commid="0" count="135" tid="0" op="" dtype="" >2.0790e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000050000000005" call="MPI_Isend" bytes="1280" orank="5" region="0" commid="0" count="61" tid="0" op="" dtype="" >1.2183e-04 9.5367e-07 2.8610e-06</hent>
<hent key="02400100000000000000050000000007" call="MPI_Isend" bytes="1280" orank="7" region="0" commid="0" count="56" tid="0" op="" dtype="" >2.0838e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000005000000000A" call="MPI_Isend" bytes="1280" orank="10" region="0" commid="0" count="151" tid="0" op="" dtype="" >2.3818e-04 9.5367e-07 8.1062e-06</hent>
<hent key="0240010000000000000005000000001E" call="MPI_Isend" bytes="1280" orank="30" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.3723e-04 4.0531e-06 1.1921e-05</hent>
<hent key="024001000000000000000500000002A6" call="MPI_Isend" bytes="1280" orank="678" region="0" commid="0" count="49" tid="0" op="" dtype="" >2.5034e-04 3.8147e-06 1.3113e-05</hent>
</hash>
<internal rank="6" log_i="1724765674.541643" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="7" mpi_size="696" stamp_init="1724765564.467086" stamp_final="1724765674.530225" username="apac4" allocationname="unknown" flags="0" pid="1612124" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10063e+02" utime="8.94561e+01" stime="1.40757e+01" mtime="7.30331e+01" gflop="0.00000e+00" gbyte="3.77975e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.30331e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003714ab5537143714be" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09925e+02" utime="8.94207e+01" stime="1.40685e+01" mtime="7.30331e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.30331e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2107e+09" > 6.0814e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2144e+09" > 2.9906e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4320e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9764e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0054e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.9141e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5783e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.5914e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0605e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3056e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4524e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="206" >
<hent key="02400100000000000000080000000003" call="MPI_Isend" bytes="2048" orank="3" region="0" commid="0" count="7" tid="0" op="" dtype="" >9.7752e-06 9.5367e-07 1.9073e-06</hent>
<hent key="0240010000000000000008000000000B" call="MPI_Isend" bytes="2048" orank="11" region="0" commid="0" count="3" tid="0" op="" dtype="" >5.9605e-06 1.9073e-06 2.1458e-06</hent>
<hent key="0240010000000000000008000000001F" call="MPI_Isend" bytes="2048" orank="31" region="0" commid="0" count="125" tid="0" op="" dtype="" >6.9332e-04 4.7684e-06 6.9141e-06</hent>
<hent key="024001000000000000000800000002A7" call="MPI_Isend" bytes="2048" orank="679" region="0" commid="0" count="165" tid="0" op="" dtype="" >7.8440e-04 3.8147e-06 6.1989e-06</hent>
<hent key="038001000000000000000E0000000004" call="MPI_Irecv" bytes="3584" orank="4" region="0" commid="0" count="123" tid="0" op="" dtype="" >1.8120e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000006" call="MPI_Irecv" bytes="3584" orank="6" region="0" commid="0" count="51" tid="0" op="" dtype="" >2.3842e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E000000001F" call="MPI_Irecv" bytes="3584" orank="31" region="0" commid="0" count="145" tid="0" op="" dtype="" >4.6253e-05 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000000E00000002A7" call="MPI_Irecv" bytes="3584" orank="679" region="0" commid="0" count="111" tid="0" op="" dtype="" >3.7432e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000E0000000004" call="MPI_Isend" bytes="3584" orank="4" region="0" commid="0" count="153" tid="0" op="" dtype="" >4.1556e-04 9.5367e-07 1.8120e-05</hent>
<hent key="024001000000000000000E0000000006" call="MPI_Isend" bytes="3584" orank="6" region="0" commid="0" count="40" tid="0" op="" dtype="" >7.3910e-05 9.5367e-07 2.8610e-06</hent>
<hent key="024001000000000000000E000000001F" call="MPI_Isend" bytes="3584" orank="31" region="0" commid="0" count="138" tid="0" op="" dtype="" >8.3995e-04 5.0068e-06 1.8120e-05</hent>
<hent key="024001000000000000000E00000002A7" call="MPI_Isend" bytes="3584" orank="679" region="0" commid="0" count="148" tid="0" op="" dtype="" >7.7391e-04 3.8147e-06 1.0014e-05</hent>
<hent key="03800100000000000000028000000003" call="MPI_Irecv" bytes="640" orank="3" region="0" commid="0" count="411" tid="0" op="" dtype="" >8.3208e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000004" call="MPI_Irecv" bytes="640" orank="4" region="0" commid="0" count="381" tid="0" op="" dtype="" >8.9645e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000006" call="MPI_Irecv" bytes="640" orank="6" region="0" commid="0" count="372" tid="0" op="" dtype="" >1.4925e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000B" call="MPI_Irecv" bytes="640" orank="11" region="0" commid="0" count="430" tid="0" op="" dtype="" >1.8716e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000001F" call="MPI_Irecv" bytes="640" orank="31" region="0" commid="0" count="24" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000014000000001F" call="MPI_Irecv" bytes="5120" orank="31" region="0" commid="0" count="950" tid="0" op="" dtype="" >1.5855e-04 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000000280000002A7" call="MPI_Irecv" bytes="640" orank="679" region="0" commid="0" count="19" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="038001000000000000001400000002A7" call="MPI_Irecv" bytes="5120" orank="679" region="0" commid="0" count="808" tid="0" op="" dtype="" >2.0480e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000A0000000001F" call="MPI_Irecv" bytes="40960" orank="31" region="0" commid="0" count="3311" tid="0" op="" dtype="" >8.2970e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000028000000003" call="MPI_Isend" bytes="640" orank="3" region="0" commid="0" count="441" tid="0" op="" dtype="" >4.9090e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000028000000004" call="MPI_Isend" bytes="640" orank="4" region="0" commid="0" count="393" tid="0" op="" dtype="" >1.2228e-03 1.9073e-06 3.0994e-05</hent>
<hent key="02400100000000000000028000000006" call="MPI_Isend" bytes="640" orank="6" region="0" commid="0" count="412" tid="0" op="" dtype="" >5.7340e-04 9.5367e-07 5.0068e-06</hent>
<hent key="0240010000000000000002800000000B" call="MPI_Isend" bytes="640" orank="11" region="0" commid="0" count="407" tid="0" op="" dtype="" >5.1141e-04 9.5367e-07 1.5020e-05</hent>
<hent key="0240010000000000000002800000001F" call="MPI_Isend" bytes="640" orank="31" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.2946e-04 3.8147e-06 5.9605e-06</hent>
<hent key="03800100000000000000A000000002A7" call="MPI_Irecv" bytes="40960" orank="679" region="0" commid="0" count="2821" tid="0" op="" dtype="" >9.5248e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000014000000001F" call="MPI_Isend" bytes="5120" orank="31" region="0" commid="0" count="890" tid="0" op="" dtype="" >3.5105e-03 1.1921e-06 1.3113e-05</hent>
<hent key="024001000000000000000280000002A7" call="MPI_Isend" bytes="640" orank="679" region="0" commid="0" count="20" tid="0" op="" dtype="" >9.0837e-05 3.8147e-06 5.9605e-06</hent>
<hent key="0380010000000000000080000000001F" call="MPI_Irecv" bytes="32768" orank="31" region="0" commid="0" count="9389" tid="0" op="" dtype="" >2.5394e-03 0.0000e+00 1.4067e-05</hent>
<hent key="024001000000000000001400000002A7" call="MPI_Isend" bytes="5120" orank="679" region="0" commid="0" count="788" tid="0" op="" dtype="" >2.8923e-03 9.5367e-07 9.0599e-06</hent>
<hent key="038001000000000000008000000002A7" call="MPI_Irecv" bytes="32768" orank="679" region="0" commid="0" count="9879" tid="0" op="" dtype="" >3.2439e-03 0.0000e+00 1.0967e-05</hent>
<hent key="03800100000000000000014000000003" call="MPI_Irecv" bytes="320" orank="3" region="0" commid="0" count="442" tid="0" op="" dtype="" >9.8228e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000004" call="MPI_Irecv" bytes="320" orank="4" region="0" commid="0" count="539" tid="0" op="" dtype="" >1.4257e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000006" call="MPI_Irecv" bytes="320" orank="6" region="0" commid="0" count="553" tid="0" op="" dtype="" >2.3842e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000001400000000B" call="MPI_Irecv" bytes="320" orank="11" region="0" commid="0" count="439" tid="0" op="" dtype="" >1.5640e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000001F" call="MPI_Irecv" bytes="320" orank="31" region="0" commid="0" count="20" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A0000000001F" call="MPI_Isend" bytes="40960" orank="31" region="0" commid="0" count="3151" tid="0" op="" dtype="" >6.2854e-02 9.0599e-06 4.0054e-05</hent>
<hent key="038001000000000000000140000002A7" call="MPI_Irecv" bytes="320" orank="679" region="0" commid="0" count="21" tid="0" op="" dtype="" >7.3910e-06 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000A000000002A7" call="MPI_Isend" bytes="40960" orank="679" region="0" commid="0" count="2712" tid="0" op="" dtype="" >3.0342e-02 7.1526e-06 3.1948e-05</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >6.4320e+00 0.0000e+00 1.1091e-01</hent>
<hent key="0240010000000000000080000000001F" call="MPI_Isend" bytes="32768" orank="31" region="0" commid="0" count="9549" tid="0" op="" dtype="" >1.8581e-01 8.8215e-06 1.7405e-04</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-05 0.0000e+00 1.8120e-05</hent>
<hent key="024001000000000000008000000002A7" call="MPI_Isend" bytes="32768" orank="679" region="0" commid="0" count="9988" tid="0" op="" dtype="" >1.0981e-01 6.9141e-06 4.9114e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000003" call="MPI_Isend" bytes="320" orank="3" region="0" commid="0" count="440" tid="0" op="" dtype="" >4.2152e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000014000000004" call="MPI_Isend" bytes="320" orank="4" region="0" commid="0" count="575" tid="0" op="" dtype="" >1.5638e-03 1.9073e-06 1.0967e-05</hent>
<hent key="02400100000000000000014000000006" call="MPI_Isend" bytes="320" orank="6" region="0" commid="0" count="556" tid="0" op="" dtype="" >6.1417e-04 0.0000e+00 8.8215e-06</hent>
<hent key="0240010000000000000001400000000B" call="MPI_Isend" bytes="320" orank="11" region="0" commid="0" count="452" tid="0" op="" dtype="" >4.5943e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000001400000001F" call="MPI_Isend" bytes="320" orank="31" region="0" commid="0" count="20" tid="0" op="" dtype="" >8.9407e-05 3.8147e-06 5.0068e-06</hent>
<hent key="024001000000000000000140000002A7" call="MPI_Isend" bytes="320" orank="679" region="0" commid="0" count="20" tid="0" op="" dtype="" >8.4877e-05 2.8610e-06 5.9605e-06</hent>
<hent key="03800100000000000000000000000003" call="MPI_Irecv" bytes="0" orank="3" region="0" commid="0" count="344" tid="0" op="" dtype="" >7.3195e-05 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000000000004" call="MPI_Irecv" bytes="0" orank="4" region="0" commid="0" count="561" tid="0" op="" dtype="" >1.3161e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000006" call="MPI_Irecv" bytes="0" orank="6" region="0" commid="0" count="538" tid="0" op="" dtype="" >1.9908e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000000000000000B" call="MPI_Irecv" bytes="0" orank="11" region="0" commid="0" count="319" tid="0" op="" dtype="" >1.0991e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000001F" call="MPI_Irecv" bytes="0" orank="31" region="0" commid="0" count="86" tid="0" op="" dtype="" >2.8849e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002A7" call="MPI_Irecv" bytes="0" orank="679" region="0" commid="0" count="85" tid="0" op="" dtype="" >2.4319e-05 0.0000e+00 2.1458e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.2626e-04 0.0000e+00 8.2970e-05</hent>
<hent key="03800100000000000000030000000003" call="MPI_Irecv" bytes="768" orank="3" region="0" commid="0" count="890" tid="0" op="" dtype="" >1.3566e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003000000000B" call="MPI_Irecv" bytes="768" orank="11" region="0" commid="0" count="574" tid="0" op="" dtype="" >1.2469e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000000000003" call="MPI_Isend" bytes="0" orank="3" region="0" commid="0" count="331" tid="0" op="" dtype="" >2.1386e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000000000004" call="MPI_Isend" bytes="0" orank="4" region="0" commid="0" count="531" tid="0" op="" dtype="" >1.0912e-03 9.5367e-07 5.4121e-05</hent>
<hent key="02400100000000000000000000000006" call="MPI_Isend" bytes="0" orank="6" region="0" commid="0" count="507" tid="0" op="" dtype="" >3.2902e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000000000000B" call="MPI_Isend" bytes="0" orank="11" region="0" commid="0" count="343" tid="0" op="" dtype="" >2.6298e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000000000000001F" call="MPI_Isend" bytes="0" orank="31" region="0" commid="0" count="88" tid="0" op="" dtype="" >3.3784e-04 9.5367e-07 5.9605e-06</hent>
<hent key="024001000000000000000000000002A7" call="MPI_Isend" bytes="0" orank="679" region="0" commid="0" count="83" tid="0" op="" dtype="" >3.1495e-04 2.1458e-06 5.0068e-06</hent>
<hent key="03800100000000000000060000000003" call="MPI_Irecv" bytes="1536" orank="3" region="0" commid="0" count="70" tid="0" op="" dtype="" >1.9312e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000004" call="MPI_Irecv" bytes="1536" orank="4" region="0" commid="0" count="16" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000060000000006" call="MPI_Irecv" bytes="1536" orank="6" region="0" commid="0" count="17" tid="0" op="" dtype="" >9.5367e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000B" call="MPI_Irecv" bytes="1536" orank="11" region="0" commid="0" count="59" tid="0" op="" dtype="" >2.3842e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000001F" call="MPI_Irecv" bytes="1536" orank="31" region="0" commid="0" count="63" tid="0" op="" dtype="" >2.2173e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002A7" call="MPI_Irecv" bytes="1536" orank="679" region="0" commid="0" count="91" tid="0" op="" dtype="" >3.0756e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000030000000003" call="MPI_Isend" bytes="768" orank="3" region="0" commid="0" count="954" tid="0" op="" dtype="" >5.5981e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000003000000000B" call="MPI_Isend" bytes="768" orank="11" region="0" commid="0" count="820" tid="0" op="" dtype="" >4.2105e-04 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 2.1458e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C000000004" call="MPI_Irecv" bytes="448" orank="4" region="0" commid="0" count="34" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000006" call="MPI_Irecv" bytes="448" orank="6" region="0" commid="0" count="14" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000003" call="MPI_Isend" bytes="1536" orank="3" region="0" commid="0" count="58" tid="0" op="" dtype="" >9.5606e-05 9.5367e-07 2.8610e-06</hent>
<hent key="02400100000000000000060000000004" call="MPI_Isend" bytes="1536" orank="4" region="0" commid="0" count="12" tid="0" op="" dtype="" >5.0068e-05 2.8610e-06 1.2159e-05</hent>
<hent key="02400100000000000000060000000006" call="MPI_Isend" bytes="1536" orank="6" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.6212e-05 1.1921e-06 2.1458e-06</hent>
<hent key="0240010000000000000006000000000B" call="MPI_Isend" bytes="1536" orank="11" region="0" commid="0" count="47" tid="0" op="" dtype="" >8.2016e-05 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000006000000001F" call="MPI_Isend" bytes="1536" orank="31" region="0" commid="0" count="72" tid="0" op="" dtype="" >3.8362e-04 4.7684e-06 7.1526e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002A7" call="MPI_Isend" bytes="1536" orank="679" region="0" commid="0" count="79" tid="0" op="" dtype="" >3.6097e-04 3.8147e-06 5.0068e-06</hent>
<hent key="038001000000000000000C000000001F" call="MPI_Irecv" bytes="3072" orank="31" region="0" commid="0" count="333" tid="0" op="" dtype="" >1.0157e-04 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002A7" call="MPI_Irecv" bytes="3072" orank="679" region="0" commid="0" count="378" tid="0" op="" dtype="" >1.3328e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000001C000000004" call="MPI_Isend" bytes="448" orank="4" region="0" commid="0" count="46" tid="0" op="" dtype="" >3.0279e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C000000006" call="MPI_Isend" bytes="448" orank="6" region="0" commid="0" count="10" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C000000001F" call="MPI_Isend" bytes="3072" orank="31" region="0" commid="0" count="334" tid="0" op="" dtype="" >1.9970e-03 4.7684e-06 1.3113e-05</hent>
<hent key="024001000000000000000C00000002A7" call="MPI_Isend" bytes="3072" orank="679" region="0" commid="0" count="351" tid="0" op="" dtype="" >1.7674e-03 3.8147e-06 6.1989e-06</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.8624e-04 1.2803e-04 1.3018e-04</hent>
<hent key="03800100000000000000038000000003" call="MPI_Irecv" bytes="896" orank="3" region="0" commid="0" count="2792" tid="0" op="" dtype="" >3.9768e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000038000000004" call="MPI_Irecv" bytes="896" orank="4" region="0" commid="0" count="145" tid="0" op="" dtype="" >3.4332e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000006" call="MPI_Irecv" bytes="896" orank="6" region="0" commid="0" count="157" tid="0" op="" dtype="" >7.5579e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000B" call="MPI_Irecv" bytes="896" orank="11" region="0" commid="0" count="3117" tid="0" op="" dtype="" >6.8378e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000003800000001F" call="MPI_Irecv" bytes="896" orank="31" region="0" commid="0" count="31" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002A7" call="MPI_Irecv" bytes="896" orank="679" region="0" commid="0" count="25" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.6703e-04 2.6703e-04 2.6703e-04</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="03800100000000000000180000000003" call="MPI_Irecv" bytes="6144" orank="3" region="0" commid="0" count="3227" tid="0" op="" dtype="" >5.2762e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000018000000000B" call="MPI_Irecv" bytes="6144" orank="11" region="0" commid="0" count="2121" tid="0" op="" dtype="" >4.1127e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000018000000001F" call="MPI_Irecv" bytes="6144" orank="31" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000001800000002A7" call="MPI_Irecv" bytes="6144" orank="679" region="0" commid="0" count="13" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000038000000003" call="MPI_Isend" bytes="896" orank="3" region="0" commid="0" count="2704" tid="0" op="" dtype="" >1.6866e-03 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000038000000004" call="MPI_Isend" bytes="896" orank="4" region="0" commid="0" count="141" tid="0" op="" dtype="" >4.6802e-04 2.8610e-06 4.0531e-06</hent>
<hent key="02400100000000000000038000000006" call="MPI_Isend" bytes="896" orank="6" region="0" commid="0" count="160" tid="0" op="" dtype="" >2.8467e-04 9.5367e-07 9.0599e-06</hent>
<hent key="0240010000000000000003800000000B" call="MPI_Isend" bytes="896" orank="11" region="0" commid="0" count="2855" tid="0" op="" dtype="" >1.7934e-03 0.0000e+00 1.6212e-05</hent>
<hent key="0240010000000000000003800000001F" call="MPI_Isend" bytes="896" orank="31" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.8525e-04 4.0531e-06 5.9605e-06</hent>
<hent key="024001000000000000000380000002A7" call="MPI_Isend" bytes="896" orank="679" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.2612e-04 3.8147e-06 5.0068e-06</hent>
<hent key="02400100000000000000180000000003" call="MPI_Isend" bytes="6144" orank="3" region="0" commid="0" count="3642" tid="0" op="" dtype="" >6.7799e-03 9.5367e-07 2.2888e-05</hent>
<hent key="0240010000000000000018000000000B" call="MPI_Isend" bytes="6144" orank="11" region="0" commid="0" count="3050" tid="0" op="" dtype="" >5.8451e-03 9.5367e-07 1.7166e-05</hent>
<hent key="0240010000000000000018000000001F" call="MPI_Isend" bytes="6144" orank="31" region="0" commid="0" count="12" tid="0" op="" dtype="" >8.9169e-05 6.9141e-06 8.1062e-06</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.9141e-05 6.9141e-05 6.9141e-05</hent>
<hent key="024001000000000000001800000002A7" call="MPI_Isend" bytes="6144" orank="679" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.3631e-05 5.9605e-06 6.9141e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >9.4466e+00 4.4823e-05 1.2667e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.4805e-04 8.4805e-04 8.4805e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.1989e-06 6.1989e-06 6.1989e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0605e-02 1.0605e-02 1.0605e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >4.6659e-03 4.6659e-03 4.6659e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.9949e-01 3.3362e-03 1.9005e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.7006e-04 5.7006e-04 5.7006e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5783e+00 3.8004e-04 2.4762e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >4.0054e-05 0.0000e+00 1.9073e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="03800100000000000000070000000003" call="MPI_Irecv" bytes="1792" orank="3" region="0" commid="0" count="16" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000004" call="MPI_Irecv" bytes="1792" orank="4" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000006" call="MPI_Irecv" bytes="1792" orank="6" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000B" call="MPI_Irecv" bytes="1792" orank="11" region="0" commid="0" count="18" tid="0" op="" dtype="" >9.2983e-06 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000007000000001F" call="MPI_Irecv" bytes="1792" orank="31" region="0" commid="0" count="106" tid="0" op="" dtype="" >4.0531e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002A7" call="MPI_Irecv" bytes="1792" orank="679" region="0" commid="0" count="122" tid="0" op="" dtype="" >4.3392e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >8.4400e-05 0.0000e+00 6.2943e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9764e+01 0.0000e+00 3.8429e+01</hent>
<hent key="038001000000000000000A000000000B" call="MPI_Irecv" bytes="2560" orank="11" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A000000001F" call="MPI_Irecv" bytes="2560" orank="31" region="0" commid="0" count="411" tid="0" op="" dtype="" >1.2207e-04 0.0000e+00 6.1989e-06</hent>
<hent key="038001000000000000000A00000002A7" call="MPI_Irecv" bytes="2560" orank="679" region="0" commid="0" count="364" tid="0" op="" dtype="" >1.1230e-04 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000070000000003" call="MPI_Isend" bytes="1792" orank="3" region="0" commid="0" count="29" tid="0" op="" dtype="" >4.9829e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000004" call="MPI_Isend" bytes="1792" orank="4" region="0" commid="0" count="1" tid="0" op="" dtype="" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000006" call="MPI_Isend" bytes="1792" orank="6" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.1683e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000007000000000B" call="MPI_Isend" bytes="1792" orank="11" region="0" commid="0" count="16" tid="0" op="" dtype="" >3.9816e-05 9.5367e-07 1.1921e-05</hent>
<hent key="0240010000000000000007000000001F" call="MPI_Isend" bytes="1792" orank="31" region="0" commid="0" count="131" tid="0" op="" dtype="" >7.1716e-04 4.7684e-06 6.9141e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3056e-02 8.3056e-02 8.3056e-02</hent>
<hent key="024001000000000000000700000002A7" call="MPI_Isend" bytes="1792" orank="679" region="0" commid="0" count="116" tid="0" op="" dtype="" >5.4455e-04 3.8147e-06 6.1989e-06</hent>
<hent key="024001000000000000000A0000000003" call="MPI_Isend" bytes="2560" orank="3" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000A000000000B" call="MPI_Isend" bytes="2560" orank="11" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="024001000000000000000A000000001F" call="MPI_Isend" bytes="2560" orank="31" region="0" commid="0" count="373" tid="0" op="" dtype="" >2.1536e-03 4.7684e-06 2.4080e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.7571e-04 1.6928e-05 8.4877e-05</hent>
<hent key="024001000000000000000A00000002A7" call="MPI_Isend" bytes="2560" orank="679" region="0" commid="0" count="366" tid="0" op="" dtype="" >1.8091e-03 3.8147e-06 2.8133e-05</hent>
<hent key="03800100000000000000100000000004" call="MPI_Irecv" bytes="4096" orank="4" region="0" commid="0" count="12577" tid="0" op="" dtype="" >2.0421e-03 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000100000000006" call="MPI_Irecv" bytes="4096" orank="6" region="0" commid="0" count="12649" tid="0" op="" dtype="" >4.7696e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000010000000001F" call="MPI_Irecv" bytes="4096" orank="31" region="0" commid="0" count="2728" tid="0" op="" dtype="" >4.6468e-04 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000001000000002A7" call="MPI_Irecv" bytes="4096" orank="679" region="0" commid="0" count="2876" tid="0" op="" dtype="" >7.5889e-04 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000100000000004" call="MPI_Isend" bytes="4096" orank="4" region="0" commid="0" count="12547" tid="0" op="" dtype="" >3.4016e-02 9.5367e-07 1.9789e-05</hent>
<hent key="02400100000000000000100000000006" call="MPI_Isend" bytes="4096" orank="6" region="0" commid="0" count="12660" tid="0" op="" dtype="" >2.2424e-02 0.0000e+00 1.9073e-05</hent>
<hent key="0240010000000000000010000000001F" call="MPI_Isend" bytes="4096" orank="31" region="0" commid="0" count="2805" tid="0" op="" dtype="" >1.1329e-02 9.5367e-07 1.3828e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.7220e-04 2.1005e-04 3.6216e-04</hent>
<hent key="024001000000000000001000000002A7" call="MPI_Isend" bytes="4096" orank="679" region="0" commid="0" count="2896" tid="0" op="" dtype="" >1.0519e-02 9.5367e-07 2.7895e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.4135e-05 6.4135e-05 6.4135e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.2612e-03 1.7095e-04 4.2915e-04</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >8.8215e-06 1.9073e-06 6.9141e-06</hent>
<hent key="03800100000000000000000400000003" call="MPI_Irecv" bytes="4" orank="3" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.2534e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000004" call="MPI_Irecv" bytes="4" orank="4" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.8556e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000006" call="MPI_Irecv" bytes="4" orank="6" region="0" commid="0" count="3400" tid="0" op="" dtype="" >9.8419e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000000040000000B" call="MPI_Irecv" bytes="4" orank="11" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.0773e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000001F" call="MPI_Irecv" bytes="4" orank="31" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.0749e-04 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000001C0000000003" call="MPI_Irecv" bytes="7168" orank="3" region="0" commid="0" count="9473" tid="0" op="" dtype="" >1.4987e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000001C000000000B" call="MPI_Irecv" bytes="7168" orank="11" region="0" commid="0" count="10579" tid="0" op="" dtype="" >2.1663e-03 0.0000e+00 8.1062e-06</hent>
<hent key="038001000000000000000004000002A7" call="MPI_Irecv" bytes="4" orank="679" region="0" commid="0" count="3400" tid="0" op="" dtype="" >7.6008e-04 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000001C00000002A7" call="MPI_Irecv" bytes="7168" orank="679" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="02400100000000000000000400000003" call="MPI_Isend" bytes="4" orank="3" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.3969e-03 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000000400000004" call="MPI_Isend" bytes="4" orank="4" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.8596e-03 0.0000e+00 6.9141e-05</hent>
<hent key="02400100000000000000000400000006" call="MPI_Isend" bytes="4" orank="6" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.5731e-03 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000040000000B" call="MPI_Isend" bytes="4" orank="11" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.7760e-03 0.0000e+00 2.9802e-05</hent>
<hent key="0240010000000000000000040000001F" call="MPI_Isend" bytes="4" orank="31" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.4932e-02 4.0531e-06 2.7204e-04</hent>
<hent key="024001000000000000001C0000000003" call="MPI_Isend" bytes="7168" orank="3" region="0" commid="0" count="9058" tid="0" op="" dtype="" >1.7356e-02 9.5367e-07 2.3842e-05</hent>
<hent key="024001000000000000001C000000000B" call="MPI_Isend" bytes="7168" orank="11" region="0" commid="0" count="9650" tid="0" op="" dtype="" >1.9054e-02 9.5367e-07 1.9073e-05</hent>
<hent key="024001000000000000000004000002A7" call="MPI_Isend" bytes="4" orank="679" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.9850e-02 3.8147e-06 6.6042e-05</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9503e-04 1.9503e-04 1.9503e-04</hent>
<hent key="03800100000000000000020000000004" call="MPI_Irecv" bytes="512" orank="4" region="0" commid="0" count="3366" tid="0" op="" dtype="" >5.7364e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000020000000006" call="MPI_Irecv" bytes="512" orank="6" region="0" commid="0" count="3386" tid="0" op="" dtype="" >5.4026e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000050000000003" call="MPI_Irecv" bytes="1280" orank="3" region="0" commid="0" count="130" tid="0" op="" dtype="" >2.2888e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000004" call="MPI_Irecv" bytes="1280" orank="4" region="0" commid="0" count="52" tid="0" op="" dtype="" >1.1444e-05 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000050000000006" call="MPI_Irecv" bytes="1280" orank="6" region="0" commid="0" count="56" tid="0" op="" dtype="" >2.2888e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000B" call="MPI_Irecv" bytes="1280" orank="11" region="0" commid="0" count="138" tid="0" op="" dtype="" >6.5327e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000005000000001F" call="MPI_Irecv" bytes="1280" orank="31" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.2636e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4872e+01 1.5974e-05 1.8457e-01</hent>
<hent key="038001000000000000000500000002A7" call="MPI_Irecv" bytes="1280" orank="679" region="0" commid="0" count="48" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000020000000004" call="MPI_Isend" bytes="512" orank="4" region="0" commid="0" count="3354" tid="0" op="" dtype="" >2.2159e-03 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000020000000006" call="MPI_Isend" bytes="512" orank="6" region="0" commid="0" count="3390" tid="0" op="" dtype="" >1.7052e-03 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000080000000003" call="MPI_Irecv" bytes="2048" orank="3" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000006" call="MPI_Irecv" bytes="2048" orank="6" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000008000000000B" call="MPI_Irecv" bytes="2048" orank="11" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000001F" call="MPI_Irecv" bytes="2048" orank="31" region="0" commid="0" count="155" tid="0" op="" dtype="" >3.7909e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002A7" call="MPI_Irecv" bytes="2048" orank="679" region="0" commid="0" count="138" tid="0" op="" dtype="" >5.1498e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000050000000003" call="MPI_Isend" bytes="1280" orank="3" region="0" commid="0" count="135" tid="0" op="" dtype="" >1.9097e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000050000000004" call="MPI_Isend" bytes="1280" orank="4" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.8263e-04 2.8610e-06 9.0599e-06</hent>
<hent key="02400100000000000000050000000006" call="MPI_Isend" bytes="1280" orank="6" region="0" commid="0" count="51" tid="0" op="" dtype="" >9.0122e-05 9.5367e-07 2.8610e-06</hent>
<hent key="0240010000000000000005000000000B" call="MPI_Isend" bytes="1280" orank="11" region="0" commid="0" count="156" tid="0" op="" dtype="" >2.4390e-04 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000005000000001F" call="MPI_Isend" bytes="1280" orank="31" region="0" commid="0" count="48" tid="0" op="" dtype="" >2.4891e-04 3.8147e-06 6.1989e-06</hent>
<hent key="024001000000000000000500000002A7" call="MPI_Isend" bytes="1280" orank="679" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.5044e-04 3.8147e-06 6.1989e-06</hent>
</hash>
<internal rank="7" log_i="1724765674.530225" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="8" mpi_size="696" stamp_init="1724765564.466059" stamp_final="1724765674.532337" username="apac4" allocationname="unknown" flags="0" pid="1612125" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10066e+02" utime="8.82258e+01" stime="1.41743e+01" mtime="7.21229e+01" gflop="0.00000e+00" gbyte="3.76701e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21229e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4451446144714355547144714a0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09929e+02" utime="8.81961e+01" stime="1.41617e+01" mtime="7.21229e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21229e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1930e+09" > 7.1034e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1836e+09" > 3.4499e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3357e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9791e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.9730e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.8610e-06 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5782e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.0518e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0586e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3099e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3572e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="206" >
<hent key="02400100000000000000080000000004" call="MPI_Isend" bytes="2048" orank="4" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="02400100000000000000080000000009" call="MPI_Isend" bytes="2048" orank="9" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.1921e-05 3.8147e-06 4.0531e-06</hent>
<hent key="0240010000000000000008000000000B" call="MPI_Isend" bytes="2048" orank="11" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="0240010000000000000008000000000C" call="MPI_Isend" bytes="2048" orank="12" region="0" commid="0" count="2" tid="0" op="" dtype="" >3.8147e-06 9.5367e-07 2.8610e-06</hent>
<hent key="02400100000000000000080000000020" call="MPI_Isend" bytes="2048" orank="32" region="0" commid="0" count="168" tid="0" op="" dtype="" >1.1783e-03 4.7684e-06 1.7881e-05</hent>
<hent key="024001000000000000000800000002A8" call="MPI_Isend" bytes="2048" orank="680" region="0" commid="0" count="147" tid="0" op="" dtype="" >8.2588e-04 3.8147e-06 1.6212e-05</hent>
<hent key="038001000000000000000E000000000B" call="MPI_Irecv" bytes="3584" orank="11" region="0" commid="0" count="60" tid="0" op="" dtype="" >2.8133e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000020" call="MPI_Irecv" bytes="3584" orank="32" region="0" commid="0" count="137" tid="0" op="" dtype="" >8.0347e-05 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000E00000002A8" call="MPI_Irecv" bytes="3584" orank="680" region="0" commid="0" count="139" tid="0" op="" dtype="" >5.3406e-05 0.0000e+00 8.1062e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="024001000000000000000E0000000009" call="MPI_Isend" bytes="3584" orank="9" region="0" commid="0" count="48" tid="0" op="" dtype="" >1.3685e-04 9.5367e-07 1.0967e-05</hent>
<hent key="024001000000000000000E000000000B" call="MPI_Isend" bytes="3584" orank="11" region="0" commid="0" count="120" tid="0" op="" dtype="" >1.9312e-04 0.0000e+00 2.8610e-06</hent>
<hent key="024001000000000000000E0000000020" call="MPI_Isend" bytes="3584" orank="32" region="0" commid="0" count="126" tid="0" op="" dtype="" >1.0066e-03 5.0068e-06 2.0981e-05</hent>
<hent key="024001000000000000000E00000002A8" call="MPI_Isend" bytes="3584" orank="680" region="0" commid="0" count="120" tid="0" op="" dtype="" >6.7973e-04 3.8147e-06 1.1921e-05</hent>
<hent key="03800100000000000000028000000004" call="MPI_Irecv" bytes="640" orank="4" region="0" commid="0" count="410" tid="0" op="" dtype="" >7.1526e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000009" call="MPI_Irecv" bytes="640" orank="9" region="0" commid="0" count="348" tid="0" op="" dtype="" >7.5817e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000B" call="MPI_Irecv" bytes="640" orank="11" region="0" commid="0" count="370" tid="0" op="" dtype="" >1.6952e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000C" call="MPI_Irecv" bytes="640" orank="12" region="0" commid="0" count="434" tid="0" op="" dtype="" >9.8944e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000020" call="MPI_Irecv" bytes="640" orank="32" region="0" commid="0" count="28" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000140000000020" call="MPI_Irecv" bytes="5120" orank="32" region="0" commid="0" count="459" tid="0" op="" dtype="" >1.3995e-04 0.0000e+00 1.0967e-05</hent>
<hent key="038001000000000000000280000002A8" call="MPI_Irecv" bytes="640" orank="680" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.0252e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000001400000002A8" call="MPI_Irecv" bytes="5120" orank="680" region="0" commid="0" count="354" tid="0" op="" dtype="" >1.3638e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000A00000000020" call="MPI_Irecv" bytes="40960" orank="32" region="0" commid="0" count="1424" tid="0" op="" dtype="" >6.4921e-04 0.0000e+00 2.0981e-05</hent>
<hent key="02400100000000000000028000000004" call="MPI_Isend" bytes="640" orank="4" region="0" commid="0" count="417" tid="0" op="" dtype="" >4.4894e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000028000000009" call="MPI_Isend" bytes="640" orank="9" region="0" commid="0" count="382" tid="0" op="" dtype="" >1.2496e-03 1.9073e-06 1.0967e-05</hent>
<hent key="0240010000000000000002800000000B" call="MPI_Isend" bytes="640" orank="11" region="0" commid="0" count="410" tid="0" op="" dtype="" >6.4826e-04 9.5367e-07 2.0981e-05</hent>
<hent key="0240010000000000000002800000000C" call="MPI_Isend" bytes="640" orank="12" region="0" commid="0" count="412" tid="0" op="" dtype="" >4.8375e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000028000000020" call="MPI_Isend" bytes="640" orank="32" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.2302e-04 4.0531e-06 9.7752e-06</hent>
<hent key="03800100000000000000A000000002A8" call="MPI_Irecv" bytes="40960" orank="680" region="0" commid="0" count="1080" tid="0" op="" dtype="" >4.0865e-04 0.0000e+00 2.6941e-05</hent>
<hent key="02400100000000000000140000000020" call="MPI_Isend" bytes="5120" orank="32" region="0" commid="0" count="507" tid="0" op="" dtype="" >2.9197e-03 1.9073e-06 6.2943e-05</hent>
<hent key="024001000000000000000280000002A8" call="MPI_Isend" bytes="640" orank="680" region="0" commid="0" count="26" tid="0" op="" dtype="" >1.4186e-04 3.8147e-06 1.2875e-05</hent>
<hent key="03800100000000000000800000000020" call="MPI_Irecv" bytes="32768" orank="32" region="0" commid="0" count="11276" tid="0" op="" dtype="" >5.0664e-03 0.0000e+00 4.1962e-05</hent>
<hent key="024001000000000000001400000002A8" call="MPI_Isend" bytes="5120" orank="680" region="0" commid="0" count="576" tid="0" op="" dtype="" >2.6820e-03 9.5367e-07 4.9829e-05</hent>
<hent key="038001000000000000008000000002A8" call="MPI_Irecv" bytes="32768" orank="680" region="0" commid="0" count="11620" tid="0" op="" dtype="" >4.3182e-03 0.0000e+00 3.4809e-05</hent>
<hent key="03800100000000000000014000000004" call="MPI_Irecv" bytes="320" orank="4" region="0" commid="0" count="454" tid="0" op="" dtype="" >8.5115e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000009" call="MPI_Irecv" bytes="320" orank="9" region="0" commid="0" count="573" tid="0" op="" dtype="" >1.5497e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000B" call="MPI_Irecv" bytes="320" orank="11" region="0" commid="0" count="540" tid="0" op="" dtype="" >2.5940e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000001400000000C" call="MPI_Irecv" bytes="320" orank="12" region="0" commid="0" count="430" tid="0" op="" dtype="" >9.1314e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000020" call="MPI_Irecv" bytes="320" orank="32" region="0" commid="0" count="21" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A00000000020" call="MPI_Isend" bytes="40960" orank="32" region="0" commid="0" count="1621" tid="0" op="" dtype="" >3.6061e-02 1.0014e-05 6.6042e-05</hent>
<hent key="038001000000000000000140000002A8" call="MPI_Irecv" bytes="320" orank="680" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A000000002A8" call="MPI_Isend" bytes="40960" orank="680" region="0" commid="0" count="1868" tid="0" op="" dtype="" >2.8178e-02 6.9141e-06 1.0395e-04</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >4.7684e-06 9.5367e-07 1.9073e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >6.3357e+00 0.0000e+00 1.1050e-01</hent>
<hent key="02400100000000000000800000000020" call="MPI_Isend" bytes="32768" orank="32" region="0" commid="0" count="11079" tid="0" op="" dtype="" >2.4330e-01 8.1062e-06 1.7381e-04</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.0981e-05 0.0000e+00 2.0027e-05</hent>
<hent key="024001000000000000008000000002A8" call="MPI_Isend" bytes="32768" orank="680" region="0" commid="0" count="10832" tid="0" op="" dtype="" >1.5869e-01 5.9605e-06 1.1992e-04</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000004" call="MPI_Isend" bytes="320" orank="4" region="0" commid="0" count="416" tid="0" op="" dtype="" >3.8290e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000009" call="MPI_Isend" bytes="320" orank="9" region="0" commid="0" count="547" tid="0" op="" dtype="" >1.6644e-03 1.9073e-06 1.3828e-05</hent>
<hent key="0240010000000000000001400000000B" call="MPI_Isend" bytes="320" orank="11" region="0" commid="0" count="573" tid="0" op="" dtype="" >7.1740e-04 9.5367e-07 6.9141e-06</hent>
<hent key="0240010000000000000001400000000C" call="MPI_Isend" bytes="320" orank="12" region="0" commid="0" count="443" tid="0" op="" dtype="" >4.2200e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000014000000020" call="MPI_Isend" bytes="320" orank="32" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.5450e-04 3.8147e-06 1.0967e-05</hent>
<hent key="024001000000000000000140000002A8" call="MPI_Isend" bytes="320" orank="680" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.2136e-04 2.8610e-06 7.1526e-06</hent>
<hent key="03800100000000000000000000000004" call="MPI_Irecv" bytes="0" orank="4" region="0" commid="0" count="317" tid="0" op="" dtype="" >6.3896e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000009" call="MPI_Irecv" bytes="0" orank="9" region="0" commid="0" count="531" tid="0" op="" dtype="" >1.3304e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000B" call="MPI_Irecv" bytes="0" orank="11" region="0" commid="0" count="533" tid="0" op="" dtype="" >2.0099e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000000000000000C" call="MPI_Irecv" bytes="0" orank="12" region="0" commid="0" count="328" tid="0" op="" dtype="" >7.8678e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000020" call="MPI_Irecv" bytes="0" orank="32" region="0" commid="0" count="83" tid="0" op="" dtype="" >2.1696e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002A8" call="MPI_Irecv" bytes="0" orank="680" region="0" commid="0" count="85" tid="0" op="" dtype="" >2.7895e-05 0.0000e+00 5.0068e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.4652e-04 0.0000e+00 8.2016e-05</hent>
<hent key="03800100000000000000030000000004" call="MPI_Irecv" bytes="768" orank="4" region="0" commid="0" count="716" tid="0" op="" dtype="" >9.2745e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003000000000C" call="MPI_Irecv" bytes="768" orank="12" region="0" commid="0" count="776" tid="0" op="" dtype="" >1.5426e-04 0.0000e+00 2.8610e-06</hent>
<hent key="02400100000000000000000000000004" call="MPI_Isend" bytes="0" orank="4" region="0" commid="0" count="341" tid="0" op="" dtype="" >2.0361e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000009" call="MPI_Isend" bytes="0" orank="9" region="0" commid="0" count="524" tid="0" op="" dtype="" >1.2496e-03 9.5367e-07 6.9857e-05</hent>
<hent key="0240010000000000000000000000000B" call="MPI_Isend" bytes="0" orank="11" region="0" commid="0" count="502" tid="0" op="" dtype="" >3.9530e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000000000000C" call="MPI_Isend" bytes="0" orank="12" region="0" commid="0" count="310" tid="0" op="" dtype="" >2.1124e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000000000000020" call="MPI_Isend" bytes="0" orank="32" region="0" commid="0" count="83" tid="0" op="" dtype="" >3.4738e-04 9.5367e-07 8.1062e-06</hent>
<hent key="024001000000000000000000000002A8" call="MPI_Isend" bytes="0" orank="680" region="0" commid="0" count="84" tid="0" op="" dtype="" >3.3021e-04 1.9073e-06 1.0967e-05</hent>
<hent key="03800100000000000000060000000004" call="MPI_Irecv" bytes="1536" orank="4" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000009" call="MPI_Irecv" bytes="1536" orank="9" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000006000000000B" call="MPI_Irecv" bytes="1536" orank="11" region="0" commid="0" count="18" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000C" call="MPI_Irecv" bytes="1536" orank="12" region="0" commid="0" count="46" tid="0" op="" dtype="" >9.5367e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000020" call="MPI_Irecv" bytes="1536" orank="32" region="0" commid="0" count="67" tid="0" op="" dtype="" >3.6001e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000600000002A8" call="MPI_Irecv" bytes="1536" orank="680" region="0" commid="0" count="82" tid="0" op="" dtype="" >3.1948e-05 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000030000000004" call="MPI_Isend" bytes="768" orank="4" region="0" commid="0" count="574" tid="0" op="" dtype="" >2.4295e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000003000000000C" call="MPI_Isend" bytes="768" orank="12" region="0" commid="0" count="670" tid="0" op="" dtype="" >3.9124e-04 0.0000e+00 3.8147e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C00000000B" call="MPI_Irecv" bytes="448" orank="11" region="0" commid="0" count="16" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000004" call="MPI_Isend" bytes="1536" orank="4" region="0" commid="0" count="56" tid="0" op="" dtype="" >8.8453e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000009" call="MPI_Isend" bytes="1536" orank="9" region="0" commid="0" count="15" tid="0" op="" dtype="" >6.5088e-05 2.8610e-06 1.3828e-05</hent>
<hent key="0240010000000000000006000000000B" call="MPI_Isend" bytes="1536" orank="11" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.3828e-05 1.9073e-06 2.1458e-06</hent>
<hent key="0240010000000000000006000000000C" call="MPI_Isend" bytes="1536" orank="12" region="0" commid="0" count="54" tid="0" op="" dtype="" >8.7261e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000020" call="MPI_Isend" bytes="1536" orank="32" region="0" commid="0" count="74" tid="0" op="" dtype="" >5.3215e-04 4.7684e-06 2.0027e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002A8" call="MPI_Isend" bytes="1536" orank="680" region="0" commid="0" count="87" tid="0" op="" dtype="" >4.5824e-04 3.8147e-06 1.5020e-05</hent>
<hent key="038001000000000000000C0000000020" call="MPI_Irecv" bytes="3072" orank="32" region="0" commid="0" count="350" tid="0" op="" dtype="" >1.9217e-04 0.0000e+00 1.3113e-05</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002A8" call="MPI_Irecv" bytes="3072" orank="680" region="0" commid="0" count="338" tid="0" op="" dtype="" >1.0371e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000001C000000009" call="MPI_Isend" bytes="448" orank="9" region="0" commid="0" count="14" tid="0" op="" dtype="" >1.2875e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C00000000B" call="MPI_Isend" bytes="448" orank="11" region="0" commid="0" count="26" tid="0" op="" dtype="" >1.0729e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000020" call="MPI_Isend" bytes="3072" orank="32" region="0" commid="0" count="366" tid="0" op="" dtype="" >2.7983e-03 5.0068e-06 3.5048e-05</hent>
<hent key="024001000000000000000C00000002A8" call="MPI_Isend" bytes="3072" orank="680" region="0" commid="0" count="378" tid="0" op="" dtype="" >2.1601e-03 3.8147e-06 2.2888e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.5037e-04 1.4710e-04 1.5211e-04</hent>
<hent key="03800100000000000000038000000004" call="MPI_Irecv" bytes="896" orank="4" region="0" commid="0" count="2980" tid="0" op="" dtype="" >4.0197e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000038000000009" call="MPI_Irecv" bytes="896" orank="9" region="0" commid="0" count="167" tid="0" op="" dtype="" >4.6253e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000B" call="MPI_Irecv" bytes="896" orank="11" region="0" commid="0" count="177" tid="0" op="" dtype="" >8.2970e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000C" call="MPI_Irecv" bytes="896" orank="12" region="0" commid="0" count="2892" tid="0" op="" dtype="" >6.5875e-04 0.0000e+00 1.9073e-05</hent>
<hent key="03800100000000000000038000000020" call="MPI_Irecv" bytes="896" orank="32" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.4544e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002A8" call="MPI_Irecv" bytes="896" orank="680" region="0" commid="0" count="29" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.0027e-05 2.0027e-05 2.0027e-05</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="03800100000000000000180000000004" call="MPI_Irecv" bytes="6144" orank="4" region="0" commid="0" count="2709" tid="0" op="" dtype="" >2.2507e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000018000000000C" call="MPI_Irecv" bytes="6144" orank="12" region="0" commid="0" count="2895" tid="0" op="" dtype="" >5.1284e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000180000000020" call="MPI_Irecv" bytes="6144" orank="32" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.3842e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001800000002A8" call="MPI_Irecv" bytes="6144" orank="680" region="0" commid="0" count="8" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000038000000004" call="MPI_Isend" bytes="896" orank="4" region="0" commid="0" count="3124" tid="0" op="" dtype="" >1.8587e-03 0.0000e+00 3.6955e-05</hent>
<hent key="02400100000000000000038000000009" call="MPI_Isend" bytes="896" orank="9" region="0" commid="0" count="171" tid="0" op="" dtype="" >6.2013e-04 1.9073e-06 1.4067e-05</hent>
<hent key="0240010000000000000003800000000B" call="MPI_Isend" bytes="896" orank="11" region="0" commid="0" count="154" tid="0" op="" dtype="" >2.8062e-04 9.5367e-07 5.9605e-06</hent>
<hent key="0240010000000000000003800000000C" call="MPI_Isend" bytes="896" orank="12" region="0" commid="0" count="3037" tid="0" op="" dtype="" >1.8811e-03 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000038000000020" call="MPI_Isend" bytes="896" orank="32" region="0" commid="0" count="23" tid="0" op="" dtype="" >1.4329e-04 4.0531e-06 1.0967e-05</hent>
<hent key="024001000000000000000380000002A8" call="MPI_Isend" bytes="896" orank="680" region="0" commid="0" count="23" tid="0" op="" dtype="" >1.1373e-04 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000180000000004" call="MPI_Isend" bytes="6144" orank="4" region="0" commid="0" count="2183" tid="0" op="" dtype="" >4.5285e-03 9.5367e-07 1.2159e-05</hent>
<hent key="0240010000000000000018000000000C" call="MPI_Isend" bytes="6144" orank="12" region="0" commid="0" count="2384" tid="0" op="" dtype="" >4.4935e-03 9.5367e-07 1.2159e-05</hent>
<hent key="02400100000000000000180000000020" call="MPI_Isend" bytes="6144" orank="32" region="0" commid="0" count="16" tid="0" op="" dtype="" >1.7405e-04 5.9605e-06 2.4080e-05</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1935e-05 2.1935e-05 2.1935e-05</hent>
<hent key="024001000000000000001800000002A8" call="MPI_Isend" bytes="6144" orank="680" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.6968e-05 5.9605e-06 7.1526e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.9555e+00 1.0967e-05 1.2668e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.9478e-04 8.9478e-04 8.9478e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0586e-02 1.0586e-02 1.0586e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >4.1368e-03 4.1368e-03 4.1368e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.8403e-01 3.2868e-03 1.7582e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.3116e-04 3.3116e-04 3.3116e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 9.5367e-07 9.5367e-07</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5782e+00 4.0197e-04 2.4793e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >3.9730e-03 1.9073e-06 2.8121e-03</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="03800100000000000000070000000004" call="MPI_Irecv" bytes="1792" orank="4" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000009" call="MPI_Irecv" bytes="1792" orank="9" region="0" commid="0" count="4" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000007000000000B" call="MPI_Irecv" bytes="1792" orank="11" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000007000000000C" call="MPI_Irecv" bytes="1792" orank="12" region="0" commid="0" count="23" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000020" call="MPI_Irecv" bytes="1792" orank="32" region="0" commid="0" count="116" tid="0" op="" dtype="" >6.0797e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000700000002A8" call="MPI_Irecv" bytes="1792" orank="680" region="0" commid="0" count="117" tid="0" op="" dtype="" >4.3154e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >9.3460e-05 0.0000e+00 6.3896e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9791e+01 0.0000e+00 3.8414e+01</hent>
<hent key="038001000000000000000A000000000C" call="MPI_Irecv" bytes="2560" orank="12" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000A0000000020" call="MPI_Irecv" bytes="2560" orank="32" region="0" commid="0" count="371" tid="0" op="" dtype="" >1.7118e-04 0.0000e+00 2.8610e-06</hent>
<hent key="038001000000000000000A00000002A8" call="MPI_Irecv" bytes="2560" orank="680" region="0" commid="0" count="386" tid="0" op="" dtype="" >1.4925e-04 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000070000000004" call="MPI_Isend" bytes="1792" orank="4" region="0" commid="0" count="10" tid="0" op="" dtype="" >1.4544e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000009" call="MPI_Isend" bytes="1792" orank="9" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.2173e-05 4.0531e-06 5.0068e-06</hent>
<hent key="0240010000000000000007000000000B" call="MPI_Isend" bytes="1792" orank="11" region="0" commid="0" count="3" tid="0" op="" dtype="" >5.7220e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0240010000000000000007000000000C" call="MPI_Isend" bytes="1792" orank="12" region="0" commid="0" count="20" tid="0" op="" dtype="" >3.5286e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000020" call="MPI_Isend" bytes="1792" orank="32" region="0" commid="0" count="121" tid="0" op="" dtype="" >8.7357e-04 5.0068e-06 2.0981e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3099e-02 8.3099e-02 8.3099e-02</hent>
<hent key="024001000000000000000700000002A8" call="MPI_Isend" bytes="1792" orank="680" region="0" commid="0" count="93" tid="0" op="" dtype="" >4.8947e-04 3.8147e-06 1.3828e-05</hent>
<hent key="024001000000000000000A000000000C" call="MPI_Isend" bytes="2560" orank="12" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="024001000000000000000A0000000020" call="MPI_Isend" bytes="2560" orank="32" region="0" commid="0" count="345" tid="0" op="" dtype="" >2.6221e-03 5.0068e-06 3.7193e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.2674e-04 2.7895e-05 1.0300e-04</hent>
<hent key="024001000000000000000A00000002A8" call="MPI_Isend" bytes="2560" orank="680" region="0" commid="0" count="368" tid="0" op="" dtype="" >2.0051e-03 3.8147e-06 2.3127e-05</hent>
<hent key="03800100000000000000100000000009" call="MPI_Irecv" bytes="4096" orank="9" region="0" commid="0" count="12700" tid="0" op="" dtype="" >2.0127e-03 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000010000000000B" call="MPI_Irecv" bytes="4096" orank="11" region="0" commid="0" count="12640" tid="0" op="" dtype="" >6.3000e-03 0.0000e+00 1.8120e-05</hent>
<hent key="03800100000000000000100000000020" call="MPI_Irecv" bytes="4096" orank="32" region="0" commid="0" count="3237" tid="0" op="" dtype="" >9.3794e-04 0.0000e+00 2.2888e-05</hent>
<hent key="038001000000000000001000000002A8" call="MPI_Irecv" bytes="4096" orank="680" region="0" commid="0" count="3336" tid="0" op="" dtype="" >1.0099e-03 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000100000000009" call="MPI_Isend" bytes="4096" orank="9" region="0" commid="0" count="12652" tid="0" op="" dtype="" >3.4611e-02 0.0000e+00 3.8862e-05</hent>
<hent key="0240010000000000000010000000000B" call="MPI_Isend" bytes="4096" orank="11" region="0" commid="0" count="12580" tid="0" op="" dtype="" >2.1888e-02 0.0000e+00 1.5974e-05</hent>
<hent key="02400100000000000000100000000020" call="MPI_Isend" bytes="4096" orank="32" region="0" commid="0" count="3179" tid="0" op="" dtype="" >1.7653e-02 1.9073e-06 8.8930e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >6.5994e-04 2.4390e-04 4.1604e-04</hent>
<hent key="024001000000000000001000000002A8" call="MPI_Isend" bytes="4096" orank="680" region="0" commid="0" count="3121" tid="0" op="" dtype="" >1.3892e-02 9.5367e-07 4.1008e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.4851e-03 2.0003e-04 5.3000e-04</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.7752e-06 3.8147e-06 5.9605e-06</hent>
<hent key="03800100000000000000000400000004" call="MPI_Irecv" bytes="4" orank="4" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.6158e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000000400000009" call="MPI_Irecv" bytes="4" orank="9" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.0725e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000000040000000B" call="MPI_Irecv" bytes="4" orank="11" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.0104e-03 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000000040000000C" call="MPI_Irecv" bytes="4" orank="12" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.6958e-04 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000000400000020" call="MPI_Irecv" bytes="4" orank="32" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.0660e-03 0.0000e+00 1.0967e-05</hent>
<hent key="038001000000000000001C0000000004" call="MPI_Irecv" bytes="7168" orank="4" region="0" commid="0" count="9991" tid="0" op="" dtype="" >9.4891e-04 0.0000e+00 2.5988e-05</hent>
<hent key="038001000000000000001C000000000C" call="MPI_Irecv" bytes="7168" orank="12" region="0" commid="0" count="9805" tid="0" op="" dtype="" >1.8737e-03 0.0000e+00 1.3113e-05</hent>
<hent key="038001000000000000001C0000000020" call="MPI_Irecv" bytes="7168" orank="32" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000004000002A8" call="MPI_Irecv" bytes="4" orank="680" region="0" commid="0" count="3400" tid="0" op="" dtype="" >9.0742e-04 0.0000e+00 2.0027e-05</hent>
<hent key="038001000000000000001C00000002A8" call="MPI_Irecv" bytes="7168" orank="680" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="02400100000000000000000400000004" call="MPI_Isend" bytes="4" orank="4" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.1479e-03 0.0000e+00 2.5988e-05</hent>
<hent key="02400100000000000000000400000009" call="MPI_Isend" bytes="4" orank="9" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.0971e-03 0.0000e+00 2.6941e-05</hent>
<hent key="0240010000000000000000040000000B" call="MPI_Isend" bytes="4" orank="11" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.7450e-03 0.0000e+00 4.7922e-05</hent>
<hent key="0240010000000000000000040000000C" call="MPI_Isend" bytes="4" orank="12" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.5204e-03 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000400000020" call="MPI_Isend" bytes="4" orank="32" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.1551e-02 3.8147e-06 4.1509e-04</hent>
<hent key="024001000000000000001C0000000004" call="MPI_Isend" bytes="7168" orank="4" region="0" commid="0" count="10517" tid="0" op="" dtype="" >2.2341e-02 9.5367e-07 3.1948e-05</hent>
<hent key="024001000000000000001C000000000C" call="MPI_Isend" bytes="7168" orank="12" region="0" commid="0" count="10316" tid="0" op="" dtype="" >1.9969e-02 9.5367e-07 2.6941e-05</hent>
<hent key="024001000000000000000004000002A8" call="MPI_Isend" bytes="4" orank="680" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.2098e-02 3.8147e-06 9.7036e-05</hent>
<hent key="024001000000000000001C00000002A8" call="MPI_Isend" bytes="7168" orank="680" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.4305e-05 7.1526e-06 7.1526e-06</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.2292e-04 2.2292e-04 2.2292e-04</hent>
<hent key="03800100000000000000020000000009" call="MPI_Irecv" bytes="512" orank="9" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.1822e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000002000000000B" call="MPI_Irecv" bytes="512" orank="11" region="0" commid="0" count="3384" tid="0" op="" dtype="" >4.4441e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000050000000004" call="MPI_Irecv" bytes="1280" orank="4" region="0" commid="0" count="146" tid="0" op="" dtype="" >2.0266e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000009" call="MPI_Irecv" bytes="1280" orank="9" region="0" commid="0" count="68" tid="0" op="" dtype="" >1.9312e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000B" call="MPI_Irecv" bytes="1280" orank="11" region="0" commid="0" count="61" tid="0" op="" dtype="" >3.0518e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000C" call="MPI_Irecv" bytes="1280" orank="12" region="0" commid="0" count="164" tid="0" op="" dtype="" >2.7418e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000020" call="MPI_Irecv" bytes="1280" orank="32" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4427e+01 1.0014e-05 1.8457e-01</hent>
<hent key="038001000000000000000500000002A8" call="MPI_Irecv" bytes="1280" orank="680" region="0" commid="0" count="31" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000020000000009" call="MPI_Isend" bytes="512" orank="9" region="0" commid="0" count="3386" tid="0" op="" dtype="" >2.4452e-03 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000002000000000B" call="MPI_Isend" bytes="512" orank="11" region="0" commid="0" count="3374" tid="0" op="" dtype="" >1.8883e-03 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000080000000004" call="MPI_Irecv" bytes="2048" orank="4" region="0" commid="0" count="5" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000C" call="MPI_Irecv" bytes="2048" orank="12" region="0" commid="0" count="6" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000020" call="MPI_Irecv" bytes="2048" orank="32" region="0" commid="0" count="161" tid="0" op="" dtype="" >8.2254e-05 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000800000002A8" call="MPI_Irecv" bytes="2048" orank="680" region="0" commid="0" count="141" tid="0" op="" dtype="" >5.1975e-05 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000050000000004" call="MPI_Isend" bytes="1280" orank="4" region="0" commid="0" count="161" tid="0" op="" dtype="" >2.3746e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000050000000009" call="MPI_Isend" bytes="1280" orank="9" region="0" commid="0" count="53" tid="0" op="" dtype="" >1.9383e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000005000000000B" call="MPI_Isend" bytes="1280" orank="11" region="0" commid="0" count="50" tid="0" op="" dtype="" >9.7275e-05 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000005000000000C" call="MPI_Isend" bytes="1280" orank="12" region="0" commid="0" count="151" tid="0" op="" dtype="" >2.4104e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000050000000020" call="MPI_Isend" bytes="1280" orank="32" region="0" commid="0" count="42" tid="0" op="" dtype="" >2.8276e-04 4.0531e-06 1.3828e-05</hent>
<hent key="024001000000000000000500000002A8" call="MPI_Isend" bytes="1280" orank="680" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.9765e-04 3.8147e-06 1.1921e-05</hent>
</hash>
<internal rank="8" log_i="1724765674.532337" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="9" mpi_size="696" stamp_init="1724765564.465370" stamp_final="1724765674.534656" username="apac4" allocationname="unknown" flags="0" pid="1612126" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10069e+02" utime="8.95186e+01" stime="1.38051e+01" mtime="7.28934e+01" gflop="0.00000e+00" gbyte="3.76564e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28934e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000591513565915721536" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09933e+02" utime="8.94854e+01" stime="1.37959e+01" mtime="7.28934e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28934e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1853e+09" > 6.0775e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1951e+09" > 2.8801e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2703e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9800e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1723e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5781e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.3607e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0595e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3088e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4511e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="204" >
<hent key="02400100000000000000080000000005" call="MPI_Isend" bytes="2048" orank="5" region="0" commid="0" count="3" tid="0" op="" dtype="" >5.0068e-06 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000008000000000A" call="MPI_Isend" bytes="2048" orank="10" region="0" commid="0" count="2" tid="0" op="" dtype="" >6.1989e-06 3.0994e-06 3.0994e-06</hent>
<hent key="0240010000000000000008000000000D" call="MPI_Isend" bytes="2048" orank="13" region="0" commid="0" count="4" tid="0" op="" dtype="" >6.9141e-06 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000080000000021" call="MPI_Isend" bytes="2048" orank="33" region="0" commid="0" count="170" tid="0" op="" dtype="" >9.8228e-04 4.7684e-06 1.1921e-05</hent>
<hent key="024001000000000000000800000002A9" call="MPI_Isend" bytes="2048" orank="681" region="0" commid="0" count="134" tid="0" op="" dtype="" >6.5255e-04 3.8147e-06 9.0599e-06</hent>
<hent key="038001000000000000000E0000000008" call="MPI_Irecv" bytes="3584" orank="8" region="0" commid="0" count="48" tid="0" op="" dtype="" >1.5974e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E000000000A" call="MPI_Irecv" bytes="3584" orank="10" region="0" commid="0" count="160" tid="0" op="" dtype="" >2.8372e-05 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000000E0000000021" call="MPI_Irecv" bytes="3584" orank="33" region="0" commid="0" count="134" tid="0" op="" dtype="" >4.5776e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E00000002A9" call="MPI_Irecv" bytes="3584" orank="681" region="0" commid="0" count="146" tid="0" op="" dtype="" >5.8413e-05 0.0000e+00 2.1458e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000E000000000A" call="MPI_Isend" bytes="3584" orank="10" region="0" commid="0" count="140" tid="0" op="" dtype="" >3.2520e-04 0.0000e+00 1.0967e-05</hent>
<hent key="024001000000000000000E0000000021" call="MPI_Isend" bytes="3584" orank="33" region="0" commid="0" count="158" tid="0" op="" dtype="" >1.0622e-03 5.9605e-06 1.3113e-05</hent>
<hent key="024001000000000000000E00000002A9" call="MPI_Isend" bytes="3584" orank="681" region="0" commid="0" count="133" tid="0" op="" dtype="" >7.2837e-04 4.0531e-06 6.9141e-06</hent>
<hent key="03800100000000000000028000000005" call="MPI_Irecv" bytes="640" orank="5" region="0" commid="0" count="372" tid="0" op="" dtype="" >8.6784e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000008" call="MPI_Irecv" bytes="640" orank="8" region="0" commid="0" count="382" tid="0" op="" dtype="" >1.6689e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000002800000000A" call="MPI_Irecv" bytes="640" orank="10" region="0" commid="0" count="354" tid="0" op="" dtype="" >8.3208e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000D" call="MPI_Irecv" bytes="640" orank="13" region="0" commid="0" count="460" tid="0" op="" dtype="" >7.8201e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000021" call="MPI_Irecv" bytes="640" orank="33" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.0252e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000140000000021" call="MPI_Irecv" bytes="5120" orank="33" region="0" commid="0" count="587" tid="0" op="" dtype="" >1.0920e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000280000002A9" call="MPI_Irecv" bytes="640" orank="681" region="0" commid="0" count="24" tid="0" op="" dtype="" >8.5831e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000001400000002A9" call="MPI_Irecv" bytes="5120" orank="681" region="0" commid="0" count="573" tid="0" op="" dtype="" >1.4186e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000A00000000021" call="MPI_Irecv" bytes="40960" orank="33" region="0" commid="0" count="1885" tid="0" op="" dtype="" >4.9472e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000028000000005" call="MPI_Isend" bytes="640" orank="5" region="0" commid="0" count="455" tid="0" op="" dtype="" >4.5252e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000028000000008" call="MPI_Isend" bytes="640" orank="8" region="0" commid="0" count="348" tid="0" op="" dtype="" >5.2166e-04 9.5367e-07 1.2875e-05</hent>
<hent key="0240010000000000000002800000000A" call="MPI_Isend" bytes="640" orank="10" region="0" commid="0" count="358" tid="0" op="" dtype="" >1.0490e-03 1.9073e-06 1.0967e-05</hent>
<hent key="0240010000000000000002800000000D" call="MPI_Isend" bytes="640" orank="13" region="0" commid="0" count="438" tid="0" op="" dtype="" >5.6744e-04 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000028000000021" call="MPI_Isend" bytes="640" orank="33" region="0" commid="0" count="21" tid="0" op="" dtype="" >1.0395e-04 4.0531e-06 5.9605e-06</hent>
<hent key="03800100000000000000A000000002A9" call="MPI_Irecv" bytes="40960" orank="681" region="0" commid="0" count="1835" tid="0" op="" dtype="" >5.3811e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000140000000021" call="MPI_Isend" bytes="5120" orank="33" region="0" commid="0" count="425" tid="0" op="" dtype="" >1.8911e-03 1.9073e-06 1.1921e-05</hent>
<hent key="024001000000000000000280000002A9" call="MPI_Isend" bytes="640" orank="681" region="0" commid="0" count="19" tid="0" op="" dtype="" >8.2970e-05 3.8147e-06 5.0068e-06</hent>
<hent key="03800100000000000000800000000021" call="MPI_Irecv" bytes="32768" orank="33" region="0" commid="0" count="10815" tid="0" op="" dtype="" >3.2740e-03 0.0000e+00 2.5988e-05</hent>
<hent key="024001000000000000001400000002A9" call="MPI_Isend" bytes="5120" orank="681" region="0" commid="0" count="378" tid="0" op="" dtype="" >1.4641e-03 9.5367e-07 8.1062e-06</hent>
<hent key="038001000000000000008000000002A9" call="MPI_Irecv" bytes="32768" orank="681" region="0" commid="0" count="10865" tid="0" op="" dtype="" >3.2580e-03 0.0000e+00 1.0967e-05</hent>
<hent key="03800100000000000000014000000005" call="MPI_Irecv" bytes="320" orank="5" region="0" commid="0" count="473" tid="0" op="" dtype="" >9.8705e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000014000000008" call="MPI_Irecv" bytes="320" orank="8" region="0" commid="0" count="547" tid="0" op="" dtype="" >2.3532e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000A" call="MPI_Irecv" bytes="320" orank="10" region="0" commid="0" count="561" tid="0" op="" dtype="" >1.2970e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000D" call="MPI_Irecv" bytes="320" orank="13" region="0" commid="0" count="410" tid="0" op="" dtype="" >5.4598e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000021" call="MPI_Irecv" bytes="320" orank="33" region="0" commid="0" count="21" tid="0" op="" dtype="" >8.5831e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A00000000021" call="MPI_Isend" bytes="40960" orank="33" region="0" commid="0" count="1380" tid="0" op="" dtype="" >2.7598e-02 8.8215e-06 4.2915e-05</hent>
<hent key="038001000000000000000140000002A9" call="MPI_Irecv" bytes="320" orank="681" region="0" commid="0" count="24" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A000000002A9" call="MPI_Isend" bytes="40960" orank="681" region="0" commid="0" count="1264" tid="0" op="" dtype="" >1.3765e-02 6.9141e-06 1.6928e-05</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >6.2703e+00 0.0000e+00 1.1070e-01</hent>
<hent key="02400100000000000000800000000021" call="MPI_Isend" bytes="32768" orank="33" region="0" commid="0" count="11320" tid="0" op="" dtype="" >2.2683e-01 7.8678e-06 7.5817e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.0027e-05 0.0000e+00 1.9073e-05</hent>
<hent key="024001000000000000008000000002A9" call="MPI_Isend" bytes="32768" orank="681" region="0" commid="0" count="11436" tid="0" op="" dtype="" >1.2368e-01 6.9141e-06 5.3167e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000005" call="MPI_Isend" bytes="320" orank="5" region="0" commid="0" count="431" tid="0" op="" dtype="" >3.2306e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000008" call="MPI_Isend" bytes="320" orank="8" region="0" commid="0" count="573" tid="0" op="" dtype="" >6.4397e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000001400000000A" call="MPI_Isend" bytes="320" orank="10" region="0" commid="0" count="552" tid="0" op="" dtype="" >1.4787e-03 1.9073e-06 1.8120e-05</hent>
<hent key="0240010000000000000001400000000D" call="MPI_Isend" bytes="320" orank="13" region="0" commid="0" count="440" tid="0" op="" dtype="" >4.8041e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000014000000021" call="MPI_Isend" bytes="320" orank="33" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.1897e-04 3.8147e-06 5.9605e-06</hent>
<hent key="024001000000000000000140000002A9" call="MPI_Isend" bytes="320" orank="681" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.3041e-04 3.8147e-06 5.0068e-06</hent>
<hent key="03800100000000000000000000000005" call="MPI_Irecv" bytes="0" orank="5" region="0" commid="0" count="362" tid="0" op="" dtype="" >8.0585e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000008" call="MPI_Irecv" bytes="0" orank="8" region="0" commid="0" count="524" tid="0" op="" dtype="" >1.5712e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000A" call="MPI_Irecv" bytes="0" orank="10" region="0" commid="0" count="533" tid="0" op="" dtype="" >1.2183e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000D" call="MPI_Irecv" bytes="0" orank="13" region="0" commid="0" count="333" tid="0" op="" dtype="" >7.1287e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000021" call="MPI_Irecv" bytes="0" orank="33" region="0" commid="0" count="82" tid="0" op="" dtype="" >1.5020e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002A9" call="MPI_Irecv" bytes="0" orank="681" region="0" commid="0" count="81" tid="0" op="" dtype="" >1.7166e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.3317e-04 0.0000e+00 8.2970e-05</hent>
<hent key="03800100000000000000030000000005" call="MPI_Irecv" bytes="768" orank="5" region="0" commid="0" count="550" tid="0" op="" dtype="" >6.9857e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003000000000D" call="MPI_Irecv" bytes="768" orank="13" region="0" commid="0" count="616" tid="0" op="" dtype="" >1.0490e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000000000005" call="MPI_Isend" bytes="0" orank="5" region="0" commid="0" count="342" tid="0" op="" dtype="" >1.6618e-04 0.0000e+00 3.0994e-06</hent>
<hent key="02400100000000000000000000000008" call="MPI_Isend" bytes="0" orank="8" region="0" commid="0" count="531" tid="0" op="" dtype="" >3.5763e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000000000000000A" call="MPI_Isend" bytes="0" orank="10" region="0" commid="0" count="550" tid="0" op="" dtype="" >1.0798e-03 0.0000e+00 3.3140e-05</hent>
<hent key="0240010000000000000000000000000D" call="MPI_Isend" bytes="0" orank="13" region="0" commid="0" count="342" tid="0" op="" dtype="" >2.6631e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000021" call="MPI_Isend" bytes="0" orank="33" region="0" commid="0" count="82" tid="0" op="" dtype="" >3.3283e-04 9.5367e-07 8.1062e-06</hent>
<hent key="024001000000000000000000000002A9" call="MPI_Isend" bytes="0" orank="681" region="0" commid="0" count="80" tid="0" op="" dtype="" >3.0851e-04 9.5367e-07 5.0068e-06</hent>
<hent key="03800100000000000000060000000005" call="MPI_Irecv" bytes="1536" orank="5" region="0" commid="0" count="61" tid="0" op="" dtype="" >1.6689e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000060000000008" call="MPI_Irecv" bytes="1536" orank="8" region="0" commid="0" count="15" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000006000000000A" call="MPI_Irecv" bytes="1536" orank="10" region="0" commid="0" count="15" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000006000000000D" call="MPI_Irecv" bytes="1536" orank="13" region="0" commid="0" count="48" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000021" call="MPI_Irecv" bytes="1536" orank="33" region="0" commid="0" count="70" tid="0" op="" dtype="" >2.8849e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002A9" call="MPI_Irecv" bytes="1536" orank="681" region="0" commid="0" count="77" tid="0" op="" dtype="" >3.3140e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000030000000005" call="MPI_Isend" bytes="768" orank="5" region="0" commid="0" count="688" tid="0" op="" dtype="" >2.9111e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000003000000000D" call="MPI_Isend" bytes="768" orank="13" region="0" commid="0" count="668" tid="0" op="" dtype="" >3.6025e-04 0.0000e+00 3.0994e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000008" call="MPI_Irecv" bytes="448" orank="8" region="0" commid="0" count="14" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C00000000A" call="MPI_Irecv" bytes="448" orank="10" region="0" commid="0" count="42" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000005" call="MPI_Isend" bytes="1536" orank="5" region="0" commid="0" count="40" tid="0" op="" dtype="" >5.8413e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000008" call="MPI_Isend" bytes="1536" orank="8" region="0" commid="0" count="9" tid="0" op="" dtype="" >2.0027e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000006000000000A" call="MPI_Isend" bytes="1536" orank="10" region="0" commid="0" count="12" tid="0" op="" dtype="" >5.8889e-05 2.8610e-06 2.0981e-05</hent>
<hent key="0240010000000000000006000000000D" call="MPI_Isend" bytes="1536" orank="13" region="0" commid="0" count="55" tid="0" op="" dtype="" >9.5367e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000021" call="MPI_Isend" bytes="1536" orank="33" region="0" commid="0" count="65" tid="0" op="" dtype="" >3.5930e-04 5.0068e-06 1.3113e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002A9" call="MPI_Isend" bytes="1536" orank="681" region="0" commid="0" count="76" tid="0" op="" dtype="" >3.6621e-04 3.8147e-06 5.9605e-06</hent>
<hent key="038001000000000000000C0000000021" call="MPI_Irecv" bytes="3072" orank="33" region="0" commid="0" count="334" tid="0" op="" dtype="" >9.4414e-05 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002A9" call="MPI_Irecv" bytes="3072" orank="681" region="0" commid="0" count="324" tid="0" op="" dtype="" >1.3566e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000001C00000000A" call="MPI_Isend" bytes="448" orank="10" region="0" commid="0" count="38" tid="0" op="" dtype="" >2.2650e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000021" call="MPI_Isend" bytes="3072" orank="33" region="0" commid="0" count="345" tid="0" op="" dtype="" >2.2538e-03 5.0068e-06 4.1008e-05</hent>
<hent key="024001000000000000000C00000002A9" call="MPI_Isend" bytes="3072" orank="681" region="0" commid="0" count="364" tid="0" op="" dtype="" >1.9374e-03 3.8147e-06 1.1206e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >5.0139e-04 1.6618e-04 1.6809e-04</hent>
<hent key="03800100000000000000038000000005" call="MPI_Irecv" bytes="896" orank="5" region="0" commid="0" count="3120" tid="0" op="" dtype="" >4.1127e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000008" call="MPI_Irecv" bytes="896" orank="8" region="0" commid="0" count="171" tid="0" op="" dtype="" >6.6042e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000A" call="MPI_Irecv" bytes="896" orank="10" region="0" commid="0" count="166" tid="0" op="" dtype="" >3.2902e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000D" call="MPI_Irecv" bytes="896" orank="13" region="0" commid="0" count="3065" tid="0" op="" dtype="" >6.0797e-04 0.0000e+00 1.9073e-05</hent>
<hent key="03800100000000000000038000000021" call="MPI_Irecv" bytes="896" orank="33" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.0252e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002A9" call="MPI_Irecv" bytes="896" orank="681" region="0" commid="0" count="23" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.6928e-05 1.6928e-05 1.6928e-05</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000180000000005" call="MPI_Irecv" bytes="6144" orank="5" region="0" commid="0" count="2043" tid="0" op="" dtype="" >3.0088e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000018000000000D" call="MPI_Irecv" bytes="6144" orank="13" region="0" commid="0" count="2295" tid="0" op="" dtype="" >4.0603e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000180000000021" call="MPI_Irecv" bytes="6144" orank="33" region="0" commid="0" count="6" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000001800000002A9" call="MPI_Irecv" bytes="6144" orank="681" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000038000000005" call="MPI_Isend" bytes="896" orank="5" region="0" commid="0" count="2980" tid="0" op="" dtype="" >1.5044e-03 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000038000000008" call="MPI_Isend" bytes="896" orank="8" region="0" commid="0" count="167" tid="0" op="" dtype="" >2.9302e-04 9.5367e-07 1.4067e-05</hent>
<hent key="0240010000000000000003800000000A" call="MPI_Isend" bytes="896" orank="10" region="0" commid="0" count="157" tid="0" op="" dtype="" >5.0783e-04 1.9073e-06 1.3828e-05</hent>
<hent key="0240010000000000000003800000000D" call="MPI_Isend" bytes="896" orank="13" region="0" commid="0" count="2998" tid="0" op="" dtype="" >1.8740e-03 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000038000000021" call="MPI_Isend" bytes="896" orank="33" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.2565e-04 3.8147e-06 5.9605e-06</hent>
<hent key="024001000000000000000380000002A9" call="MPI_Isend" bytes="896" orank="681" region="0" commid="0" count="19" tid="0" op="" dtype="" >8.7976e-05 3.8147e-06 5.0068e-06</hent>
<hent key="02400100000000000000180000000005" call="MPI_Isend" bytes="6144" orank="5" region="0" commid="0" count="2555" tid="0" op="" dtype="" >4.5483e-03 9.5367e-07 1.5020e-05</hent>
<hent key="0240010000000000000018000000000D" call="MPI_Isend" bytes="6144" orank="13" region="0" commid="0" count="2407" tid="0" op="" dtype="" >4.9319e-03 9.5367e-07 1.0014e-05</hent>
<hent key="02400100000000000000180000000021" call="MPI_Isend" bytes="6144" orank="33" region="0" commid="0" count="9" tid="0" op="" dtype="" >6.5327e-05 6.9141e-06 8.1062e-06</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0041e-05 3.0041e-05 3.0041e-05</hent>
<hent key="024001000000000000001800000002A9" call="MPI_Isend" bytes="6144" orank="681" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.7166e-05 5.0068e-06 6.1989e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >9.4214e+00 2.5034e-05 1.2668e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >9.1505e-04 9.1505e-04 9.1505e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0595e-02 1.0595e-02 1.0595e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >5.4600e-03 5.4600e-03 5.4600e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.9926e-01 3.3751e-03 1.9089e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.7101e-04 5.7101e-04 5.7101e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 9.5367e-07 1.9073e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5781e+00 4.2510e-04 2.4789e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.4067e-05 0.0000e+00 5.0068e-06</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="03800100000000000000070000000005" call="MPI_Irecv" bytes="1792" orank="5" region="0" commid="0" count="15" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000008" call="MPI_Irecv" bytes="1792" orank="8" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000A" call="MPI_Irecv" bytes="1792" orank="10" region="0" commid="0" count="5" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000007000000000D" call="MPI_Irecv" bytes="1792" orank="13" region="0" commid="0" count="24" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000021" call="MPI_Irecv" bytes="1792" orank="33" region="0" commid="0" count="132" tid="0" op="" dtype="" >4.7445e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002A9" call="MPI_Irecv" bytes="1792" orank="681" region="0" commid="0" count="107" tid="0" op="" dtype="" >2.3127e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >1.2493e-04 9.5367e-07 6.4850e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9799e+01 0.0000e+00 3.8419e+01</hent>
<hent key="038001000000000000000A000000000D" call="MPI_Irecv" bytes="2560" orank="13" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000021" call="MPI_Irecv" bytes="2560" orank="33" region="0" commid="0" count="369" tid="0" op="" dtype="" >1.0467e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002A9" call="MPI_Irecv" bytes="2560" orank="681" region="0" commid="0" count="390" tid="0" op="" dtype="" >1.5235e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000005" call="MPI_Isend" bytes="1792" orank="5" region="0" commid="0" count="13" tid="0" op="" dtype="" >1.6212e-05 9.5367e-07 1.9073e-06</hent>
<hent key="02400100000000000000070000000008" call="MPI_Isend" bytes="1792" orank="8" region="0" commid="0" count="4" tid="0" op="" dtype="" >8.1062e-06 1.9073e-06 2.1458e-06</hent>
<hent key="0240010000000000000007000000000A" call="MPI_Isend" bytes="1792" orank="10" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.4080e-05 3.8147e-06 4.0531e-06</hent>
<hent key="0240010000000000000007000000000D" call="MPI_Isend" bytes="1792" orank="13" region="0" commid="0" count="25" tid="0" op="" dtype="" >5.5552e-05 9.5367e-07 1.2875e-05</hent>
<hent key="02400100000000000000070000000021" call="MPI_Isend" bytes="1792" orank="33" region="0" commid="0" count="100" tid="0" op="" dtype="" >5.5170e-04 4.7684e-06 1.2875e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3088e-02 8.3088e-02 8.3088e-02</hent>
<hent key="024001000000000000000700000002A9" call="MPI_Isend" bytes="1792" orank="681" region="0" commid="0" count="105" tid="0" op="" dtype="" >5.0783e-04 3.8147e-06 6.1989e-06</hent>
<hent key="024001000000000000000A0000000005" call="MPI_Isend" bytes="2560" orank="5" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000A0000000021" call="MPI_Isend" bytes="2560" orank="33" region="0" commid="0" count="385" tid="0" op="" dtype="" >2.3439e-03 5.0068e-06 7.8678e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.3961e-04 2.9802e-05 1.0896e-04</hent>
<hent key="024001000000000000000A00000002A9" call="MPI_Isend" bytes="2560" orank="681" region="0" commid="0" count="386" tid="0" op="" dtype="" >1.9491e-03 3.8147e-06 1.0967e-05</hent>
<hent key="03800100000000000000100000000008" call="MPI_Irecv" bytes="4096" orank="8" region="0" commid="0" count="12652" tid="0" op="" dtype="" >4.8213e-03 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000010000000000A" call="MPI_Irecv" bytes="4096" orank="10" region="0" commid="0" count="12540" tid="0" op="" dtype="" >2.0018e-03 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000100000000021" call="MPI_Irecv" bytes="4096" orank="33" region="0" commid="0" count="3103" tid="0" op="" dtype="" >5.1165e-04 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000001000000002A9" call="MPI_Irecv" bytes="4096" orank="681" region="0" commid="0" count="3130" tid="0" op="" dtype="" >7.3218e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000100000000008" call="MPI_Isend" bytes="4096" orank="8" region="0" commid="0" count="12700" tid="0" op="" dtype="" >2.2104e-02 0.0000e+00 2.2173e-05</hent>
<hent key="0240010000000000000010000000000A" call="MPI_Isend" bytes="4096" orank="10" region="0" commid="0" count="12560" tid="0" op="" dtype="" >3.1565e-02 0.0000e+00 2.4080e-05</hent>
<hent key="02400100000000000000100000000021" call="MPI_Isend" bytes="4096" orank="33" region="0" commid="0" count="3243" tid="0" op="" dtype="" >1.3289e-02 1.9073e-06 3.6001e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >7.4100e-04 2.7204e-04 4.6897e-04</hent>
<hent key="024001000000000000001000000002A9" call="MPI_Isend" bytes="4096" orank="681" region="0" commid="0" count="3323" tid="0" op="" dtype="" >1.2226e-02 9.5367e-07 2.1935e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.8862e-05 3.8862e-05 3.8862e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.6270e-03 2.2101e-04 5.5504e-04</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.0967e-05 3.0994e-06 7.8678e-06</hent>
<hent key="03800100000000000000000400000005" call="MPI_Irecv" bytes="4" orank="5" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.7207e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000008" call="MPI_Irecv" bytes="4" orank="8" region="0" commid="0" count="3400" tid="0" op="" dtype="" >9.9707e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0380010000000000000000040000000A" call="MPI_Irecv" bytes="4" orank="10" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.6182e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000000D" call="MPI_Irecv" bytes="4" orank="13" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.5896e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000021" call="MPI_Irecv" bytes="4" orank="33" region="0" commid="0" count="3400" tid="0" op="" dtype="" >7.2241e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001C0000000005" call="MPI_Irecv" bytes="7168" orank="5" region="0" commid="0" count="10657" tid="0" op="" dtype="" >1.6677e-03 0.0000e+00 2.1935e-05</hent>
<hent key="038001000000000000001C000000000D" call="MPI_Irecv" bytes="7168" orank="13" region="0" commid="0" count="10405" tid="0" op="" dtype="" >1.9240e-03 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000004000002A9" call="MPI_Irecv" bytes="4" orank="681" region="0" commid="0" count="3400" tid="0" op="" dtype="" >8.1301e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000000400000005" call="MPI_Isend" bytes="4" orank="5" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.7507e-03 0.0000e+00 2.7895e-05</hent>
<hent key="02400100000000000000000400000008" call="MPI_Isend" bytes="4" orank="8" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.5566e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000000040000000A" call="MPI_Isend" bytes="4" orank="10" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.7986e-03 0.0000e+00 5.8889e-05</hent>
<hent key="0240010000000000000000040000000D" call="MPI_Isend" bytes="4" orank="13" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.4248e-03 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000000400000021" call="MPI_Isend" bytes="4" orank="33" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.3681e-02 4.0531e-06 2.9516e-04</hent>
<hent key="024001000000000000001C0000000005" call="MPI_Isend" bytes="7168" orank="5" region="0" commid="0" count="10145" tid="0" op="" dtype="" >1.8697e-02 9.5367e-07 2.1935e-05</hent>
<hent key="024001000000000000001C000000000D" call="MPI_Isend" bytes="7168" orank="13" region="0" commid="0" count="10293" tid="0" op="" dtype="" >2.1691e-02 9.5367e-07 3.5048e-05</hent>
<hent key="024001000000000000000004000002A9" call="MPI_Isend" bytes="4" orank="681" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.8644e-02 3.8147e-06 6.1035e-05</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.4700e-04 2.4700e-04 2.4700e-04</hent>
<hent key="03800100000000000000020000000008" call="MPI_Irecv" bytes="512" orank="8" region="0" commid="0" count="3386" tid="0" op="" dtype="" >4.4322e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002000000000A" call="MPI_Irecv" bytes="512" orank="10" region="0" commid="0" count="3358" tid="0" op="" dtype="" >4.4131e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000005" call="MPI_Irecv" bytes="1280" orank="5" region="0" commid="0" count="144" tid="0" op="" dtype="" >3.6478e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000008" call="MPI_Irecv" bytes="1280" orank="8" region="0" commid="0" count="53" tid="0" op="" dtype="" >2.5272e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000A" call="MPI_Irecv" bytes="1280" orank="10" region="0" commid="0" count="65" tid="0" op="" dtype="" >2.0981e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000D" call="MPI_Irecv" bytes="1280" orank="13" region="0" commid="0" count="138" tid="0" op="" dtype="" >2.1935e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000021" call="MPI_Irecv" bytes="1280" orank="33" region="0" commid="0" count="46" tid="0" op="" dtype="" >1.5497e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4884e+01 1.1921e-05 1.8455e-01</hent>
<hent key="038001000000000000000500000002A9" call="MPI_Irecv" bytes="1280" orank="681" region="0" commid="0" count="42" tid="0" op="" dtype="" >1.6451e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000020000000008" call="MPI_Isend" bytes="512" orank="8" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.9634e-03 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000002000000000A" call="MPI_Isend" bytes="512" orank="10" region="0" commid="0" count="3362" tid="0" op="" dtype="" >2.4269e-03 0.0000e+00 1.1921e-05</hent>
<hent key="03800100000000000000080000000005" call="MPI_Irecv" bytes="2048" orank="5" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000080000000008" call="MPI_Irecv" bytes="2048" orank="8" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000A" call="MPI_Irecv" bytes="2048" orank="10" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000008000000000D" call="MPI_Irecv" bytes="2048" orank="13" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000080000000021" call="MPI_Irecv" bytes="2048" orank="33" region="0" commid="0" count="161" tid="0" op="" dtype="" >5.7220e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002A9" call="MPI_Irecv" bytes="2048" orank="681" region="0" commid="0" count="153" tid="0" op="" dtype="" >5.2452e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000050000000005" call="MPI_Isend" bytes="1280" orank="5" region="0" commid="0" count="147" tid="0" op="" dtype="" >2.0385e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000050000000008" call="MPI_Isend" bytes="1280" orank="8" region="0" commid="0" count="68" tid="0" op="" dtype="" >1.2565e-04 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000005000000000A" call="MPI_Isend" bytes="1280" orank="10" region="0" commid="0" count="63" tid="0" op="" dtype="" >2.0909e-04 1.9073e-06 4.0531e-06</hent>
<hent key="0240010000000000000005000000000D" call="MPI_Isend" bytes="1280" orank="13" region="0" commid="0" count="130" tid="0" op="" dtype="" >2.0313e-04 9.5367e-07 6.1989e-06</hent>
<hent key="02400100000000000000050000000021" call="MPI_Isend" bytes="1280" orank="33" region="0" commid="0" count="47" tid="0" op="" dtype="" >2.5725e-04 5.0068e-06 7.1526e-06</hent>
<hent key="024001000000000000000500000002A9" call="MPI_Isend" bytes="1280" orank="681" region="0" commid="0" count="50" tid="0" op="" dtype="" >2.3651e-04 3.8147e-06 7.1526e-06</hent>
</hash>
<internal rank="9" log_i="1724765674.534656" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="10" mpi_size="696" stamp_init="1724765564.465348" stamp_final="1724765674.526166" username="apac4" allocationname="unknown" flags="0" pid="1612127" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10061e+02" utime="8.79324e+01" stime="1.42835e+01" mtime="7.16061e+01" gflop="0.00000e+00" gbyte="3.77331e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16061e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ce14cf14d1148255d114d014de" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09923e+02" utime="8.78983e+01" stime="1.42746e+01" mtime="7.16061e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16061e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2169e+09" > 7.3498e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2243e+09" > 4.0451e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8799e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9788e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.3419e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.2214e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5770e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.5906e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0625e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3090e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3489e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="209" >
<hent key="02400100000000000000080000000006" call="MPI_Isend" bytes="2048" orank="6" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.0252e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000080000000009" call="MPI_Isend" bytes="2048" orank="9" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="0240010000000000000008000000000B" call="MPI_Isend" bytes="2048" orank="11" region="0" commid="0" count="2" tid="0" op="" dtype="" >6.1989e-06 3.0994e-06 3.0994e-06</hent>
<hent key="0240010000000000000008000000000E" call="MPI_Isend" bytes="2048" orank="14" region="0" commid="0" count="3" tid="0" op="" dtype="" >4.7684e-06 9.5367e-07 1.9073e-06</hent>
<hent key="02400100000000000000080000000022" call="MPI_Isend" bytes="2048" orank="34" region="0" commid="0" count="149" tid="0" op="" dtype="" >1.0419e-03 4.7684e-06 2.8849e-05</hent>
<hent key="024001000000000000000800000002AA" call="MPI_Isend" bytes="2048" orank="682" region="0" commid="0" count="152" tid="0" op="" dtype="" >8.2898e-04 3.8147e-06 1.5974e-05</hent>
<hent key="038001000000000000000E0000000009" call="MPI_Irecv" bytes="3584" orank="9" region="0" commid="0" count="140" tid="0" op="" dtype="" >5.8889e-05 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000E000000000B" call="MPI_Irecv" bytes="3584" orank="11" region="0" commid="0" count="100" tid="0" op="" dtype="" >1.7166e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000022" call="MPI_Irecv" bytes="3584" orank="34" region="0" commid="0" count="159" tid="0" op="" dtype="" >7.4625e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E00000002AA" call="MPI_Irecv" bytes="3584" orank="682" region="0" commid="0" count="154" tid="0" op="" dtype="" >1.7262e-04 0.0000e+00 2.0981e-05</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000E0000000009" call="MPI_Isend" bytes="3584" orank="9" region="0" commid="0" count="160" tid="0" op="" dtype="" >2.4557e-04 0.0000e+00 2.1458e-06</hent>
<hent key="024001000000000000000E000000000B" call="MPI_Isend" bytes="3584" orank="11" region="0" commid="0" count="338" tid="0" op="" dtype="" >7.7796e-04 0.0000e+00 1.0967e-05</hent>
<hent key="024001000000000000000E0000000022" call="MPI_Isend" bytes="3584" orank="34" region="0" commid="0" count="130" tid="0" op="" dtype="" >9.7680e-04 5.0068e-06 1.8120e-05</hent>
<hent key="024001000000000000000E00000002AA" call="MPI_Isend" bytes="3584" orank="682" region="0" commid="0" count="158" tid="0" op="" dtype="" >1.0591e-03 4.0531e-06 1.9789e-05</hent>
<hent key="03800100000000000000028000000006" call="MPI_Irecv" bytes="640" orank="6" region="0" commid="0" count="408" tid="0" op="" dtype="" >1.1158e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000028000000009" call="MPI_Irecv" bytes="640" orank="9" region="0" commid="0" count="358" tid="0" op="" dtype="" >1.6356e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000B" call="MPI_Irecv" bytes="640" orank="11" region="0" commid="0" count="381" tid="0" op="" dtype="" >2.2411e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000002800000000E" call="MPI_Irecv" bytes="640" orank="14" region="0" commid="0" count="411" tid="0" op="" dtype="" >9.6083e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000022" call="MPI_Irecv" bytes="640" orank="34" region="0" commid="0" count="21" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000140000000022" call="MPI_Irecv" bytes="5120" orank="34" region="0" commid="0" count="1061" tid="0" op="" dtype="" >2.9445e-04 0.0000e+00 1.0967e-05</hent>
<hent key="038001000000000000000280000002AA" call="MPI_Irecv" bytes="640" orank="682" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.3351e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="038001000000000000001400000002AA" call="MPI_Irecv" bytes="5120" orank="682" region="0" commid="0" count="1020" tid="0" op="" dtype="" >3.8671e-04 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000A00000000022" call="MPI_Irecv" bytes="40960" orank="34" region="0" commid="0" count="3817" tid="0" op="" dtype="" >2.3959e-03 0.0000e+00 3.2902e-05</hent>
<hent key="02400100000000000000028000000006" call="MPI_Isend" bytes="640" orank="6" region="0" commid="0" count="409" tid="0" op="" dtype="" >5.0020e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000028000000009" call="MPI_Isend" bytes="640" orank="9" region="0" commid="0" count="354" tid="0" op="" dtype="" >5.5337e-04 9.5367e-07 1.1921e-05</hent>
<hent key="0240010000000000000002800000000B" call="MPI_Isend" bytes="640" orank="11" region="0" commid="0" count="375" tid="0" op="" dtype="" >1.2176e-03 1.9073e-06 1.5020e-05</hent>
<hent key="0240010000000000000002800000000E" call="MPI_Isend" bytes="640" orank="14" region="0" commid="0" count="436" tid="0" op="" dtype="" >5.4383e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000028000000022" call="MPI_Isend" bytes="640" orank="34" region="0" commid="0" count="26" tid="0" op="" dtype="" >1.4496e-04 4.0531e-06 1.5020e-05</hent>
<hent key="03800100000000000000A000000002AA" call="MPI_Irecv" bytes="40960" orank="682" region="0" commid="0" count="3546" tid="0" op="" dtype="" >1.5616e-03 0.0000e+00 2.5988e-05</hent>
<hent key="02400100000000000000140000000022" call="MPI_Isend" bytes="5120" orank="34" region="0" commid="0" count="957" tid="0" op="" dtype="" >5.5230e-03 1.9073e-06 6.0081e-05</hent>
<hent key="024001000000000000000280000002AA" call="MPI_Isend" bytes="640" orank="682" region="0" commid="0" count="21" tid="0" op="" dtype="" >9.9182e-05 4.0531e-06 8.8215e-06</hent>
<hent key="03800100000000000000800000000022" call="MPI_Irecv" bytes="32768" orank="34" region="0" commid="0" count="8883" tid="0" op="" dtype="" >6.4497e-03 0.0000e+00 3.5048e-05</hent>
<hent key="024001000000000000001400000002AA" call="MPI_Isend" bytes="5120" orank="682" region="0" commid="0" count="937" tid="0" op="" dtype="" >4.7507e-03 9.5367e-07 3.5048e-05</hent>
<hent key="038001000000000000008000000002AA" call="MPI_Irecv" bytes="32768" orank="682" region="0" commid="0" count="9154" tid="0" op="" dtype="" >3.3855e-03 0.0000e+00 3.6955e-05</hent>
<hent key="03800100000000000000014000000006" call="MPI_Irecv" bytes="320" orank="6" region="0" commid="0" count="446" tid="0" op="" dtype="" >1.1826e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000009" call="MPI_Irecv" bytes="320" orank="9" region="0" commid="0" count="552" tid="0" op="" dtype="" >2.4915e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000B" call="MPI_Irecv" bytes="320" orank="11" region="0" commid="0" count="554" tid="0" op="" dtype="" >2.9898e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000001400000000E" call="MPI_Irecv" bytes="320" orank="14" region="0" commid="0" count="438" tid="0" op="" dtype="" >1.1730e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000022" call="MPI_Irecv" bytes="320" orank="34" region="0" commid="0" count="21" tid="0" op="" dtype="" >6.6757e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000A00000000022" call="MPI_Isend" bytes="40960" orank="34" region="0" commid="0" count="3221" tid="0" op="" dtype="" >7.5544e-02 1.0014e-05 8.0109e-05</hent>
<hent key="038001000000000000000140000002AA" call="MPI_Irecv" bytes="320" orank="682" region="0" commid="0" count="21" tid="0" op="" dtype="" >1.4305e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A000000002AA" call="MPI_Isend" bytes="40960" orank="682" region="0" commid="0" count="3242" tid="0" op="" dtype="" >5.1346e-02 6.9141e-06 1.4997e-04</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >5.7220e-06 0.0000e+00 2.8610e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >5.8799e+00 0.0000e+00 1.1043e-01</hent>
<hent key="02400100000000000000800000000022" call="MPI_Isend" bytes="32768" orank="34" region="0" commid="0" count="9479" tid="0" op="" dtype="" >2.2189e-01 8.8215e-06 3.3784e-04</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.0981e-05 0.0000e+00 2.0027e-05</hent>
<hent key="024001000000000000008000000002AA" call="MPI_Isend" bytes="32768" orank="682" region="0" commid="0" count="9458" tid="0" op="" dtype="" >1.4837e-01 6.9141e-06 2.6584e-04</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000006" call="MPI_Isend" bytes="320" orank="6" region="0" commid="0" count="416" tid="0" op="" dtype="" >4.2272e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000014000000009" call="MPI_Isend" bytes="320" orank="9" region="0" commid="0" count="561" tid="0" op="" dtype="" >6.9189e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000001400000000B" call="MPI_Isend" bytes="320" orank="11" region="0" commid="0" count="540" tid="0" op="" dtype="" >1.6026e-03 1.9073e-06 1.0967e-05</hent>
<hent key="0240010000000000000001400000000E" call="MPI_Isend" bytes="320" orank="14" region="0" commid="0" count="404" tid="0" op="" dtype="" >4.3154e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000014000000022" call="MPI_Isend" bytes="320" orank="34" region="0" commid="0" count="26" tid="0" op="" dtype="" >1.4782e-04 3.8147e-06 1.6212e-05</hent>
<hent key="024001000000000000000140000002AA" call="MPI_Isend" bytes="320" orank="682" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.3542e-04 3.8147e-06 1.0014e-05</hent>
<hent key="03800100000000000000000000000006" call="MPI_Irecv" bytes="0" orank="6" region="0" commid="0" count="338" tid="0" op="" dtype="" >7.5579e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000009" call="MPI_Irecv" bytes="0" orank="9" region="0" commid="0" count="550" tid="0" op="" dtype="" >1.8716e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000000000000000B" call="MPI_Irecv" bytes="0" orank="11" region="0" commid="0" count="509" tid="0" op="" dtype="" >2.0695e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000000000000000E" call="MPI_Irecv" bytes="0" orank="14" region="0" commid="0" count="336" tid="0" op="" dtype="" >9.9659e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000022" call="MPI_Irecv" bytes="0" orank="34" region="0" commid="0" count="86" tid="0" op="" dtype="" >2.3603e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002AA" call="MPI_Irecv" bytes="0" orank="682" region="0" commid="0" count="83" tid="0" op="" dtype="" >2.1219e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.0933e-04 0.0000e+00 8.1062e-05</hent>
<hent key="03800100000000000000030000000006" call="MPI_Irecv" bytes="768" orank="6" region="0" commid="0" count="812" tid="0" op="" dtype="" >1.2565e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0380010000000000000003000000000E" call="MPI_Irecv" bytes="768" orank="14" region="0" commid="0" count="728" tid="0" op="" dtype="" >1.6928e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000006" call="MPI_Isend" bytes="0" orank="6" region="0" commid="0" count="345" tid="0" op="" dtype="" >2.5773e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000000000009" call="MPI_Isend" bytes="0" orank="9" region="0" commid="0" count="533" tid="0" op="" dtype="" >3.9864e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000000000000000B" call="MPI_Isend" bytes="0" orank="11" region="0" commid="0" count="525" tid="0" op="" dtype="" >1.1363e-03 9.5367e-07 6.8188e-05</hent>
<hent key="0240010000000000000000000000000E" call="MPI_Isend" bytes="0" orank="14" region="0" commid="0" count="340" tid="0" op="" dtype="" >2.3842e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000022" call="MPI_Isend" bytes="0" orank="34" region="0" commid="0" count="82" tid="0" op="" dtype="" >3.6001e-04 2.8610e-06 1.0967e-05</hent>
<hent key="024001000000000000000000000002AA" call="MPI_Isend" bytes="0" orank="682" region="0" commid="0" count="83" tid="0" op="" dtype="" >3.4714e-04 1.9073e-06 1.0967e-05</hent>
<hent key="03800100000000000000060000000006" call="MPI_Irecv" bytes="1536" orank="6" region="0" commid="0" count="60" tid="0" op="" dtype="" >1.7405e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000009" call="MPI_Irecv" bytes="1536" orank="9" region="0" commid="0" count="12" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000006000000000B" call="MPI_Irecv" bytes="1536" orank="11" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.6689e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000006000000000E" call="MPI_Irecv" bytes="1536" orank="14" region="0" commid="0" count="54" tid="0" op="" dtype="" >1.5497e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000022" call="MPI_Irecv" bytes="1536" orank="34" region="0" commid="0" count="70" tid="0" op="" dtype="" >3.2902e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002AA" call="MPI_Irecv" bytes="1536" orank="682" region="0" commid="0" count="77" tid="0" op="" dtype="" >5.4121e-05 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000030000000006" call="MPI_Isend" bytes="768" orank="6" region="0" commid="0" count="768" tid="0" op="" dtype="" >4.1938e-04 0.0000e+00 2.6941e-05</hent>
<hent key="0240010000000000000003000000000E" call="MPI_Isend" bytes="768" orank="14" region="0" commid="0" count="798" tid="0" op="" dtype="" >4.1389e-04 0.0000e+00 6.1989e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C000000009" call="MPI_Irecv" bytes="448" orank="9" region="0" commid="0" count="38" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C00000000B" call="MPI_Irecv" bytes="448" orank="11" region="0" commid="0" count="24" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000006" call="MPI_Isend" bytes="1536" orank="6" region="0" commid="0" count="58" tid="0" op="" dtype="" >9.9659e-05 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000060000000009" call="MPI_Isend" bytes="1536" orank="9" region="0" commid="0" count="15" tid="0" op="" dtype="" >3.0279e-05 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000006000000000B" call="MPI_Isend" bytes="1536" orank="11" region="0" commid="0" count="18" tid="0" op="" dtype="" >7.6294e-05 3.0994e-06 1.0014e-05</hent>
<hent key="0240010000000000000006000000000E" call="MPI_Isend" bytes="1536" orank="14" region="0" commid="0" count="57" tid="0" op="" dtype="" >9.7275e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000022" call="MPI_Isend" bytes="1536" orank="34" region="0" commid="0" count="73" tid="0" op="" dtype="" >4.6992e-04 5.0068e-06 1.7881e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002AA" call="MPI_Isend" bytes="1536" orank="682" region="0" commid="0" count="73" tid="0" op="" dtype="" >4.2200e-04 3.0994e-06 1.5974e-05</hent>
<hent key="038001000000000000000C0000000022" call="MPI_Irecv" bytes="3072" orank="34" region="0" commid="0" count="359" tid="0" op="" dtype="" >1.7309e-04 0.0000e+00 1.2159e-05</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002AA" call="MPI_Irecv" bytes="3072" orank="682" region="0" commid="0" count="339" tid="0" op="" dtype="" >2.7704e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0240010000000000000001C000000009" call="MPI_Isend" bytes="448" orank="9" region="0" commid="0" count="42" tid="0" op="" dtype="" >2.5034e-05 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000001C00000000B" call="MPI_Isend" bytes="448" orank="11" region="0" commid="0" count="90" tid="0" op="" dtype="" >6.2704e-05 0.0000e+00 8.1062e-06</hent>
<hent key="024001000000000000000C0000000022" call="MPI_Isend" bytes="3072" orank="34" region="0" commid="0" count="351" tid="0" op="" dtype="" >2.5973e-03 4.7684e-06 2.5988e-05</hent>
<hent key="024001000000000000000C00000002AA" call="MPI_Isend" bytes="3072" orank="682" region="0" commid="0" count="345" tid="0" op="" dtype="" >2.0871e-03 3.8147e-06 1.7881e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >5.2285e-04 1.5783e-04 1.8597e-04</hent>
<hent key="03800100000000000000038000000006" call="MPI_Irecv" bytes="896" orank="6" region="0" commid="0" count="2858" tid="0" op="" dtype="" >4.2510e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000038000000009" call="MPI_Irecv" bytes="896" orank="9" region="0" commid="0" count="157" tid="0" op="" dtype="" >5.8413e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000B" call="MPI_Irecv" bytes="896" orank="11" region="0" commid="0" count="173" tid="0" op="" dtype="" >1.2255e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000003800000000E" call="MPI_Irecv" bytes="896" orank="14" region="0" commid="0" count="2959" tid="0" op="" dtype="" >7.4792e-04 0.0000e+00 3.0041e-05</hent>
<hent key="03800100000000000000038000000022" call="MPI_Irecv" bytes="896" orank="34" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.7405e-05 0.0000e+00 3.0994e-06</hent>
<hent key="038001000000000000000380000002AA" call="MPI_Irecv" bytes="896" orank="682" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.7405e-05 0.0000e+00 4.0531e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.5702e-04 2.5702e-04 2.5702e-04</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="03800100000000000000180000000006" call="MPI_Irecv" bytes="6144" orank="6" region="0" commid="0" count="3186" tid="0" op="" dtype="" >5.7483e-04 0.0000e+00 4.7684e-06</hent>
<hent key="0380010000000000000018000000000E" call="MPI_Irecv" bytes="6144" orank="14" region="0" commid="0" count="2750" tid="0" op="" dtype="" >5.4526e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000180000000022" call="MPI_Irecv" bytes="6144" orank="34" region="0" commid="0" count="15" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000001800000002AA" call="MPI_Irecv" bytes="6144" orank="682" region="0" commid="0" count="8" tid="0" op="" dtype="" >9.2983e-06 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000038000000006" call="MPI_Isend" bytes="896" orank="6" region="0" commid="0" count="2930" tid="0" op="" dtype="" >1.7438e-03 0.0000e+00 2.4080e-05</hent>
<hent key="02400100000000000000038000000009" call="MPI_Isend" bytes="896" orank="9" region="0" commid="0" count="166" tid="0" op="" dtype="" >3.0994e-04 9.5367e-07 1.0967e-05</hent>
<hent key="0240010000000000000003800000000B" call="MPI_Isend" bytes="896" orank="11" region="0" commid="0" count="173" tid="0" op="" dtype="" >6.4230e-04 2.8610e-06 1.5020e-05</hent>
<hent key="0240010000000000000003800000000E" call="MPI_Isend" bytes="896" orank="14" region="0" commid="0" count="2903" tid="0" op="" dtype="" >1.8010e-03 0.0000e+00 2.0027e-05</hent>
<hent key="02400100000000000000038000000022" call="MPI_Isend" bytes="896" orank="34" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.5688e-04 3.8147e-06 8.1062e-06</hent>
<hent key="024001000000000000000380000002AA" call="MPI_Isend" bytes="896" orank="682" region="0" commid="0" count="24" tid="0" op="" dtype="" >1.1230e-04 3.8147e-06 7.8678e-06</hent>
<hent key="02400100000000000000180000000006" call="MPI_Isend" bytes="6144" orank="6" region="0" commid="0" count="2858" tid="0" op="" dtype="" >5.3411e-03 9.5367e-07 2.6941e-05</hent>
<hent key="0240010000000000000018000000000E" call="MPI_Isend" bytes="6144" orank="14" region="0" commid="0" count="2841" tid="0" op="" dtype="" >6.2480e-03 9.5367e-07 2.1935e-05</hent>
<hent key="02400100000000000000180000000022" call="MPI_Isend" bytes="6144" orank="34" region="0" commid="0" count="11" tid="0" op="" dtype="" >9.3222e-05 6.9141e-06 1.3113e-05</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.5075e-05 5.5075e-05 5.5075e-05</hent>
<hent key="024001000000000000001800000002AA" call="MPI_Isend" bytes="6144" orank="682" region="0" commid="0" count="11" tid="0" op="" dtype="" >7.8917e-05 5.0068e-06 1.3113e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.8743e+00 1.3828e-05 1.2666e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.6308e-04 8.6308e-04 8.6308e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.8147e-06 3.8147e-06 3.8147e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0625e-02 1.0625e-02 1.0625e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >5.3730e-03 5.3730e-03 5.3730e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.8964e-01 3.2921e-03 1.8127e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.6407e-04 3.6407e-04 3.6407e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 9.5367e-07 9.5367e-07</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5770e+00 4.0507e-04 2.4783e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >6.3419e-05 2.1458e-06 3.6001e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="03800100000000000000070000000006" call="MPI_Irecv" bytes="1792" orank="6" region="0" commid="0" count="22" tid="0" op="" dtype="" >7.6294e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000009" call="MPI_Irecv" bytes="1792" orank="9" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000B" call="MPI_Irecv" bytes="1792" orank="11" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000007000000000E" call="MPI_Irecv" bytes="1792" orank="14" region="0" commid="0" count="19" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000022" call="MPI_Irecv" bytes="1792" orank="34" region="0" commid="0" count="103" tid="0" op="" dtype="" >4.8161e-05 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000700000002AA" call="MPI_Irecv" bytes="1792" orank="682" region="0" commid="0" count="106" tid="0" op="" dtype="" >6.1035e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >9.4414e-05 9.5367e-07 6.3896e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9787e+01 0.0000e+00 3.8414e+01</hent>
<hent key="038001000000000000000A0000000006" call="MPI_Irecv" bytes="2560" orank="6" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000000E" call="MPI_Irecv" bytes="2560" orank="14" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.1921e-06 1.1921e-06 1.1921e-06</hent>
<hent key="038001000000000000000A0000000022" call="MPI_Irecv" bytes="2560" orank="34" region="0" commid="0" count="379" tid="0" op="" dtype="" >1.6713e-04 0.0000e+00 2.8610e-06</hent>
<hent key="038001000000000000000A00000002AA" call="MPI_Irecv" bytes="2560" orank="682" region="0" commid="0" count="382" tid="0" op="" dtype="" >2.7037e-04 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000070000000006" call="MPI_Isend" bytes="1792" orank="6" region="0" commid="0" count="28" tid="0" op="" dtype="" >5.0783e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000009" call="MPI_Isend" bytes="1792" orank="9" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.2875e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000007000000000B" call="MPI_Isend" bytes="1792" orank="11" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.2875e-05 3.8147e-06 5.0068e-06</hent>
<hent key="0240010000000000000007000000000E" call="MPI_Isend" bytes="1792" orank="14" region="0" commid="0" count="24" tid="0" op="" dtype="" >4.5776e-05 9.5367e-07 2.8610e-06</hent>
<hent key="02400100000000000000070000000022" call="MPI_Isend" bytes="1792" orank="34" region="0" commid="0" count="120" tid="0" op="" dtype="" >7.8058e-04 3.8147e-06 1.5974e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3090e-02 8.3090e-02 8.3090e-02</hent>
<hent key="024001000000000000000700000002AA" call="MPI_Isend" bytes="1792" orank="682" region="0" commid="0" count="94" tid="0" op="" dtype="" >5.2023e-04 3.0994e-06 1.0967e-05</hent>
<hent key="024001000000000000000A0000000006" call="MPI_Isend" bytes="2560" orank="6" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="024001000000000000000A0000000022" call="MPI_Isend" bytes="2560" orank="34" region="0" commid="0" count="356" tid="0" op="" dtype="" >2.4760e-03 4.0531e-06 2.4796e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.3413e-04 2.8849e-05 1.1015e-04</hent>
<hent key="024001000000000000000A00000002AA" call="MPI_Isend" bytes="2560" orank="682" region="0" commid="0" count="391" tid="0" op="" dtype="" >2.2321e-03 3.8147e-06 1.9073e-05</hent>
<hent key="03800100000000000000100000000009" call="MPI_Irecv" bytes="4096" orank="9" region="0" commid="0" count="12560" tid="0" op="" dtype="" >5.7015e-03 0.0000e+00 2.0027e-05</hent>
<hent key="0380010000000000000010000000000B" call="MPI_Irecv" bytes="4096" orank="11" region="0" commid="0" count="12600" tid="0" op="" dtype="" >2.3634e-03 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000100000000022" call="MPI_Irecv" bytes="4096" orank="34" region="0" commid="0" count="2617" tid="0" op="" dtype="" >7.9536e-04 0.0000e+00 2.4080e-05</hent>
<hent key="038001000000000000001000000002AA" call="MPI_Irecv" bytes="4096" orank="682" region="0" commid="0" count="2656" tid="0" op="" dtype="" >9.6107e-04 0.0000e+00 2.1935e-05</hent>
<hent key="02400100000000000000100000000009" call="MPI_Isend" bytes="4096" orank="9" region="0" commid="0" count="12540" tid="0" op="" dtype="" >2.0195e-02 0.0000e+00 1.9073e-05</hent>
<hent key="0240010000000000000010000000000B" call="MPI_Isend" bytes="4096" orank="11" region="0" commid="0" count="12362" tid="0" op="" dtype="" >2.8713e-02 9.5367e-07 3.2187e-05</hent>
<hent key="02400100000000000000100000000022" call="MPI_Isend" bytes="4096" orank="34" region="0" commid="0" count="2747" tid="0" op="" dtype="" >1.4853e-02 1.9073e-06 5.1975e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >7.8225e-04 2.8610e-04 4.9615e-04</hent>
<hent key="024001000000000000001000000002AA" call="MPI_Isend" bytes="4096" orank="682" region="0" commid="0" count="2734" tid="0" op="" dtype="" >1.3144e-02 9.5367e-07 6.6042e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.8161e-05 4.8161e-05 4.8161e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.7788e-03 2.6798e-04 5.9700e-04</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.0599e-06 2.1458e-06 6.9141e-06</hent>
<hent key="03800100000000000000000400000006" call="MPI_Irecv" bytes="4" orank="6" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.6039e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000009" call="MPI_Irecv" bytes="4" orank="9" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.1344e-03 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000000040000000B" call="MPI_Irecv" bytes="4" orank="11" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.2667e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000040000000E" call="MPI_Irecv" bytes="4" orank="14" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.1999e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000022" call="MPI_Irecv" bytes="4" orank="34" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.1714e-03 0.0000e+00 2.0981e-05</hent>
<hent key="038001000000000000001C0000000006" call="MPI_Irecv" bytes="7168" orank="6" region="0" commid="0" count="9514" tid="0" op="" dtype="" >1.5733e-03 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000001C000000000E" call="MPI_Irecv" bytes="7168" orank="14" region="0" commid="0" count="9950" tid="0" op="" dtype="" >1.9934e-03 0.0000e+00 1.7881e-05</hent>
<hent key="038001000000000000000004000002AA" call="MPI_Irecv" bytes="4" orank="682" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.0259e-03 0.0000e+00 2.0027e-05</hent>
<hent key="038001000000000000001C00000002AA" call="MPI_Irecv" bytes="7168" orank="682" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="02400100000000000000000400000006" call="MPI_Isend" bytes="4" orank="6" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.7469e-03 0.0000e+00 2.5988e-05</hent>
<hent key="02400100000000000000000400000009" call="MPI_Isend" bytes="4" orank="9" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.6072e-03 0.0000e+00 4.9114e-05</hent>
<hent key="0240010000000000000000040000000B" call="MPI_Isend" bytes="4" orank="11" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.0780e-03 0.0000e+00 2.4796e-05</hent>
<hent key="0240010000000000000000040000000E" call="MPI_Isend" bytes="4" orank="14" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.7221e-03 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000000400000022" call="MPI_Isend" bytes="4" orank="34" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.9759e-02 4.0531e-06 4.3488e-04</hent>
<hent key="024001000000000000001C0000000006" call="MPI_Isend" bytes="7168" orank="6" region="0" commid="0" count="9842" tid="0" op="" dtype="" >1.8729e-02 9.5367e-07 3.0041e-05</hent>
<hent key="024001000000000000001C000000000E" call="MPI_Isend" bytes="7168" orank="14" region="0" commid="0" count="9859" tid="0" op="" dtype="" >2.2302e-02 9.5367e-07 2.9087e-05</hent>
<hent key="024001000000000000000004000002AA" call="MPI_Isend" bytes="4" orank="682" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.2804e-02 3.8147e-06 1.6785e-04</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.6488e-04 2.6488e-04 2.6488e-04</hent>
<hent key="03800100000000000000020000000009" call="MPI_Irecv" bytes="512" orank="9" region="0" commid="0" count="3362" tid="0" op="" dtype="" >5.6958e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0380010000000000000002000000000B" call="MPI_Irecv" bytes="512" orank="11" region="0" commid="0" count="3376" tid="0" op="" dtype="" >4.5991e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000050000000006" call="MPI_Irecv" bytes="1280" orank="6" region="0" commid="0" count="151" tid="0" op="" dtype="" >4.1485e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000009" call="MPI_Irecv" bytes="1280" orank="9" region="0" commid="0" count="63" tid="0" op="" dtype="" >2.3127e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000B" call="MPI_Irecv" bytes="1280" orank="11" region="0" commid="0" count="56" tid="0" op="" dtype="" >4.6968e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000005000000000E" call="MPI_Irecv" bytes="1280" orank="14" region="0" commid="0" count="150" tid="0" op="" dtype="" >3.0518e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000022" call="MPI_Irecv" bytes="1280" orank="34" region="0" commid="0" count="44" tid="0" op="" dtype="" >2.2888e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4418e+01 9.0599e-06 1.8487e-01</hent>
<hent key="038001000000000000000500000002AA" call="MPI_Irecv" bytes="1280" orank="682" region="0" commid="0" count="50" tid="0" op="" dtype="" >3.0041e-05 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000020000000009" call="MPI_Isend" bytes="512" orank="9" region="0" commid="0" count="3358" tid="0" op="" dtype="" >1.8141e-03 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000002000000000B" call="MPI_Isend" bytes="512" orank="11" region="0" commid="0" count="3310" tid="0" op="" dtype="" >2.0959e-03 0.0000e+00 1.2875e-05</hent>
<hent key="03800100000000000000080000000006" call="MPI_Irecv" bytes="2048" orank="6" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000009" call="MPI_Irecv" bytes="2048" orank="9" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000B" call="MPI_Irecv" bytes="2048" orank="11" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="0380010000000000000008000000000E" call="MPI_Irecv" bytes="2048" orank="14" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000022" call="MPI_Irecv" bytes="2048" orank="34" region="0" commid="0" count="133" tid="0" op="" dtype="" >5.4359e-05 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000800000002AA" call="MPI_Irecv" bytes="2048" orank="682" region="0" commid="0" count="148" tid="0" op="" dtype="" >1.2755e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000050000000006" call="MPI_Isend" bytes="1280" orank="6" region="0" commid="0" count="139" tid="0" op="" dtype="" >2.2411e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000050000000009" call="MPI_Isend" bytes="1280" orank="9" region="0" commid="0" count="65" tid="0" op="" dtype="" >1.3304e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000005000000000B" call="MPI_Isend" bytes="1280" orank="11" region="0" commid="0" count="64" tid="0" op="" dtype="" >2.5153e-04 2.8610e-06 1.4067e-05</hent>
<hent key="0240010000000000000005000000000E" call="MPI_Isend" bytes="1280" orank="14" region="0" commid="0" count="135" tid="0" op="" dtype="" >2.2554e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000050000000022" call="MPI_Isend" bytes="1280" orank="34" region="0" commid="0" count="42" tid="0" op="" dtype="" >2.6584e-04 4.0531e-06 1.3828e-05</hent>
<hent key="024001000000000000000500000002AA" call="MPI_Isend" bytes="1280" orank="682" region="0" commid="0" count="49" tid="0" op="" dtype="" >2.4462e-04 3.8147e-06 1.0014e-05</hent>
</hash>
<internal rank="10" log_i="1724765674.526166" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="11" mpi_size="696" stamp_init="1724765564.465384" stamp_final="1724765674.525601" username="apac4" allocationname="unknown" flags="0" pid="1612128" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10060e+02" utime="8.92670e+01" stime="1.40601e+01" mtime="7.31597e+01" gflop="0.00000e+00" gbyte="3.74874e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.31597e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09923e+02" utime="8.92285e+01" stime="1.40562e+01" mtime="7.31597e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.31597e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1903e+09" > 6.3674e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1941e+09" > 2.5626e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3251e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9794e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3842e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.6253e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5773e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.0491e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0584e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3072e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4703e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="209" >
<hent key="02400100000000000000080000000007" call="MPI_Isend" bytes="2048" orank="7" region="0" commid="0" count="5" tid="0" op="" dtype="" >6.9141e-06 9.5367e-07 1.9073e-06</hent>
<hent key="0240010000000000000008000000000A" call="MPI_Isend" bytes="2048" orank="10" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0240010000000000000008000000000F" call="MPI_Isend" bytes="2048" orank="15" region="0" commid="0" count="16" tid="0" op="" dtype="" >4.1962e-05 9.5367e-07 1.4067e-05</hent>
<hent key="02400100000000000000080000000023" call="MPI_Isend" bytes="2048" orank="35" region="0" commid="0" count="168" tid="0" op="" dtype="" >9.4438e-04 4.7684e-06 1.0967e-05</hent>
<hent key="024001000000000000000800000002AB" call="MPI_Isend" bytes="2048" orank="683" region="0" commid="0" count="169" tid="0" op="" dtype="" >7.7009e-04 3.8147e-06 6.9141e-06</hent>
<hent key="038001000000000000000E0000000008" call="MPI_Irecv" bytes="3584" orank="8" region="0" commid="0" count="120" tid="0" op="" dtype="" >1.5974e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E000000000A" call="MPI_Irecv" bytes="3584" orank="10" region="0" commid="0" count="338" tid="0" op="" dtype="" >1.5283e-04 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000000E0000000023" call="MPI_Irecv" bytes="3584" orank="35" region="0" commid="0" count="136" tid="0" op="" dtype="" >4.6492e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E00000002AB" call="MPI_Irecv" bytes="3584" orank="683" region="0" commid="0" count="127" tid="0" op="" dtype="" >3.3617e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000E0000000008" call="MPI_Isend" bytes="3584" orank="8" region="0" commid="0" count="60" tid="0" op="" dtype="" >1.4710e-04 9.5367e-07 1.0014e-05</hent>
<hent key="024001000000000000000E000000000A" call="MPI_Isend" bytes="3584" orank="10" region="0" commid="0" count="100" tid="0" op="" dtype="" >1.5283e-04 9.5367e-07 6.9141e-06</hent>
<hent key="024001000000000000000E0000000023" call="MPI_Isend" bytes="3584" orank="35" region="0" commid="0" count="135" tid="0" op="" dtype="" >8.2469e-04 4.7684e-06 1.2875e-05</hent>
<hent key="024001000000000000000E00000002AB" call="MPI_Isend" bytes="3584" orank="683" region="0" commid="0" count="146" tid="0" op="" dtype="" >7.1955e-04 3.8147e-06 8.1062e-06</hent>
<hent key="03800100000000000000028000000007" call="MPI_Irecv" bytes="640" orank="7" region="0" commid="0" count="407" tid="0" op="" dtype="" >1.1945e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000028000000008" call="MPI_Irecv" bytes="640" orank="8" region="0" commid="0" count="410" tid="0" op="" dtype="" >9.3937e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000A" call="MPI_Irecv" bytes="640" orank="10" region="0" commid="0" count="375" tid="0" op="" dtype="" >1.3518e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000F" call="MPI_Irecv" bytes="640" orank="15" region="0" commid="0" count="375" tid="0" op="" dtype="" >6.9141e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000023" call="MPI_Irecv" bytes="640" orank="35" region="0" commid="0" count="24" tid="0" op="" dtype="" >7.6294e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000140000000023" call="MPI_Irecv" bytes="5120" orank="35" region="0" commid="0" count="662" tid="0" op="" dtype="" >1.0538e-04 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000280000002AB" call="MPI_Irecv" bytes="640" orank="683" region="0" commid="0" count="26" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="038001000000000000001400000002AB" call="MPI_Irecv" bytes="5120" orank="683" region="0" commid="0" count="531" tid="0" op="" dtype="" >1.2636e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000A00000000023" call="MPI_Irecv" bytes="40960" orank="35" region="0" commid="0" count="2175" tid="0" op="" dtype="" >4.3774e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000028000000007" call="MPI_Isend" bytes="640" orank="7" region="0" commid="0" count="430" tid="0" op="" dtype="" >4.7278e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000028000000008" call="MPI_Isend" bytes="640" orank="8" region="0" commid="0" count="370" tid="0" op="" dtype="" >1.1096e-03 1.9073e-06 1.4067e-05</hent>
<hent key="0240010000000000000002800000000A" call="MPI_Isend" bytes="640" orank="10" region="0" commid="0" count="381" tid="0" op="" dtype="" >5.4479e-04 9.5367e-07 1.4067e-05</hent>
<hent key="0240010000000000000002800000000F" call="MPI_Isend" bytes="640" orank="15" region="0" commid="0" count="438" tid="0" op="" dtype="" >4.9543e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000028000000023" call="MPI_Isend" bytes="640" orank="35" region="0" commid="0" count="23" tid="0" op="" dtype="" >1.1039e-04 3.8147e-06 6.1989e-06</hent>
<hent key="03800100000000000000A000000002AB" call="MPI_Irecv" bytes="40960" orank="683" region="0" commid="0" count="1662" tid="0" op="" dtype="" >3.9387e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000140000000023" call="MPI_Isend" bytes="5120" orank="35" region="0" commid="0" count="439" tid="0" op="" dtype="" >2.1281e-03 1.9073e-06 2.6608e-04</hent>
<hent key="024001000000000000000280000002AB" call="MPI_Isend" bytes="640" orank="683" region="0" commid="0" count="21" tid="0" op="" dtype="" >8.7738e-05 2.8610e-06 5.0068e-06</hent>
<hent key="03800100000000000000800000000023" call="MPI_Irecv" bytes="32768" orank="35" region="0" commid="0" count="10525" tid="0" op="" dtype="" >2.1164e-03 0.0000e+00 7.8678e-06</hent>
<hent key="024001000000000000001400000002AB" call="MPI_Isend" bytes="5120" orank="683" region="0" commid="0" count="539" tid="0" op="" dtype="" >1.9209e-03 9.5367e-07 7.1526e-06</hent>
<hent key="038001000000000000008000000002AB" call="MPI_Irecv" bytes="32768" orank="683" region="0" commid="0" count="11038" tid="0" op="" dtype="" >2.4996e-03 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000014000000007" call="MPI_Irecv" bytes="320" orank="7" region="0" commid="0" count="452" tid="0" op="" dtype="" >1.1992e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000008" call="MPI_Irecv" bytes="320" orank="8" region="0" commid="0" count="573" tid="0" op="" dtype="" >1.3137e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000A" call="MPI_Irecv" bytes="320" orank="10" region="0" commid="0" count="540" tid="0" op="" dtype="" >2.4509e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000001400000000F" call="MPI_Irecv" bytes="320" orank="15" region="0" commid="0" count="474" tid="0" op="" dtype="" >1.0347e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000023" call="MPI_Irecv" bytes="320" orank="35" region="0" commid="0" count="22" tid="0" op="" dtype="" >9.7752e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A00000000023" call="MPI_Isend" bytes="40960" orank="35" region="0" commid="0" count="1428" tid="0" op="" dtype="" >3.2751e-02 1.0014e-05 4.2915e-05</hent>
<hent key="038001000000000000000140000002AB" call="MPI_Irecv" bytes="320" orank="683" region="0" commid="0" count="28" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A000000002AB" call="MPI_Isend" bytes="40960" orank="683" region="0" commid="0" count="1830" tid="0" op="" dtype="" >1.9670e-02 7.1526e-06 1.9073e-05</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >6.3251e+00 0.0000e+00 1.1058e-01</hent>
<hent key="02400100000000000000800000000023" call="MPI_Isend" bytes="32768" orank="35" region="0" commid="0" count="11272" tid="0" op="" dtype="" >2.5831e-01 1.0014e-05 1.6999e-04</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.0266e-05 0.0000e+00 1.9073e-05</hent>
<hent key="024001000000000000008000000002AB" call="MPI_Isend" bytes="32768" orank="683" region="0" commid="0" count="10870" tid="0" op="" dtype="" >1.1574e-01 6.9141e-06 4.5061e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000007" call="MPI_Isend" bytes="320" orank="7" region="0" commid="0" count="439" tid="0" op="" dtype="" >4.0627e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000008" call="MPI_Isend" bytes="320" orank="8" region="0" commid="0" count="540" tid="0" op="" dtype="" >1.4427e-03 1.9073e-06 1.0014e-05</hent>
<hent key="0240010000000000000001400000000A" call="MPI_Isend" bytes="320" orank="10" region="0" commid="0" count="554" tid="0" op="" dtype="" >6.3562e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0240010000000000000001400000000F" call="MPI_Isend" bytes="320" orank="15" region="0" commid="0" count="449" tid="0" op="" dtype="" >4.4465e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000014000000023" call="MPI_Isend" bytes="320" orank="35" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.2541e-04 4.0531e-06 5.9605e-06</hent>
<hent key="024001000000000000000140000002AB" call="MPI_Isend" bytes="320" orank="683" region="0" commid="0" count="21" tid="0" op="" dtype="" >8.7738e-05 3.8147e-06 5.0068e-06</hent>
<hent key="03800100000000000000000000000007" call="MPI_Irecv" bytes="0" orank="7" region="0" commid="0" count="343" tid="0" op="" dtype="" >8.9884e-05 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000000000000008" call="MPI_Irecv" bytes="0" orank="8" region="0" commid="0" count="502" tid="0" op="" dtype="" >1.1897e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000A" call="MPI_Irecv" bytes="0" orank="10" region="0" commid="0" count="525" tid="0" op="" dtype="" >1.6618e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000000000000000F" call="MPI_Irecv" bytes="0" orank="15" region="0" commid="0" count="314" tid="0" op="" dtype="" >6.3896e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000023" call="MPI_Irecv" bytes="0" orank="35" region="0" commid="0" count="82" tid="0" op="" dtype="" >1.8835e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002AB" call="MPI_Irecv" bytes="0" orank="683" region="0" commid="0" count="83" tid="0" op="" dtype="" >1.9550e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >1.9479e-04 0.0000e+00 8.2970e-05</hent>
<hent key="03800100000000000000030000000007" call="MPI_Irecv" bytes="768" orank="7" region="0" commid="0" count="820" tid="0" op="" dtype="" >1.0872e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003000000000F" call="MPI_Irecv" bytes="768" orank="15" region="0" commid="0" count="860" tid="0" op="" dtype="" >1.8668e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000000000007" call="MPI_Isend" bytes="0" orank="7" region="0" commid="0" count="319" tid="0" op="" dtype="" >1.9789e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000000000008" call="MPI_Isend" bytes="0" orank="8" region="0" commid="0" count="533" tid="0" op="" dtype="" >1.0388e-03 0.0000e+00 5.0068e-05</hent>
<hent key="0240010000000000000000000000000A" call="MPI_Isend" bytes="0" orank="10" region="0" commid="0" count="509" tid="0" op="" dtype="" >3.5810e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0240010000000000000000000000000F" call="MPI_Isend" bytes="0" orank="15" region="0" commid="0" count="294" tid="0" op="" dtype="" >2.2149e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000000000000023" call="MPI_Isend" bytes="0" orank="35" region="0" commid="0" count="91" tid="0" op="" dtype="" >3.4904e-04 9.5367e-07 5.0068e-06</hent>
<hent key="024001000000000000000000000002AB" call="MPI_Isend" bytes="0" orank="683" region="0" commid="0" count="83" tid="0" op="" dtype="" >3.1948e-04 9.5367e-07 8.8215e-06</hent>
<hent key="03800100000000000000060000000007" call="MPI_Irecv" bytes="1536" orank="7" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.1921e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000008" call="MPI_Irecv" bytes="1536" orank="8" region="0" commid="0" count="7" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000006000000000A" call="MPI_Irecv" bytes="1536" orank="10" region="0" commid="0" count="18" tid="0" op="" dtype="" >1.0490e-05 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000006000000000F" call="MPI_Irecv" bytes="1536" orank="15" region="0" commid="0" count="57" tid="0" op="" dtype="" >9.2983e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000023" call="MPI_Irecv" bytes="1536" orank="35" region="0" commid="0" count="70" tid="0" op="" dtype="" >1.4544e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002AB" call="MPI_Irecv" bytes="1536" orank="683" region="0" commid="0" count="82" tid="0" op="" dtype="" >1.9312e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000030000000007" call="MPI_Isend" bytes="768" orank="7" region="0" commid="0" count="574" tid="0" op="" dtype="" >2.8014e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000003000000000F" call="MPI_Isend" bytes="768" orank="15" region="0" commid="0" count="880" tid="0" op="" dtype="" >4.0722e-04 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C000000008" call="MPI_Irecv" bytes="448" orank="8" region="0" commid="0" count="26" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C00000000A" call="MPI_Irecv" bytes="448" orank="10" region="0" commid="0" count="90" tid="0" op="" dtype="" >1.7881e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000007" call="MPI_Isend" bytes="1536" orank="7" region="0" commid="0" count="59" tid="0" op="" dtype="" >9.0837e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000008" call="MPI_Isend" bytes="1536" orank="8" region="0" commid="0" count="18" tid="0" op="" dtype="" >6.1274e-05 2.8610e-06 4.0531e-06</hent>
<hent key="0240010000000000000006000000000A" call="MPI_Isend" bytes="1536" orank="10" region="0" commid="0" count="22" tid="0" op="" dtype="" >4.6492e-05 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000006000000000F" call="MPI_Isend" bytes="1536" orank="15" region="0" commid="0" count="55" tid="0" op="" dtype="" >8.3447e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000023" call="MPI_Isend" bytes="1536" orank="35" region="0" commid="0" count="72" tid="0" op="" dtype="" >3.9554e-04 4.0531e-06 6.9141e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002AB" call="MPI_Isend" bytes="1536" orank="683" region="0" commid="0" count="70" tid="0" op="" dtype="" >3.1400e-04 3.8147e-06 1.0014e-05</hent>
<hent key="038001000000000000000C0000000023" call="MPI_Irecv" bytes="3072" orank="35" region="0" commid="0" count="340" tid="0" op="" dtype="" >1.0681e-04 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002AB" call="MPI_Irecv" bytes="3072" orank="683" region="0" commid="0" count="368" tid="0" op="" dtype="" >1.0729e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000001C000000008" call="MPI_Isend" bytes="448" orank="8" region="0" commid="0" count="16" tid="0" op="" dtype="" >1.0729e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C00000000A" call="MPI_Isend" bytes="448" orank="10" region="0" commid="0" count="24" tid="0" op="" dtype="" >1.6928e-05 0.0000e+00 5.0068e-06</hent>
<hent key="024001000000000000000C0000000023" call="MPI_Isend" bytes="3072" orank="35" region="0" commid="0" count="355" tid="0" op="" dtype="" >2.1236e-03 5.0068e-06 1.2875e-05</hent>
<hent key="024001000000000000000C00000002AB" call="MPI_Isend" bytes="3072" orank="683" region="0" commid="0" count="343" tid="0" op="" dtype="" >1.6758e-03 3.8147e-06 1.0014e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >5.8818e-04 1.9312e-04 1.9908e-04</hent>
<hent key="03800100000000000000038000000007" call="MPI_Irecv" bytes="896" orank="7" region="0" commid="0" count="2855" tid="0" op="" dtype="" >3.9697e-04 0.0000e+00 1.1206e-05</hent>
<hent key="03800100000000000000038000000008" call="MPI_Irecv" bytes="896" orank="8" region="0" commid="0" count="154" tid="0" op="" dtype="" >4.2439e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000A" call="MPI_Irecv" bytes="896" orank="10" region="0" commid="0" count="173" tid="0" op="" dtype="" >7.3195e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000003800000000F" call="MPI_Irecv" bytes="896" orank="15" region="0" commid="0" count="2851" tid="0" op="" dtype="" >6.4683e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000023" call="MPI_Irecv" bytes="896" orank="35" region="0" commid="0" count="31" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002AB" call="MPI_Irecv" bytes="896" orank="683" region="0" commid="0" count="26" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.4400e-04 1.4400e-04 1.4400e-04</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="03800100000000000000180000000007" call="MPI_Irecv" bytes="6144" orank="7" region="0" commid="0" count="3050" tid="0" op="" dtype="" >2.6894e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000018000000000F" call="MPI_Irecv" bytes="6144" orank="15" region="0" commid="0" count="3108" tid="0" op="" dtype="" >4.1270e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000180000000023" call="MPI_Irecv" bytes="6144" orank="35" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000001800000002AB" call="MPI_Irecv" bytes="6144" orank="683" region="0" commid="0" count="8" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000038000000007" call="MPI_Isend" bytes="896" orank="7" region="0" commid="0" count="3117" tid="0" op="" dtype="" >1.7972e-03 0.0000e+00 6.2943e-05</hent>
<hent key="02400100000000000000038000000008" call="MPI_Isend" bytes="896" orank="8" region="0" commid="0" count="177" tid="0" op="" dtype="" >5.7530e-04 1.9073e-06 1.8120e-05</hent>
<hent key="0240010000000000000003800000000A" call="MPI_Isend" bytes="896" orank="10" region="0" commid="0" count="173" tid="0" op="" dtype="" >2.8276e-04 9.5367e-07 5.9605e-06</hent>
<hent key="0240010000000000000003800000000F" call="MPI_Isend" bytes="896" orank="15" region="0" commid="0" count="2814" tid="0" op="" dtype="" >1.5707e-03 0.0000e+00 1.7166e-05</hent>
<hent key="02400100000000000000038000000023" call="MPI_Isend" bytes="896" orank="35" region="0" commid="0" count="33" tid="0" op="" dtype="" >1.7428e-04 4.0531e-06 8.8215e-06</hent>
<hent key="024001000000000000000380000002AB" call="MPI_Isend" bytes="896" orank="683" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.2159e-04 3.8147e-06 5.9605e-06</hent>
<hent key="02400100000000000000180000000007" call="MPI_Isend" bytes="6144" orank="7" region="0" commid="0" count="2121" tid="0" op="" dtype="" >3.7320e-03 9.5367e-07 1.5020e-05</hent>
<hent key="0240010000000000000018000000000F" call="MPI_Isend" bytes="6144" orank="15" region="0" commid="0" count="3090" tid="0" op="" dtype="" >6.1011e-03 9.5367e-07 2.0027e-05</hent>
<hent key="02400100000000000000180000000023" call="MPI_Isend" bytes="6144" orank="35" region="0" commid="0" count="9" tid="0" op="" dtype="" >6.3181e-05 6.9141e-06 7.1526e-06</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.5088e-05 6.5088e-05 6.5088e-05</hent>
<hent key="024001000000000000001800000002AB" call="MPI_Isend" bytes="6144" orank="683" region="0" commid="0" count="12" tid="0" op="" dtype="" >7.0095e-05 5.0068e-06 6.1989e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >9.4631e+00 8.1062e-06 1.2668e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.5902e-04 8.5902e-04 8.5902e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0584e-02 1.0584e-02 1.0584e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >4.9760e-03 4.9760e-03 4.9760e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.2388e-01 3.3500e-03 2.1557e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.6696e-04 5.6696e-04 5.6696e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5773e+00 3.9005e-04 2.4780e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.3842e-05 0.0000e+00 1.0014e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="03800100000000000000070000000007" call="MPI_Irecv" bytes="1792" orank="7" region="0" commid="0" count="16" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000008" call="MPI_Irecv" bytes="1792" orank="8" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000007000000000A" call="MPI_Irecv" bytes="1792" orank="10" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000007000000000F" call="MPI_Irecv" bytes="1792" orank="15" region="0" commid="0" count="29" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000023" call="MPI_Irecv" bytes="1792" orank="35" region="0" commid="0" count="121" tid="0" op="" dtype="" >3.7909e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002AB" call="MPI_Irecv" bytes="1792" orank="683" region="0" commid="0" count="107" tid="0" op="" dtype="" >3.0994e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >8.4877e-05 0.0000e+00 6.3896e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9793e+01 0.0000e+00 3.8421e+01</hent>
<hent key="038001000000000000000A0000000007" call="MPI_Irecv" bytes="2560" orank="7" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000023" call="MPI_Irecv" bytes="2560" orank="35" region="0" commid="0" count="386" tid="0" op="" dtype="" >1.0967e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002AB" call="MPI_Irecv" bytes="2560" orank="683" region="0" commid="0" count="360" tid="0" op="" dtype="" >8.0109e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000007" call="MPI_Isend" bytes="1792" orank="7" region="0" commid="0" count="18" tid="0" op="" dtype="" >2.8372e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000008" call="MPI_Isend" bytes="1792" orank="8" region="0" commid="0" count="1" tid="0" op="" dtype="" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="0240010000000000000007000000000A" call="MPI_Isend" bytes="1792" orank="10" region="0" commid="0" count="4" tid="0" op="" dtype="" >8.8215e-06 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000007000000000F" call="MPI_Isend" bytes="1792" orank="15" region="0" commid="0" count="15" tid="0" op="" dtype="" >2.3365e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000023" call="MPI_Isend" bytes="1792" orank="35" region="0" commid="0" count="120" tid="0" op="" dtype="" >6.6185e-04 3.8147e-06 1.2159e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3072e-02 8.3072e-02 8.3072e-02</hent>
<hent key="024001000000000000000700000002AB" call="MPI_Isend" bytes="1792" orank="683" region="0" commid="0" count="102" tid="0" op="" dtype="" >4.5753e-04 3.8147e-06 1.2159e-05</hent>
<hent key="024001000000000000000A0000000007" call="MPI_Isend" bytes="2560" orank="7" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000A0000000023" call="MPI_Isend" bytes="2560" orank="35" region="0" commid="0" count="371" tid="0" op="" dtype="" >2.1660e-03 4.7684e-06 2.0981e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.7275e-04 3.1948e-05 1.2279e-04</hent>
<hent key="024001000000000000000A00000002AB" call="MPI_Isend" bytes="2560" orank="683" region="0" commid="0" count="388" tid="0" op="" dtype="" >1.8175e-03 3.8147e-06 9.0599e-06</hent>
<hent key="03800100000000000000100000000008" call="MPI_Irecv" bytes="4096" orank="8" region="0" commid="0" count="12580" tid="0" op="" dtype="" >1.7798e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000010000000000A" call="MPI_Irecv" bytes="4096" orank="10" region="0" commid="0" count="12362" tid="0" op="" dtype="" >5.4986e-03 0.0000e+00 1.3828e-05</hent>
<hent key="03800100000000000000100000000023" call="MPI_Irecv" bytes="4096" orank="35" region="0" commid="0" count="3021" tid="0" op="" dtype="" >5.0831e-04 0.0000e+00 2.8610e-06</hent>
<hent key="038001000000000000001000000002AB" call="MPI_Irecv" bytes="4096" orank="683" region="0" commid="0" count="3164" tid="0" op="" dtype="" >6.7329e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000100000000008" call="MPI_Isend" bytes="4096" orank="8" region="0" commid="0" count="12640" tid="0" op="" dtype="" >2.9104e-02 0.0000e+00 2.4080e-05</hent>
<hent key="0240010000000000000010000000000A" call="MPI_Isend" bytes="4096" orank="10" region="0" commid="0" count="12600" tid="0" op="" dtype="" >2.0099e-02 0.0000e+00 1.8120e-05</hent>
<hent key="02400100000000000000100000000023" call="MPI_Isend" bytes="4096" orank="35" region="0" commid="0" count="3213" tid="0" op="" dtype="" >1.2578e-02 1.1921e-06 6.5088e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >8.8000e-04 3.1400e-04 5.6601e-04</hent>
<hent key="024001000000000000001000000002AB" call="MPI_Isend" bytes="4096" orank="683" region="0" commid="0" count="3143" tid="0" op="" dtype="" >1.1072e-02 9.5367e-07 3.0041e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.1008e-05 4.1008e-05 4.1008e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.0170e-03 2.9683e-04 7.0000e-04</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.1921e-05 1.9073e-06 1.0014e-05</hent>
<hent key="03800100000000000000000400000007" call="MPI_Irecv" bytes="4" orank="7" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.4012e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000008" call="MPI_Irecv" bytes="4" orank="8" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.2714e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000000A" call="MPI_Irecv" bytes="4" orank="10" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.5804e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000000F" call="MPI_Irecv" bytes="4" orank="15" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.7220e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000023" call="MPI_Irecv" bytes="4" orank="35" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.7483e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001C0000000007" call="MPI_Irecv" bytes="7168" orank="7" region="0" commid="0" count="9650" tid="0" op="" dtype="" >9.6154e-04 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000001C000000000F" call="MPI_Irecv" bytes="7168" orank="15" region="0" commid="0" count="9592" tid="0" op="" dtype="" >1.2586e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000001C0000000023" call="MPI_Irecv" bytes="7168" orank="35" region="0" commid="0" count="3" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000004000002AB" call="MPI_Irecv" bytes="4" orank="683" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.0879e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001C00000002AB" call="MPI_Irecv" bytes="7168" orank="683" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="02400100000000000000000400000007" call="MPI_Isend" bytes="4" orank="7" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.3609e-03 0.0000e+00 2.8133e-05</hent>
<hent key="02400100000000000000000400000008" call="MPI_Isend" bytes="4" orank="8" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.2726e-03 0.0000e+00 5.6028e-05</hent>
<hent key="0240010000000000000000040000000A" call="MPI_Isend" bytes="4" orank="10" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.2581e-03 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000040000000F" call="MPI_Isend" bytes="4" orank="15" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.5314e-03 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000000400000023" call="MPI_Isend" bytes="4" orank="35" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.6770e-02 4.0531e-06 2.5988e-04</hent>
<hent key="024001000000000000001C0000000007" call="MPI_Isend" bytes="7168" orank="7" region="0" commid="0" count="10579" tid="0" op="" dtype="" >1.9143e-02 9.5367e-07 2.2888e-05</hent>
<hent key="024001000000000000001C000000000F" call="MPI_Isend" bytes="7168" orank="15" region="0" commid="0" count="9610" tid="0" op="" dtype="" >1.9923e-02 9.5367e-07 2.0027e-05</hent>
<hent key="024001000000000000001C0000000023" call="MPI_Isend" bytes="7168" orank="35" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.5020e-05 7.1526e-06 7.8678e-06</hent>
<hent key="024001000000000000000004000002AB" call="MPI_Isend" bytes="4" orank="683" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.8855e-02 3.8147e-06 6.5088e-05</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8896e-04 2.8896e-04 2.8896e-04</hent>
<hent key="03800100000000000000020000000008" call="MPI_Irecv" bytes="512" orank="8" region="0" commid="0" count="3374" tid="0" op="" dtype="" >5.8627e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000002000000000A" call="MPI_Irecv" bytes="512" orank="10" region="0" commid="0" count="3310" tid="0" op="" dtype="" >6.8641e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000007" call="MPI_Irecv" bytes="1280" orank="7" region="0" commid="0" count="156" tid="0" op="" dtype="" >3.6716e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000050000000008" call="MPI_Irecv" bytes="1280" orank="8" region="0" commid="0" count="50" tid="0" op="" dtype="" >1.3351e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000A" call="MPI_Irecv" bytes="1280" orank="10" region="0" commid="0" count="64" tid="0" op="" dtype="" >2.5272e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000F" call="MPI_Irecv" bytes="1280" orank="15" region="0" commid="0" count="133" tid="0" op="" dtype="" >2.6226e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000023" call="MPI_Irecv" bytes="1280" orank="35" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.6928e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.5010e+01 1.0967e-05 1.8464e-01</hent>
<hent key="038001000000000000000500000002AB" call="MPI_Irecv" bytes="1280" orank="683" region="0" commid="0" count="44" tid="0" op="" dtype="" >9.7752e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000020000000008" call="MPI_Isend" bytes="512" orank="8" region="0" commid="0" count="3384" tid="0" op="" dtype="" >2.1544e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000002000000000A" call="MPI_Isend" bytes="512" orank="10" region="0" commid="0" count="3376" tid="0" op="" dtype="" >1.7276e-03 0.0000e+00 1.4067e-05</hent>
<hent key="03800100000000000000080000000007" call="MPI_Irecv" bytes="2048" orank="7" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000008" call="MPI_Irecv" bytes="2048" orank="8" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000008000000000A" call="MPI_Irecv" bytes="2048" orank="10" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000008000000000F" call="MPI_Irecv" bytes="2048" orank="15" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000023" call="MPI_Irecv" bytes="2048" orank="35" region="0" commid="0" count="147" tid="0" op="" dtype="" >4.6730e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002AB" call="MPI_Irecv" bytes="2048" orank="683" region="0" commid="0" count="145" tid="0" op="" dtype="" >3.1471e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000050000000007" call="MPI_Isend" bytes="1280" orank="7" region="0" commid="0" count="138" tid="0" op="" dtype="" >2.1243e-04 9.5367e-07 7.1526e-06</hent>
<hent key="02400100000000000000050000000008" call="MPI_Isend" bytes="1280" orank="8" region="0" commid="0" count="61" tid="0" op="" dtype="" >2.0552e-04 2.1458e-06 4.0531e-06</hent>
<hent key="0240010000000000000005000000000A" call="MPI_Isend" bytes="1280" orank="10" region="0" commid="0" count="56" tid="0" op="" dtype="" >1.1373e-04 9.5367e-07 9.0599e-06</hent>
<hent key="0240010000000000000005000000000F" call="MPI_Isend" bytes="1280" orank="15" region="0" commid="0" count="139" tid="0" op="" dtype="" >2.1052e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000050000000023" call="MPI_Isend" bytes="1280" orank="35" region="0" commid="0" count="42" tid="0" op="" dtype="" >2.3127e-04 3.8147e-06 1.6928e-05</hent>
<hent key="024001000000000000000500000002AB" call="MPI_Isend" bytes="1280" orank="683" region="0" commid="0" count="36" tid="0" op="" dtype="" >1.5521e-04 3.8147e-06 5.0068e-06</hent>
</hash>
<internal rank="11" log_i="1724765674.525601" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="12" mpi_size="696" stamp_init="1724765564.465673" stamp_final="1724765674.523005" username="apac4" allocationname="unknown" flags="0" pid="1612129" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10057e+02" utime="8.68977e+01" stime="1.44431e+01" mtime="7.18365e+01" gflop="0.00000e+00" gbyte="3.78338e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18365e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e015e215e3158e56e315e31513" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09919e+02" utime="8.68627e+01" stime="1.44347e+01" mtime="7.18365e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18365e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1866e+09" > 8.4915e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1895e+09" > 4.3637e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1613e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9783e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3280e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1737e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5766e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.4458e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0572e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3142e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3323e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="209" >
<hent key="02400100000000000000080000000008" call="MPI_Isend" bytes="2048" orank="8" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.1683e-05 1.9073e-06 2.1458e-06</hent>
<hent key="0240010000000000000008000000000D" call="MPI_Isend" bytes="2048" orank="13" region="0" commid="0" count="1" tid="0" op="" dtype="" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="0240010000000000000008000000000F" call="MPI_Isend" bytes="2048" orank="15" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="02400100000000000000080000000010" call="MPI_Isend" bytes="2048" orank="16" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.7418e-05 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000080000000024" call="MPI_Isend" bytes="2048" orank="36" region="0" commid="0" count="141" tid="0" op="" dtype="" >1.0140e-03 5.0068e-06 4.0054e-05</hent>
<hent key="024001000000000000000800000002AC" call="MPI_Isend" bytes="2048" orank="684" region="0" commid="0" count="135" tid="0" op="" dtype="" >7.4100e-04 3.8147e-06 1.4067e-05</hent>
<hent key="038001000000000000000E000000000D" call="MPI_Irecv" bytes="3584" orank="13" region="0" commid="0" count="60" tid="0" op="" dtype="" >9.7752e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E000000000F" call="MPI_Irecv" bytes="3584" orank="15" region="0" commid="0" count="84" tid="0" op="" dtype="" >5.8174e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000024" call="MPI_Irecv" bytes="3584" orank="36" region="0" commid="0" count="134" tid="0" op="" dtype="" >9.2506e-05 0.0000e+00 3.8147e-06</hent>
<hent key="038001000000000000000E00000002AC" call="MPI_Irecv" bytes="3584" orank="684" region="0" commid="0" count="131" tid="0" op="" dtype="" >6.8426e-05 0.0000e+00 7.8678e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000E000000000D" call="MPI_Isend" bytes="3584" orank="13" region="0" commid="0" count="84" tid="0" op="" dtype="" >2.2984e-04 9.5367e-07 5.0068e-06</hent>
<hent key="024001000000000000000E000000000F" call="MPI_Isend" bytes="3584" orank="15" region="0" commid="0" count="182" tid="0" op="" dtype="" >3.4761e-04 0.0000e+00 8.8215e-06</hent>
<hent key="024001000000000000000E0000000024" call="MPI_Isend" bytes="3584" orank="36" region="0" commid="0" count="111" tid="0" op="" dtype="" >9.0647e-04 5.0068e-06 2.5988e-05</hent>
<hent key="024001000000000000000E00000002AC" call="MPI_Isend" bytes="3584" orank="684" region="0" commid="0" count="157" tid="0" op="" dtype="" >9.7275e-04 3.8147e-06 1.5974e-05</hent>
<hent key="03800100000000000000028000000008" call="MPI_Irecv" bytes="640" orank="8" region="0" commid="0" count="412" tid="0" op="" dtype="" >8.9645e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000002800000000D" call="MPI_Irecv" bytes="640" orank="13" region="0" commid="0" count="363" tid="0" op="" dtype="" >9.4175e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000F" call="MPI_Irecv" bytes="640" orank="15" region="0" commid="0" count="373" tid="0" op="" dtype="" >1.6713e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000028000000010" call="MPI_Irecv" bytes="640" orank="16" region="0" commid="0" count="375" tid="0" op="" dtype="" >7.3195e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000024" call="MPI_Irecv" bytes="640" orank="36" region="0" commid="0" count="23" tid="0" op="" dtype="" >7.6294e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000140000000024" call="MPI_Irecv" bytes="5120" orank="36" region="0" commid="0" count="542" tid="0" op="" dtype="" >1.7858e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="038001000000000000000280000002AC" call="MPI_Irecv" bytes="640" orank="684" region="0" commid="0" count="21" tid="0" op="" dtype="" >1.0014e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001400000002AC" call="MPI_Irecv" bytes="5120" orank="684" region="0" commid="0" count="438" tid="0" op="" dtype="" >3.0923e-04 0.0000e+00 5.0068e-05</hent>
<hent key="03800100000000000000A00000000024" call="MPI_Irecv" bytes="40960" orank="36" region="0" commid="0" count="1777" tid="0" op="" dtype="" >9.0647e-04 0.0000e+00 2.8133e-05</hent>
<hent key="02400100000000000000028000000008" call="MPI_Isend" bytes="640" orank="8" region="0" commid="0" count="434" tid="0" op="" dtype="" >5.1904e-04 9.5367e-07 1.0014e-05</hent>
<hent key="0240010000000000000002800000000D" call="MPI_Isend" bytes="640" orank="13" region="0" commid="0" count="352" tid="0" op="" dtype="" >1.1396e-03 1.9073e-06 1.5020e-05</hent>
<hent key="0240010000000000000002800000000F" call="MPI_Isend" bytes="640" orank="15" region="0" commid="0" count="342" tid="0" op="" dtype="" >5.3334e-04 9.5367e-07 1.5974e-05</hent>
<hent key="02400100000000000000028000000010" call="MPI_Isend" bytes="640" orank="16" region="0" commid="0" count="426" tid="0" op="" dtype="" >5.1093e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000028000000024" call="MPI_Isend" bytes="640" orank="36" region="0" commid="0" count="21" tid="0" op="" dtype="" >1.2136e-04 4.0531e-06 1.5020e-05</hent>
<hent key="03800100000000000000A000000002AC" call="MPI_Irecv" bytes="40960" orank="684" region="0" commid="0" count="1387" tid="0" op="" dtype="" >8.6045e-04 0.0000e+00 5.7936e-05</hent>
<hent key="02400100000000000000140000000024" call="MPI_Isend" bytes="5120" orank="36" region="0" commid="0" count="453" tid="0" op="" dtype="" >3.2973e-03 1.9073e-06 5.6028e-05</hent>
<hent key="024001000000000000000280000002AC" call="MPI_Isend" bytes="640" orank="684" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.0610e-04 3.8147e-06 8.1062e-06</hent>
<hent key="03800100000000000000800000000024" call="MPI_Irecv" bytes="32768" orank="36" region="0" commid="0" count="10923" tid="0" op="" dtype="" >5.2085e-03 0.0000e+00 3.0994e-05</hent>
<hent key="024001000000000000001400000002AC" call="MPI_Isend" bytes="5120" orank="684" region="0" commid="0" count="427" tid="0" op="" dtype="" >2.2316e-03 9.5367e-07 3.3140e-05</hent>
<hent key="038001000000000000008000000002AC" call="MPI_Irecv" bytes="32768" orank="684" region="0" commid="0" count="11313" tid="0" op="" dtype="" >7.0214e-03 0.0000e+00 5.0068e-05</hent>
<hent key="03800100000000000000014000000008" call="MPI_Irecv" bytes="320" orank="8" region="0" commid="0" count="443" tid="0" op="" dtype="" >9.9421e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000D" call="MPI_Irecv" bytes="320" orank="13" region="0" commid="0" count="542" tid="0" op="" dtype="" >1.2112e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000F" call="MPI_Irecv" bytes="320" orank="15" region="0" commid="0" count="512" tid="0" op="" dtype="" >2.1529e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000010" call="MPI_Irecv" bytes="320" orank="16" region="0" commid="0" count="421" tid="0" op="" dtype="" >6.5088e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000024" call="MPI_Irecv" bytes="320" orank="36" region="0" commid="0" count="23" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A00000000024" call="MPI_Isend" bytes="40960" orank="36" region="0" commid="0" count="1378" tid="0" op="" dtype="" >4.0696e-02 8.8215e-06 1.1802e-04</hent>
<hent key="038001000000000000000140000002AC" call="MPI_Irecv" bytes="320" orank="684" region="0" commid="0" count="17" tid="0" op="" dtype="" >1.3351e-05 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000A000000002AC" call="MPI_Isend" bytes="40960" orank="684" region="0" commid="0" count="1413" tid="0" op="" dtype="" >2.3369e-02 6.9141e-06 8.0824e-05</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >4.7684e-06 0.0000e+00 1.9073e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >6.1613e+00 0.0000e+00 1.1032e-01</hent>
<hent key="02400100000000000000800000000024" call="MPI_Isend" bytes="32768" orank="36" region="0" commid="0" count="11322" tid="0" op="" dtype="" >3.4476e-01 8.8215e-06 1.6403e-04</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.0027e-05 0.0000e+00 2.0027e-05</hent>
<hent key="024001000000000000008000000002AC" call="MPI_Isend" bytes="32768" orank="684" region="0" commid="0" count="11287" tid="0" op="" dtype="" >1.7746e-01 5.9605e-06 8.7976e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 0.0000e+00 2.8610e-06</hent>
<hent key="02400100000000000000014000000008" call="MPI_Isend" bytes="320" orank="8" region="0" commid="0" count="430" tid="0" op="" dtype="" >4.3917e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000001400000000D" call="MPI_Isend" bytes="320" orank="13" region="0" commid="0" count="584" tid="0" op="" dtype="" >1.6589e-03 1.9073e-06 6.9141e-06</hent>
<hent key="0240010000000000000001400000000F" call="MPI_Isend" bytes="320" orank="15" region="0" commid="0" count="561" tid="0" op="" dtype="" >7.1549e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000014000000010" call="MPI_Isend" bytes="320" orank="16" region="0" commid="0" count="438" tid="0" op="" dtype="" >4.5204e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000014000000024" call="MPI_Isend" bytes="320" orank="36" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.5116e-04 3.8147e-06 1.1921e-05</hent>
<hent key="024001000000000000000140000002AC" call="MPI_Isend" bytes="320" orank="684" region="0" commid="0" count="24" tid="0" op="" dtype="" >1.1277e-04 3.8147e-06 1.3113e-05</hent>
<hent key="03800100000000000000000000000008" call="MPI_Irecv" bytes="0" orank="8" region="0" commid="0" count="310" tid="0" op="" dtype="" >6.7711e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000D" call="MPI_Irecv" bytes="0" orank="13" region="0" commid="0" count="557" tid="0" op="" dtype="" >1.3494e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000F" call="MPI_Irecv" bytes="0" orank="15" region="0" commid="0" count="559" tid="0" op="" dtype="" >1.9550e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000000000010" call="MPI_Irecv" bytes="0" orank="16" region="0" commid="0" count="364" tid="0" op="" dtype="" >7.1526e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000024" call="MPI_Irecv" bytes="0" orank="36" region="0" commid="0" count="88" tid="0" op="" dtype="" >3.0041e-05 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000000000002AC" call="MPI_Irecv" bytes="0" orank="684" region="0" commid="0" count="90" tid="0" op="" dtype="" >3.0518e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.0885e-04 0.0000e+00 8.2016e-05</hent>
<hent key="03800100000000000000030000000008" call="MPI_Irecv" bytes="768" orank="8" region="0" commid="0" count="670" tid="0" op="" dtype="" >9.2030e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000030000000010" call="MPI_Irecv" bytes="768" orank="16" region="0" commid="0" count="772" tid="0" op="" dtype="" >1.3900e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000008" call="MPI_Isend" bytes="0" orank="8" region="0" commid="0" count="328" tid="0" op="" dtype="" >2.1243e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000000000000000D" call="MPI_Isend" bytes="0" orank="13" region="0" commid="0" count="530" tid="0" op="" dtype="" >1.1363e-03 9.5367e-07 3.2902e-05</hent>
<hent key="0240010000000000000000000000000F" call="MPI_Isend" bytes="0" orank="15" region="0" commid="0" count="534" tid="0" op="" dtype="" >4.1413e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000010" call="MPI_Isend" bytes="0" orank="16" region="0" commid="0" count="363" tid="0" op="" dtype="" >2.5964e-04 0.0000e+00 8.8215e-06</hent>
<hent key="02400100000000000000000000000024" call="MPI_Isend" bytes="0" orank="36" region="0" commid="0" count="78" tid="0" op="" dtype="" >3.4785e-04 9.5367e-07 1.1921e-05</hent>
<hent key="024001000000000000000000000002AC" call="MPI_Isend" bytes="0" orank="684" region="0" commid="0" count="86" tid="0" op="" dtype="" >3.3236e-04 2.1458e-06 7.8678e-06</hent>
<hent key="03800100000000000000060000000008" call="MPI_Irecv" bytes="1536" orank="8" region="0" commid="0" count="54" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000D" call="MPI_Irecv" bytes="1536" orank="13" region="0" commid="0" count="13" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000006000000000F" call="MPI_Irecv" bytes="1536" orank="15" region="0" commid="0" count="14" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000010" call="MPI_Irecv" bytes="1536" orank="16" region="0" commid="0" count="54" tid="0" op="" dtype="" >8.5831e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000060000000024" call="MPI_Irecv" bytes="1536" orank="36" region="0" commid="0" count="70" tid="0" op="" dtype="" >4.5538e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000600000002AC" call="MPI_Irecv" bytes="1536" orank="684" region="0" commid="0" count="79" tid="0" op="" dtype="" >5.2929e-05 0.0000e+00 1.1206e-05</hent>
<hent key="02400100000000000000030000000008" call="MPI_Isend" bytes="768" orank="8" region="0" commid="0" count="776" tid="0" op="" dtype="" >4.1175e-04 0.0000e+00 2.5034e-05</hent>
<hent key="02400100000000000000030000000010" call="MPI_Isend" bytes="768" orank="16" region="0" commid="0" count="566" tid="0" op="" dtype="" >2.8348e-04 0.0000e+00 4.0531e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 2.1458e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C00000000D" call="MPI_Irecv" bytes="448" orank="13" region="0" commid="0" count="16" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C00000000F" call="MPI_Irecv" bytes="448" orank="15" region="0" commid="0" count="20" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000008" call="MPI_Isend" bytes="1536" orank="8" region="0" commid="0" count="46" tid="0" op="" dtype="" >8.5831e-05 9.5367e-07 8.1062e-06</hent>
<hent key="0240010000000000000006000000000D" call="MPI_Isend" bytes="1536" orank="13" region="0" commid="0" count="15" tid="0" op="" dtype="" >5.9128e-05 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000006000000000F" call="MPI_Isend" bytes="1536" orank="15" region="0" commid="0" count="17" tid="0" op="" dtype="" >3.6240e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000060000000010" call="MPI_Isend" bytes="1536" orank="16" region="0" commid="0" count="56" tid="0" op="" dtype="" >1.0085e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000060000000024" call="MPI_Isend" bytes="1536" orank="36" region="0" commid="0" count="78" tid="0" op="" dtype="" >5.8150e-04 4.7684e-06 1.8835e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002AC" call="MPI_Isend" bytes="1536" orank="684" region="0" commid="0" count="76" tid="0" op="" dtype="" >3.9649e-04 3.8147e-06 1.1921e-05</hent>
<hent key="038001000000000000000C0000000024" call="MPI_Irecv" bytes="3072" orank="36" region="0" commid="0" count="371" tid="0" op="" dtype="" >2.7609e-04 0.0000e+00 8.1062e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002AC" call="MPI_Irecv" bytes="3072" orank="684" region="0" commid="0" count="352" tid="0" op="" dtype="" >2.0123e-04 0.0000e+00 1.4782e-05</hent>
<hent key="0240010000000000000001C00000000D" call="MPI_Isend" bytes="448" orank="13" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.3828e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C00000000F" call="MPI_Isend" bytes="448" orank="15" region="0" commid="0" count="42" tid="0" op="" dtype="" >1.4067e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000024" call="MPI_Isend" bytes="3072" orank="36" region="0" commid="0" count="395" tid="0" op="" dtype="" >3.1104e-03 4.7684e-06 2.8133e-05</hent>
<hent key="024001000000000000000C00000002AC" call="MPI_Isend" bytes="3072" orank="684" region="0" commid="0" count="343" tid="0" op="" dtype="" >2.0349e-03 3.8147e-06 1.5974e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >6.4397e-04 2.1005e-04 2.1887e-04</hent>
<hent key="03800100000000000000038000000008" call="MPI_Irecv" bytes="896" orank="8" region="0" commid="0" count="3037" tid="0" op="" dtype="" >5.0259e-04 0.0000e+00 6.1989e-06</hent>
<hent key="0380010000000000000003800000000D" call="MPI_Irecv" bytes="896" orank="13" region="0" commid="0" count="155" tid="0" op="" dtype="" >3.7909e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000F" call="MPI_Irecv" bytes="896" orank="15" region="0" commid="0" count="181" tid="0" op="" dtype="" >8.0824e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000010" call="MPI_Irecv" bytes="896" orank="16" region="0" commid="0" count="2933" tid="0" op="" dtype="" >6.0344e-04 0.0000e+00 5.0068e-05</hent>
<hent key="03800100000000000000038000000024" call="MPI_Irecv" bytes="896" orank="36" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.6212e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000380000002AC" call="MPI_Irecv" bytes="896" orank="684" region="0" commid="0" count="26" tid="0" op="" dtype="" >1.0014e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.0218e-04 2.0218e-04 2.0218e-04</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.1921e-06 1.1921e-06 1.1921e-06</hent>
<hent key="03800100000000000000180000000008" call="MPI_Irecv" bytes="6144" orank="8" region="0" commid="0" count="2384" tid="0" op="" dtype="" >4.3368e-04 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000180000000010" call="MPI_Irecv" bytes="6144" orank="16" region="0" commid="0" count="2806" tid="0" op="" dtype="" >6.1893e-04 0.0000e+00 1.4067e-05</hent>
<hent key="03800100000000000000180000000024" call="MPI_Irecv" bytes="6144" orank="36" region="0" commid="0" count="11" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000001800000002AC" call="MPI_Irecv" bytes="6144" orank="684" region="0" commid="0" count="11" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000038000000008" call="MPI_Isend" bytes="896" orank="8" region="0" commid="0" count="2892" tid="0" op="" dtype="" >1.9379e-03 0.0000e+00 6.4850e-05</hent>
<hent key="0240010000000000000003800000000D" call="MPI_Isend" bytes="896" orank="13" region="0" commid="0" count="154" tid="0" op="" dtype="" >5.3358e-04 2.8610e-06 1.3113e-05</hent>
<hent key="0240010000000000000003800000000F" call="MPI_Isend" bytes="896" orank="15" region="0" commid="0" count="189" tid="0" op="" dtype="" >3.4928e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000038000000010" call="MPI_Isend" bytes="896" orank="16" region="0" commid="0" count="3105" tid="0" op="" dtype="" >1.7197e-03 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000038000000024" call="MPI_Isend" bytes="896" orank="36" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.6785e-04 5.0068e-06 1.2875e-05</hent>
<hent key="024001000000000000000380000002AC" call="MPI_Isend" bytes="896" orank="684" region="0" commid="0" count="24" tid="0" op="" dtype="" >1.1754e-04 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000180000000008" call="MPI_Isend" bytes="6144" orank="8" region="0" commid="0" count="2895" tid="0" op="" dtype="" >5.8925e-03 9.5367e-07 4.7922e-05</hent>
<hent key="02400100000000000000180000000010" call="MPI_Isend" bytes="6144" orank="16" region="0" commid="0" count="1969" tid="0" op="" dtype="" >3.7565e-03 9.5367e-07 4.6968e-05</hent>
<hent key="02400100000000000000180000000024" call="MPI_Isend" bytes="6144" orank="36" region="0" commid="0" count="10" tid="0" op="" dtype="" >8.7261e-05 6.9141e-06 1.8120e-05</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.4823e-05 4.4823e-05 4.4823e-05</hent>
<hent key="024001000000000000001800000002AC" call="MPI_Isend" bytes="6144" orank="684" region="0" commid="0" count="9" tid="0" op="" dtype="" >7.2002e-05 5.9605e-06 1.5020e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.6131e+00 1.0967e-05 1.2665e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.7595e-04 8.7595e-04 8.7595e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0572e-02 1.0572e-02 1.0572e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >4.4200e-03 4.4200e-03 4.4200e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.0880e-01 3.2861e-03 2.0082e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.9199e-04 5.9199e-04 5.9199e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >3.8147e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5766e+00 3.9315e-04 2.4773e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.3280e-03 9.5367e-07 1.2279e-03</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000070000000008" call="MPI_Irecv" bytes="1792" orank="8" region="0" commid="0" count="20" tid="0" op="" dtype="" >6.6757e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000007000000000F" call="MPI_Irecv" bytes="1792" orank="15" region="0" commid="0" count="4" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000070000000010" call="MPI_Irecv" bytes="1792" orank="16" region="0" commid="0" count="14" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000024" call="MPI_Irecv" bytes="1792" orank="36" region="0" commid="0" count="121" tid="0" op="" dtype="" >9.2268e-05 0.0000e+00 8.1062e-06</hent>
<hent key="038001000000000000000700000002AC" call="MPI_Irecv" bytes="1792" orank="684" region="0" commid="0" count="114" tid="0" op="" dtype="" >4.1962e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >8.5115e-05 0.0000e+00 6.0797e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9783e+01 0.0000e+00 3.8414e+01</hent>
<hent key="038001000000000000000A0000000008" call="MPI_Irecv" bytes="2560" orank="8" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.1921e-06 1.1921e-06 1.1921e-06</hent>
<hent key="038001000000000000000A0000000010" call="MPI_Irecv" bytes="2560" orank="16" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000024" call="MPI_Irecv" bytes="2560" orank="36" region="0" commid="0" count="373" tid="0" op="" dtype="" >2.7204e-04 0.0000e+00 1.2159e-05</hent>
<hent key="038001000000000000000A00000002AC" call="MPI_Irecv" bytes="2560" orank="684" region="0" commid="0" count="369" tid="0" op="" dtype="" >1.9336e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000070000000008" call="MPI_Isend" bytes="1792" orank="8" region="0" commid="0" count="23" tid="0" op="" dtype="" >4.3631e-05 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000007000000000D" call="MPI_Isend" bytes="1792" orank="13" region="0" commid="0" count="2" tid="0" op="" dtype="" >8.1062e-06 4.0531e-06 4.0531e-06</hent>
<hent key="0240010000000000000007000000000F" call="MPI_Isend" bytes="1792" orank="15" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.6212e-05 1.9073e-06 8.1062e-06</hent>
<hent key="02400100000000000000070000000010" call="MPI_Isend" bytes="1792" orank="16" region="0" commid="0" count="20" tid="0" op="" dtype="" >3.3855e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000024" call="MPI_Isend" bytes="1792" orank="36" region="0" commid="0" count="140" tid="0" op="" dtype="" >9.9230e-04 5.0068e-06 1.8120e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3142e-02 8.3142e-02 8.3142e-02</hent>
<hent key="024001000000000000000700000002AC" call="MPI_Isend" bytes="1792" orank="684" region="0" commid="0" count="124" tid="0" op="" dtype="" >6.7878e-04 3.8147e-06 1.5974e-05</hent>
<hent key="024001000000000000000A0000000008" call="MPI_Isend" bytes="2560" orank="8" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000A0000000010" call="MPI_Isend" bytes="2560" orank="16" region="0" commid="0" count="2" tid="0" op="" dtype="" >5.0068e-06 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000024" call="MPI_Isend" bytes="2560" orank="36" region="0" commid="0" count="355" tid="0" op="" dtype="" >2.6224e-03 5.0068e-06 2.0981e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.0017e-04 3.6001e-05 1.3399e-04</hent>
<hent key="024001000000000000000A00000002AC" call="MPI_Isend" bytes="2560" orank="684" region="0" commid="0" count="366" tid="0" op="" dtype="" >2.0764e-03 3.8147e-06 1.5974e-05</hent>
<hent key="0380010000000000000010000000000D" call="MPI_Irecv" bytes="4096" orank="13" region="0" commid="0" count="12640" tid="0" op="" dtype="" >3.2458e-03 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000010000000000F" call="MPI_Irecv" bytes="4096" orank="15" region="0" commid="0" count="12616" tid="0" op="" dtype="" >6.6595e-03 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000100000000024" call="MPI_Irecv" bytes="4096" orank="36" region="0" commid="0" count="3122" tid="0" op="" dtype="" >9.8228e-04 0.0000e+00 2.2173e-05</hent>
<hent key="038001000000000000001000000002AC" call="MPI_Irecv" bytes="4096" orank="684" region="0" commid="0" count="3249" tid="0" op="" dtype="" >1.8084e-03 0.0000e+00 7.1049e-05</hent>
<hent key="0240010000000000000010000000000D" call="MPI_Isend" bytes="4096" orank="13" region="0" commid="0" count="12616" tid="0" op="" dtype="" >3.7444e-02 9.5367e-07 2.0981e-05</hent>
<hent key="0240010000000000000010000000000F" call="MPI_Isend" bytes="4096" orank="15" region="0" commid="0" count="12518" tid="0" op="" dtype="" >2.3436e-02 0.0000e+00 3.1948e-05</hent>
<hent key="02400100000000000000100000000024" call="MPI_Isend" bytes="4096" orank="36" region="0" commid="0" count="3226" tid="0" op="" dtype="" >2.3506e-02 1.9073e-06 1.2302e-04</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.7322e-04 3.5906e-04 6.1417e-04</hent>
<hent key="024001000000000000001000000002AC" call="MPI_Isend" bytes="4096" orank="684" region="0" commid="0" count="3255" tid="0" op="" dtype="" >1.6166e-02 9.5367e-07 5.4121e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.4823e-05 4.4823e-05 4.4823e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.2068e-03 3.1686e-04 7.7701e-04</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >6.6757e-06 1.9073e-06 4.7684e-06</hent>
<hent key="03800100000000000000000400000008" call="MPI_Irecv" bytes="4" orank="8" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.9138e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000040000000D" call="MPI_Irecv" bytes="4" orank="13" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.2714e-04 0.0000e+00 3.0994e-06</hent>
<hent key="0380010000000000000000040000000F" call="MPI_Irecv" bytes="4" orank="15" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.3809e-03 0.0000e+00 1.0967e-05</hent>
<hent key="03800100000000000000000400000010" call="MPI_Irecv" bytes="4" orank="16" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.2738e-04 0.0000e+00 1.7166e-05</hent>
<hent key="03800100000000000000000400000024" call="MPI_Irecv" bytes="4" orank="36" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.0061e-03 0.0000e+00 1.7166e-05</hent>
<hent key="038001000000000000001C0000000008" call="MPI_Irecv" bytes="7168" orank="8" region="0" commid="0" count="10316" tid="0" op="" dtype="" >1.9851e-03 0.0000e+00 2.0027e-05</hent>
<hent key="038001000000000000001C0000000010" call="MPI_Irecv" bytes="7168" orank="16" region="0" commid="0" count="9894" tid="0" op="" dtype="" >2.2569e-03 0.0000e+00 8.8930e-05</hent>
<hent key="038001000000000000001C0000000024" call="MPI_Irecv" bytes="7168" orank="36" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000004000002AC" call="MPI_Irecv" bytes="4" orank="684" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.1458e-03 0.0000e+00 4.3869e-05</hent>
<hent key="02400100000000000000000400000008" call="MPI_Isend" bytes="4" orank="8" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.1918e-03 0.0000e+00 1.5974e-05</hent>
<hent key="0240010000000000000000040000000D" call="MPI_Isend" bytes="4" orank="13" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.1842e-03 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000000040000000F" call="MPI_Isend" bytes="4" orank="15" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.9379e-03 0.0000e+00 4.4823e-05</hent>
<hent key="02400100000000000000000400000010" call="MPI_Isend" bytes="4" orank="16" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.6639e-03 0.0000e+00 2.4796e-05</hent>
<hent key="02400100000000000000000400000024" call="MPI_Isend" bytes="4" orank="36" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.3842e-02 3.8147e-06 4.0603e-04</hent>
<hent key="024001000000000000001C0000000008" call="MPI_Isend" bytes="7168" orank="8" region="0" commid="0" count="9805" tid="0" op="" dtype="" >2.0794e-02 9.5367e-07 6.4135e-05</hent>
<hent key="024001000000000000001C0000000010" call="MPI_Isend" bytes="7168" orank="16" region="0" commid="0" count="10731" tid="0" op="" dtype="" >2.1076e-02 9.5367e-07 2.5988e-05</hent>
<hent key="024001000000000000000004000002AC" call="MPI_Isend" bytes="4" orank="684" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.3880e-02 3.8147e-06 7.2002e-05</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.1304e-04 3.1304e-04 3.1304e-04</hent>
<hent key="0380010000000000000002000000000D" call="MPI_Irecv" bytes="512" orank="13" region="0" commid="0" count="3384" tid="0" op="" dtype="" >6.7687e-04 0.0000e+00 8.8215e-06</hent>
<hent key="0380010000000000000002000000000F" call="MPI_Irecv" bytes="512" orank="15" region="0" commid="0" count="3380" tid="0" op="" dtype="" >6.2418e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000050000000008" call="MPI_Irecv" bytes="1280" orank="8" region="0" commid="0" count="151" tid="0" op="" dtype="" >3.9339e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000005000000000D" call="MPI_Irecv" bytes="1280" orank="13" region="0" commid="0" count="68" tid="0" op="" dtype="" >1.3828e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000F" call="MPI_Irecv" bytes="1280" orank="15" region="0" commid="0" count="55" tid="0" op="" dtype="" >2.4557e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000050000000010" call="MPI_Irecv" bytes="1280" orank="16" region="0" commid="0" count="158" tid="0" op="" dtype="" >3.1710e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000024" call="MPI_Irecv" bytes="1280" orank="36" region="0" commid="0" count="42" tid="0" op="" dtype="" >2.1935e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4495e+01 1.8835e-05 1.8461e-01</hent>
<hent key="038001000000000000000500000002AC" call="MPI_Irecv" bytes="1280" orank="684" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.5511e-05 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000002000000000D" call="MPI_Isend" bytes="512" orank="13" region="0" commid="0" count="3378" tid="0" op="" dtype="" >2.3000e-03 0.0000e+00 2.0981e-05</hent>
<hent key="0240010000000000000002000000000F" call="MPI_Isend" bytes="512" orank="15" region="0" commid="0" count="3358" tid="0" op="" dtype="" >1.9464e-03 0.0000e+00 5.1022e-05</hent>
<hent key="03800100000000000000080000000008" call="MPI_Irecv" bytes="2048" orank="8" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.9073e-06 9.5367e-07 9.5367e-07</hent>
<hent key="0380010000000000000008000000000D" call="MPI_Irecv" bytes="2048" orank="13" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000008000000000F" call="MPI_Irecv" bytes="2048" orank="15" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000080000000010" call="MPI_Irecv" bytes="2048" orank="16" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000024" call="MPI_Irecv" bytes="2048" orank="36" region="0" commid="0" count="154" tid="0" op="" dtype="" >1.2302e-04 0.0000e+00 9.0599e-06</hent>
<hent key="038001000000000000000800000002AC" call="MPI_Irecv" bytes="2048" orank="684" region="0" commid="0" count="160" tid="0" op="" dtype="" >6.4611e-05 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000050000000008" call="MPI_Isend" bytes="1280" orank="8" region="0" commid="0" count="164" tid="0" op="" dtype="" >2.6059e-04 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000005000000000D" call="MPI_Isend" bytes="1280" orank="13" region="0" commid="0" count="62" tid="0" op="" dtype="" >2.2149e-04 2.8610e-06 8.1062e-06</hent>
<hent key="0240010000000000000005000000000F" call="MPI_Isend" bytes="1280" orank="15" region="0" commid="0" count="51" tid="0" op="" dtype="" >1.0157e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000010" call="MPI_Isend" bytes="1280" orank="16" region="0" commid="0" count="113" tid="0" op="" dtype="" >1.7786e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000050000000024" call="MPI_Isend" bytes="1280" orank="36" region="0" commid="0" count="37" tid="0" op="" dtype="" >2.8300e-04 4.7684e-06 2.3842e-05</hent>
<hent key="024001000000000000000500000002AC" call="MPI_Isend" bytes="1280" orank="684" region="0" commid="0" count="52" tid="0" op="" dtype="" >2.6536e-04 3.8147e-06 1.0967e-05</hent>
</hash>
<internal rank="12" log_i="1724765674.523005" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="13" mpi_size="696" stamp_init="1724765564.465368" stamp_final="1724765674.524930" username="apac4" allocationname="unknown" flags="0" pid="1612130" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10060e+02" utime="8.91629e+01" stime="1.38527e+01" mtime="7.29350e+01" gflop="0.00000e+00" gbyte="3.78311e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.29350e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09921e+02" utime="8.91388e+01" stime="1.38338e+01" mtime="7.29350e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.29350e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1970e+09" > 6.0757e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1929e+09" > 2.6782e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1001e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9800e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.9857e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5762e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.7681e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0631e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3063e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4726e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="209" >
<hent key="02400100000000000000080000000009" call="MPI_Isend" bytes="2048" orank="9" region="0" commid="0" count="3" tid="0" op="" dtype="" >5.9605e-06 1.9073e-06 2.1458e-06</hent>
<hent key="0240010000000000000008000000000C" call="MPI_Isend" bytes="2048" orank="12" region="0" commid="0" count="2" tid="0" op="" dtype="" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="0240010000000000000008000000000E" call="MPI_Isend" bytes="2048" orank="14" region="0" commid="0" count="2" tid="0" op="" dtype="" >7.1526e-06 3.0994e-06 4.0531e-06</hent>
<hent key="02400100000000000000080000000011" call="MPI_Isend" bytes="2048" orank="17" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.0967e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000080000000025" call="MPI_Isend" bytes="2048" orank="37" region="0" commid="0" count="153" tid="0" op="" dtype="" >8.6737e-04 4.7684e-06 1.1921e-05</hent>
<hent key="024001000000000000000800000002AD" call="MPI_Isend" bytes="2048" orank="685" region="0" commid="0" count="147" tid="0" op="" dtype="" >6.9332e-04 3.8147e-06 1.1206e-05</hent>
<hent key="038001000000000000000E000000000C" call="MPI_Irecv" bytes="3584" orank="12" region="0" commid="0" count="84" tid="0" op="" dtype="" >4.0054e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E000000000E" call="MPI_Irecv" bytes="3584" orank="14" region="0" commid="0" count="91" tid="0" op="" dtype="" >9.5367e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000E0000000025" call="MPI_Irecv" bytes="3584" orank="37" region="0" commid="0" count="126" tid="0" op="" dtype="" >3.8862e-05 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000E00000002AD" call="MPI_Irecv" bytes="3584" orank="685" region="0" commid="0" count="112" tid="0" op="" dtype="" >3.0756e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000E000000000C" call="MPI_Isend" bytes="3584" orank="12" region="0" commid="0" count="60" tid="0" op="" dtype="" >9.4652e-05 9.5367e-07 2.1458e-06</hent>
<hent key="024001000000000000000E000000000E" call="MPI_Isend" bytes="3584" orank="14" region="0" commid="0" count="98" tid="0" op="" dtype="" >2.2864e-04 9.5367e-07 5.9605e-06</hent>
<hent key="024001000000000000000E0000000025" call="MPI_Isend" bytes="3584" orank="37" region="0" commid="0" count="119" tid="0" op="" dtype="" >7.1740e-04 5.0068e-06 7.1526e-06</hent>
<hent key="024001000000000000000E00000002AD" call="MPI_Isend" bytes="3584" orank="685" region="0" commid="0" count="136" tid="0" op="" dtype="" >6.8831e-04 3.8147e-06 9.7752e-06</hent>
<hent key="03800100000000000000028000000009" call="MPI_Irecv" bytes="640" orank="9" region="0" commid="0" count="438" tid="0" op="" dtype="" >9.9421e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000C" call="MPI_Irecv" bytes="640" orank="12" region="0" commid="0" count="352" tid="0" op="" dtype="" >1.3232e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000E" call="MPI_Irecv" bytes="640" orank="14" region="0" commid="0" count="325" tid="0" op="" dtype="" >9.2268e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000011" call="MPI_Irecv" bytes="640" orank="17" region="0" commid="0" count="430" tid="0" op="" dtype="" >1.6832e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000025" call="MPI_Irecv" bytes="640" orank="37" region="0" commid="0" count="28" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000140000000025" call="MPI_Irecv" bytes="5120" orank="37" region="0" commid="0" count="472" tid="0" op="" dtype="" >9.5844e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="038001000000000000000280000002AD" call="MPI_Irecv" bytes="640" orank="685" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.8358e-05 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000001400000002AD" call="MPI_Irecv" bytes="5120" orank="685" region="0" commid="0" count="633" tid="0" op="" dtype="" >1.3113e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000A00000000025" call="MPI_Irecv" bytes="40960" orank="37" region="0" commid="0" count="1517" tid="0" op="" dtype="" >2.8110e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000028000000009" call="MPI_Isend" bytes="640" orank="9" region="0" commid="0" count="460" tid="0" op="" dtype="" >5.4240e-04 9.5367e-07 8.8215e-06</hent>
<hent key="0240010000000000000002800000000C" call="MPI_Isend" bytes="640" orank="12" region="0" commid="0" count="363" tid="0" op="" dtype="" >5.5671e-04 9.5367e-07 1.2875e-05</hent>
<hent key="0240010000000000000002800000000E" call="MPI_Isend" bytes="640" orank="14" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.0762e-03 1.9073e-06 1.6928e-05</hent>
<hent key="02400100000000000000028000000011" call="MPI_Isend" bytes="640" orank="17" region="0" commid="0" count="422" tid="0" op="" dtype="" >5.0855e-04 9.5367e-07 7.8678e-06</hent>
<hent key="02400100000000000000028000000025" call="MPI_Isend" bytes="640" orank="37" region="0" commid="0" count="18" tid="0" op="" dtype="" >9.7036e-05 4.0531e-06 1.0014e-05</hent>
<hent key="03800100000000000000A000000002AD" call="MPI_Irecv" bytes="40960" orank="685" region="0" commid="0" count="2077" tid="0" op="" dtype="" >4.9853e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000140000000025" call="MPI_Isend" bytes="5120" orank="37" region="0" commid="0" count="560" tid="0" op="" dtype="" >2.2640e-03 1.1921e-06 7.1526e-06</hent>
<hent key="024001000000000000000280000002AD" call="MPI_Isend" bytes="640" orank="685" region="0" commid="0" count="22" tid="0" op="" dtype="" >9.2745e-05 3.8147e-06 5.0068e-06</hent>
<hent key="03800100000000000000800000000025" call="MPI_Irecv" bytes="32768" orank="37" region="0" commid="0" count="11183" tid="0" op="" dtype="" >2.1873e-03 0.0000e+00 2.8849e-05</hent>
<hent key="024001000000000000001400000002AD" call="MPI_Isend" bytes="5120" orank="685" region="0" commid="0" count="650" tid="0" op="" dtype="" >2.3777e-03 9.5367e-07 7.1526e-06</hent>
<hent key="038001000000000000008000000002AD" call="MPI_Irecv" bytes="32768" orank="685" region="0" commid="0" count="10623" tid="0" op="" dtype="" >2.6262e-03 0.0000e+00 1.5020e-05</hent>
<hent key="03800100000000000000014000000009" call="MPI_Irecv" bytes="320" orank="9" region="0" commid="0" count="440" tid="0" op="" dtype="" >1.2064e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000C" call="MPI_Irecv" bytes="320" orank="12" region="0" commid="0" count="584" tid="0" op="" dtype="" >2.0146e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000E" call="MPI_Irecv" bytes="320" orank="14" region="0" commid="0" count="583" tid="0" op="" dtype="" >1.4496e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000011" call="MPI_Irecv" bytes="320" orank="17" region="0" commid="0" count="449" tid="0" op="" dtype="" >1.7500e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000014000000025" call="MPI_Irecv" bytes="320" orank="37" region="0" commid="0" count="29" tid="0" op="" dtype="" >9.7752e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A00000000025" call="MPI_Isend" bytes="40960" orank="37" region="0" commid="0" count="1809" tid="0" op="" dtype="" >3.6416e-02 1.0014e-05 4.4823e-05</hent>
<hent key="038001000000000000000140000002AD" call="MPI_Irecv" bytes="320" orank="685" region="0" commid="0" count="23" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000A000000002AD" call="MPI_Isend" bytes="40960" orank="685" region="0" commid="0" count="2119" tid="0" op="" dtype="" >2.2697e-02 6.9141e-06 1.8835e-05</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >6.1001e+00 0.0000e+00 1.1098e-01</hent>
<hent key="02400100000000000000800000000025" call="MPI_Isend" bytes="32768" orank="37" region="0" commid="0" count="10891" tid="0" op="" dtype="" >2.2175e-01 9.7752e-06 1.6904e-04</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-05 0.0000e+00 1.8120e-05</hent>
<hent key="024001000000000000008000000002AD" call="MPI_Isend" bytes="32768" orank="685" region="0" commid="0" count="10581" tid="0" op="" dtype="" >1.1301e-01 5.9605e-06 5.8889e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000009" call="MPI_Isend" bytes="320" orank="9" region="0" commid="0" count="410" tid="0" op="" dtype="" >4.0030e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000001400000000C" call="MPI_Isend" bytes="320" orank="12" region="0" commid="0" count="542" tid="0" op="" dtype="" >6.5589e-04 0.0000e+00 1.2875e-05</hent>
<hent key="0240010000000000000001400000000E" call="MPI_Isend" bytes="320" orank="14" region="0" commid="0" count="564" tid="0" op="" dtype="" >1.4899e-03 1.9073e-06 8.1062e-06</hent>
<hent key="02400100000000000000014000000011" call="MPI_Isend" bytes="320" orank="17" region="0" commid="0" count="462" tid="0" op="" dtype="" >4.6539e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000014000000025" call="MPI_Isend" bytes="320" orank="37" region="0" commid="0" count="24" tid="0" op="" dtype="" >1.0991e-04 3.8147e-06 5.0068e-06</hent>
<hent key="024001000000000000000140000002AD" call="MPI_Isend" bytes="320" orank="685" region="0" commid="0" count="23" tid="0" op="" dtype="" >1.0037e-04 3.8147e-06 5.0068e-06</hent>
<hent key="03800100000000000000000000000009" call="MPI_Irecv" bytes="0" orank="9" region="0" commid="0" count="342" tid="0" op="" dtype="" >7.7963e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000C" call="MPI_Irecv" bytes="0" orank="12" region="0" commid="0" count="530" tid="0" op="" dtype="" >1.4997e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000000000000000E" call="MPI_Irecv" bytes="0" orank="14" region="0" commid="0" count="573" tid="0" op="" dtype="" >1.4234e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000011" call="MPI_Irecv" bytes="0" orank="17" region="0" commid="0" count="335" tid="0" op="" dtype="" >1.0157e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000025" call="MPI_Irecv" bytes="0" orank="37" region="0" commid="0" count="79" tid="0" op="" dtype="" >1.4067e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002AD" call="MPI_Irecv" bytes="0" orank="685" region="0" commid="0" count="83" tid="0" op="" dtype="" >1.9312e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.0313e-04 0.0000e+00 8.5115e-05</hent>
<hent key="03800100000000000000030000000009" call="MPI_Irecv" bytes="768" orank="9" region="0" commid="0" count="668" tid="0" op="" dtype="" >7.8440e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000030000000011" call="MPI_Irecv" bytes="768" orank="17" region="0" commid="0" count="792" tid="0" op="" dtype="" >1.3709e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000009" call="MPI_Isend" bytes="0" orank="9" region="0" commid="0" count="333" tid="0" op="" dtype="" >2.2125e-04 0.0000e+00 3.0994e-06</hent>
<hent key="0240010000000000000000000000000C" call="MPI_Isend" bytes="0" orank="12" region="0" commid="0" count="557" tid="0" op="" dtype="" >4.1270e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000000000000000E" call="MPI_Isend" bytes="0" orank="14" region="0" commid="0" count="554" tid="0" op="" dtype="" >1.1261e-03 9.5367e-07 3.9101e-05</hent>
<hent key="02400100000000000000000000000011" call="MPI_Isend" bytes="0" orank="17" region="0" commid="0" count="324" tid="0" op="" dtype="" >2.2650e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000025" call="MPI_Isend" bytes="0" orank="37" region="0" commid="0" count="88" tid="0" op="" dtype="" >3.5357e-04 9.5367e-07 5.0068e-06</hent>
<hent key="024001000000000000000000000002AD" call="MPI_Isend" bytes="0" orank="685" region="0" commid="0" count="83" tid="0" op="" dtype="" >3.1447e-04 9.5367e-07 5.9605e-06</hent>
<hent key="03800100000000000000060000000009" call="MPI_Irecv" bytes="1536" orank="9" region="0" commid="0" count="55" tid="0" op="" dtype="" >1.4067e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000C" call="MPI_Irecv" bytes="1536" orank="12" region="0" commid="0" count="15" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000006000000000E" call="MPI_Irecv" bytes="1536" orank="14" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.3842e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000011" call="MPI_Irecv" bytes="1536" orank="17" region="0" commid="0" count="47" tid="0" op="" dtype="" >2.5034e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000025" call="MPI_Irecv" bytes="1536" orank="37" region="0" commid="0" count="74" tid="0" op="" dtype="" >2.7418e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002AD" call="MPI_Irecv" bytes="1536" orank="685" region="0" commid="0" count="64" tid="0" op="" dtype="" >1.8358e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000030000000009" call="MPI_Isend" bytes="768" orank="9" region="0" commid="0" count="616" tid="0" op="" dtype="" >2.6798e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000030000000011" call="MPI_Isend" bytes="768" orank="17" region="0" commid="0" count="520" tid="0" op="" dtype="" >2.6441e-04 0.0000e+00 1.3113e-05</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C00000000C" call="MPI_Irecv" bytes="448" orank="12" region="0" commid="0" count="22" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C00000000E" call="MPI_Irecv" bytes="448" orank="14" region="0" commid="0" count="34" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000009" call="MPI_Isend" bytes="1536" orank="9" region="0" commid="0" count="48" tid="0" op="" dtype="" >7.9393e-05 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000006000000000C" call="MPI_Isend" bytes="1536" orank="12" region="0" commid="0" count="13" tid="0" op="" dtype="" >2.9802e-05 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000006000000000E" call="MPI_Isend" bytes="1536" orank="14" region="0" commid="0" count="10" tid="0" op="" dtype="" >3.7909e-05 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000060000000011" call="MPI_Isend" bytes="1536" orank="17" region="0" commid="0" count="46" tid="0" op="" dtype="" >7.2002e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000025" call="MPI_Isend" bytes="1536" orank="37" region="0" commid="0" count="81" tid="0" op="" dtype="" >4.3631e-04 3.8147e-06 7.1526e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002AD" call="MPI_Isend" bytes="1536" orank="685" region="0" commid="0" count="82" tid="0" op="" dtype="" >3.7408e-04 3.8147e-06 5.9605e-06</hent>
<hent key="038001000000000000000C0000000025" call="MPI_Irecv" bytes="3072" orank="37" region="0" commid="0" count="347" tid="0" op="" dtype="" >1.2875e-04 0.0000e+00 1.9073e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002AD" call="MPI_Irecv" bytes="3072" orank="685" region="0" commid="0" count="322" tid="0" op="" dtype="" >8.7261e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C00000000C" call="MPI_Isend" bytes="448" orank="12" region="0" commid="0" count="16" tid="0" op="" dtype="" >1.1206e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C00000000E" call="MPI_Isend" bytes="448" orank="14" region="0" commid="0" count="24" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000025" call="MPI_Isend" bytes="3072" orank="37" region="0" commid="0" count="367" tid="0" op="" dtype="" >2.1954e-03 4.7684e-06 1.4067e-05</hent>
<hent key="024001000000000000000C00000002AD" call="MPI_Isend" bytes="3072" orank="685" region="0" commid="0" count="345" tid="0" op="" dtype="" >1.7092e-03 3.8147e-06 1.0014e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >6.8498e-04 2.2697e-04 2.3007e-04</hent>
<hent key="03800100000000000000038000000009" call="MPI_Irecv" bytes="896" orank="9" region="0" commid="0" count="2998" tid="0" op="" dtype="" >4.0865e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000C" call="MPI_Irecv" bytes="896" orank="12" region="0" commid="0" count="154" tid="0" op="" dtype="" >6.0797e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000E" call="MPI_Irecv" bytes="896" orank="14" region="0" commid="0" count="160" tid="0" op="" dtype="" >4.5300e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000011" call="MPI_Irecv" bytes="896" orank="17" region="0" commid="0" count="2878" tid="0" op="" dtype="" >5.0783e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000025" call="MPI_Irecv" bytes="896" orank="37" region="0" commid="0" count="20" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002AD" call="MPI_Irecv" bytes="896" orank="685" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.0504e-04 2.0504e-04 2.0504e-04</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="03800100000000000000180000000009" call="MPI_Irecv" bytes="6144" orank="9" region="0" commid="0" count="2407" tid="0" op="" dtype="" >2.9969e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000180000000011" call="MPI_Irecv" bytes="6144" orank="17" region="0" commid="0" count="2994" tid="0" op="" dtype="" >5.5313e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000180000000025" call="MPI_Irecv" bytes="6144" orank="37" region="0" commid="0" count="8" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001800000002AD" call="MPI_Irecv" bytes="6144" orank="685" region="0" commid="0" count="20" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000038000000009" call="MPI_Isend" bytes="896" orank="9" region="0" commid="0" count="3065" tid="0" op="" dtype="" >1.6739e-03 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000003800000000C" call="MPI_Isend" bytes="896" orank="12" region="0" commid="0" count="155" tid="0" op="" dtype="" >3.0231e-04 9.5367e-07 1.0014e-05</hent>
<hent key="0240010000000000000003800000000E" call="MPI_Isend" bytes="896" orank="14" region="0" commid="0" count="159" tid="0" op="" dtype="" >5.3191e-04 2.1458e-06 1.0967e-05</hent>
<hent key="02400100000000000000038000000011" call="MPI_Isend" bytes="896" orank="17" region="0" commid="0" count="3165" tid="0" op="" dtype="" >1.7624e-03 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000038000000025" call="MPI_Isend" bytes="896" orank="37" region="0" commid="0" count="24" tid="0" op="" dtype="" >1.2302e-04 4.0531e-06 5.9605e-06</hent>
<hent key="024001000000000000000380000002AD" call="MPI_Isend" bytes="896" orank="685" region="0" commid="0" count="21" tid="0" op="" dtype="" >9.2983e-05 3.8147e-06 5.0068e-06</hent>
<hent key="02400100000000000000180000000009" call="MPI_Isend" bytes="6144" orank="9" region="0" commid="0" count="2295" tid="0" op="" dtype="" >4.1039e-03 9.5367e-07 2.4080e-05</hent>
<hent key="02400100000000000000180000000011" call="MPI_Isend" bytes="6144" orank="17" region="0" commid="0" count="1884" tid="0" op="" dtype="" >3.9473e-03 9.5367e-07 1.1921e-05</hent>
<hent key="02400100000000000000180000000025" call="MPI_Isend" bytes="6144" orank="37" region="0" commid="0" count="15" tid="0" op="" dtype="" >1.0490e-04 5.9605e-06 8.1062e-06</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.5075e-05 5.5075e-05 5.5075e-05</hent>
<hent key="024001000000000000001800000002AD" call="MPI_Isend" bytes="6144" orank="685" region="0" commid="0" count="10" tid="0" op="" dtype="" >6.1512e-05 5.0068e-06 6.9141e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >9.5466e+00 1.6928e-05 1.2669e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.6999e-04 8.6999e-04 8.6999e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0631e-02 1.0631e-02 1.0631e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >5.4541e-03 5.4541e-03 5.4541e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.2033e-01 3.3562e-03 2.1200e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.5909e-04 5.5909e-04 5.5909e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5762e+00 4.2796e-04 2.4769e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.6928e-05 0.0000e+00 6.9141e-06</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000070000000009" call="MPI_Irecv" bytes="1792" orank="9" region="0" commid="0" count="25" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000007000000000C" call="MPI_Irecv" bytes="1792" orank="12" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000007000000000E" call="MPI_Irecv" bytes="1792" orank="14" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000070000000011" call="MPI_Irecv" bytes="1792" orank="17" region="0" commid="0" count="21" tid="0" op="" dtype="" >1.2398e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000070000000025" call="MPI_Irecv" bytes="1792" orank="37" region="0" commid="0" count="103" tid="0" op="" dtype="" >3.9339e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002AD" call="MPI_Irecv" bytes="1792" orank="685" region="0" commid="0" count="115" tid="0" op="" dtype="" >3.3379e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >8.4162e-05 0.0000e+00 6.3896e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9800e+01 0.0000e+00 3.8429e+01</hent>
<hent key="038001000000000000000A0000000011" call="MPI_Irecv" bytes="2560" orank="17" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000025" call="MPI_Irecv" bytes="2560" orank="37" region="0" commid="0" count="372" tid="0" op="" dtype="" >1.1492e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002AD" call="MPI_Irecv" bytes="2560" orank="685" region="0" commid="0" count="374" tid="0" op="" dtype="" >1.1039e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000009" call="MPI_Isend" bytes="1792" orank="9" region="0" commid="0" count="24" tid="0" op="" dtype="" >4.1723e-05 9.5367e-07 2.8610e-06</hent>
<hent key="0240010000000000000007000000000E" call="MPI_Isend" bytes="1792" orank="14" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.6226e-05 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000070000000011" call="MPI_Isend" bytes="1792" orank="17" region="0" commid="0" count="16" tid="0" op="" dtype="" >2.7418e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000025" call="MPI_Isend" bytes="1792" orank="37" region="0" commid="0" count="120" tid="0" op="" dtype="" >6.6209e-04 4.7684e-06 6.9141e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3063e-02 8.3063e-02 8.3063e-02</hent>
<hent key="024001000000000000000700000002AD" call="MPI_Isend" bytes="1792" orank="685" region="0" commid="0" count="126" tid="0" op="" dtype="" >5.8246e-04 3.8147e-06 1.0014e-05</hent>
<hent key="024001000000000000000A0000000009" call="MPI_Isend" bytes="2560" orank="9" region="0" commid="0" count="3" tid="0" op="" dtype="" >4.2915e-06 9.5367e-07 2.1458e-06</hent>
<hent key="024001000000000000000A0000000011" call="MPI_Isend" bytes="2560" orank="17" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000A0000000025" call="MPI_Isend" bytes="2560" orank="37" region="0" commid="0" count="351" tid="0" op="" dtype="" >2.0299e-03 5.0068e-06 1.3113e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.2401e-04 3.7909e-05 1.4496e-04</hent>
<hent key="024001000000000000000A00000002AD" call="MPI_Isend" bytes="2560" orank="685" region="0" commid="0" count="364" tid="0" op="" dtype="" >1.7595e-03 3.8147e-06 1.0967e-05</hent>
<hent key="0380010000000000000010000000000C" call="MPI_Irecv" bytes="4096" orank="12" region="0" commid="0" count="12616" tid="0" op="" dtype="" >5.3990e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000010000000000E" call="MPI_Irecv" bytes="4096" orank="14" region="0" commid="0" count="12609" tid="0" op="" dtype="" >1.8640e-03 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000100000000025" call="MPI_Irecv" bytes="4096" orank="37" region="0" commid="0" count="3218" tid="0" op="" dtype="" >5.2905e-04 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000001000000002AD" call="MPI_Irecv" bytes="4096" orank="685" region="0" commid="0" count="3073" tid="0" op="" dtype="" >7.4434e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000010000000000C" call="MPI_Isend" bytes="4096" orank="12" region="0" commid="0" count="12640" tid="0" op="" dtype="" >2.1769e-02 0.0000e+00 1.5020e-05</hent>
<hent key="0240010000000000000010000000000E" call="MPI_Isend" bytes="4096" orank="14" region="0" commid="0" count="12602" tid="0" op="" dtype="" >3.0619e-02 0.0000e+00 2.0027e-05</hent>
<hent key="02400100000000000000100000000025" call="MPI_Isend" bytes="4096" orank="37" region="0" commid="0" count="3131" tid="0" op="" dtype="" >1.2341e-02 9.5367e-07 2.4080e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.0600e-03 3.9101e-04 6.6900e-04</hent>
<hent key="024001000000000000001000000002AD" call="MPI_Isend" bytes="4096" orank="685" region="0" commid="0" count="3032" tid="0" op="" dtype="" >1.0750e-02 9.5367e-07 2.6941e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.6042e-05 6.6042e-05 6.6042e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.3661e-03 3.3593e-04 8.2684e-04</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >7.8678e-06 1.9073e-06 5.9605e-06</hent>
<hent key="03800100000000000000000400000009" call="MPI_Irecv" bytes="4" orank="9" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.5943e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000000C" call="MPI_Irecv" bytes="4" orank="12" region="0" commid="0" count="3400" tid="0" op="" dtype="" >9.1267e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000040000000E" call="MPI_Irecv" bytes="4" orank="14" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.7207e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000011" call="MPI_Irecv" bytes="4" orank="17" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.5323e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000400000025" call="MPI_Irecv" bytes="4" orank="37" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.3668e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001C0000000009" call="MPI_Irecv" bytes="7168" orank="9" region="0" commid="0" count="10293" tid="0" op="" dtype="" >1.2372e-03 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000001C0000000011" call="MPI_Irecv" bytes="7168" orank="17" region="0" commid="0" count="9706" tid="0" op="" dtype="" >1.8125e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000001C0000000025" call="MPI_Irecv" bytes="7168" orank="37" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000004000002AD" call="MPI_Irecv" bytes="4" orank="685" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.1236e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000400000009" call="MPI_Isend" bytes="4" orank="9" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.3149e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000000040000000C" call="MPI_Isend" bytes="4" orank="12" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.5481e-03 0.0000e+00 1.9073e-06</hent>
<hent key="0240010000000000000000040000000E" call="MPI_Isend" bytes="4" orank="14" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.0289e-03 0.0000e+00 1.0490e-04</hent>
<hent key="02400100000000000000000400000011" call="MPI_Isend" bytes="4" orank="17" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.7092e-03 0.0000e+00 2.9087e-05</hent>
<hent key="02400100000000000000000400000025" call="MPI_Isend" bytes="4" orank="37" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.5037e-02 4.0531e-06 3.2091e-04</hent>
<hent key="024001000000000000001C0000000009" call="MPI_Isend" bytes="7168" orank="9" region="0" commid="0" count="10405" tid="0" op="" dtype="" >1.9273e-02 9.5367e-07 2.7895e-05</hent>
<hent key="024001000000000000001C0000000011" call="MPI_Isend" bytes="7168" orank="17" region="0" commid="0" count="10816" tid="0" op="" dtype="" >2.2252e-02 9.5367e-07 1.8120e-05</hent>
<hent key="024001000000000000001C0000000025" call="MPI_Isend" bytes="7168" orank="37" region="0" commid="0" count="1" tid="0" op="" dtype="" >6.9141e-06 6.9141e-06 6.9141e-06</hent>
<hent key="024001000000000000000004000002AD" call="MPI_Isend" bytes="4" orank="685" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.9261e-02 3.8147e-06 7.4697e-04</hent>
<hent key="024001000000000000001C00000002AD" call="MPI_Isend" bytes="7168" orank="685" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.3307e-04 3.3307e-04 3.3307e-04</hent>
<hent key="0380010000000000000002000000000C" call="MPI_Irecv" bytes="512" orank="12" region="0" commid="0" count="3378" tid="0" op="" dtype="" >5.0950e-04 0.0000e+00 1.7166e-05</hent>
<hent key="0380010000000000000002000000000E" call="MPI_Irecv" bytes="512" orank="14" region="0" commid="0" count="3366" tid="0" op="" dtype="" >6.8760e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000050000000009" call="MPI_Irecv" bytes="1280" orank="9" region="0" commid="0" count="130" tid="0" op="" dtype="" >3.7193e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000C" call="MPI_Irecv" bytes="1280" orank="12" region="0" commid="0" count="62" tid="0" op="" dtype="" >2.4080e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000E" call="MPI_Irecv" bytes="1280" orank="14" region="0" commid="0" count="46" tid="0" op="" dtype="" >1.8835e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000011" call="MPI_Irecv" bytes="1280" orank="17" region="0" commid="0" count="139" tid="0" op="" dtype="" >6.9380e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000050000000025" call="MPI_Irecv" bytes="1280" orank="37" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.1206e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4952e+01 1.0014e-05 1.8461e-01</hent>
<hent key="038001000000000000000500000002AD" call="MPI_Irecv" bytes="1280" orank="685" region="0" commid="0" count="56" tid="0" op="" dtype="" >1.6928e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000002000000000C" call="MPI_Isend" bytes="512" orank="12" region="0" commid="0" count="3384" tid="0" op="" dtype="" >1.5285e-03 0.0000e+00 1.2159e-05</hent>
<hent key="0240010000000000000002000000000E" call="MPI_Isend" bytes="512" orank="14" region="0" commid="0" count="3376" tid="0" op="" dtype="" >2.0013e-03 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000080000000009" call="MPI_Irecv" bytes="2048" orank="9" region="0" commid="0" count="4" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000008000000000C" call="MPI_Irecv" bytes="2048" orank="12" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000080000000011" call="MPI_Irecv" bytes="2048" orank="17" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000025" call="MPI_Irecv" bytes="2048" orank="37" region="0" commid="0" count="186" tid="0" op="" dtype="" >5.1022e-05 0.0000e+00 7.1526e-06</hent>
<hent key="038001000000000000000800000002AD" call="MPI_Irecv" bytes="2048" orank="685" region="0" commid="0" count="166" tid="0" op="" dtype="" >5.0306e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000050000000009" call="MPI_Isend" bytes="1280" orank="9" region="0" commid="0" count="138" tid="0" op="" dtype="" >2.0671e-04 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000005000000000C" call="MPI_Isend" bytes="1280" orank="12" region="0" commid="0" count="68" tid="0" op="" dtype="" >1.3542e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000005000000000E" call="MPI_Isend" bytes="1280" orank="14" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.3971e-04 2.8610e-06 6.9141e-06</hent>
<hent key="02400100000000000000050000000011" call="MPI_Isend" bytes="1280" orank="17" region="0" commid="0" count="138" tid="0" op="" dtype="" >2.3246e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000050000000025" call="MPI_Isend" bytes="1280" orank="37" region="0" commid="0" count="48" tid="0" op="" dtype="" >2.5463e-04 4.0531e-06 6.1989e-06</hent>
<hent key="024001000000000000000500000002AD" call="MPI_Isend" bytes="1280" orank="685" region="0" commid="0" count="58" tid="0" op="" dtype="" >2.5392e-04 3.8147e-06 5.0068e-06</hent>
</hash>
<internal rank="13" log_i="1724765674.524930" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="14" mpi_size="696" stamp_init="1724765564.467118" stamp_final="1724765674.533762" username="apac4" allocationname="unknown" flags="0" pid="1612131" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10067e+02" utime="8.83961e+01" stime="1.39540e+01" mtime="7.19111e+01" gflop="0.00000e+00" gbyte="3.76583e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19111e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4761577157915fa55791578154d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09929e+02" utime="8.83616e+01" stime="1.39464e+01" mtime="7.19111e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19111e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2013e+09" > 7.3231e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1897e+09" > 3.6815e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8540e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9795e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3901e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9843e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5760e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.0354e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0622e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3152e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3804e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="208" >
<hent key="0240010000000000000008000000000A" call="MPI_Isend" bytes="2048" orank="10" region="0" commid="0" count="4" tid="0" op="" dtype="" >6.9141e-06 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000080000000012" call="MPI_Isend" bytes="2048" orank="18" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.7643e-05 1.1921e-06 2.1458e-06</hent>
<hent key="02400100000000000000080000000026" call="MPI_Isend" bytes="2048" orank="38" region="0" commid="0" count="165" tid="0" op="" dtype="" >1.0579e-03 4.7684e-06 1.7881e-05</hent>
<hent key="024001000000000000000800000002AE" call="MPI_Isend" bytes="2048" orank="686" region="0" commid="0" count="138" tid="0" op="" dtype="" >7.3099e-04 3.8147e-06 1.5020e-05</hent>
<hent key="038001000000000000000E000000000D" call="MPI_Irecv" bytes="3584" orank="13" region="0" commid="0" count="98" tid="0" op="" dtype="" >4.0293e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E000000000F" call="MPI_Irecv" bytes="3584" orank="15" region="0" commid="0" count="168" tid="0" op="" dtype="" >3.4094e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000026" call="MPI_Irecv" bytes="3584" orank="38" region="0" commid="0" count="127" tid="0" op="" dtype="" >7.8917e-05 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000E00000002AE" call="MPI_Irecv" bytes="3584" orank="686" region="0" commid="0" count="117" tid="0" op="" dtype="" >6.6757e-05 0.0000e+00 8.1062e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000E000000000D" call="MPI_Isend" bytes="3584" orank="13" region="0" commid="0" count="91" tid="0" op="" dtype="" >1.4305e-04 0.0000e+00 2.1458e-06</hent>
<hent key="024001000000000000000E000000000F" call="MPI_Isend" bytes="3584" orank="15" region="0" commid="0" count="226" tid="0" op="" dtype="" >6.1870e-04 9.5367e-07 5.9605e-06</hent>
<hent key="024001000000000000000E0000000026" call="MPI_Isend" bytes="3584" orank="38" region="0" commid="0" count="127" tid="0" op="" dtype="" >9.4819e-04 5.0068e-06 1.6928e-05</hent>
<hent key="024001000000000000000E00000002AE" call="MPI_Isend" bytes="3584" orank="686" region="0" commid="0" count="131" tid="0" op="" dtype="" >7.8869e-04 3.8147e-06 1.5974e-05</hent>
<hent key="0380010000000000000002800000000A" call="MPI_Irecv" bytes="640" orank="10" region="0" commid="0" count="436" tid="0" op="" dtype="" >1.0562e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000D" call="MPI_Irecv" bytes="640" orank="13" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.6904e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000F" call="MPI_Irecv" bytes="640" orank="15" region="0" commid="0" count="372" tid="0" op="" dtype="" >9.4652e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000012" call="MPI_Irecv" bytes="640" orank="18" region="0" commid="0" count="411" tid="0" op="" dtype="" >1.0037e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000026" call="MPI_Irecv" bytes="640" orank="38" region="0" commid="0" count="24" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000140000000026" call="MPI_Irecv" bytes="5120" orank="38" region="0" commid="0" count="683" tid="0" op="" dtype="" >2.5463e-04 0.0000e+00 3.3855e-05</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000000280000002AE" call="MPI_Irecv" bytes="640" orank="686" region="0" commid="0" count="19" tid="0" op="" dtype="" >6.6757e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000001400000002AE" call="MPI_Irecv" bytes="5120" orank="686" region="0" commid="0" count="344" tid="0" op="" dtype="" >1.2279e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000A00000000026" call="MPI_Irecv" bytes="40960" orank="38" region="0" commid="0" count="2208" tid="0" op="" dtype="" >1.0629e-03 0.0000e+00 2.9087e-05</hent>
<hent key="0240010000000000000002800000000A" call="MPI_Isend" bytes="640" orank="10" region="0" commid="0" count="411" tid="0" op="" dtype="" >4.3392e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000002800000000D" call="MPI_Isend" bytes="640" orank="13" region="0" commid="0" count="325" tid="0" op="" dtype="" >4.9257e-04 9.5367e-07 1.3113e-05</hent>
<hent key="0240010000000000000002800000000F" call="MPI_Isend" bytes="640" orank="15" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.1482e-03 1.9073e-06 9.0599e-06</hent>
<hent key="02400100000000000000028000000012" call="MPI_Isend" bytes="640" orank="18" region="0" commid="0" count="414" tid="0" op="" dtype="" >4.5896e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000028000000026" call="MPI_Isend" bytes="640" orank="38" region="0" commid="0" count="26" tid="0" op="" dtype="" >1.3351e-04 4.0531e-06 6.9141e-06</hent>
<hent key="03800100000000000000A000000002AE" call="MPI_Irecv" bytes="40960" orank="686" region="0" commid="0" count="950" tid="0" op="" dtype="" >2.8586e-04 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000140000000026" call="MPI_Isend" bytes="5120" orank="38" region="0" commid="0" count="750" tid="0" op="" dtype="" >3.7811e-03 1.9073e-06 3.6001e-05</hent>
<hent key="024001000000000000000280000002AE" call="MPI_Isend" bytes="640" orank="686" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.2875e-04 3.8147e-06 5.0068e-06</hent>
<hent key="03800100000000000000800000000026" call="MPI_Irecv" bytes="32768" orank="38" region="0" commid="0" count="10492" tid="0" op="" dtype="" >6.5329e-03 0.0000e+00 3.0041e-05</hent>
<hent key="024001000000000000001400000002AE" call="MPI_Isend" bytes="5120" orank="686" region="0" commid="0" count="613" tid="0" op="" dtype="" >2.8970e-03 9.5367e-07 4.6015e-05</hent>
<hent key="038001000000000000008000000002AE" call="MPI_Irecv" bytes="32768" orank="686" region="0" commid="0" count="11750" tid="0" op="" dtype="" >3.9294e-03 0.0000e+00 3.8147e-05</hent>
<hent key="0380010000000000000001400000000A" call="MPI_Irecv" bytes="320" orank="10" region="0" commid="0" count="404" tid="0" op="" dtype="" >1.0395e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000001400000000D" call="MPI_Irecv" bytes="320" orank="13" region="0" commid="0" count="564" tid="0" op="" dtype="" >2.5868e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000F" call="MPI_Irecv" bytes="320" orank="15" region="0" commid="0" count="526" tid="0" op="" dtype="" >1.3423e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000012" call="MPI_Irecv" bytes="320" orank="18" region="0" commid="0" count="403" tid="0" op="" dtype="" >9.3699e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000014000000026" call="MPI_Irecv" bytes="320" orank="38" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.2398e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A00000000026" call="MPI_Isend" bytes="40960" orank="38" region="0" commid="0" count="2584" tid="0" op="" dtype="" >6.1176e-02 1.0967e-05 8.9884e-05</hent>
<hent key="038001000000000000000140000002AE" call="MPI_Irecv" bytes="320" orank="686" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.4544e-05 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000A000000002AE" call="MPI_Isend" bytes="40960" orank="686" region="0" commid="0" count="2058" tid="0" op="" dtype="" >2.9514e-02 6.9141e-06 2.4009e-04</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >5.8540e+00 0.0000e+00 1.1041e-01</hent>
<hent key="02400100000000000000800000000026" call="MPI_Isend" bytes="32768" orank="38" region="0" commid="0" count="10116" tid="0" op="" dtype="" >2.6217e-01 1.0014e-05 2.5105e-04</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.8120e-05 0.0000e+00 1.8120e-05</hent>
<hent key="024001000000000000008000000002AE" call="MPI_Isend" bytes="32768" orank="686" region="0" commid="0" count="10642" tid="0" op="" dtype="" >1.4415e-01 5.9605e-06 1.3804e-04</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 0.0000e+00 1.9073e-06</hent>
<hent key="0240010000000000000001400000000A" call="MPI_Isend" bytes="320" orank="10" region="0" commid="0" count="438" tid="0" op="" dtype="" >3.7050e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000001400000000D" call="MPI_Isend" bytes="320" orank="13" region="0" commid="0" count="583" tid="0" op="" dtype="" >6.6257e-04 0.0000e+00 1.4067e-05</hent>
<hent key="0240010000000000000001400000000F" call="MPI_Isend" bytes="320" orank="15" region="0" commid="0" count="551" tid="0" op="" dtype="" >1.6279e-03 1.9073e-06 1.3828e-05</hent>
<hent key="02400100000000000000014000000012" call="MPI_Isend" bytes="320" orank="18" region="0" commid="0" count="423" tid="0" op="" dtype="" >3.8743e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000014000000026" call="MPI_Isend" bytes="320" orank="38" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.1945e-04 3.8147e-06 1.5020e-05</hent>
<hent key="024001000000000000000140000002AE" call="MPI_Isend" bytes="320" orank="686" region="0" commid="0" count="19" tid="0" op="" dtype="" >8.7500e-05 3.0994e-06 7.1526e-06</hent>
<hent key="0380010000000000000000000000000A" call="MPI_Irecv" bytes="0" orank="10" region="0" commid="0" count="340" tid="0" op="" dtype="" >8.8692e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000000000000000D" call="MPI_Irecv" bytes="0" orank="13" region="0" commid="0" count="554" tid="0" op="" dtype="" >1.8907e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000F" call="MPI_Irecv" bytes="0" orank="15" region="0" commid="0" count="572" tid="0" op="" dtype="" >1.6117e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000012" call="MPI_Irecv" bytes="0" orank="18" region="0" commid="0" count="324" tid="0" op="" dtype="" >8.0824e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000026" call="MPI_Irecv" bytes="0" orank="38" region="0" commid="0" count="83" tid="0" op="" dtype="" >2.7180e-05 0.0000e+00 2.8610e-06</hent>
<hent key="038001000000000000000000000002AE" call="MPI_Irecv" bytes="0" orank="686" region="0" commid="0" count="83" tid="0" op="" dtype="" >2.6226e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.1124e-04 0.0000e+00 7.7963e-05</hent>
<hent key="02400100000000000000200000000026" call="MPI_Isend" bytes="8192" orank="38" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.1206e-05 1.1206e-05 1.1206e-05</hent>
<hent key="0380010000000000000003000000000A" call="MPI_Irecv" bytes="768" orank="10" region="0" commid="0" count="798" tid="0" op="" dtype="" >1.0252e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000030000000012" call="MPI_Irecv" bytes="768" orank="18" region="0" commid="0" count="570" tid="0" op="" dtype="" >1.0037e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000000000000000A" call="MPI_Isend" bytes="0" orank="10" region="0" commid="0" count="336" tid="0" op="" dtype="" >2.0742e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000000000000000D" call="MPI_Isend" bytes="0" orank="13" region="0" commid="0" count="573" tid="0" op="" dtype="" >3.4189e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000000000000000F" call="MPI_Isend" bytes="0" orank="15" region="0" commid="0" count="561" tid="0" op="" dtype="" >1.1475e-03 9.5367e-07 3.2187e-05</hent>
<hent key="02400100000000000000000000000012" call="MPI_Isend" bytes="0" orank="18" region="0" commid="0" count="335" tid="0" op="" dtype="" >2.0933e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000026" call="MPI_Isend" bytes="0" orank="38" region="0" commid="0" count="94" tid="0" op="" dtype="" >3.9101e-04 9.5367e-07 1.0967e-05</hent>
<hent key="024001000000000000000000000002AE" call="MPI_Isend" bytes="0" orank="686" region="0" commid="0" count="89" tid="0" op="" dtype="" >3.2425e-04 1.9073e-06 9.0599e-06</hent>
<hent key="0380010000000000000006000000000A" call="MPI_Irecv" bytes="1536" orank="10" region="0" commid="0" count="57" tid="0" op="" dtype="" >9.5367e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000006000000000D" call="MPI_Irecv" bytes="1536" orank="13" region="0" commid="0" count="10" tid="0" op="" dtype="" >5.4836e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000F" call="MPI_Irecv" bytes="1536" orank="15" region="0" commid="0" count="16" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000012" call="MPI_Irecv" bytes="1536" orank="18" region="0" commid="0" count="85" tid="0" op="" dtype="" >1.8358e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000026" call="MPI_Irecv" bytes="1536" orank="38" region="0" commid="0" count="78" tid="0" op="" dtype="" >3.6478e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002AE" call="MPI_Irecv" bytes="1536" orank="686" region="0" commid="0" count="87" tid="0" op="" dtype="" >3.6001e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000003000000000A" call="MPI_Isend" bytes="768" orank="10" region="0" commid="0" count="728" tid="0" op="" dtype="" >3.4976e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000030000000012" call="MPI_Isend" bytes="768" orank="18" region="0" commid="0" count="846" tid="0" op="" dtype="" >4.0603e-04 0.0000e+00 6.9141e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C00000000D" call="MPI_Irecv" bytes="448" orank="13" region="0" commid="0" count="24" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C00000000F" call="MPI_Irecv" bytes="448" orank="15" region="0" commid="0" count="42" tid="0" op="" dtype="" >7.3910e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000006000000000A" call="MPI_Isend" bytes="1536" orank="10" region="0" commid="0" count="54" tid="0" op="" dtype="" >8.1301e-05 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000006000000000D" call="MPI_Isend" bytes="1536" orank="13" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.2650e-05 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000006000000000F" call="MPI_Isend" bytes="1536" orank="15" region="0" commid="0" count="11" tid="0" op="" dtype="" >4.1485e-05 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000060000000012" call="MPI_Isend" bytes="1536" orank="18" region="0" commid="0" count="72" tid="0" op="" dtype="" >1.2422e-04 9.5367e-07 1.4782e-05</hent>
<hent key="02400100000000000000060000000026" call="MPI_Isend" bytes="1536" orank="38" region="0" commid="0" count="79" tid="0" op="" dtype="" >5.3263e-04 4.0531e-06 2.2173e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002AE" call="MPI_Isend" bytes="1536" orank="686" region="0" commid="0" count="77" tid="0" op="" dtype="" >3.8743e-04 3.8147e-06 1.2159e-05</hent>
<hent key="038001000000000000000C0000000026" call="MPI_Irecv" bytes="3072" orank="38" region="0" commid="0" count="349" tid="0" op="" dtype="" >2.3508e-04 0.0000e+00 8.1062e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002AE" call="MPI_Irecv" bytes="3072" orank="686" region="0" commid="0" count="298" tid="0" op="" dtype="" >1.1396e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000001C00000000D" call="MPI_Isend" bytes="448" orank="13" region="0" commid="0" count="34" tid="0" op="" dtype="" >1.6928e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C00000000F" call="MPI_Isend" bytes="448" orank="15" region="0" commid="0" count="72" tid="0" op="" dtype="" >4.4584e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000026" call="MPI_Isend" bytes="3072" orank="38" region="0" commid="0" count="310" tid="0" op="" dtype="" >2.2261e-03 4.7684e-06 2.0981e-05</hent>
<hent key="024001000000000000000C00000002AE" call="MPI_Isend" bytes="3072" orank="686" region="0" commid="0" count="342" tid="0" op="" dtype="" >1.8866e-03 3.8147e-06 1.9073e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >7.2718e-04 2.3913e-04 2.4700e-04</hent>
<hent key="0380010000000000000003800000000A" call="MPI_Irecv" bytes="896" orank="10" region="0" commid="0" count="2903" tid="0" op="" dtype="" >4.3106e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000D" call="MPI_Irecv" bytes="896" orank="13" region="0" commid="0" count="159" tid="0" op="" dtype="" >7.6771e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000F" call="MPI_Irecv" bytes="896" orank="15" region="0" commid="0" count="163" tid="0" op="" dtype="" >4.7922e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000012" call="MPI_Irecv" bytes="896" orank="18" region="0" commid="0" count="3137" tid="0" op="" dtype="" >7.3290e-04 0.0000e+00 1.9073e-05</hent>
<hent key="03800100000000000000038000000026" call="MPI_Irecv" bytes="896" orank="38" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.3113e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002AE" call="MPI_Irecv" bytes="896" orank="686" region="0" commid="0" count="54" tid="0" op="" dtype="" >2.9564e-05 0.0000e+00 2.1458e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1206e-05 1.1206e-05 1.1206e-05</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0380010000000000000018000000000A" call="MPI_Irecv" bytes="6144" orank="10" region="0" commid="0" count="2841" tid="0" op="" dtype="" >4.6062e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000180000000012" call="MPI_Irecv" bytes="6144" orank="18" region="0" commid="0" count="2218" tid="0" op="" dtype="" >4.6253e-04 0.0000e+00 1.0967e-05</hent>
<hent key="03800100000000000000180000000026" call="MPI_Irecv" bytes="6144" orank="38" region="0" commid="0" count="16" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000001800000002AE" call="MPI_Irecv" bytes="6144" orank="686" region="0" commid="0" count="17" tid="0" op="" dtype="" >1.6212e-05 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000003800000000A" call="MPI_Isend" bytes="896" orank="10" region="0" commid="0" count="2959" tid="0" op="" dtype="" >1.6830e-03 0.0000e+00 1.1206e-05</hent>
<hent key="0240010000000000000003800000000D" call="MPI_Isend" bytes="896" orank="13" region="0" commid="0" count="160" tid="0" op="" dtype="" >2.7394e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000003800000000F" call="MPI_Isend" bytes="896" orank="15" region="0" commid="0" count="148" tid="0" op="" dtype="" >5.4002e-04 1.9073e-06 2.0027e-05</hent>
<hent key="02400100000000000000038000000012" call="MPI_Isend" bytes="896" orank="18" region="0" commid="0" count="2831" tid="0" op="" dtype="" >1.7087e-03 0.0000e+00 1.9073e-05</hent>
<hent key="02400100000000000000038000000026" call="MPI_Isend" bytes="896" orank="38" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.6403e-04 3.8147e-06 1.4067e-05</hent>
<hent key="024001000000000000000380000002AE" call="MPI_Isend" bytes="896" orank="686" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.7262e-04 3.0994e-06 2.0027e-05</hent>
<hent key="0240010000000000000018000000000A" call="MPI_Isend" bytes="6144" orank="10" region="0" commid="0" count="2750" tid="0" op="" dtype="" >5.2679e-03 9.5367e-07 2.1219e-05</hent>
<hent key="02400100000000000000180000000012" call="MPI_Isend" bytes="6144" orank="18" region="0" commid="0" count="3190" tid="0" op="" dtype="" >6.2306e-03 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000180000000026" call="MPI_Isend" bytes="6144" orank="38" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.7333e-04 6.9141e-06 1.0967e-05</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.3896e-05 6.3896e-05 6.3896e-05</hent>
<hent key="024001000000000000001800000002AE" call="MPI_Isend" bytes="6144" orank="686" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.7309e-04 5.0068e-06 1.2875e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >9.1138e+00 1.4067e-05 1.2665e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.6498e-04 8.6498e-04 8.6498e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0622e-02 1.0622e-02 1.0622e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >4.9982e-03 4.9982e-03 4.9982e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.0288e-01 3.2818e-03 1.9445e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >6.0296e-04 6.0296e-04 6.0296e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5760e+00 3.9816e-04 2.4762e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.3901e-02 1.9073e-06 1.2674e-02</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0380010000000000000007000000000A" call="MPI_Irecv" bytes="1792" orank="10" region="0" commid="0" count="24" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000D" call="MPI_Irecv" bytes="1792" orank="13" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000F" call="MPI_Irecv" bytes="1792" orank="15" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000070000000012" call="MPI_Irecv" bytes="1792" orank="18" region="0" commid="0" count="27" tid="0" op="" dtype="" >6.6757e-06 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000070000000026" call="MPI_Irecv" bytes="1792" orank="38" region="0" commid="0" count="101" tid="0" op="" dtype="" >5.1260e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002AE" call="MPI_Irecv" bytes="1792" orank="686" region="0" commid="0" count="112" tid="0" op="" dtype="" >5.2691e-05 0.0000e+00 5.9605e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >1.3828e-04 9.5367e-07 6.4135e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9794e+01 0.0000e+00 3.8416e+01</hent>
<hent key="038001000000000000000A0000000026" call="MPI_Irecv" bytes="2560" orank="38" region="0" commid="0" count="363" tid="0" op="" dtype="" >2.4247e-04 0.0000e+00 1.3828e-05</hent>
<hent key="038001000000000000000A00000002AE" call="MPI_Irecv" bytes="2560" orank="686" region="0" commid="0" count="338" tid="0" op="" dtype="" >1.3971e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000007000000000A" call="MPI_Isend" bytes="1792" orank="10" region="0" commid="0" count="19" tid="0" op="" dtype="" >3.0756e-05 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000007000000000D" call="MPI_Isend" bytes="1792" orank="13" region="0" commid="0" count="2" tid="0" op="" dtype="" >4.2915e-06 2.1458e-06 2.1458e-06</hent>
<hent key="0240010000000000000007000000000F" call="MPI_Isend" bytes="1792" orank="15" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.0742e-05 2.8610e-06 4.0531e-06</hent>
<hent key="02400100000000000000070000000012" call="MPI_Isend" bytes="1792" orank="18" region="0" commid="0" count="24" tid="0" op="" dtype="" >3.9577e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000026" call="MPI_Isend" bytes="1792" orank="38" region="0" commid="0" count="116" tid="0" op="" dtype="" >7.4482e-04 4.0531e-06 1.9789e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3152e-02 8.3152e-02 8.3152e-02</hent>
<hent key="024001000000000000000700000002AE" call="MPI_Isend" bytes="1792" orank="686" region="0" commid="0" count="101" tid="0" op="" dtype="" >5.1522e-04 2.8610e-06 1.2159e-05</hent>
<hent key="024001000000000000000A000000000A" call="MPI_Isend" bytes="2560" orank="10" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000A0000000012" call="MPI_Isend" bytes="2560" orank="18" region="0" commid="0" count="2" tid="0" op="" dtype="" >3.8147e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000A0000000026" call="MPI_Isend" bytes="2560" orank="38" region="0" commid="0" count="362" tid="0" op="" dtype="" >2.4683e-03 4.7684e-06 2.2888e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.3808e-04 3.8147e-05 1.5187e-04</hent>
<hent key="024001000000000000000A00000002AE" call="MPI_Isend" bytes="2560" orank="686" region="0" commid="0" count="326" tid="0" op="" dtype="" >1.7023e-03 3.8147e-06 1.5020e-05</hent>
<hent key="0380010000000000000010000000000D" call="MPI_Irecv" bytes="4096" orank="13" region="0" commid="0" count="12602" tid="0" op="" dtype="" >4.8234e-03 0.0000e+00 1.8120e-05</hent>
<hent key="0380010000000000000010000000000F" call="MPI_Irecv" bytes="4096" orank="15" region="0" commid="0" count="12532" tid="0" op="" dtype="" >2.7297e-03 0.0000e+00 1.5974e-05</hent>
<hent key="03800100000000000000100000000026" call="MPI_Irecv" bytes="4096" orank="38" region="0" commid="0" count="3023" tid="0" op="" dtype="" >9.2196e-04 0.0000e+00 2.1935e-05</hent>
<hent key="038001000000000000001000000002AE" call="MPI_Irecv" bytes="4096" orank="686" region="0" commid="0" count="3402" tid="0" op="" dtype="" >1.0059e-03 0.0000e+00 2.1935e-05</hent>
<hent key="0240010000000000000010000000000D" call="MPI_Isend" bytes="4096" orank="13" region="0" commid="0" count="12609" tid="0" op="" dtype="" >2.1803e-02 0.0000e+00 1.7166e-05</hent>
<hent key="0240010000000000000010000000000F" call="MPI_Isend" bytes="4096" orank="15" region="0" commid="0" count="12474" tid="0" op="" dtype="" >3.5362e-02 0.0000e+00 2.9492e-04</hent>
<hent key="02400100000000000000100000000026" call="MPI_Isend" bytes="4096" orank="38" region="0" commid="0" count="2957" tid="0" op="" dtype="" >1.4729e-02 1.9073e-06 4.2915e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.1311e-03 4.1509e-04 7.1597e-04</hent>
<hent key="024001000000000000001000000002AE" call="MPI_Isend" bytes="4096" orank="686" region="0" commid="0" count="3123" tid="0" op="" dtype="" >1.3450e-02 9.5367e-07 1.5593e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.4836e-05 5.4836e-05 5.4836e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.4829e-03 3.5596e-04 8.3494e-04</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >6.6757e-06 1.9073e-06 4.7684e-06</hent>
<hent key="0380010000000000000000040000000A" call="MPI_Irecv" bytes="4" orank="10" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.1795e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000040000000D" call="MPI_Irecv" bytes="4" orank="13" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.0309e-03 0.0000e+00 3.8147e-06</hent>
<hent key="0380010000000000000000040000000F" call="MPI_Irecv" bytes="4" orank="15" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.1522e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000012" call="MPI_Irecv" bytes="4" orank="18" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.9877e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000000400000026" call="MPI_Irecv" bytes="4" orank="38" region="0" commid="0" count="3400" tid="0" op="" dtype="" >9.2745e-04 0.0000e+00 8.8215e-06</hent>
<hent key="038001000000000000001C000000000A" call="MPI_Irecv" bytes="7168" orank="10" region="0" commid="0" count="9859" tid="0" op="" dtype="" >1.7450e-03 0.0000e+00 1.4067e-05</hent>
<hent key="038001000000000000001C0000000012" call="MPI_Irecv" bytes="7168" orank="18" region="0" commid="0" count="10482" tid="0" op="" dtype="" >2.3928e-03 0.0000e+00 1.3113e-05</hent>
<hent key="038001000000000000000004000002AE" call="MPI_Irecv" bytes="4" orank="686" region="0" commid="0" count="3400" tid="0" op="" dtype="" >8.2946e-04 0.0000e+00 1.0967e-05</hent>
<hent key="038001000000000000001C00000002AE" call="MPI_Irecv" bytes="7168" orank="686" region="0" commid="0" count="4" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.9073e-06</hent>
<hent key="0240010000000000000000040000000A" call="MPI_Isend" bytes="4" orank="10" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.4253e-03 0.0000e+00 2.5988e-05</hent>
<hent key="0240010000000000000000040000000D" call="MPI_Isend" bytes="4" orank="13" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.7543e-03 0.0000e+00 4.4107e-05</hent>
<hent key="0240010000000000000000040000000F" call="MPI_Isend" bytes="4" orank="15" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.9397e-03 0.0000e+00 2.0981e-05</hent>
<hent key="02400100000000000000000400000012" call="MPI_Isend" bytes="4" orank="18" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.4887e-03 0.0000e+00 2.4080e-05</hent>
<hent key="02400100000000000000000400000026" call="MPI_Isend" bytes="4" orank="38" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.8672e-02 3.8147e-06 4.0102e-04</hent>
<hent key="024001000000000000001C000000000A" call="MPI_Isend" bytes="7168" orank="10" region="0" commid="0" count="9950" tid="0" op="" dtype="" >1.9714e-02 9.5367e-07 4.1008e-05</hent>
<hent key="024001000000000000001C0000000012" call="MPI_Isend" bytes="7168" orank="18" region="0" commid="0" count="9510" tid="0" op="" dtype="" >1.9435e-02 9.5367e-07 2.1935e-05</hent>
<hent key="024001000000000000000004000002AE" call="MPI_Isend" bytes="4" orank="686" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.1501e-02 3.8147e-06 6.0081e-05</hent>
<hent key="024001000000000000001C00000002AE" call="MPI_Isend" bytes="7168" orank="686" region="0" commid="0" count="3" tid="0" op="" dtype="" >2.6226e-05 6.1989e-06 1.0967e-05</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.5310e-04 3.5310e-04 3.5310e-04</hent>
<hent key="0380010000000000000002000000000D" call="MPI_Irecv" bytes="512" orank="13" region="0" commid="0" count="3376" tid="0" op="" dtype="" >6.4969e-04 0.0000e+00 2.2888e-05</hent>
<hent key="0380010000000000000002000000000F" call="MPI_Irecv" bytes="512" orank="15" region="0" commid="0" count="3358" tid="0" op="" dtype="" >4.9138e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0380010000000000000005000000000A" call="MPI_Irecv" bytes="1280" orank="10" region="0" commid="0" count="135" tid="0" op="" dtype="" >4.2439e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000D" call="MPI_Irecv" bytes="1280" orank="13" region="0" commid="0" count="39" tid="0" op="" dtype="" >2.2888e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000F" call="MPI_Irecv" bytes="1280" orank="15" region="0" commid="0" count="48" tid="0" op="" dtype="" >1.5974e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000012" call="MPI_Irecv" bytes="1280" orank="18" region="0" commid="0" count="134" tid="0" op="" dtype="" >2.2411e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000026" call="MPI_Irecv" bytes="1280" orank="38" region="0" commid="0" count="49" tid="0" op="" dtype="" >2.8610e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4481e+01 7.8678e-06 1.8477e-01</hent>
<hent key="038001000000000000000500000002AE" call="MPI_Irecv" bytes="1280" orank="686" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.5736e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000002000000000D" call="MPI_Isend" bytes="512" orank="13" region="0" commid="0" count="3366" tid="0" op="" dtype="" >1.9007e-03 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000002000000000F" call="MPI_Isend" bytes="512" orank="15" region="0" commid="0" count="3328" tid="0" op="" dtype="" >2.1415e-03 0.0000e+00 2.0027e-05</hent>
<hent key="0380010000000000000008000000000A" call="MPI_Irecv" bytes="2048" orank="10" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000D" call="MPI_Irecv" bytes="2048" orank="13" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.9073e-06 9.5367e-07 9.5367e-07</hent>
<hent key="0380010000000000000008000000000F" call="MPI_Irecv" bytes="2048" orank="15" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000080000000012" call="MPI_Irecv" bytes="2048" orank="18" region="0" commid="0" count="9" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000080000000026" call="MPI_Irecv" bytes="2048" orank="38" region="0" commid="0" count="146" tid="0" op="" dtype="" >8.0347e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000800000002AE" call="MPI_Irecv" bytes="2048" orank="686" region="0" commid="0" count="147" tid="0" op="" dtype="" >6.5327e-05 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000005000000000A" call="MPI_Isend" bytes="1280" orank="10" region="0" commid="0" count="150" tid="0" op="" dtype="" >2.3174e-04 9.5367e-07 1.0967e-05</hent>
<hent key="0240010000000000000005000000000D" call="MPI_Isend" bytes="1280" orank="13" region="0" commid="0" count="46" tid="0" op="" dtype="" >8.7023e-05 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000005000000000F" call="MPI_Isend" bytes="1280" orank="15" region="0" commid="0" count="58" tid="0" op="" dtype="" >2.1935e-04 2.8610e-06 8.1062e-06</hent>
<hent key="02400100000000000000050000000012" call="MPI_Isend" bytes="1280" orank="18" region="0" commid="0" count="144" tid="0" op="" dtype="" >2.0242e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000050000000026" call="MPI_Isend" bytes="1280" orank="38" region="0" commid="0" count="41" tid="0" op="" dtype="" >2.3007e-04 3.8147e-06 1.4067e-05</hent>
<hent key="024001000000000000000500000002AE" call="MPI_Isend" bytes="1280" orank="686" region="0" commid="0" count="53" tid="0" op="" dtype="" >2.7418e-04 3.0994e-06 1.5974e-05</hent>
</hash>
<internal rank="14" log_i="1724765674.533762" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="15" mpi_size="696" stamp_init="1724765564.465690" stamp_final="1724765674.523140" username="apac4" allocationname="unknown" flags="0" pid="1612132" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10057e+02" utime="8.95328e+01" stime="1.37335e+01" mtime="7.30058e+01" gflop="0.00000e+00" gbyte="3.76522e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.30058e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004715b755471542153d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09923e+02" utime="8.95011e+01" stime="1.37227e+01" mtime="7.30058e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.30058e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2186e+09" > 6.0345e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2140e+09" > 2.8392e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1578e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9796e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2159e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1723e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5759e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.4579e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0580e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3102e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4745e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="212" >
<hent key="0240010000000000000008000000000B" call="MPI_Isend" bytes="2048" orank="11" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.3113e-05 1.1921e-06 2.1458e-06</hent>
<hent key="0240010000000000000008000000000C" call="MPI_Isend" bytes="2048" orank="12" region="0" commid="0" count="2" tid="0" op="" dtype="" >5.7220e-06 2.8610e-06 2.8610e-06</hent>
<hent key="0240010000000000000008000000000E" call="MPI_Isend" bytes="2048" orank="14" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="02400100000000000000080000000013" call="MPI_Isend" bytes="2048" orank="19" region="0" commid="0" count="6" tid="0" op="" dtype="" >9.7752e-06 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000080000000027" call="MPI_Isend" bytes="2048" orank="39" region="0" commid="0" count="140" tid="0" op="" dtype="" >7.8034e-04 4.7684e-06 1.1921e-05</hent>
<hent key="024001000000000000000800000002AF" call="MPI_Isend" bytes="2048" orank="687" region="0" commid="0" count="156" tid="0" op="" dtype="" >7.0477e-04 3.8147e-06 9.0599e-06</hent>
<hent key="038001000000000000000E000000000C" call="MPI_Irecv" bytes="3584" orank="12" region="0" commid="0" count="182" tid="0" op="" dtype="" >2.5749e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E000000000E" call="MPI_Irecv" bytes="3584" orank="14" region="0" commid="0" count="226" tid="0" op="" dtype="" >7.8917e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000027" call="MPI_Irecv" bytes="3584" orank="39" region="0" commid="0" count="129" tid="0" op="" dtype="" >5.2929e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000E00000002AF" call="MPI_Irecv" bytes="3584" orank="687" region="0" commid="0" count="126" tid="0" op="" dtype="" >2.5272e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000E000000000C" call="MPI_Isend" bytes="3584" orank="12" region="0" commid="0" count="84" tid="0" op="" dtype="" >1.7858e-04 9.5367e-07 4.0531e-06</hent>
<hent key="024001000000000000000E000000000E" call="MPI_Isend" bytes="3584" orank="14" region="0" commid="0" count="168" tid="0" op="" dtype="" >2.8944e-04 0.0000e+00 1.0014e-05</hent>
<hent key="024001000000000000000E0000000027" call="MPI_Isend" bytes="3584" orank="39" region="0" commid="0" count="128" tid="0" op="" dtype="" >8.0061e-04 5.0068e-06 7.1526e-06</hent>
<hent key="024001000000000000000E00000002AF" call="MPI_Isend" bytes="3584" orank="687" region="0" commid="0" count="124" tid="0" op="" dtype="" >6.5565e-04 3.8147e-06 1.1921e-05</hent>
<hent key="0380010000000000000002800000000B" call="MPI_Irecv" bytes="640" orank="11" region="0" commid="0" count="438" tid="0" op="" dtype="" >7.8201e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000C" call="MPI_Irecv" bytes="640" orank="12" region="0" commid="0" count="342" tid="0" op="" dtype="" >8.6069e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000E" call="MPI_Irecv" bytes="640" orank="14" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.3876e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000013" call="MPI_Irecv" bytes="640" orank="19" region="0" commid="0" count="424" tid="0" op="" dtype="" >1.2708e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000028000000027" call="MPI_Irecv" bytes="640" orank="39" region="0" commid="0" count="26" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000140000000027" call="MPI_Irecv" bytes="5120" orank="39" region="0" commid="0" count="915" tid="0" op="" dtype="" >1.8549e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000000280000002AF" call="MPI_Irecv" bytes="640" orank="687" region="0" commid="0" count="31" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000001400000002AF" call="MPI_Irecv" bytes="5120" orank="687" region="0" commid="0" count="961" tid="0" op="" dtype="" >1.9193e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000A00000000027" call="MPI_Irecv" bytes="40960" orank="39" region="0" commid="0" count="2942" tid="0" op="" dtype="" >1.0488e-03 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000002800000000B" call="MPI_Isend" bytes="640" orank="11" region="0" commid="0" count="375" tid="0" op="" dtype="" >4.5776e-04 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000002800000000C" call="MPI_Isend" bytes="640" orank="12" region="0" commid="0" count="373" tid="0" op="" dtype="" >1.0300e-03 1.9073e-06 7.1526e-06</hent>
<hent key="0240010000000000000002800000000E" call="MPI_Isend" bytes="640" orank="14" region="0" commid="0" count="372" tid="0" op="" dtype="" >5.5170e-04 9.5367e-07 1.2875e-05</hent>
<hent key="02400100000000000000028000000013" call="MPI_Isend" bytes="640" orank="19" region="0" commid="0" count="432" tid="0" op="" dtype="" >5.0974e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000028000000027" call="MPI_Isend" bytes="640" orank="39" region="0" commid="0" count="24" tid="0" op="" dtype="" >1.1611e-04 3.8147e-06 5.9605e-06</hent>
<hent key="03800100000000000000A000000002AF" call="MPI_Irecv" bytes="40960" orank="687" region="0" commid="0" count="3283" tid="0" op="" dtype="" >7.4768e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000140000000027" call="MPI_Isend" bytes="5120" orank="39" region="0" commid="0" count="1003" tid="0" op="" dtype="" >4.0486e-03 9.5367e-07 1.4067e-05</hent>
<hent key="024001000000000000000280000002AF" call="MPI_Isend" bytes="640" orank="687" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.2374e-04 2.8610e-06 5.0068e-06</hent>
<hent key="03800100000000000000800000000027" call="MPI_Irecv" bytes="32768" orank="39" region="0" commid="0" count="9758" tid="0" op="" dtype="" >3.4266e-03 0.0000e+00 1.4782e-05</hent>
<hent key="024001000000000000001400000002AF" call="MPI_Isend" bytes="5120" orank="687" region="0" commid="0" count="966" tid="0" op="" dtype="" >3.3247e-03 9.5367e-07 7.1526e-06</hent>
<hent key="038001000000000000008000000002AF" call="MPI_Irecv" bytes="32768" orank="687" region="0" commid="0" count="9417" tid="0" op="" dtype="" >2.1157e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0380010000000000000001400000000B" call="MPI_Irecv" bytes="320" orank="11" region="0" commid="0" count="449" tid="0" op="" dtype="" >7.1764e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000C" call="MPI_Irecv" bytes="320" orank="12" region="0" commid="0" count="561" tid="0" op="" dtype="" >1.4949e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000E" call="MPI_Irecv" bytes="320" orank="14" region="0" commid="0" count="551" tid="0" op="" dtype="" >2.0576e-04 0.0000e+00 3.8147e-06</hent>
<hent key="03800100000000000000014000000013" call="MPI_Irecv" bytes="320" orank="19" region="0" commid="0" count="448" tid="0" op="" dtype="" >9.9659e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000027" call="MPI_Irecv" bytes="320" orank="39" region="0" commid="0" count="31" tid="0" op="" dtype="" >1.1683e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A00000000027" call="MPI_Isend" bytes="40960" orank="39" region="0" commid="0" count="3421" tid="0" op="" dtype="" >7.2055e-02 1.0014e-05 5.1975e-05</hent>
<hent key="038001000000000000000140000002AF" call="MPI_Irecv" bytes="320" orank="687" region="0" commid="0" count="18" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000A000000002AF" call="MPI_Isend" bytes="40960" orank="687" region="0" commid="0" count="3284" tid="0" op="" dtype="" >3.5596e-02 5.9605e-06 3.0041e-05</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >6.1578e+00 0.0000e+00 1.1077e-01</hent>
<hent key="02400100000000000000800000000027" call="MPI_Isend" bytes="32768" orank="39" region="0" commid="0" count="9279" tid="0" op="" dtype="" >1.8380e-01 9.0599e-06 1.1802e-04</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-05 0.0000e+00 1.8120e-05</hent>
<hent key="024001000000000000008000000002AF" call="MPI_Isend" bytes="32768" orank="687" region="0" commid="0" count="9416" tid="0" op="" dtype="" >1.0092e-01 5.9605e-06 5.6982e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0240010000000000000001400000000B" call="MPI_Isend" bytes="320" orank="11" region="0" commid="0" count="474" tid="0" op="" dtype="" >4.6563e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000001400000000C" call="MPI_Isend" bytes="320" orank="12" region="0" commid="0" count="512" tid="0" op="" dtype="" >1.2245e-03 9.5367e-07 8.1062e-06</hent>
<hent key="0240010000000000000001400000000E" call="MPI_Isend" bytes="320" orank="14" region="0" commid="0" count="526" tid="0" op="" dtype="" >6.0248e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000014000000013" call="MPI_Isend" bytes="320" orank="19" region="0" commid="0" count="424" tid="0" op="" dtype="" >4.1795e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000014000000027" call="MPI_Isend" bytes="320" orank="39" region="0" commid="0" count="36" tid="0" op="" dtype="" >1.6117e-04 3.8147e-06 5.9605e-06</hent>
<hent key="03800100000000000000200000000027" call="MPI_Irecv" bytes="8192" orank="39" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="024001000000000000000140000002AF" call="MPI_Isend" bytes="320" orank="687" region="0" commid="0" count="23" tid="0" op="" dtype="" >1.0252e-04 3.0994e-06 9.0599e-06</hent>
<hent key="038001000000000000002000000002AF" call="MPI_Irecv" bytes="8192" orank="687" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000000000000000B" call="MPI_Irecv" bytes="0" orank="11" region="0" commid="0" count="294" tid="0" op="" dtype="" >5.5552e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000C" call="MPI_Irecv" bytes="0" orank="12" region="0" commid="0" count="534" tid="0" op="" dtype="" >1.5092e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000000000000000E" call="MPI_Irecv" bytes="0" orank="14" region="0" commid="0" count="561" tid="0" op="" dtype="" >1.9121e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000013" call="MPI_Irecv" bytes="0" orank="19" region="0" commid="0" count="346" tid="0" op="" dtype="" >7.9393e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000027" call="MPI_Irecv" bytes="0" orank="39" region="0" commid="0" count="85" tid="0" op="" dtype="" >1.1921e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002AF" call="MPI_Irecv" bytes="0" orank="687" region="0" commid="0" count="88" tid="0" op="" dtype="" >1.9312e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.0337e-04 0.0000e+00 8.5115e-05</hent>
<hent key="0380010000000000000003000000000B" call="MPI_Irecv" bytes="768" orank="11" region="0" commid="0" count="880" tid="0" op="" dtype="" >1.0824e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000030000000013" call="MPI_Irecv" bytes="768" orank="19" region="0" commid="0" count="950" tid="0" op="" dtype="" >1.6856e-04 0.0000e+00 1.2875e-05</hent>
<hent key="0240010000000000000000000000000B" call="MPI_Isend" bytes="0" orank="11" region="0" commid="0" count="314" tid="0" op="" dtype="" >2.1434e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000000000000000C" call="MPI_Isend" bytes="0" orank="12" region="0" commid="0" count="559" tid="0" op="" dtype="" >9.8205e-04 9.5367e-07 3.4809e-05</hent>
<hent key="0240010000000000000000000000000E" call="MPI_Isend" bytes="0" orank="14" region="0" commid="0" count="572" tid="0" op="" dtype="" >3.9792e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000013" call="MPI_Isend" bytes="0" orank="19" region="0" commid="0" count="345" tid="0" op="" dtype="" >2.4104e-04 0.0000e+00 8.8215e-06</hent>
<hent key="02400100000000000000000000000027" call="MPI_Isend" bytes="0" orank="39" region="0" commid="0" count="86" tid="0" op="" dtype="" >3.3474e-04 9.5367e-07 5.0068e-06</hent>
<hent key="024001000000000000000000000002AF" call="MPI_Isend" bytes="0" orank="687" region="0" commid="0" count="82" tid="0" op="" dtype="" >2.9016e-04 9.5367e-07 4.0531e-06</hent>
<hent key="0380010000000000000006000000000B" call="MPI_Irecv" bytes="1536" orank="11" region="0" commid="0" count="55" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000C" call="MPI_Irecv" bytes="1536" orank="12" region="0" commid="0" count="17" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000E" call="MPI_Irecv" bytes="1536" orank="14" region="0" commid="0" count="11" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000013" call="MPI_Irecv" bytes="1536" orank="19" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.8120e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000027" call="MPI_Irecv" bytes="1536" orank="39" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.2663e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002AF" call="MPI_Irecv" bytes="1536" orank="687" region="0" commid="0" count="83" tid="0" op="" dtype="" >2.8610e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000003000000000B" call="MPI_Isend" bytes="768" orank="11" region="0" commid="0" count="860" tid="0" op="" dtype="" >4.0960e-04 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000030000000013" call="MPI_Isend" bytes="768" orank="19" region="0" commid="0" count="808" tid="0" op="" dtype="" >4.3392e-04 0.0000e+00 5.4836e-05</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C00000000C" call="MPI_Irecv" bytes="448" orank="12" region="0" commid="0" count="42" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C00000000E" call="MPI_Irecv" bytes="448" orank="14" region="0" commid="0" count="72" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000006000000000B" call="MPI_Isend" bytes="1536" orank="11" region="0" commid="0" count="57" tid="0" op="" dtype="" >9.1791e-05 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000006000000000C" call="MPI_Isend" bytes="1536" orank="12" region="0" commid="0" count="14" tid="0" op="" dtype="" >4.5776e-05 1.9073e-06 4.0531e-06</hent>
<hent key="0240010000000000000006000000000E" call="MPI_Isend" bytes="1536" orank="14" region="0" commid="0" count="16" tid="0" op="" dtype="" >3.2425e-05 1.1921e-06 3.0994e-06</hent>
<hent key="02400100000000000000060000000013" call="MPI_Isend" bytes="1536" orank="19" region="0" commid="0" count="49" tid="0" op="" dtype="" >9.3699e-05 9.5367e-07 1.0014e-05</hent>
<hent key="02400100000000000000060000000027" call="MPI_Isend" bytes="1536" orank="39" region="0" commid="0" count="77" tid="0" op="" dtype="" >4.0674e-04 4.0531e-06 7.1526e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002AF" call="MPI_Isend" bytes="1536" orank="687" region="0" commid="0" count="90" tid="0" op="" dtype="" >3.9673e-04 2.8610e-06 5.0068e-06</hent>
<hent key="038001000000000000000C0000000013" call="MPI_Irecv" bytes="3072" orank="19" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C0000000027" call="MPI_Irecv" bytes="3072" orank="39" region="0" commid="0" count="313" tid="0" op="" dtype="" >1.1516e-04 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002AF" call="MPI_Irecv" bytes="3072" orank="687" region="0" commid="0" count="324" tid="0" op="" dtype="" >8.3685e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C00000000C" call="MPI_Isend" bytes="448" orank="12" region="0" commid="0" count="20" tid="0" op="" dtype="" >1.0014e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C00000000E" call="MPI_Isend" bytes="448" orank="14" region="0" commid="0" count="42" tid="0" op="" dtype="" >1.3828e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000027" call="MPI_Isend" bytes="3072" orank="39" region="0" commid="0" count="318" tid="0" op="" dtype="" >1.9069e-03 4.7684e-06 1.3113e-05</hent>
<hent key="024001000000000000000C00000002AF" call="MPI_Isend" bytes="3072" orank="687" region="0" commid="0" count="292" tid="0" op="" dtype="" >1.4682e-03 3.8147e-06 1.4067e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >7.7510e-04 2.5606e-04 2.6202e-04</hent>
<hent key="0380010000000000000003800000000B" call="MPI_Irecv" bytes="896" orank="11" region="0" commid="0" count="2814" tid="0" op="" dtype="" >3.2830e-04 0.0000e+00 6.1989e-06</hent>
<hent key="0380010000000000000003800000000C" call="MPI_Irecv" bytes="896" orank="12" region="0" commid="0" count="189" tid="0" op="" dtype="" >4.2200e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000E" call="MPI_Irecv" bytes="896" orank="14" region="0" commid="0" count="148" tid="0" op="" dtype="" >5.6982e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000038000000013" call="MPI_Irecv" bytes="896" orank="19" region="0" commid="0" count="2719" tid="0" op="" dtype="" >4.8113e-04 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000038000000027" call="MPI_Irecv" bytes="896" orank="39" region="0" commid="0" count="36" tid="0" op="" dtype="" >7.3910e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002AF" call="MPI_Irecv" bytes="896" orank="687" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.4305e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.4009e-04 2.4009e-04 2.4009e-04</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="0380010000000000000018000000000B" call="MPI_Irecv" bytes="6144" orank="11" region="0" commid="0" count="3090" tid="0" op="" dtype="" >5.5933e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000180000000013" call="MPI_Irecv" bytes="6144" orank="19" region="0" commid="0" count="3473" tid="0" op="" dtype="" >4.6349e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000180000000027" call="MPI_Irecv" bytes="6144" orank="39" region="0" commid="0" count="27" tid="0" op="" dtype="" >9.2983e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001800000002AF" call="MPI_Irecv" bytes="6144" orank="687" region="0" commid="0" count="21" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000003800000000B" call="MPI_Isend" bytes="896" orank="11" region="0" commid="0" count="2851" tid="0" op="" dtype="" >1.6806e-03 0.0000e+00 7.1526e-06</hent>
<hent key="0240010000000000000003800000000C" call="MPI_Isend" bytes="896" orank="12" region="0" commid="0" count="181" tid="0" op="" dtype="" >5.5671e-04 1.9073e-06 1.0967e-05</hent>
<hent key="0240010000000000000003800000000E" call="MPI_Isend" bytes="896" orank="14" region="0" commid="0" count="163" tid="0" op="" dtype="" >2.9612e-04 9.5367e-07 7.1526e-06</hent>
<hent key="02400100000000000000038000000013" call="MPI_Isend" bytes="896" orank="19" region="0" commid="0" count="2876" tid="0" op="" dtype="" >1.6499e-03 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000038000000027" call="MPI_Isend" bytes="896" orank="39" region="0" commid="0" count="23" tid="0" op="" dtype="" >1.1516e-04 4.0531e-06 6.1989e-06</hent>
<hent key="024001000000000000000380000002AF" call="MPI_Isend" bytes="896" orank="687" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.6427e-04 3.0994e-06 5.0068e-06</hent>
<hent key="0240010000000000000018000000000B" call="MPI_Isend" bytes="6144" orank="11" region="0" commid="0" count="3108" tid="0" op="" dtype="" >5.8560e-03 9.5367e-07 2.1935e-05</hent>
<hent key="02400100000000000000180000000013" call="MPI_Isend" bytes="6144" orank="19" region="0" commid="0" count="2982" tid="0" op="" dtype="" >5.3649e-03 9.5367e-07 1.4782e-05</hent>
<hent key="02400100000000000000180000000027" call="MPI_Isend" bytes="6144" orank="39" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.9646e-04 5.9605e-06 8.1062e-06</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.2915e-05 4.2915e-05 4.2915e-05</hent>
<hent key="024001000000000000001800000002AF" call="MPI_Isend" bytes="6144" orank="687" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.3232e-04 5.0068e-06 7.1526e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >9.4385e+00 1.5974e-05 1.2674e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.5592e-04 8.5592e-04 8.5592e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0580e-02 1.0580e-02 1.0580e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >4.5762e-03 4.5762e-03 4.5762e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.1886e-01 3.3641e-03 2.1055e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.6314e-04 5.6314e-04 5.6314e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5759e+00 4.3011e-04 2.4758e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.2159e-05 0.0000e+00 5.9605e-06</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0380010000000000000007000000000B" call="MPI_Irecv" bytes="1792" orank="11" region="0" commid="0" count="15" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000007000000000C" call="MPI_Irecv" bytes="1792" orank="12" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000E" call="MPI_Irecv" bytes="1792" orank="14" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000013" call="MPI_Irecv" bytes="1792" orank="19" region="0" commid="0" count="19" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000070000000027" call="MPI_Irecv" bytes="1792" orank="39" region="0" commid="0" count="112" tid="0" op="" dtype="" >3.7909e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002AF" call="MPI_Irecv" bytes="1792" orank="687" region="0" commid="0" count="119" tid="0" op="" dtype="" >2.7895e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >1.1039e-04 0.0000e+00 6.2943e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9796e+01 0.0000e+00 3.8416e+01</hent>
<hent key="038001000000000000000A0000000027" call="MPI_Irecv" bytes="2560" orank="39" region="0" commid="0" count="335" tid="0" op="" dtype="" >1.1325e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002AF" call="MPI_Irecv" bytes="2560" orank="687" region="0" commid="0" count="327" tid="0" op="" dtype="" >9.7036e-05 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000007000000000B" call="MPI_Isend" bytes="1792" orank="11" region="0" commid="0" count="29" tid="0" op="" dtype="" >4.7684e-05 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000007000000000C" call="MPI_Isend" bytes="1792" orank="12" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.9073e-05 2.8610e-06 8.1062e-06</hent>
<hent key="0240010000000000000007000000000E" call="MPI_Isend" bytes="1792" orank="14" region="0" commid="0" count="2" tid="0" op="" dtype="" >3.8147e-06 1.9073e-06 1.9073e-06</hent>
<hent key="02400100000000000000070000000013" call="MPI_Isend" bytes="1792" orank="19" region="0" commid="0" count="24" tid="0" op="" dtype="" >5.2452e-05 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000070000000027" call="MPI_Isend" bytes="1792" orank="39" region="0" commid="0" count="115" tid="0" op="" dtype="" >6.2799e-04 5.0068e-06 1.1206e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3102e-02 8.3102e-02 8.3102e-02</hent>
<hent key="024001000000000000000700000002AF" call="MPI_Isend" bytes="1792" orank="687" region="0" commid="0" count="125" tid="0" op="" dtype="" >5.4789e-04 2.8610e-06 8.8215e-06</hent>
<hent key="024001000000000000000A0000000013" call="MPI_Isend" bytes="2560" orank="19" region="0" commid="0" count="4" tid="0" op="" dtype="" >8.1062e-06 1.9073e-06 2.1458e-06</hent>
<hent key="024001000000000000000A0000000027" call="MPI_Isend" bytes="2560" orank="39" region="0" commid="0" count="322" tid="0" op="" dtype="" >1.8778e-03 4.7684e-06 6.1989e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.5906e-04 4.1008e-05 1.5998e-04</hent>
<hent key="024001000000000000000A00000002AF" call="MPI_Isend" bytes="2560" orank="687" region="0" commid="0" count="360" tid="0" op="" dtype="" >1.6754e-03 3.8147e-06 1.0014e-05</hent>
<hent key="0380010000000000000010000000000C" call="MPI_Irecv" bytes="4096" orank="12" region="0" commid="0" count="12518" tid="0" op="" dtype="" >2.3472e-03 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000010000000000E" call="MPI_Irecv" bytes="4096" orank="14" region="0" commid="0" count="12474" tid="0" op="" dtype="" >4.7088e-03 0.0000e+00 3.0041e-05</hent>
<hent key="03800100000000000000100000000027" call="MPI_Irecv" bytes="4096" orank="39" region="0" commid="0" count="2796" tid="0" op="" dtype="" >6.0797e-04 0.0000e+00 8.7023e-05</hent>
<hent key="038001000000000000001000000002AF" call="MPI_Irecv" bytes="4096" orank="687" region="0" commid="0" count="2758" tid="0" op="" dtype="" >5.8508e-04 0.0000e+00 6.1989e-06</hent>
<hent key="0240010000000000000010000000000C" call="MPI_Isend" bytes="4096" orank="12" region="0" commid="0" count="12616" tid="0" op="" dtype="" >2.8938e-02 0.0000e+00 2.0981e-05</hent>
<hent key="0240010000000000000010000000000E" call="MPI_Isend" bytes="4096" orank="14" region="0" commid="0" count="12532" tid="0" op="" dtype="" >2.2332e-02 0.0000e+00 1.7881e-05</hent>
<hent key="02400100000000000000100000000027" call="MPI_Isend" bytes="4096" orank="39" region="0" commid="0" count="2731" tid="0" op="" dtype="" >1.0776e-02 9.5367e-07 3.0041e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.2169e-03 4.4298e-04 7.7391e-04</hent>
<hent key="024001000000000000001000000002AF" call="MPI_Isend" bytes="4096" orank="687" region="0" commid="0" count="2732" tid="0" op="" dtype="" >9.4707e-03 9.5367e-07 2.0981e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.7909e-05 3.7909e-05 3.7909e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.7268e-03 3.7599e-04 9.7203e-04</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.0252e-05 2.1458e-06 8.1062e-06</hent>
<hent key="0380010000000000000000040000000B" call="MPI_Irecv" bytes="4" orank="11" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.5538e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000000C" call="MPI_Irecv" bytes="4" orank="12" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.9329e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000000E" call="MPI_Irecv" bytes="4" orank="14" region="0" commid="0" count="3400" tid="0" op="" dtype="" >9.5057e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000000400000013" call="MPI_Irecv" bytes="4" orank="19" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.8889e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000000400000027" call="MPI_Irecv" bytes="4" orank="39" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.1965e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001C000000000B" call="MPI_Irecv" bytes="7168" orank="11" region="0" commid="0" count="9610" tid="0" op="" dtype="" >1.6370e-03 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000001C0000000013" call="MPI_Irecv" bytes="7168" orank="19" region="0" commid="0" count="9227" tid="0" op="" dtype="" >1.3216e-03 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000001C0000000027" call="MPI_Irecv" bytes="7168" orank="39" region="0" commid="0" count="3" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000004000002AF" call="MPI_Irecv" bytes="4" orank="687" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.9748e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001C00000002AF" call="MPI_Irecv" bytes="7168" orank="687" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.1921e-06 1.1921e-06 1.1921e-06</hent>
<hent key="0240010000000000000000040000000B" call="MPI_Isend" bytes="4" orank="11" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.2519e-03 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000040000000C" call="MPI_Isend" bytes="4" orank="12" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.4576e-03 0.0000e+00 5.6982e-05</hent>
<hent key="0240010000000000000000040000000E" call="MPI_Isend" bytes="4" orank="14" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.4784e-03 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000000400000013" call="MPI_Isend" bytes="4" orank="19" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.4315e-03 0.0000e+00 3.4094e-05</hent>
<hent key="02400100000000000000000400000027" call="MPI_Isend" bytes="4" orank="39" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.6519e-02 3.8147e-06 3.1996e-04</hent>
<hent key="024001000000000000001C000000000B" call="MPI_Isend" bytes="7168" orank="11" region="0" commid="0" count="9592" tid="0" op="" dtype="" >1.8603e-02 9.5367e-07 2.0027e-05</hent>
<hent key="024001000000000000001C0000000013" call="MPI_Isend" bytes="7168" orank="19" region="0" commid="0" count="9718" tid="0" op="" dtype="" >1.8218e-02 9.5367e-07 1.5807e-04</hent>
<hent key="024001000000000000001C0000000027" call="MPI_Isend" bytes="7168" orank="39" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.4067e-05 1.4067e-05 1.4067e-05</hent>
<hent key="024001000000000000000004000002AF" call="MPI_Isend" bytes="4" orank="687" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.9185e-02 3.8147e-06 6.4850e-05</hent>
<hent key="024001000000000000001C00000002AF" call="MPI_Isend" bytes="7168" orank="687" region="0" commid="0" count="6" tid="0" op="" dtype="" >4.2439e-05 6.9141e-06 7.1526e-06</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.7909e-04 3.7909e-04 3.7909e-04</hent>
<hent key="0380010000000000000002000000000C" call="MPI_Irecv" bytes="512" orank="12" region="0" commid="0" count="3358" tid="0" op="" dtype="" >5.2357e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000002000000000E" call="MPI_Irecv" bytes="512" orank="14" region="0" commid="0" count="3328" tid="0" op="" dtype="" >4.9305e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000B" call="MPI_Irecv" bytes="1280" orank="11" region="0" commid="0" count="139" tid="0" op="" dtype="" >2.8849e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000C" call="MPI_Irecv" bytes="1280" orank="12" region="0" commid="0" count="51" tid="0" op="" dtype="" >1.5020e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000E" call="MPI_Irecv" bytes="1280" orank="14" region="0" commid="0" count="58" tid="0" op="" dtype="" >1.3113e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000013" call="MPI_Irecv" bytes="1280" orank="19" region="0" commid="0" count="137" tid="0" op="" dtype="" >3.9816e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000050000000027" call="MPI_Irecv" bytes="1280" orank="39" region="0" commid="0" count="68" tid="0" op="" dtype="" >1.9550e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.5081e+01 1.0967e-05 1.8447e-01</hent>
<hent key="038001000000000000000500000002AF" call="MPI_Irecv" bytes="1280" orank="687" region="0" commid="0" count="57" tid="0" op="" dtype="" >9.5367e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0240010000000000000002000000000C" call="MPI_Isend" bytes="512" orank="12" region="0" commid="0" count="3380" tid="0" op="" dtype="" >2.0702e-03 0.0000e+00 1.5020e-05</hent>
<hent key="0240010000000000000002000000000E" call="MPI_Isend" bytes="512" orank="14" region="0" commid="0" count="3358" tid="0" op="" dtype="" >1.7865e-03 0.0000e+00 1.2159e-05</hent>
<hent key="0380010000000000000008000000000B" call="MPI_Irecv" bytes="2048" orank="11" region="0" commid="0" count="16" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000C" call="MPI_Irecv" bytes="2048" orank="12" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000080000000013" call="MPI_Irecv" bytes="2048" orank="19" region="0" commid="0" count="9" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000027" call="MPI_Irecv" bytes="2048" orank="39" region="0" commid="0" count="149" tid="0" op="" dtype="" >4.9353e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002AF" call="MPI_Irecv" bytes="2048" orank="687" region="0" commid="0" count="143" tid="0" op="" dtype="" >4.0531e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000005000000000B" call="MPI_Isend" bytes="1280" orank="11" region="0" commid="0" count="133" tid="0" op="" dtype="" >2.1219e-04 9.5367e-07 1.0967e-05</hent>
<hent key="0240010000000000000005000000000C" call="MPI_Isend" bytes="1280" orank="12" region="0" commid="0" count="55" tid="0" op="" dtype="" >1.7738e-04 1.9073e-06 4.0531e-06</hent>
<hent key="0240010000000000000005000000000E" call="MPI_Isend" bytes="1280" orank="14" region="0" commid="0" count="48" tid="0" op="" dtype="" >9.6321e-05 9.5367e-07 7.8678e-06</hent>
<hent key="02400100000000000000050000000013" call="MPI_Isend" bytes="1280" orank="19" region="0" commid="0" count="132" tid="0" op="" dtype="" >2.0480e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000050000000027" call="MPI_Isend" bytes="1280" orank="39" region="0" commid="0" count="68" tid="0" op="" dtype="" >3.6478e-04 3.8147e-06 1.2875e-05</hent>
<hent key="024001000000000000000500000002AF" call="MPI_Isend" bytes="1280" orank="687" region="0" commid="0" count="52" tid="0" op="" dtype="" >2.2912e-04 2.8610e-06 1.0014e-05</hent>
</hash>
<internal rank="15" log_i="1724765674.523140" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="16" mpi_size="696" stamp_init="1724765564.466902" stamp_final="1724765674.538598" username="apac4" allocationname="unknown" flags="0" pid="1612133" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10072e+02" utime="8.58631e+01" stime="1.48888e+01" mtime="7.18704e+01" gflop="0.00000e+00" gbyte="3.76137e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18704e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ab15aa1548" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09935e+02" utime="8.58323e+01" stime="1.48761e+01" mtime="7.18704e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18704e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2152e+09" > 1.0363e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2147e+09" > 7.1023e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1220e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9719e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.5179e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.0068e-06 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5754e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.6813e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0596e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3134e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3182e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="209" >
<hent key="0240010000000000000008000000000C" call="MPI_Isend" bytes="2048" orank="12" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.0252e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000080000000011" call="MPI_Isend" bytes="2048" orank="17" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.2159e-05 5.0068e-06 7.1526e-06</hent>
<hent key="02400100000000000000080000000014" call="MPI_Isend" bytes="2048" orank="20" region="0" commid="0" count="5" tid="0" op="" dtype="" >9.7752e-06 1.9073e-06 2.1458e-06</hent>
<hent key="02400100000000000000080000000028" call="MPI_Isend" bytes="2048" orank="40" region="0" commid="0" count="156" tid="0" op="" dtype="" >1.1518e-03 4.7684e-06 2.7180e-05</hent>
<hent key="024001000000000000000800000002B0" call="MPI_Isend" bytes="2048" orank="688" region="0" commid="0" count="176" tid="0" op="" dtype="" >9.8372e-04 3.8147e-06 1.4067e-05</hent>
<hent key="038001000000000000000E0000000011" call="MPI_Irecv" bytes="3584" orank="17" region="0" commid="0" count="208" tid="0" op="" dtype="" >3.0518e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000013" call="MPI_Irecv" bytes="3584" orank="19" region="0" commid="0" count="171" tid="0" op="" dtype="" >5.8174e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000028" call="MPI_Irecv" bytes="3584" orank="40" region="0" commid="0" count="128" tid="0" op="" dtype="" >9.3699e-05 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000E00000002B0" call="MPI_Irecv" bytes="3584" orank="688" region="0" commid="0" count="123" tid="0" op="" dtype="" >6.5088e-05 0.0000e+00 1.0014e-05</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000E0000000011" call="MPI_Isend" bytes="3584" orank="17" region="0" commid="0" count="102" tid="0" op="" dtype="" >2.6917e-04 0.0000e+00 5.0068e-06</hent>
<hent key="024001000000000000000E0000000013" call="MPI_Isend" bytes="3584" orank="19" region="0" commid="0" count="65" tid="0" op="" dtype="" >9.8944e-05 9.5367e-07 2.1458e-06</hent>
<hent key="024001000000000000000E0000000028" call="MPI_Isend" bytes="3584" orank="40" region="0" commid="0" count="124" tid="0" op="" dtype="" >1.0207e-03 5.0068e-06 2.3127e-05</hent>
<hent key="024001000000000000000E00000002B0" call="MPI_Isend" bytes="3584" orank="688" region="0" commid="0" count="152" tid="0" op="" dtype="" >9.3651e-04 3.8147e-06 1.8835e-05</hent>
<hent key="0380010000000000000002800000000C" call="MPI_Irecv" bytes="640" orank="12" region="0" commid="0" count="426" tid="0" op="" dtype="" >8.3447e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000011" call="MPI_Irecv" bytes="640" orank="17" region="0" commid="0" count="379" tid="0" op="" dtype="" >1.0085e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000013" call="MPI_Irecv" bytes="640" orank="19" region="0" commid="0" count="336" tid="0" op="" dtype="" >2.8801e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000028000000014" call="MPI_Irecv" bytes="640" orank="20" region="0" commid="0" count="386" tid="0" op="" dtype="" >8.4639e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000028" call="MPI_Irecv" bytes="640" orank="40" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.3351e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000140000000028" call="MPI_Irecv" bytes="5120" orank="40" region="0" commid="0" count="923" tid="0" op="" dtype="" >4.9186e-04 0.0000e+00 2.8133e-05</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002B0" call="MPI_Irecv" bytes="640" orank="688" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.8835e-05 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000001400000002B0" call="MPI_Irecv" bytes="5120" orank="688" region="0" commid="0" count="884" tid="0" op="" dtype="" >7.2241e-04 0.0000e+00 6.1035e-05</hent>
<hent key="03800100000000000000A00000000028" call="MPI_Irecv" bytes="40960" orank="40" region="0" commid="0" count="3191" tid="0" op="" dtype="" >2.1691e-03 0.0000e+00 5.0783e-05</hent>
<hent key="0240010000000000000002800000000C" call="MPI_Isend" bytes="640" orank="12" region="0" commid="0" count="375" tid="0" op="" dtype="" >4.3440e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000028000000011" call="MPI_Isend" bytes="640" orank="17" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.7612e-03 2.8610e-06 1.8120e-05</hent>
<hent key="02400100000000000000028000000013" call="MPI_Isend" bytes="640" orank="19" region="0" commid="0" count="348" tid="0" op="" dtype="" >5.5194e-04 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000028000000014" call="MPI_Isend" bytes="640" orank="20" region="0" commid="0" count="415" tid="0" op="" dtype="" >5.4479e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000028000000028" call="MPI_Isend" bytes="640" orank="40" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.8096e-04 4.0531e-06 1.5020e-05</hent>
<hent key="03800100000000000000A000000002B0" call="MPI_Irecv" bytes="40960" orank="688" region="0" commid="0" count="2951" tid="0" op="" dtype="" >2.1057e-03 0.0000e+00 7.7009e-05</hent>
<hent key="02400100000000000000140000000028" call="MPI_Isend" bytes="5120" orank="40" region="0" commid="0" count="895" tid="0" op="" dtype="" >7.2169e-03 1.9073e-06 1.0204e-04</hent>
<hent key="024001000000000000000280000002B0" call="MPI_Isend" bytes="640" orank="688" region="0" commid="0" count="16" tid="0" op="" dtype="" >8.8453e-05 3.0994e-06 1.4067e-05</hent>
<hent key="03800100000000000000800000000028" call="MPI_Irecv" bytes="32768" orank="40" region="0" commid="0" count="9509" tid="0" op="" dtype="" >6.4838e-03 0.0000e+00 7.0810e-05</hent>
<hent key="024001000000000000001400000002B0" call="MPI_Isend" bytes="5120" orank="688" region="0" commid="0" count="952" tid="0" op="" dtype="" >5.1684e-03 9.5367e-07 5.6028e-05</hent>
<hent key="038001000000000000008000000002B0" call="MPI_Irecv" bytes="32768" orank="688" region="0" commid="0" count="9749" tid="0" op="" dtype="" >3.4185e-02 0.0000e+00 2.7438e-02</hent>
<hent key="0380010000000000000001400000000C" call="MPI_Irecv" bytes="320" orank="12" region="0" commid="0" count="438" tid="0" op="" dtype="" >7.5340e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000011" call="MPI_Irecv" bytes="320" orank="17" region="0" commid="0" count="551" tid="0" op="" dtype="" >1.5140e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000013" call="MPI_Irecv" bytes="320" orank="19" region="0" commid="0" count="599" tid="0" op="" dtype="" >5.1475e-04 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000014000000014" call="MPI_Irecv" bytes="320" orank="20" region="0" commid="0" count="460" tid="0" op="" dtype="" >1.0109e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000028" call="MPI_Irecv" bytes="320" orank="40" region="0" commid="0" count="17" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A00000000028" call="MPI_Isend" bytes="40960" orank="40" region="0" commid="0" count="2980" tid="0" op="" dtype="" >1.1979e-01 1.0967e-05 1.8311e-04</hent>
<hent key="038001000000000000000140000002B0" call="MPI_Irecv" bytes="320" orank="688" region="0" commid="0" count="25" tid="0" op="" dtype="" >7.3910e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A000000002B0" call="MPI_Isend" bytes="40960" orank="688" region="0" commid="0" count="3279" tid="0" op="" dtype="" >5.9487e-02 6.9141e-06 1.0800e-04</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.5974e-05 1.9073e-06 5.9605e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >6.1220e+00 0.0000e+00 1.1036e-01</hent>
<hent key="02400100000000000000800000000028" call="MPI_Isend" bytes="32768" orank="40" region="0" commid="0" count="9720" tid="0" op="" dtype="" >4.1534e-01 1.0014e-05 1.0792e-02</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.2888e-05 9.5367e-07 1.9073e-05</hent>
<hent key="024001000000000000008000000002B0" call="MPI_Isend" bytes="32768" orank="688" region="0" commid="0" count="9421" tid="0" op="" dtype="" >1.7216e-01 5.9605e-06 1.3518e-04</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0240010000000000000001400000000C" call="MPI_Isend" bytes="320" orank="12" region="0" commid="0" count="421" tid="0" op="" dtype="" >4.0293e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000011" call="MPI_Isend" bytes="320" orank="17" region="0" commid="0" count="546" tid="0" op="" dtype="" >2.4171e-03 1.9073e-06 1.2159e-05</hent>
<hent key="02400100000000000000014000000013" call="MPI_Isend" bytes="320" orank="19" region="0" commid="0" count="590" tid="0" op="" dtype="" >7.1549e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000014000000014" call="MPI_Isend" bytes="320" orank="20" region="0" commid="0" count="457" tid="0" op="" dtype="" >5.0974e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000014000000028" call="MPI_Isend" bytes="320" orank="40" region="0" commid="0" count="16" tid="0" op="" dtype="" >7.4148e-05 3.8147e-06 6.1989e-06</hent>
<hent key="024001000000000000000140000002B0" call="MPI_Isend" bytes="320" orank="688" region="0" commid="0" count="31" tid="0" op="" dtype="" >1.5450e-04 3.0994e-06 1.1921e-05</hent>
<hent key="038001000000000000002000000002B0" call="MPI_Irecv" bytes="8192" orank="688" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000000000000000C" call="MPI_Irecv" bytes="0" orank="12" region="0" commid="0" count="363" tid="0" op="" dtype="" >7.1287e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000011" call="MPI_Irecv" bytes="0" orank="17" region="0" commid="0" count="550" tid="0" op="" dtype="" >1.4257e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000013" call="MPI_Irecv" bytes="0" orank="19" region="0" commid="0" count="517" tid="0" op="" dtype="" >3.3593e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000000000000014" call="MPI_Irecv" bytes="0" orank="20" region="0" commid="0" count="341" tid="0" op="" dtype="" >8.7976e-05 0.0000e+00 1.1921e-05</hent>
<hent key="03800100000000000000000000000028" call="MPI_Irecv" bytes="0" orank="40" region="0" commid="0" count="87" tid="0" op="" dtype="" >3.0994e-05 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000000000002B0" call="MPI_Irecv" bytes="0" orank="688" region="0" commid="0" count="88" tid="0" op="" dtype="" >5.6982e-05 0.0000e+00 1.0014e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.1505e-04 0.0000e+00 7.7963e-05</hent>
<hent key="0380010000000000000003000000000C" call="MPI_Irecv" bytes="768" orank="12" region="0" commid="0" count="566" tid="0" op="" dtype="" >8.2493e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000030000000014" call="MPI_Irecv" bytes="768" orank="20" region="0" commid="0" count="846" tid="0" op="" dtype="" >1.4353e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000000000000C" call="MPI_Isend" bytes="0" orank="12" region="0" commid="0" count="364" tid="0" op="" dtype="" >2.4867e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000000000000011" call="MPI_Isend" bytes="0" orank="17" region="0" commid="0" count="537" tid="0" op="" dtype="" >1.4458e-03 9.5367e-07 7.0095e-05</hent>
<hent key="02400100000000000000000000000013" call="MPI_Isend" bytes="0" orank="19" region="0" commid="0" count="513" tid="0" op="" dtype="" >3.9697e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000014" call="MPI_Isend" bytes="0" orank="20" region="0" commid="0" count="335" tid="0" op="" dtype="" >2.7752e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000000000000028" call="MPI_Isend" bytes="0" orank="40" region="0" commid="0" count="86" tid="0" op="" dtype="" >4.4775e-04 1.9073e-06 1.9073e-05</hent>
<hent key="024001000000000000000000000002B0" call="MPI_Isend" bytes="0" orank="688" region="0" commid="0" count="82" tid="0" op="" dtype="" >3.6263e-04 2.8610e-06 1.2159e-05</hent>
<hent key="0380010000000000000006000000000C" call="MPI_Irecv" bytes="1536" orank="12" region="0" commid="0" count="56" tid="0" op="" dtype="" >1.3828e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000011" call="MPI_Irecv" bytes="1536" orank="17" region="0" commid="0" count="11" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000013" call="MPI_Irecv" bytes="1536" orank="19" region="0" commid="0" count="14" tid="0" op="" dtype="" >1.5020e-05 9.5367e-07 2.1458e-06</hent>
<hent key="03800100000000000000060000000014" call="MPI_Irecv" bytes="1536" orank="20" region="0" commid="0" count="61" tid="0" op="" dtype="" >1.5497e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000028" call="MPI_Irecv" bytes="1536" orank="40" region="0" commid="0" count="81" tid="0" op="" dtype="" >4.3154e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000600000002B0" call="MPI_Irecv" bytes="1536" orank="688" region="0" commid="0" count="83" tid="0" op="" dtype="" >4.1008e-05 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000003000000000C" call="MPI_Isend" bytes="768" orank="12" region="0" commid="0" count="772" tid="0" op="" dtype="" >4.3392e-04 0.0000e+00 3.9816e-05</hent>
<hent key="02400100000000000000030000000014" call="MPI_Isend" bytes="768" orank="20" region="0" commid="0" count="776" tid="0" op="" dtype="" >4.7350e-04 0.0000e+00 7.1526e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C000000011" call="MPI_Irecv" bytes="448" orank="17" region="0" commid="0" count="60" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000013" call="MPI_Irecv" bytes="448" orank="19" region="0" commid="0" count="50" tid="0" op="" dtype="" >9.5367e-06 0.0000e+00 3.0994e-06</hent>
<hent key="0240010000000000000006000000000C" call="MPI_Isend" bytes="1536" orank="12" region="0" commid="0" count="54" tid="0" op="" dtype="" >8.2493e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000011" call="MPI_Isend" bytes="1536" orank="17" region="0" commid="0" count="17" tid="0" op="" dtype="" >9.2983e-05 4.0531e-06 7.1526e-06</hent>
<hent key="02400100000000000000060000000013" call="MPI_Isend" bytes="1536" orank="19" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.3603e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000060000000014" call="MPI_Isend" bytes="1536" orank="20" region="0" commid="0" count="50" tid="0" op="" dtype="" >8.8453e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000028" call="MPI_Isend" bytes="1536" orank="40" region="0" commid="0" count="61" tid="0" op="" dtype="" >4.7493e-04 4.7684e-06 2.5034e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002B0" call="MPI_Isend" bytes="1536" orank="688" region="0" commid="0" count="84" tid="0" op="" dtype="" >4.3392e-04 3.8147e-06 1.4067e-05</hent>
<hent key="038001000000000000000C0000000028" call="MPI_Irecv" bytes="3072" orank="40" region="0" commid="0" count="371" tid="0" op="" dtype="" >2.7823e-04 0.0000e+00 1.3113e-05</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002B0" call="MPI_Irecv" bytes="3072" orank="688" region="0" commid="0" count="340" tid="0" op="" dtype="" >1.6785e-04 0.0000e+00 1.2875e-05</hent>
<hent key="0240010000000000000001C000000011" call="MPI_Isend" bytes="448" orank="17" region="0" commid="0" count="32" tid="0" op="" dtype="" >3.1471e-05 0.0000e+00 8.8215e-06</hent>
<hent key="0240010000000000000001C000000013" call="MPI_Isend" bytes="448" orank="19" region="0" commid="0" count="16" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000028" call="MPI_Isend" bytes="3072" orank="40" region="0" commid="0" count="355" tid="0" op="" dtype="" >2.9175e-03 5.0068e-06 5.6982e-05</hent>
<hent key="024001000000000000000C00000002B0" call="MPI_Isend" bytes="3072" orank="688" region="0" commid="0" count="363" tid="0" op="" dtype="" >2.1698e-03 3.8147e-06 1.5020e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >8.0705e-04 2.6703e-04 2.7204e-04</hent>
<hent key="0380010000000000000003800000000C" call="MPI_Irecv" bytes="896" orank="12" region="0" commid="0" count="3105" tid="0" op="" dtype="" >4.8351e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000038000000011" call="MPI_Irecv" bytes="896" orank="17" region="0" commid="0" count="145" tid="0" op="" dtype="" >3.8862e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000013" call="MPI_Irecv" bytes="896" orank="19" region="0" commid="0" count="174" tid="0" op="" dtype="" >1.4329e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000038000000014" call="MPI_Irecv" bytes="896" orank="20" region="0" commid="0" count="2843" tid="0" op="" dtype="" >6.1965e-04 0.0000e+00 1.8120e-05</hent>
<hent key="03800100000000000000038000000028" call="MPI_Irecv" bytes="896" orank="40" region="0" commid="0" count="17" tid="0" op="" dtype="" >1.4067e-05 0.0000e+00 7.1526e-06</hent>
<hent key="038001000000000000000380000002B0" call="MPI_Irecv" bytes="896" orank="688" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.5736e-05 0.0000e+00 8.1062e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1195e-04 2.1195e-04 2.1195e-04</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0380010000000000000018000000000C" call="MPI_Irecv" bytes="6144" orank="12" region="0" commid="0" count="1969" tid="0" op="" dtype="" >3.4237e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000180000000014" call="MPI_Irecv" bytes="6144" orank="20" region="0" commid="0" count="3126" tid="0" op="" dtype="" >3.8218e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000180000000028" call="MPI_Irecv" bytes="6144" orank="40" region="0" commid="0" count="10" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000001800000002B0" call="MPI_Irecv" bytes="6144" orank="688" region="0" commid="0" count="11" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000003800000000C" call="MPI_Isend" bytes="896" orank="12" region="0" commid="0" count="2933" tid="0" op="" dtype="" >2.4278e-03 0.0000e+00 1.0586e-04</hent>
<hent key="02400100000000000000038000000011" call="MPI_Isend" bytes="896" orank="17" region="0" commid="0" count="179" tid="0" op="" dtype="" >9.0742e-04 3.0994e-06 9.7752e-06</hent>
<hent key="02400100000000000000038000000013" call="MPI_Isend" bytes="896" orank="19" region="0" commid="0" count="173" tid="0" op="" dtype="" >3.3212e-04 9.5367e-07 1.2875e-05</hent>
<hent key="02400100000000000000038000000014" call="MPI_Isend" bytes="896" orank="20" region="0" commid="0" count="2911" tid="0" op="" dtype="" >1.9753e-03 0.0000e+00 3.0041e-05</hent>
<hent key="02400100000000000000038000000028" call="MPI_Isend" bytes="896" orank="40" region="0" commid="0" count="32" tid="0" op="" dtype="" >2.3317e-04 3.8147e-06 1.9789e-05</hent>
<hent key="024001000000000000000380000002B0" call="MPI_Isend" bytes="896" orank="688" region="0" commid="0" count="22" tid="0" op="" dtype="" >9.9182e-05 3.0994e-06 7.8678e-06</hent>
<hent key="0240010000000000000018000000000C" call="MPI_Isend" bytes="6144" orank="12" region="0" commid="0" count="2806" tid="0" op="" dtype="" >5.3525e-03 9.5367e-07 2.0027e-05</hent>
<hent key="02400100000000000000180000000014" call="MPI_Isend" bytes="6144" orank="20" region="0" commid="0" count="2895" tid="0" op="" dtype="" >5.7428e-03 9.5367e-07 7.2956e-05</hent>
<hent key="02400100000000000000180000000028" call="MPI_Isend" bytes="6144" orank="40" region="0" commid="0" count="9" tid="0" op="" dtype="" >8.2016e-05 6.9141e-06 1.5020e-05</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.6928e-05 1.6928e-05 1.6928e-05</hent>
<hent key="024001000000000000001800000002B0" call="MPI_Isend" bytes="6144" orank="688" region="0" commid="0" count="7" tid="0" op="" dtype="" >5.8174e-05 5.9605e-06 1.3113e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.7628e+00 1.0967e-05 1.2661e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.7595e-04 8.7595e-04 8.7595e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0596e-02 1.0596e-02 1.0596e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >4.7209e-03 4.7209e-03 4.7209e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.7990e-01 3.3090e-03 1.7208e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.7599e-04 3.7599e-04 3.7599e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5754e+00 4.4799e-04 2.4752e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >6.5179e-02 0.0000e+00 6.4633e-02</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.1989e-06 6.1989e-06 6.1989e-06</hent>
<hent key="0380010000000000000007000000000C" call="MPI_Irecv" bytes="1792" orank="12" region="0" commid="0" count="20" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000011" call="MPI_Irecv" bytes="1792" orank="17" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000070000000013" call="MPI_Irecv" bytes="1792" orank="19" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000014" call="MPI_Irecv" bytes="1792" orank="20" region="0" commid="0" count="19" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000028" call="MPI_Irecv" bytes="1792" orank="40" region="0" commid="0" count="117" tid="0" op="" dtype="" >7.3671e-05 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000700000002B0" call="MPI_Irecv" bytes="1792" orank="688" region="0" commid="0" count="131" tid="0" op="" dtype="" >6.9857e-05 0.0000e+00 1.7166e-05</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >8.9169e-05 0.0000e+00 6.6996e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9718e+01 0.0000e+00 3.8353e+01</hent>
<hent key="038001000000000000000A000000000C" call="MPI_Irecv" bytes="2560" orank="12" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000014" call="MPI_Irecv" bytes="2560" orank="20" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000028" call="MPI_Irecv" bytes="2560" orank="40" region="0" commid="0" count="363" tid="0" op="" dtype="" >2.4319e-04 0.0000e+00 1.1921e-05</hent>
<hent key="038001000000000000000A00000002B0" call="MPI_Irecv" bytes="2560" orank="688" region="0" commid="0" count="334" tid="0" op="" dtype="" >9.6798e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000007000000000C" call="MPI_Isend" bytes="1792" orank="12" region="0" commid="0" count="14" tid="0" op="" dtype="" >2.0742e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000011" call="MPI_Isend" bytes="1792" orank="17" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.0014e-05 4.0531e-06 5.9605e-06</hent>
<hent key="02400100000000000000070000000013" call="MPI_Isend" bytes="1792" orank="19" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.2159e-05 1.9073e-06 2.1458e-06</hent>
<hent key="02400100000000000000070000000014" call="MPI_Isend" bytes="1792" orank="20" region="0" commid="0" count="17" tid="0" op="" dtype="" >4.3154e-05 1.9073e-06 1.0014e-05</hent>
<hent key="02400100000000000000070000000028" call="MPI_Isend" bytes="1792" orank="40" region="0" commid="0" count="128" tid="0" op="" dtype="" >9.4104e-04 4.7684e-06 2.0027e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3134e-02 8.3134e-02 8.3134e-02</hent>
<hent key="024001000000000000000700000002B0" call="MPI_Isend" bytes="1792" orank="688" region="0" commid="0" count="111" tid="0" op="" dtype="" >6.2895e-04 3.8147e-06 3.3140e-05</hent>
<hent key="024001000000000000000A000000000C" call="MPI_Isend" bytes="2560" orank="12" region="0" commid="0" count="3" tid="0" op="" dtype="" >4.7684e-06 9.5367e-07 1.9073e-06</hent>
<hent key="024001000000000000000A0000000014" call="MPI_Isend" bytes="2560" orank="20" region="0" commid="0" count="2" tid="0" op="" dtype="" >5.0068e-06 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000028" call="MPI_Isend" bytes="2560" orank="40" region="0" commid="0" count="393" tid="0" op="" dtype="" >2.8930e-03 4.7684e-06 2.5988e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.8218e-04 3.9101e-05 1.7309e-04</hent>
<hent key="024001000000000000000A00000002B0" call="MPI_Isend" bytes="2560" orank="688" region="0" commid="0" count="348" tid="0" op="" dtype="" >1.9426e-03 3.8147e-06 5.5790e-05</hent>
<hent key="03800100000000000000100000000011" call="MPI_Irecv" bytes="4096" orank="17" region="0" commid="0" count="12492" tid="0" op="" dtype="" >2.6147e-03 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000100000000013" call="MPI_Irecv" bytes="4096" orank="19" region="0" commid="0" count="12529" tid="0" op="" dtype="" >4.2574e-03 0.0000e+00 2.0027e-05</hent>
<hent key="03800100000000000000100000000028" call="MPI_Irecv" bytes="4096" orank="40" region="0" commid="0" count="2757" tid="0" op="" dtype="" >1.3287e-03 0.0000e+00 4.6968e-05</hent>
<hent key="038001000000000000001000000002B0" call="MPI_Irecv" bytes="4096" orank="688" region="0" commid="0" count="2843" tid="0" op="" dtype="" >1.6685e-03 0.0000e+00 8.0824e-05</hent>
<hent key="02400100000000000000100000000011" call="MPI_Isend" bytes="4096" orank="17" region="0" commid="0" count="12598" tid="0" op="" dtype="" >3.5253e-02 0.0000e+00 3.6001e-05</hent>
<hent key="02400100000000000000100000000013" call="MPI_Isend" bytes="4096" orank="19" region="0" commid="0" count="12635" tid="0" op="" dtype="" >2.1875e-02 0.0000e+00 1.2875e-05</hent>
<hent key="02400100000000000000100000000028" call="MPI_Isend" bytes="4096" orank="40" region="0" commid="0" count="2779" tid="0" op="" dtype="" >2.3935e-02 1.9073e-06 1.2994e-04</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.2891e-03 4.6802e-04 8.2111e-04</hent>
<hent key="024001000000000000001000000002B0" call="MPI_Isend" bytes="4096" orank="688" region="0" commid="0" count="2714" tid="0" op="" dtype="" >1.4396e-02 9.5367e-07 9.1076e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.7909e-03 3.8099e-04 9.5201e-04</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.0014e-05 4.0531e-06 5.9605e-06</hent>
<hent key="0380010000000000000000040000000C" call="MPI_Irecv" bytes="4" orank="12" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.4751e-04 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000000400000011" call="MPI_Irecv" bytes="4" orank="17" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.1522e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000400000013" call="MPI_Irecv" bytes="4" orank="19" region="0" commid="0" count="3400" tid="0" op="" dtype="" >9.5963e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000000400000014" call="MPI_Irecv" bytes="4" orank="20" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.7541e-04 0.0000e+00 1.5020e-05</hent>
<hent key="03800100000000000000000400000028" call="MPI_Irecv" bytes="4" orank="40" region="0" commid="0" count="3400" tid="0" op="" dtype="" >9.9373e-04 0.0000e+00 2.7895e-05</hent>
<hent key="038001000000000000001C000000000C" call="MPI_Irecv" bytes="7168" orank="12" region="0" commid="0" count="10731" tid="0" op="" dtype="" >1.8597e-03 0.0000e+00 2.0027e-05</hent>
<hent key="038001000000000000001C0000000014" call="MPI_Irecv" bytes="7168" orank="20" region="0" commid="0" count="9574" tid="0" op="" dtype="" >1.4555e-03 0.0000e+00 6.0081e-05</hent>
<hent key="038001000000000000000004000002B0" call="MPI_Irecv" bytes="4" orank="688" region="0" commid="0" count="3400" tid="0" op="" dtype="" >9.2936e-04 0.0000e+00 3.6955e-05</hent>
<hent key="0240010000000000000000040000000C" call="MPI_Isend" bytes="4" orank="12" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.7560e-03 0.0000e+00 2.5988e-05</hent>
<hent key="02400100000000000000000400000011" call="MPI_Isend" bytes="4" orank="17" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.6380e-03 0.0000e+00 2.4080e-05</hent>
<hent key="02400100000000000000000400000013" call="MPI_Isend" bytes="4" orank="19" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.4153e-03 0.0000e+00 4.7922e-05</hent>
<hent key="02400100000000000000000400000014" call="MPI_Isend" bytes="4" orank="20" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.2772e-03 0.0000e+00 1.5974e-05</hent>
<hent key="02400100000000000000000400000028" call="MPI_Isend" bytes="4" orank="40" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.6466e-02 4.7684e-06 3.7909e-04</hent>
<hent key="024001000000000000001C000000000C" call="MPI_Isend" bytes="7168" orank="12" region="0" commid="0" count="9894" tid="0" op="" dtype="" >1.9619e-02 9.5367e-07 1.0300e-04</hent>
<hent key="024001000000000000001C0000000014" call="MPI_Isend" bytes="7168" orank="20" region="0" commid="0" count="9805" tid="0" op="" dtype="" >1.9861e-02 9.5367e-07 1.2088e-04</hent>
<hent key="024001000000000000000004000002B0" call="MPI_Isend" bytes="4" orank="688" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.5435e-02 3.8147e-06 9.8944e-05</hent>
<hent key="024001000000000000001C00000002B0" call="MPI_Isend" bytes="7168" orank="688" region="0" commid="0" count="1" tid="0" op="" dtype="" >7.1526e-06 7.1526e-06 7.1526e-06</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.9196e-04 3.9196e-04 3.9196e-04</hent>
<hent key="03800100000000000000020000000011" call="MPI_Irecv" bytes="512" orank="17" region="0" commid="0" count="3340" tid="0" op="" dtype="" >7.1931e-04 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000020000000013" call="MPI_Irecv" bytes="512" orank="19" region="0" commid="0" count="3350" tid="0" op="" dtype="" >5.4216e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0380010000000000000005000000000C" call="MPI_Irecv" bytes="1280" orank="12" region="0" commid="0" count="113" tid="0" op="" dtype="" >3.2663e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000011" call="MPI_Irecv" bytes="1280" orank="17" region="0" commid="0" count="61" tid="0" op="" dtype="" >1.5259e-05 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000050000000013" call="MPI_Irecv" bytes="1280" orank="19" region="0" commid="0" count="55" tid="0" op="" dtype="" >4.6015e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000050000000014" call="MPI_Irecv" bytes="1280" orank="20" region="0" commid="0" count="135" tid="0" op="" dtype="" >2.3127e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000028" call="MPI_Irecv" bytes="1280" orank="40" region="0" commid="0" count="49" tid="0" op="" dtype="" >3.1233e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4234e+01 1.0967e-05 1.8486e-01</hent>
<hent key="038001000000000000000500000002B0" call="MPI_Irecv" bytes="1280" orank="688" region="0" commid="0" count="41" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000020000000011" call="MPI_Isend" bytes="512" orank="17" region="0" commid="0" count="3368" tid="0" op="" dtype="" >2.4722e-03 0.0000e+00 2.1935e-05</hent>
<hent key="02400100000000000000020000000013" call="MPI_Isend" bytes="512" orank="19" region="0" commid="0" count="3384" tid="0" op="" dtype="" >1.9562e-03 0.0000e+00 1.3113e-05</hent>
<hent key="0380010000000000000008000000000C" call="MPI_Irecv" bytes="2048" orank="12" region="0" commid="0" count="11" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000080000000013" call="MPI_Irecv" bytes="2048" orank="19" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000080000000014" call="MPI_Irecv" bytes="2048" orank="20" region="0" commid="0" count="8" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000028" call="MPI_Irecv" bytes="2048" orank="40" region="0" commid="0" count="153" tid="0" op="" dtype="" >1.1206e-04 0.0000e+00 7.8678e-06</hent>
<hent key="038001000000000000000800000002B0" call="MPI_Irecv" bytes="2048" orank="688" region="0" commid="0" count="140" tid="0" op="" dtype="" >4.9591e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000005000000000C" call="MPI_Isend" bytes="1280" orank="12" region="0" commid="0" count="158" tid="0" op="" dtype="" >2.4676e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000050000000011" call="MPI_Isend" bytes="1280" orank="17" region="0" commid="0" count="52" tid="0" op="" dtype="" >2.8419e-04 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000050000000013" call="MPI_Isend" bytes="1280" orank="19" region="0" commid="0" count="59" tid="0" op="" dtype="" >1.3494e-04 9.5367e-07 2.0027e-05</hent>
<hent key="02400100000000000000050000000014" call="MPI_Isend" bytes="1280" orank="20" region="0" commid="0" count="132" tid="0" op="" dtype="" >2.2316e-04 9.5367e-07 1.0014e-05</hent>
<hent key="02400100000000000000050000000028" call="MPI_Isend" bytes="1280" orank="40" region="0" commid="0" count="38" tid="0" op="" dtype="" >2.1625e-04 4.0531e-06 1.2159e-05</hent>
<hent key="024001000000000000000500000002B0" call="MPI_Isend" bytes="1280" orank="688" region="0" commid="0" count="41" tid="0" op="" dtype="" >2.1529e-04 3.8147e-06 1.5974e-05</hent>
</hash>
<internal rank="16" log_i="1724765674.538598" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="17" mpi_size="696" stamp_init="1724765564.465364" stamp_final="1724765674.529922" username="apac4" allocationname="unknown" flags="0" pid="1612134" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10065e+02" utime="8.95020e+01" stime="1.40368e+01" mtime="7.26739e+01" gflop="0.00000e+00" gbyte="3.77205e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.26739e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000011141114c3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09927e+02" utime="8.94720e+01" stime="1.40236e+01" mtime="7.26739e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.26739e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2029e+09" > 6.4475e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2078e+09" > 2.4337e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2866e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9780e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1948e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.8876e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5753e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.0503e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0575e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3079e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4263e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="206" >
<hent key="0240010000000000000008000000000D" call="MPI_Isend" bytes="2048" orank="13" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.5259e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000015" call="MPI_Isend" bytes="2048" orank="21" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.1206e-05 1.1921e-06 2.1458e-06</hent>
<hent key="02400100000000000000080000000029" call="MPI_Isend" bytes="2048" orank="41" region="0" commid="0" count="150" tid="0" op="" dtype="" >8.3709e-04 4.0531e-06 1.1921e-05</hent>
<hent key="024001000000000000000800000002B1" call="MPI_Isend" bytes="2048" orank="689" region="0" commid="0" count="137" tid="0" op="" dtype="" >6.3729e-04 3.8147e-06 6.1989e-06</hent>
<hent key="038001000000000000000E0000000010" call="MPI_Irecv" bytes="3584" orank="16" region="0" commid="0" count="102" tid="0" op="" dtype="" >3.9577e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000012" call="MPI_Irecv" bytes="3584" orank="18" region="0" commid="0" count="197" tid="0" op="" dtype="" >1.8597e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000029" call="MPI_Irecv" bytes="3584" orank="41" region="0" commid="0" count="140" tid="0" op="" dtype="" >3.2187e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E00000002B1" call="MPI_Irecv" bytes="3584" orank="689" region="0" commid="0" count="146" tid="0" op="" dtype="" >4.6015e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="024001000000000000000E0000000010" call="MPI_Isend" bytes="3584" orank="16" region="0" commid="0" count="208" tid="0" op="" dtype="" >3.5667e-04 0.0000e+00 3.0994e-06</hent>
<hent key="024001000000000000000E0000000012" call="MPI_Isend" bytes="3584" orank="18" region="0" commid="0" count="103" tid="0" op="" dtype="" >2.1839e-04 9.5367e-07 5.0068e-06</hent>
<hent key="024001000000000000000E0000000029" call="MPI_Isend" bytes="3584" orank="41" region="0" commid="0" count="143" tid="0" op="" dtype="" >9.4056e-04 5.0068e-06 1.2875e-05</hent>
<hent key="024001000000000000000E00000002B1" call="MPI_Isend" bytes="3584" orank="689" region="0" commid="0" count="145" tid="0" op="" dtype="" >7.7200e-04 3.8147e-06 1.0014e-05</hent>
<hent key="0380010000000000000002800000000D" call="MPI_Irecv" bytes="640" orank="13" region="0" commid="0" count="422" tid="0" op="" dtype="" >7.9155e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000010" call="MPI_Irecv" bytes="640" orank="16" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.4329e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000012" call="MPI_Irecv" bytes="640" orank="18" region="0" commid="0" count="349" tid="0" op="" dtype="" >7.5817e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000015" call="MPI_Irecv" bytes="640" orank="21" region="0" commid="0" count="428" tid="0" op="" dtype="" >9.1076e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000029" call="MPI_Irecv" bytes="640" orank="41" region="0" commid="0" count="30" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000140000000029" call="MPI_Irecv" bytes="5120" orank="41" region="0" commid="0" count="678" tid="0" op="" dtype="" >1.1349e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000000280000002B1" call="MPI_Irecv" bytes="640" orank="689" region="0" commid="0" count="15" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001400000002B1" call="MPI_Irecv" bytes="5120" orank="689" region="0" commid="0" count="808" tid="0" op="" dtype="" >1.5354e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000A00000000029" call="MPI_Irecv" bytes="40960" orank="41" region="0" commid="0" count="2342" tid="0" op="" dtype="" >5.1856e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0240010000000000000002800000000D" call="MPI_Isend" bytes="640" orank="13" region="0" commid="0" count="430" tid="0" op="" dtype="" >4.8637e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000028000000010" call="MPI_Isend" bytes="640" orank="16" region="0" commid="0" count="379" tid="0" op="" dtype="" >5.6243e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000028000000012" call="MPI_Isend" bytes="640" orank="18" region="0" commid="0" count="336" tid="0" op="" dtype="" >9.3317e-04 1.9073e-06 8.1062e-06</hent>
<hent key="02400100000000000000028000000015" call="MPI_Isend" bytes="640" orank="21" region="0" commid="0" count="442" tid="0" op="" dtype="" >5.2834e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000028000000029" call="MPI_Isend" bytes="640" orank="41" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.1897e-04 3.8147e-06 5.9605e-06</hent>
<hent key="03800100000000000000A000000002B1" call="MPI_Irecv" bytes="40960" orank="689" region="0" commid="0" count="2877" tid="0" op="" dtype="" >5.3334e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000140000000029" call="MPI_Isend" bytes="5120" orank="41" region="0" commid="0" count="662" tid="0" op="" dtype="" >2.6345e-03 9.5367e-07 2.5034e-05</hent>
<hent key="024001000000000000000280000002B1" call="MPI_Isend" bytes="640" orank="689" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.1325e-04 3.0994e-06 5.0068e-06</hent>
<hent key="03800100000000000000800000000029" call="MPI_Irecv" bytes="32768" orank="41" region="0" commid="0" count="10358" tid="0" op="" dtype="" >2.2240e-03 0.0000e+00 1.0014e-05</hent>
<hent key="024001000000000000001400000002B1" call="MPI_Isend" bytes="5120" orank="689" region="0" commid="0" count="729" tid="0" op="" dtype="" >2.6219e-03 9.5367e-07 1.8120e-05</hent>
<hent key="038001000000000000008000000002B1" call="MPI_Irecv" bytes="32768" orank="689" region="0" commid="0" count="9823" tid="0" op="" dtype="" >1.8408e-03 0.0000e+00 1.0014e-05</hent>
<hent key="0380010000000000000001400000000D" call="MPI_Irecv" bytes="320" orank="13" region="0" commid="0" count="462" tid="0" op="" dtype="" >8.0347e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000010" call="MPI_Irecv" bytes="320" orank="16" region="0" commid="0" count="546" tid="0" op="" dtype="" >2.2888e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000012" call="MPI_Irecv" bytes="320" orank="18" region="0" commid="0" count="583" tid="0" op="" dtype="" >1.4782e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000015" call="MPI_Irecv" bytes="320" orank="21" region="0" commid="0" count="429" tid="0" op="" dtype="" >8.7738e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000029" call="MPI_Irecv" bytes="320" orank="41" region="0" commid="0" count="18" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000A00000000029" call="MPI_Isend" bytes="40960" orank="41" region="0" commid="0" count="2298" tid="0" op="" dtype="" >5.2455e-02 1.0967e-05 6.1989e-05</hent>
<hent key="038001000000000000000140000002B1" call="MPI_Irecv" bytes="320" orank="689" region="0" commid="0" count="29" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A000000002B1" call="MPI_Isend" bytes="40960" orank="689" region="0" commid="0" count="2539" tid="0" op="" dtype="" >2.8283e-02 6.9141e-06 2.0027e-05</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >6.2866e+00 0.0000e+00 1.1083e-01</hent>
<hent key="02400100000000000000800000000029" call="MPI_Isend" bytes="32768" orank="41" region="0" commid="0" count="10402" tid="0" op="" dtype="" >2.3635e-01 1.0014e-05 5.9843e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.0027e-05 0.0000e+00 1.9073e-05</hent>
<hent key="024001000000000000008000000002B1" call="MPI_Isend" bytes="32768" orank="689" region="0" commid="0" count="10161" tid="0" op="" dtype="" >1.1230e-01 6.9141e-06 4.1008e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 0.0000e+00 1.9073e-06</hent>
<hent key="0240010000000000000001400000000D" call="MPI_Isend" bytes="320" orank="13" region="0" commid="0" count="449" tid="0" op="" dtype="" >4.0722e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000010" call="MPI_Isend" bytes="320" orank="16" region="0" commid="0" count="551" tid="0" op="" dtype="" >6.2156e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000012" call="MPI_Isend" bytes="320" orank="18" region="0" commid="0" count="566" tid="0" op="" dtype="" >1.4062e-03 1.9073e-06 1.4067e-05</hent>
<hent key="02400100000000000000014000000015" call="MPI_Isend" bytes="320" orank="21" region="0" commid="0" count="430" tid="0" op="" dtype="" >4.2462e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000014000000029" call="MPI_Isend" bytes="320" orank="41" region="0" commid="0" count="24" tid="0" op="" dtype="" >1.0395e-04 3.8147e-06 5.0068e-06</hent>
<hent key="024001000000000000000140000002B1" call="MPI_Isend" bytes="320" orank="689" region="0" commid="0" count="21" tid="0" op="" dtype="" >8.8453e-05 3.0994e-06 5.0068e-06</hent>
<hent key="0380010000000000000000000000000D" call="MPI_Irecv" bytes="0" orank="13" region="0" commid="0" count="324" tid="0" op="" dtype="" >7.1526e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000010" call="MPI_Irecv" bytes="0" orank="16" region="0" commid="0" count="537" tid="0" op="" dtype="" >1.8048e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000000000000012" call="MPI_Irecv" bytes="0" orank="18" region="0" commid="0" count="554" tid="0" op="" dtype="" >1.3351e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000015" call="MPI_Irecv" bytes="0" orank="21" region="0" commid="0" count="329" tid="0" op="" dtype="" >6.8426e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000029" call="MPI_Irecv" bytes="0" orank="41" region="0" commid="0" count="82" tid="0" op="" dtype="" >2.2650e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002B1" call="MPI_Irecv" bytes="0" orank="689" region="0" commid="0" count="82" tid="0" op="" dtype="" >1.0729e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.1911e-04 0.0000e+00 8.2970e-05</hent>
<hent key="0380010000000000000003000000000D" call="MPI_Irecv" bytes="768" orank="13" region="0" commid="0" count="520" tid="0" op="" dtype="" >7.3433e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000030000000015" call="MPI_Irecv" bytes="768" orank="21" region="0" commid="0" count="608" tid="0" op="" dtype="" >6.7949e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000000000000000D" call="MPI_Isend" bytes="0" orank="13" region="0" commid="0" count="335" tid="0" op="" dtype="" >2.2531e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000000000000010" call="MPI_Isend" bytes="0" orank="16" region="0" commid="0" count="550" tid="0" op="" dtype="" >4.1962e-04 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000000000000012" call="MPI_Isend" bytes="0" orank="18" region="0" commid="0" count="567" tid="0" op="" dtype="" >1.0347e-03 0.0000e+00 5.1975e-05</hent>
<hent key="02400100000000000000000000000015" call="MPI_Isend" bytes="0" orank="21" region="0" commid="0" count="324" tid="0" op="" dtype="" >2.3389e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000000000000029" call="MPI_Isend" bytes="0" orank="41" region="0" commid="0" count="82" tid="0" op="" dtype="" >3.2640e-04 9.5367e-07 8.1062e-06</hent>
<hent key="024001000000000000000000000002B1" call="MPI_Isend" bytes="0" orank="689" region="0" commid="0" count="87" tid="0" op="" dtype="" >3.2401e-04 9.5367e-07 5.0068e-06</hent>
<hent key="0380010000000000000006000000000D" call="MPI_Irecv" bytes="1536" orank="13" region="0" commid="0" count="46" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000060000000010" call="MPI_Irecv" bytes="1536" orank="16" region="0" commid="0" count="17" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000012" call="MPI_Irecv" bytes="1536" orank="18" region="0" commid="0" count="10" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000060000000015" call="MPI_Irecv" bytes="1536" orank="21" region="0" commid="0" count="65" tid="0" op="" dtype="" >1.2875e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000029" call="MPI_Irecv" bytes="1536" orank="41" region="0" commid="0" count="79" tid="0" op="" dtype="" >2.1219e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002B1" call="MPI_Irecv" bytes="1536" orank="689" region="0" commid="0" count="82" tid="0" op="" dtype="" >3.8147e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000003000000000D" call="MPI_Isend" bytes="768" orank="13" region="0" commid="0" count="792" tid="0" op="" dtype="" >3.4523e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000030000000015" call="MPI_Isend" bytes="768" orank="21" region="0" commid="0" count="744" tid="0" op="" dtype="" >3.8266e-04 0.0000e+00 7.8678e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 3.0994e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000010" call="MPI_Irecv" bytes="448" orank="16" region="0" commid="0" count="32" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C000000012" call="MPI_Irecv" bytes="448" orank="18" region="0" commid="0" count="54" tid="0" op="" dtype="" >6.4373e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000006000000000D" call="MPI_Isend" bytes="1536" orank="13" region="0" commid="0" count="47" tid="0" op="" dtype="" >8.6308e-05 9.5367e-07 1.0014e-05</hent>
<hent key="02400100000000000000060000000010" call="MPI_Isend" bytes="1536" orank="16" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.2888e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000060000000012" call="MPI_Isend" bytes="1536" orank="18" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.0518e-05 2.8610e-06 4.0531e-06</hent>
<hent key="02400100000000000000060000000015" call="MPI_Isend" bytes="1536" orank="21" region="0" commid="0" count="58" tid="0" op="" dtype="" >9.6321e-05 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000060000000029" call="MPI_Isend" bytes="1536" orank="41" region="0" commid="0" count="84" tid="0" op="" dtype="" >4.6182e-04 4.0531e-06 1.3113e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002B1" call="MPI_Isend" bytes="1536" orank="689" region="0" commid="0" count="73" tid="0" op="" dtype="" >3.2473e-04 3.8147e-06 5.9605e-06</hent>
<hent key="038001000000000000000C0000000029" call="MPI_Irecv" bytes="3072" orank="41" region="0" commid="0" count="361" tid="0" op="" dtype="" >7.9870e-05 0.0000e+00 1.9073e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002B1" call="MPI_Irecv" bytes="3072" orank="689" region="0" commid="0" count="361" tid="0" op="" dtype="" >1.2827e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C000000010" call="MPI_Isend" bytes="448" orank="16" region="0" commid="0" count="60" tid="0" op="" dtype="" >2.4319e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C000000012" call="MPI_Isend" bytes="448" orank="18" region="0" commid="0" count="30" tid="0" op="" dtype="" >2.0504e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000029" call="MPI_Isend" bytes="3072" orank="41" region="0" commid="0" count="359" tid="0" op="" dtype="" >2.2597e-03 5.0068e-06 1.2875e-05</hent>
<hent key="024001000000000000000C00000002B1" call="MPI_Isend" bytes="3072" orank="689" region="0" commid="0" count="318" tid="0" op="" dtype="" >1.6227e-03 3.8147e-06 1.0014e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >8.5783e-04 2.8396e-04 2.8801e-04</hent>
<hent key="0380010000000000000003800000000D" call="MPI_Irecv" bytes="896" orank="13" region="0" commid="0" count="3165" tid="0" op="" dtype="" >4.7565e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000010" call="MPI_Irecv" bytes="896" orank="16" region="0" commid="0" count="179" tid="0" op="" dtype="" >7.7248e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000012" call="MPI_Irecv" bytes="896" orank="18" region="0" commid="0" count="154" tid="0" op="" dtype="" >3.7670e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000015" call="MPI_Irecv" bytes="896" orank="21" region="0" commid="0" count="3065" tid="0" op="" dtype="" >4.4990e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000029" call="MPI_Irecv" bytes="896" orank="41" region="0" commid="0" count="30" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002B1" call="MPI_Irecv" bytes="896" orank="689" region="0" commid="0" count="34" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.2984e-04 2.2984e-04 2.2984e-04</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="0380010000000000000018000000000D" call="MPI_Irecv" bytes="6144" orank="13" region="0" commid="0" count="1884" tid="0" op="" dtype="" >2.4033e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000180000000015" call="MPI_Irecv" bytes="6144" orank="21" region="0" commid="0" count="2304" tid="0" op="" dtype="" >3.1734e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000180000000029" call="MPI_Irecv" bytes="6144" orank="41" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001800000002B1" call="MPI_Irecv" bytes="6144" orank="689" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000003800000000D" call="MPI_Isend" bytes="896" orank="13" region="0" commid="0" count="2878" tid="0" op="" dtype="" >1.5223e-03 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000038000000010" call="MPI_Isend" bytes="896" orank="16" region="0" commid="0" count="145" tid="0" op="" dtype="" >2.5749e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000038000000012" call="MPI_Isend" bytes="896" orank="18" region="0" commid="0" count="154" tid="0" op="" dtype="" >4.8041e-04 1.9073e-06 9.0599e-06</hent>
<hent key="02400100000000000000038000000015" call="MPI_Isend" bytes="896" orank="21" region="0" commid="0" count="2916" tid="0" op="" dtype="" >1.6220e-03 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000038000000029" call="MPI_Isend" bytes="896" orank="41" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.2207e-04 3.8147e-06 6.1989e-06</hent>
<hent key="024001000000000000000380000002B1" call="MPI_Isend" bytes="896" orank="689" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.3757e-04 3.0994e-06 5.0068e-06</hent>
<hent key="0240010000000000000018000000000D" call="MPI_Isend" bytes="6144" orank="13" region="0" commid="0" count="2994" tid="0" op="" dtype="" >5.7511e-03 9.5367e-07 1.7166e-05</hent>
<hent key="02400100000000000000180000000015" call="MPI_Isend" bytes="6144" orank="21" region="0" commid="0" count="2745" tid="0" op="" dtype="" >5.7297e-03 9.5367e-07 8.8215e-06</hent>
<hent key="02400100000000000000180000000029" call="MPI_Isend" bytes="6144" orank="41" region="0" commid="0" count="6" tid="0" op="" dtype="" >4.1246e-05 5.9605e-06 8.1062e-06</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.5988e-05 2.5988e-05 2.5988e-05</hent>
<hent key="024001000000000000001800000002B1" call="MPI_Isend" bytes="6144" orank="689" region="0" commid="0" count="13" tid="0" op="" dtype="" >8.3685e-05 5.0068e-06 1.1921e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >9.3006e+00 1.3113e-05 1.2672e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.6093e-04 8.6093e-04 8.6093e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0575e-02 1.0575e-02 1.0575e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >3.8471e-03 3.8471e-03 3.8471e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.1587e-01 3.3400e-03 2.0761e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.6100e-04 5.6100e-04 5.6100e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5753e+00 4.4084e-04 2.4748e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >3.1948e-05 0.0000e+00 1.5020e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0967e-05 1.0967e-05 1.0967e-05</hent>
<hent key="0380010000000000000007000000000D" call="MPI_Irecv" bytes="1792" orank="13" region="0" commid="0" count="16" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000010" call="MPI_Irecv" bytes="1792" orank="16" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000012" call="MPI_Irecv" bytes="1792" orank="18" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000015" call="MPI_Irecv" bytes="1792" orank="21" region="0" commid="0" count="22" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000029" call="MPI_Irecv" bytes="1792" orank="41" region="0" commid="0" count="111" tid="0" op="" dtype="" >2.1458e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002B1" call="MPI_Irecv" bytes="1792" orank="689" region="0" commid="0" count="108" tid="0" op="" dtype="" >3.5524e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >9.1076e-05 9.5367e-07 6.6996e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9780e+01 0.0000e+00 3.8414e+01</hent>
<hent key="038001000000000000000A000000000D" call="MPI_Irecv" bytes="2560" orank="13" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000015" call="MPI_Irecv" bytes="2560" orank="21" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000029" call="MPI_Irecv" bytes="2560" orank="41" region="0" commid="0" count="382" tid="0" op="" dtype="" >8.9169e-05 0.0000e+00 1.2159e-05</hent>
<hent key="038001000000000000000A00000002B1" call="MPI_Irecv" bytes="2560" orank="689" region="0" commid="0" count="380" tid="0" op="" dtype="" >1.0800e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000007000000000D" call="MPI_Isend" bytes="1792" orank="13" region="0" commid="0" count="21" tid="0" op="" dtype="" >3.4332e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000010" call="MPI_Isend" bytes="1792" orank="16" region="0" commid="0" count="3" tid="0" op="" dtype="" >7.6294e-06 1.9073e-06 2.8610e-06</hent>
<hent key="02400100000000000000070000000012" call="MPI_Isend" bytes="1792" orank="18" region="0" commid="0" count="8" tid="0" op="" dtype="" >3.7432e-05 3.0994e-06 1.2875e-05</hent>
<hent key="02400100000000000000070000000015" call="MPI_Isend" bytes="1792" orank="21" region="0" commid="0" count="10" tid="0" op="" dtype="" >1.5497e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000029" call="MPI_Isend" bytes="1792" orank="41" region="0" commid="0" count="106" tid="0" op="" dtype="" >5.6648e-04 4.0531e-06 6.1989e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3079e-02 8.3079e-02 8.3079e-02</hent>
<hent key="024001000000000000000700000002B1" call="MPI_Isend" bytes="1792" orank="689" region="0" commid="0" count="130" tid="0" op="" dtype="" >6.0320e-04 3.8147e-06 1.0014e-05</hent>
<hent key="024001000000000000000A000000000D" call="MPI_Isend" bytes="2560" orank="13" region="0" commid="0" count="2" tid="0" op="" dtype="" >3.8147e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000A0000000015" call="MPI_Isend" bytes="2560" orank="21" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000A0000000029" call="MPI_Isend" bytes="2560" orank="41" region="0" commid="0" count="372" tid="0" op="" dtype="" >2.1381e-03 4.7684e-06 1.3113e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.9816e-04 4.0054e-05 1.8191e-04</hent>
<hent key="024001000000000000000A00000002B1" call="MPI_Isend" bytes="2560" orank="689" region="0" commid="0" count="385" tid="0" op="" dtype="" >1.8768e-03 3.8147e-06 1.0014e-05</hent>
<hent key="03800100000000000000100000000010" call="MPI_Irecv" bytes="4096" orank="16" region="0" commid="0" count="12598" tid="0" op="" dtype="" >4.7860e-03 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000100000000012" call="MPI_Irecv" bytes="4096" orank="18" region="0" commid="0" count="12503" tid="0" op="" dtype="" >1.4074e-03 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000100000000029" call="MPI_Irecv" bytes="4096" orank="41" region="0" commid="0" count="2986" tid="0" op="" dtype="" >5.1308e-04 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000001000000002B1" call="MPI_Irecv" bytes="4096" orank="689" region="0" commid="0" count="2852" tid="0" op="" dtype="" >5.7101e-04 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000100000000010" call="MPI_Isend" bytes="4096" orank="16" region="0" commid="0" count="12492" tid="0" op="" dtype="" >2.2072e-02 0.0000e+00 3.0041e-05</hent>
<hent key="02400100000000000000100000000012" call="MPI_Isend" bytes="4096" orank="18" region="0" commid="0" count="12597" tid="0" op="" dtype="" >2.9356e-02 9.5367e-07 1.8120e-05</hent>
<hent key="02400100000000000000100000000029" call="MPI_Isend" bytes="4096" orank="41" region="0" commid="0" count="3015" tid="0" op="" dtype="" >1.1911e-02 1.1921e-06 2.4080e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.3762e-03 4.9615e-04 8.8000e-04</hent>
<hent key="024001000000000000001000000002B1" call="MPI_Isend" bytes="4096" orank="689" region="0" commid="0" count="2950" tid="0" op="" dtype="" >1.0382e-02 9.5367e-07 1.9073e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.4107e-05 4.4107e-05 4.4107e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.9981e-03 4.0412e-04 1.0569e-03</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.7752e-06 1.9073e-06 7.8678e-06</hent>
<hent key="0380010000000000000000040000000D" call="MPI_Irecv" bytes="4" orank="13" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.8232e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000010" call="MPI_Irecv" bytes="4" orank="16" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.3848e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000400000012" call="MPI_Irecv" bytes="4" orank="18" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.2609e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000400000015" call="MPI_Irecv" bytes="4" orank="21" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.5514e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000029" call="MPI_Irecv" bytes="4" orank="41" region="0" commid="0" count="3400" tid="0" op="" dtype="" >7.6270e-04 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000001C000000000D" call="MPI_Irecv" bytes="7168" orank="13" region="0" commid="0" count="10816" tid="0" op="" dtype="" >1.3845e-03 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000001C0000000015" call="MPI_Irecv" bytes="7168" orank="21" region="0" commid="0" count="10396" tid="0" op="" dtype="" >1.3785e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000004000002B1" call="MPI_Irecv" bytes="4" orank="689" region="0" commid="0" count="3400" tid="0" op="" dtype="" >7.2575e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000000040000000D" call="MPI_Isend" bytes="4" orank="13" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.6646e-03 0.0000e+00 2.9802e-05</hent>
<hent key="02400100000000000000000400000010" call="MPI_Isend" bytes="4" orank="16" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.3874e-03 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000400000012" call="MPI_Isend" bytes="4" orank="18" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.4939e-03 0.0000e+00 5.8889e-05</hent>
<hent key="02400100000000000000000400000015" call="MPI_Isend" bytes="4" orank="21" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.5426e-03 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000400000029" call="MPI_Isend" bytes="4" orank="41" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.4777e-02 4.0531e-06 3.0303e-04</hent>
<hent key="024001000000000000001C000000000D" call="MPI_Isend" bytes="7168" orank="13" region="0" commid="0" count="9706" tid="0" op="" dtype="" >1.9078e-02 9.5367e-07 1.6928e-05</hent>
<hent key="024001000000000000001C0000000015" call="MPI_Isend" bytes="7168" orank="21" region="0" commid="0" count="9955" tid="0" op="" dtype="" >2.1336e-02 9.5367e-07 3.2902e-05</hent>
<hent key="024001000000000000000004000002B1" call="MPI_Isend" bytes="4" orank="689" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.9857e-02 3.8147e-06 6.4850e-05</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.1890e-04 4.1890e-04 4.1890e-04</hent>
<hent key="03800100000000000000020000000010" call="MPI_Irecv" bytes="512" orank="16" region="0" commid="0" count="3368" tid="0" op="" dtype="" >5.5027e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000020000000012" call="MPI_Irecv" bytes="512" orank="18" region="0" commid="0" count="3346" tid="0" op="" dtype="" >5.5599e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000005000000000D" call="MPI_Irecv" bytes="1280" orank="13" region="0" commid="0" count="138" tid="0" op="" dtype="" >3.1233e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000010" call="MPI_Irecv" bytes="1280" orank="16" region="0" commid="0" count="52" tid="0" op="" dtype="" >2.2888e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000012" call="MPI_Irecv" bytes="1280" orank="18" region="0" commid="0" count="48" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000015" call="MPI_Irecv" bytes="1280" orank="21" region="0" commid="0" count="148" tid="0" op="" dtype="" >2.8372e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000029" call="MPI_Irecv" bytes="1280" orank="41" region="0" commid="0" count="39" tid="0" op="" dtype="" >8.8215e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4741e+01 1.5020e-05 1.8462e-01</hent>
<hent key="038001000000000000000500000002B1" call="MPI_Irecv" bytes="1280" orank="689" region="0" commid="0" count="45" tid="0" op="" dtype="" >1.4782e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000020000000010" call="MPI_Isend" bytes="512" orank="16" region="0" commid="0" count="3340" tid="0" op="" dtype="" >1.7712e-03 0.0000e+00 1.7881e-05</hent>
<hent key="02400100000000000000020000000012" call="MPI_Isend" bytes="512" orank="18" region="0" commid="0" count="3370" tid="0" op="" dtype="" >2.3096e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0380010000000000000008000000000D" call="MPI_Irecv" bytes="2048" orank="13" region="0" commid="0" count="6" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000010" call="MPI_Irecv" bytes="2048" orank="16" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000015" call="MPI_Irecv" bytes="2048" orank="21" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000029" call="MPI_Irecv" bytes="2048" orank="41" region="0" commid="0" count="156" tid="0" op="" dtype="" >3.6478e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002B1" call="MPI_Irecv" bytes="2048" orank="689" region="0" commid="0" count="154" tid="0" op="" dtype="" >5.8651e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000005000000000D" call="MPI_Isend" bytes="1280" orank="13" region="0" commid="0" count="139" tid="0" op="" dtype="" >2.0146e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000050000000010" call="MPI_Isend" bytes="1280" orank="16" region="0" commid="0" count="61" tid="0" op="" dtype="" >1.2636e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000050000000012" call="MPI_Isend" bytes="1280" orank="18" region="0" commid="0" count="60" tid="0" op="" dtype="" >2.1887e-04 2.8610e-06 1.4067e-05</hent>
<hent key="02400100000000000000050000000015" call="MPI_Isend" bytes="1280" orank="21" region="0" commid="0" count="169" tid="0" op="" dtype="" >2.7394e-04 9.5367e-07 1.1921e-05</hent>
<hent key="02400100000000000000050000000029" call="MPI_Isend" bytes="1280" orank="41" region="0" commid="0" count="47" tid="0" op="" dtype="" >2.4056e-04 3.8147e-06 5.9605e-06</hent>
<hent key="024001000000000000000500000002B1" call="MPI_Isend" bytes="1280" orank="689" region="0" commid="0" count="53" tid="0" op="" dtype="" >2.4629e-04 3.8147e-06 5.0068e-06</hent>
</hash>
<internal rank="17" log_i="1724765674.529922" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="18" mpi_size="696" stamp_init="1724765564.465353" stamp_final="1724765674.534381" username="apac4" allocationname="unknown" flags="0" pid="1612135" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10069e+02" utime="8.69830e+01" stime="1.47569e+01" mtime="7.19216e+01" gflop="0.00000e+00" gbyte="3.76808e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19216e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4161417141814a25518141814bb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09933e+02" utime="8.69535e+01" stime="1.47439e+01" mtime="7.19216e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19216e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2005e+09" > 7.9845e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2013e+09" > 4.2542e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9786e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9774e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.6280e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.7207e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5747e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.3639e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0586e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3089e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3653e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="210" >
<hent key="0240010000000000000008000000000E" call="MPI_Isend" bytes="2048" orank="14" region="0" commid="0" count="9" tid="0" op="" dtype="" >2.7657e-05 1.9073e-06 1.0014e-05</hent>
<hent key="02400100000000000000080000000016" call="MPI_Isend" bytes="2048" orank="22" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.5020e-05 1.9073e-06 2.8610e-06</hent>
<hent key="0240010000000000000008000000002A" call="MPI_Isend" bytes="2048" orank="42" region="0" commid="0" count="139" tid="0" op="" dtype="" >9.4366e-04 4.7684e-06 1.5020e-05</hent>
<hent key="024001000000000000000800000002B2" call="MPI_Isend" bytes="2048" orank="690" region="0" commid="0" count="148" tid="0" op="" dtype="" >8.5115e-04 3.8147e-06 1.2159e-05</hent>
<hent key="038001000000000000000E0000000011" call="MPI_Irecv" bytes="3584" orank="17" region="0" commid="0" count="103" tid="0" op="" dtype="" >4.7445e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000013" call="MPI_Irecv" bytes="3584" orank="19" region="0" commid="0" count="126" tid="0" op="" dtype="" >2.0742e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E000000002A" call="MPI_Irecv" bytes="3584" orank="42" region="0" commid="0" count="133" tid="0" op="" dtype="" >5.8174e-05 0.0000e+00 8.1062e-06</hent>
<hent key="038001000000000000000E00000002B2" call="MPI_Irecv" bytes="3584" orank="690" region="0" commid="0" count="133" tid="0" op="" dtype="" >1.1730e-04 0.0000e+00 8.1062e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000E0000000011" call="MPI_Isend" bytes="3584" orank="17" region="0" commid="0" count="197" tid="0" op="" dtype="" >3.0160e-04 0.0000e+00 2.1458e-06</hent>
<hent key="024001000000000000000E0000000013" call="MPI_Isend" bytes="3584" orank="19" region="0" commid="0" count="68" tid="0" op="" dtype="" >1.7285e-04 0.0000e+00 5.0068e-06</hent>
<hent key="024001000000000000000E000000002A" call="MPI_Isend" bytes="3584" orank="42" region="0" commid="0" count="142" tid="0" op="" dtype="" >1.0819e-03 5.9605e-06 2.3127e-05</hent>
<hent key="024001000000000000000E00000002B2" call="MPI_Isend" bytes="3584" orank="690" region="0" commid="0" count="150" tid="0" op="" dtype="" >1.0071e-03 4.7684e-06 3.1948e-05</hent>
<hent key="0380010000000000000002800000000E" call="MPI_Irecv" bytes="640" orank="14" region="0" commid="0" count="414" tid="0" op="" dtype="" >8.6069e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000011" call="MPI_Irecv" bytes="640" orank="17" region="0" commid="0" count="336" tid="0" op="" dtype="" >1.4782e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000013" call="MPI_Irecv" bytes="640" orank="19" region="0" commid="0" count="364" tid="0" op="" dtype="" >2.0766e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000028000000016" call="MPI_Irecv" bytes="640" orank="22" region="0" commid="0" count="477" tid="0" op="" dtype="" >8.6069e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000002A" call="MPI_Irecv" bytes="640" orank="42" region="0" commid="0" count="25" tid="0" op="" dtype="" >9.7752e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000014000000002A" call="MPI_Irecv" bytes="5120" orank="42" region="0" commid="0" count="855" tid="0" op="" dtype="" >2.9922e-04 0.0000e+00 5.6982e-05</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="038001000000000000000280000002B2" call="MPI_Irecv" bytes="640" orank="690" region="0" commid="0" count="30" tid="0" op="" dtype="" >2.0266e-05 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000001400000002B2" call="MPI_Irecv" bytes="5120" orank="690" region="0" commid="0" count="562" tid="0" op="" dtype="" >3.6168e-04 0.0000e+00 3.4809e-05</hent>
<hent key="03800100000000000000A0000000002A" call="MPI_Irecv" bytes="40960" orank="42" region="0" commid="0" count="3011" tid="0" op="" dtype="" >3.5071e-03 0.0000e+00 7.9870e-05</hent>
<hent key="0240010000000000000002800000000E" call="MPI_Isend" bytes="640" orank="14" region="0" commid="0" count="411" tid="0" op="" dtype="" >4.9472e-04 9.5367e-07 7.8678e-06</hent>
<hent key="02400100000000000000028000000011" call="MPI_Isend" bytes="640" orank="17" region="0" commid="0" count="349" tid="0" op="" dtype="" >5.0712e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000028000000013" call="MPI_Isend" bytes="640" orank="19" region="0" commid="0" count="353" tid="0" op="" dtype="" >1.0717e-03 1.9073e-06 9.7752e-06</hent>
<hent key="02400100000000000000028000000016" call="MPI_Isend" bytes="640" orank="22" region="0" commid="0" count="424" tid="0" op="" dtype="" >5.4526e-04 9.5367e-07 7.1526e-06</hent>
<hent key="0240010000000000000002800000002A" call="MPI_Isend" bytes="640" orank="42" region="0" commid="0" count="26" tid="0" op="" dtype="" >1.2541e-04 3.8147e-06 5.9605e-06</hent>
<hent key="03800100000000000000A000000002B2" call="MPI_Irecv" bytes="40960" orank="690" region="0" commid="0" count="1752" tid="0" op="" dtype="" >7.0572e-04 0.0000e+00 2.4796e-05</hent>
<hent key="0240010000000000000014000000002A" call="MPI_Isend" bytes="5120" orank="42" region="0" commid="0" count="682" tid="0" op="" dtype="" >3.8383e-03 1.1921e-06 7.2002e-05</hent>
<hent key="024001000000000000000280000002B2" call="MPI_Isend" bytes="640" orank="690" region="0" commid="0" count="24" tid="0" op="" dtype="" >1.1897e-04 3.8147e-06 1.0014e-05</hent>
<hent key="0380010000000000000080000000002A" call="MPI_Irecv" bytes="32768" orank="42" region="0" commid="0" count="9689" tid="0" op="" dtype="" >8.7411e-03 0.0000e+00 6.9141e-05</hent>
<hent key="024001000000000000001400000002B2" call="MPI_Isend" bytes="5120" orank="690" region="0" commid="0" count="667" tid="0" op="" dtype="" >3.7262e-03 9.5367e-07 1.0419e-04</hent>
<hent key="038001000000000000008000000002B2" call="MPI_Irecv" bytes="32768" orank="690" region="0" commid="0" count="10948" tid="0" op="" dtype="" >3.7780e-03 0.0000e+00 8.4162e-05</hent>
<hent key="0380010000000000000001400000000E" call="MPI_Irecv" bytes="320" orank="14" region="0" commid="0" count="423" tid="0" op="" dtype="" >8.6546e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000011" call="MPI_Irecv" bytes="320" orank="17" region="0" commid="0" count="566" tid="0" op="" dtype="" >2.4796e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000013" call="MPI_Irecv" bytes="320" orank="19" region="0" commid="0" count="547" tid="0" op="" dtype="" >2.9826e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000014000000016" call="MPI_Irecv" bytes="320" orank="22" region="0" commid="0" count="413" tid="0" op="" dtype="" >7.3671e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000002A" call="MPI_Irecv" bytes="320" orank="42" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A0000000002A" call="MPI_Isend" bytes="40960" orank="42" region="0" commid="0" count="2215" tid="0" op="" dtype="" >5.1595e-02 1.0967e-05 7.3910e-05</hent>
<hent key="038001000000000000000140000002B2" call="MPI_Irecv" bytes="320" orank="690" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.2636e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A000000002B2" call="MPI_Isend" bytes="40960" orank="690" region="0" commid="0" count="2315" tid="0" op="" dtype="" >4.4188e-02 6.9141e-06 1.0395e-04</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >5.9786e+00 0.0000e+00 1.1031e-01</hent>
<hent key="0240010000000000000080000000002A" call="MPI_Isend" bytes="32768" orank="42" region="0" commid="0" count="10485" tid="0" op="" dtype="" >2.5856e-01 1.0967e-05 8.7023e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.2888e-05 9.5367e-07 1.9073e-05</hent>
<hent key="024001000000000000008000000002B2" call="MPI_Isend" bytes="32768" orank="690" region="0" commid="0" count="10385" tid="0" op="" dtype="" >1.9976e-01 6.9141e-06 1.2398e-04</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000001400000000E" call="MPI_Isend" bytes="320" orank="14" region="0" commid="0" count="403" tid="0" op="" dtype="" >4.1533e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000014000000011" call="MPI_Isend" bytes="320" orank="17" region="0" commid="0" count="583" tid="0" op="" dtype="" >6.9022e-04 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000014000000013" call="MPI_Isend" bytes="320" orank="19" region="0" commid="0" count="557" tid="0" op="" dtype="" >1.5969e-03 1.9073e-06 1.5974e-05</hent>
<hent key="02400100000000000000014000000016" call="MPI_Isend" bytes="320" orank="22" region="0" commid="0" count="451" tid="0" op="" dtype="" >4.7612e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000001400000002A" call="MPI_Isend" bytes="320" orank="42" region="0" commid="0" count="23" tid="0" op="" dtype="" >1.2279e-04 3.8147e-06 1.2159e-05</hent>
<hent key="03800100000000000000200000000016" call="MPI_Irecv" bytes="8192" orank="22" region="0" commid="0" count="14" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000140000002B2" call="MPI_Isend" bytes="320" orank="690" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.2183e-04 3.8147e-06 5.9605e-06</hent>
<hent key="0380010000000000000000000000000E" call="MPI_Irecv" bytes="0" orank="14" region="0" commid="0" count="335" tid="0" op="" dtype="" >6.7234e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000011" call="MPI_Irecv" bytes="0" orank="17" region="0" commid="0" count="567" tid="0" op="" dtype="" >1.7309e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000013" call="MPI_Irecv" bytes="0" orank="19" region="0" commid="0" count="530" tid="0" op="" dtype="" >2.3460e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000016" call="MPI_Irecv" bytes="0" orank="22" region="0" commid="0" count="365" tid="0" op="" dtype="" >5.8413e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000002A" call="MPI_Irecv" bytes="0" orank="42" region="0" commid="0" count="82" tid="0" op="" dtype="" >2.5034e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002B2" call="MPI_Irecv" bytes="0" orank="690" region="0" commid="0" count="85" tid="0" op="" dtype="" >3.0279e-05 0.0000e+00 4.0531e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.4843e-04 0.0000e+00 8.1062e-05</hent>
<hent key="02400100000000000000200000000016" call="MPI_Isend" bytes="8192" orank="22" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.2493e-04 9.5367e-07 1.1492e-04</hent>
<hent key="0380010000000000000003000000000E" call="MPI_Irecv" bytes="768" orank="14" region="0" commid="0" count="846" tid="0" op="" dtype="" >1.5306e-04 0.0000e+00 1.3113e-05</hent>
<hent key="03800100000000000000030000000016" call="MPI_Irecv" bytes="768" orank="22" region="0" commid="0" count="950" tid="0" op="" dtype="" >1.4067e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000000000000000E" call="MPI_Isend" bytes="0" orank="14" region="0" commid="0" count="324" tid="0" op="" dtype="" >2.4676e-04 0.0000e+00 1.9073e-05</hent>
<hent key="02400100000000000000000000000011" call="MPI_Isend" bytes="0" orank="17" region="0" commid="0" count="554" tid="0" op="" dtype="" >4.3702e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000000000000013" call="MPI_Isend" bytes="0" orank="19" region="0" commid="0" count="563" tid="0" op="" dtype="" >1.2093e-03 9.5367e-07 3.0994e-05</hent>
<hent key="02400100000000000000000000000016" call="MPI_Isend" bytes="0" orank="22" region="0" commid="0" count="350" tid="0" op="" dtype="" >2.2173e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000000000002A" call="MPI_Isend" bytes="0" orank="42" region="0" commid="0" count="87" tid="0" op="" dtype="" >4.0460e-04 1.9073e-06 8.8215e-06</hent>
<hent key="024001000000000000000000000002B2" call="MPI_Isend" bytes="0" orank="690" region="0" commid="0" count="83" tid="0" op="" dtype="" >3.5453e-04 2.1458e-06 9.0599e-06</hent>
<hent key="0380010000000000000006000000000E" call="MPI_Irecv" bytes="1536" orank="14" region="0" commid="0" count="72" tid="0" op="" dtype="" >1.5497e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000011" call="MPI_Irecv" bytes="1536" orank="17" region="0" commid="0" count="9" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000060000000013" call="MPI_Irecv" bytes="1536" orank="19" region="0" commid="0" count="13" tid="0" op="" dtype="" >9.7752e-06 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000060000000016" call="MPI_Irecv" bytes="1536" orank="22" region="0" commid="0" count="53" tid="0" op="" dtype="" >1.4067e-05 0.0000e+00 9.0599e-06</hent>
<hent key="0380010000000000000006000000002A" call="MPI_Irecv" bytes="1536" orank="42" region="0" commid="0" count="78" tid="0" op="" dtype="" >2.1696e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002B2" call="MPI_Irecv" bytes="1536" orank="690" region="0" commid="0" count="81" tid="0" op="" dtype="" >5.1260e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000003000000000E" call="MPI_Isend" bytes="768" orank="14" region="0" commid="0" count="570" tid="0" op="" dtype="" >2.7585e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000030000000016" call="MPI_Isend" bytes="768" orank="22" region="0" commid="0" count="934" tid="0" op="" dtype="" >4.5753e-04 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 3.0994e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000011" call="MPI_Irecv" bytes="448" orank="17" region="0" commid="0" count="30" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000013" call="MPI_Irecv" bytes="448" orank="19" region="0" commid="0" count="30" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000006000000000E" call="MPI_Isend" bytes="1536" orank="14" region="0" commid="0" count="85" tid="0" op="" dtype="" >1.2922e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000011" call="MPI_Isend" bytes="1536" orank="17" region="0" commid="0" count="10" tid="0" op="" dtype="" >2.0742e-05 1.1921e-06 3.0994e-06</hent>
<hent key="02400100000000000000060000000013" call="MPI_Isend" bytes="1536" orank="19" region="0" commid="0" count="19" tid="0" op="" dtype="" >7.2241e-05 2.8610e-06 4.7684e-06</hent>
<hent key="02400100000000000000060000000016" call="MPI_Isend" bytes="1536" orank="22" region="0" commid="0" count="56" tid="0" op="" dtype="" >9.0361e-05 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000006000000002A" call="MPI_Isend" bytes="1536" orank="42" region="0" commid="0" count="81" tid="0" op="" dtype="" >4.9353e-04 4.7684e-06 2.6941e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002B2" call="MPI_Isend" bytes="1536" orank="690" region="0" commid="0" count="83" tid="0" op="" dtype="" >4.9901e-04 3.8147e-06 2.9802e-05</hent>
<hent key="038001000000000000000C000000002A" call="MPI_Irecv" bytes="3072" orank="42" region="0" commid="0" count="350" tid="0" op="" dtype="" >1.5759e-04 0.0000e+00 9.0599e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002B2" call="MPI_Irecv" bytes="3072" orank="690" region="0" commid="0" count="323" tid="0" op="" dtype="" >2.6035e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000001C000000011" call="MPI_Isend" bytes="448" orank="17" region="0" commid="0" count="54" tid="0" op="" dtype="" >2.7657e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C000000013" call="MPI_Isend" bytes="448" orank="19" region="0" commid="0" count="26" tid="0" op="" dtype="" >1.5497e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C000000002A" call="MPI_Isend" bytes="3072" orank="42" region="0" commid="0" count="334" tid="0" op="" dtype="" >2.3849e-03 4.7684e-06 2.0027e-05</hent>
<hent key="024001000000000000000C00000002B2" call="MPI_Isend" bytes="3072" orank="690" region="0" commid="0" count="338" tid="0" op="" dtype="" >2.2435e-03 3.8147e-06 2.3127e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.1100e-04 2.9898e-04 3.1209e-04</hent>
<hent key="0380010000000000000003800000000E" call="MPI_Irecv" bytes="896" orank="14" region="0" commid="0" count="2831" tid="0" op="" dtype="" >5.4169e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000038000000011" call="MPI_Irecv" bytes="896" orank="17" region="0" commid="0" count="154" tid="0" op="" dtype="" >5.7697e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000013" call="MPI_Irecv" bytes="896" orank="19" region="0" commid="0" count="191" tid="0" op="" dtype="" >1.1587e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000038000000016" call="MPI_Irecv" bytes="896" orank="22" region="0" commid="0" count="2676" tid="0" op="" dtype="" >3.7241e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000003800000002A" call="MPI_Irecv" bytes="896" orank="42" region="0" commid="0" count="36" tid="0" op="" dtype="" >1.0014e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002B2" call="MPI_Irecv" bytes="896" orank="690" region="0" commid="0" count="26" tid="0" op="" dtype="" >1.4067e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1921e-05 1.1921e-05 1.1921e-05</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0380010000000000000018000000000E" call="MPI_Irecv" bytes="6144" orank="14" region="0" commid="0" count="3190" tid="0" op="" dtype="" >4.6897e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000180000000016" call="MPI_Irecv" bytes="6144" orank="22" region="0" commid="0" count="3662" tid="0" op="" dtype="" >6.6495e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000018000000002A" call="MPI_Irecv" bytes="6144" orank="42" region="0" commid="0" count="13" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001800000002B2" call="MPI_Irecv" bytes="6144" orank="690" region="0" commid="0" count="14" tid="0" op="" dtype="" >2.1696e-05 0.0000e+00 8.8215e-06</hent>
<hent key="0240010000000000000003800000000E" call="MPI_Isend" bytes="896" orank="14" region="0" commid="0" count="3137" tid="0" op="" dtype="" >1.8303e-03 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000038000000011" call="MPI_Isend" bytes="896" orank="17" region="0" commid="0" count="154" tid="0" op="" dtype="" >2.8205e-04 9.5367e-07 7.1526e-06</hent>
<hent key="02400100000000000000038000000013" call="MPI_Isend" bytes="896" orank="19" region="0" commid="0" count="151" tid="0" op="" dtype="" >5.1570e-04 1.9073e-06 1.2875e-05</hent>
<hent key="02400100000000000000038000000016" call="MPI_Isend" bytes="896" orank="22" region="0" commid="0" count="2729" tid="0" op="" dtype="" >1.6778e-03 0.0000e+00 1.6212e-05</hent>
<hent key="0240010000000000000003800000002A" call="MPI_Isend" bytes="896" orank="42" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.6499e-04 4.0531e-06 1.8120e-05</hent>
<hent key="024001000000000000000380000002B2" call="MPI_Isend" bytes="896" orank="690" region="0" commid="0" count="36" tid="0" op="" dtype="" >1.9097e-04 4.0531e-06 1.0967e-05</hent>
<hent key="0240010000000000000018000000000E" call="MPI_Isend" bytes="6144" orank="14" region="0" commid="0" count="2218" tid="0" op="" dtype="" >4.3209e-03 9.5367e-07 1.7166e-05</hent>
<hent key="02400100000000000000180000000016" call="MPI_Isend" bytes="6144" orank="22" region="0" commid="0" count="3453" tid="0" op="" dtype="" >6.9556e-03 9.5367e-07 8.8215e-06</hent>
<hent key="0240010000000000000018000000002A" call="MPI_Isend" bytes="6144" orank="42" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.9802e-05 6.9141e-06 8.1062e-06</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.6941e-05 2.6941e-05 2.6941e-05</hent>
<hent key="024001000000000000001800000002B2" call="MPI_Isend" bytes="6144" orank="690" region="0" commid="0" count="11" tid="0" op="" dtype="" >7.9870e-05 5.9605e-06 1.0014e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.9992e+00 1.5020e-05 1.2665e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.8596e-04 8.8596e-04 8.8596e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0586e-02 1.0586e-02 1.0586e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >4.6780e-03 4.6780e-03 4.6780e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.0524e-01 3.2952e-03 1.9721e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.8293e-04 5.8293e-04 5.8293e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5747e+00 4.4394e-04 2.4742e-01</hent>
<hent key="03800100000000000000040000000016" call="MPI_Irecv" bytes="1024" orank="22" region="0" commid="0" count="4" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >6.6280e-05 9.5367e-07 3.0994e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="0380010000000000000007000000000E" call="MPI_Irecv" bytes="1792" orank="14" region="0" commid="0" count="24" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000011" call="MPI_Irecv" bytes="1792" orank="17" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000013" call="MPI_Irecv" bytes="1792" orank="19" region="0" commid="0" count="3" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000070000000016" call="MPI_Irecv" bytes="1792" orank="22" region="0" commid="0" count="26" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000002A" call="MPI_Irecv" bytes="1792" orank="42" region="0" commid="0" count="112" tid="0" op="" dtype="" >5.0306e-05 0.0000e+00 6.1989e-06</hent>
<hent key="038001000000000000000700000002B2" call="MPI_Irecv" bytes="1792" orank="690" region="0" commid="0" count="127" tid="0" op="" dtype="" >9.8944e-05 0.0000e+00 1.0014e-05</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >1.0443e-04 0.0000e+00 6.4850e-05</hent>
<hent key="02400100000000000000040000000016" call="MPI_Isend" bytes="1024" orank="22" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9773e+01 0.0000e+00 3.8412e+01</hent>
<hent key="038001000000000000000A000000000E" call="MPI_Irecv" bytes="2560" orank="14" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000016" call="MPI_Irecv" bytes="2560" orank="22" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A000000002A" call="MPI_Irecv" bytes="2560" orank="42" region="0" commid="0" count="389" tid="0" op="" dtype="" >1.7500e-04 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000A00000002B2" call="MPI_Irecv" bytes="2560" orank="690" region="0" commid="0" count="359" tid="0" op="" dtype="" >2.8229e-04 0.0000e+00 1.4067e-05</hent>
<hent key="0240010000000000000007000000000E" call="MPI_Isend" bytes="1792" orank="14" region="0" commid="0" count="27" tid="0" op="" dtype="" >4.8876e-05 9.5367e-07 2.8610e-06</hent>
<hent key="02400100000000000000070000000011" call="MPI_Isend" bytes="1792" orank="17" region="0" commid="0" count="2" tid="0" op="" dtype="" >3.8147e-06 1.9073e-06 1.9073e-06</hent>
<hent key="02400100000000000000070000000013" call="MPI_Isend" bytes="1792" orank="19" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.8358e-05 3.0994e-06 4.0531e-06</hent>
<hent key="02400100000000000000070000000016" call="MPI_Isend" bytes="1792" orank="22" region="0" commid="0" count="16" tid="0" op="" dtype="" >2.8849e-05 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000007000000002A" call="MPI_Isend" bytes="1792" orank="42" region="0" commid="0" count="103" tid="0" op="" dtype="" >6.5231e-04 5.0068e-06 2.9087e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3089e-02 8.3089e-02 8.3089e-02</hent>
<hent key="024001000000000000000700000002B2" call="MPI_Isend" bytes="1792" orank="690" region="0" commid="0" count="109" tid="0" op="" dtype="" >5.8937e-04 3.8147e-06 1.5974e-05</hent>
<hent key="024001000000000000000A0000000013" call="MPI_Isend" bytes="2560" orank="19" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="024001000000000000000A0000000016" call="MPI_Isend" bytes="2560" orank="22" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000A000000002A" call="MPI_Isend" bytes="2560" orank="42" region="0" commid="0" count="381" tid="0" op="" dtype="" >2.5704e-03 4.7684e-06 2.5988e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.1604e-04 4.0054e-05 1.9002e-04</hent>
<hent key="024001000000000000000A00000002B2" call="MPI_Isend" bytes="2560" orank="690" region="0" commid="0" count="365" tid="0" op="" dtype="" >2.2151e-03 3.8147e-06 2.5988e-05</hent>
<hent key="03800100000000000000100000000011" call="MPI_Irecv" bytes="4096" orank="17" region="0" commid="0" count="12597" tid="0" op="" dtype="" >6.0937e-03 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000100000000013" call="MPI_Irecv" bytes="4096" orank="19" region="0" commid="0" count="12574" tid="0" op="" dtype="" >2.3167e-03 0.0000e+00 7.1526e-06</hent>
<hent key="0380010000000000000010000000002A" call="MPI_Irecv" bytes="4096" orank="42" region="0" commid="0" count="2833" tid="0" op="" dtype="" >1.0443e-03 0.0000e+00 7.5102e-05</hent>
<hent key="038001000000000000001000000002B2" call="MPI_Irecv" bytes="4096" orank="690" region="0" commid="0" count="3133" tid="0" op="" dtype="" >1.2250e-03 0.0000e+00 3.1948e-05</hent>
<hent key="02400100000000000000100000000011" call="MPI_Isend" bytes="4096" orank="17" region="0" commid="0" count="12503" tid="0" op="" dtype="" >2.1033e-02 0.0000e+00 1.8120e-05</hent>
<hent key="02400100000000000000100000000013" call="MPI_Isend" bytes="4096" orank="19" region="0" commid="0" count="12632" tid="0" op="" dtype="" >3.4947e-02 0.0000e+00 2.1935e-05</hent>
<hent key="0240010000000000000010000000002A" call="MPI_Isend" bytes="4096" orank="42" region="0" commid="0" count="3029" tid="0" op="" dtype="" >1.5708e-02 1.9073e-06 7.2956e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.4429e-03 5.1689e-04 9.2602e-04</hent>
<hent key="024001000000000000001000000002B2" call="MPI_Isend" bytes="4096" orank="690" region="0" commid="0" count="3017" tid="0" op="" dtype="" >1.5314e-02 9.5367e-07 9.8944e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.2915e-05 4.2915e-05 4.2915e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.1552e-03 4.1819e-04 1.1060e-03</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.4782e-05 9.5367e-07 1.3828e-05</hent>
<hent key="0380010000000000000000040000000E" call="MPI_Irecv" bytes="4" orank="14" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.1509e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000000400000011" call="MPI_Irecv" bytes="4" orank="17" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.0145e-03 0.0000e+00 1.6928e-05</hent>
<hent key="03800100000000000000000400000013" call="MPI_Irecv" bytes="4" orank="19" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.9520e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000016" call="MPI_Irecv" bytes="4" orank="22" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.0436e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000002A" call="MPI_Irecv" bytes="4" orank="42" region="0" commid="0" count="3400" tid="0" op="" dtype="" >9.0551e-04 0.0000e+00 3.9101e-05</hent>
<hent key="038001000000000000001C000000000E" call="MPI_Irecv" bytes="7168" orank="14" region="0" commid="0" count="9510" tid="0" op="" dtype="" >1.6179e-03 0.0000e+00 1.2875e-05</hent>
<hent key="038001000000000000001C0000000016" call="MPI_Irecv" bytes="7168" orank="22" region="0" commid="0" count="9024" tid="0" op="" dtype="" >1.5452e-03 0.0000e+00 1.2875e-05</hent>
<hent key="038001000000000000000004000002B2" call="MPI_Irecv" bytes="4" orank="690" region="0" commid="0" count="3400" tid="0" op="" dtype="" >8.1682e-04 0.0000e+00 1.1921e-05</hent>
<hent key="0240010000000000000000040000000E" call="MPI_Isend" bytes="4" orank="14" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.4946e-03 0.0000e+00 2.5988e-05</hent>
<hent key="02400100000000000000000400000011" call="MPI_Isend" bytes="4" orank="17" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.4784e-03 0.0000e+00 4.0054e-05</hent>
<hent key="02400100000000000000000400000013" call="MPI_Isend" bytes="4" orank="19" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.4401e-03 0.0000e+00 8.8215e-06</hent>
<hent key="02400100000000000000000400000016" call="MPI_Isend" bytes="4" orank="22" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.8196e-03 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000040000002A" call="MPI_Isend" bytes="4" orank="42" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.8152e-02 4.7684e-06 4.0698e-04</hent>
<hent key="024001000000000000001C000000000E" call="MPI_Isend" bytes="7168" orank="14" region="0" commid="0" count="10482" tid="0" op="" dtype="" >2.0906e-02 9.5367e-07 2.3127e-05</hent>
<hent key="024001000000000000001C0000000016" call="MPI_Isend" bytes="7168" orank="22" region="0" commid="0" count="9240" tid="0" op="" dtype="" >1.9516e-02 9.5367e-07 1.9073e-05</hent>
<hent key="024001000000000000000004000002B2" call="MPI_Isend" bytes="4" orank="690" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.4971e-02 3.8147e-06 8.1062e-05</hent>
<hent key="024001000000000000001C00000002B2" call="MPI_Isend" bytes="7168" orank="690" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.3583e-04 4.3583e-04 4.3583e-04</hent>
<hent key="03800100000000000000020000000011" call="MPI_Irecv" bytes="512" orank="17" region="0" commid="0" count="3370" tid="0" op="" dtype="" >5.0926e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000020000000013" call="MPI_Irecv" bytes="512" orank="19" region="0" commid="0" count="3370" tid="0" op="" dtype="" >5.1737e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0380010000000000000005000000000E" call="MPI_Irecv" bytes="1280" orank="14" region="0" commid="0" count="144" tid="0" op="" dtype="" >3.8862e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000011" call="MPI_Irecv" bytes="1280" orank="17" region="0" commid="0" count="60" tid="0" op="" dtype="" >2.4319e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000013" call="MPI_Irecv" bytes="1280" orank="19" region="0" commid="0" count="52" tid="0" op="" dtype="" >3.3617e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000050000000016" call="MPI_Irecv" bytes="1280" orank="22" region="0" commid="0" count="128" tid="0" op="" dtype="" >1.7881e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000002A" call="MPI_Irecv" bytes="1280" orank="42" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4443e+01 9.0599e-06 1.8480e-01</hent>
<hent key="038001000000000000000500000002B2" call="MPI_Irecv" bytes="1280" orank="690" region="0" commid="0" count="51" tid="0" op="" dtype="" >3.1710e-05 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000020000000011" call="MPI_Isend" bytes="512" orank="17" region="0" commid="0" count="3346" tid="0" op="" dtype="" >1.8537e-03 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000020000000013" call="MPI_Isend" bytes="512" orank="19" region="0" commid="0" count="3374" tid="0" op="" dtype="" >2.3968e-03 0.0000e+00 1.7881e-05</hent>
<hent key="0380010000000000000008000000000E" call="MPI_Irecv" bytes="2048" orank="14" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000016" call="MPI_Irecv" bytes="2048" orank="22" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000002A" call="MPI_Irecv" bytes="2048" orank="42" region="0" commid="0" count="130" tid="0" op="" dtype="" >6.6280e-05 0.0000e+00 8.1062e-06</hent>
<hent key="038001000000000000000800000002B2" call="MPI_Irecv" bytes="2048" orank="690" region="0" commid="0" count="151" tid="0" op="" dtype="" >1.4234e-04 0.0000e+00 1.8120e-05</hent>
<hent key="0240010000000000000005000000000E" call="MPI_Isend" bytes="1280" orank="14" region="0" commid="0" count="134" tid="0" op="" dtype="" >2.1267e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000050000000011" call="MPI_Isend" bytes="1280" orank="17" region="0" commid="0" count="48" tid="0" op="" dtype="" >9.5844e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000013" call="MPI_Isend" bytes="1280" orank="19" region="0" commid="0" count="51" tid="0" op="" dtype="" >1.9407e-04 2.8610e-06 1.4067e-05</hent>
<hent key="02400100000000000000050000000016" call="MPI_Isend" bytes="1280" orank="22" region="0" commid="0" count="130" tid="0" op="" dtype="" >2.0957e-04 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000005000000002A" call="MPI_Isend" bytes="1280" orank="42" region="0" commid="0" count="44" tid="0" op="" dtype="" >2.5392e-04 5.0068e-06 1.2159e-05</hent>
<hent key="024001000000000000000500000002B2" call="MPI_Isend" bytes="1280" orank="690" region="0" commid="0" count="41" tid="0" op="" dtype="" >2.0599e-04 3.8147e-06 8.1062e-06</hent>
</hash>
<internal rank="18" log_i="1724765674.534381" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="19" mpi_size="696" stamp_init="1724765564.465393" stamp_final="1724765674.526136" username="apac4" allocationname="unknown" flags="0" pid="1612136" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10061e+02" utime="8.93614e+01" stime="1.41555e+01" mtime="7.33087e+01" gflop="0.00000e+00" gbyte="3.77659e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.33087e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e414e614e714fe55e714e714d9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09923e+02" utime="8.93355e+01" stime="1.41387e+01" mtime="7.33087e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.33087e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1895e+09" > 6.0586e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1864e+09" > 2.7894e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6895e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9782e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6716e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3644e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5747e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.7549e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0601e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3079e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4528e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="208" >
<hent key="0240010000000000000008000000000F" call="MPI_Isend" bytes="2048" orank="15" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.5259e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000080000000010" call="MPI_Isend" bytes="2048" orank="16" region="0" commid="0" count="1" tid="0" op="" dtype="" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="02400100000000000000080000000017" call="MPI_Isend" bytes="2048" orank="23" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.6689e-05 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000008000000002B" call="MPI_Isend" bytes="2048" orank="43" region="0" commid="0" count="144" tid="0" op="" dtype="" >8.5521e-04 4.7684e-06 1.2875e-05</hent>
<hent key="024001000000000000000800000002B3" call="MPI_Isend" bytes="2048" orank="691" region="0" commid="0" count="157" tid="0" op="" dtype="" >8.0967e-04 3.8147e-06 1.1921e-05</hent>
<hent key="038001000000000000000E0000000010" call="MPI_Irecv" bytes="3584" orank="16" region="0" commid="0" count="65" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000E0000000012" call="MPI_Irecv" bytes="3584" orank="18" region="0" commid="0" count="68" tid="0" op="" dtype="" >2.5749e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E000000002B" call="MPI_Irecv" bytes="3584" orank="43" region="0" commid="0" count="131" tid="0" op="" dtype="" >5.4121e-05 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000E00000002B3" call="MPI_Irecv" bytes="3584" orank="691" region="0" commid="0" count="138" tid="0" op="" dtype="" >3.0994e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="024001000000000000000E0000000010" call="MPI_Isend" bytes="3584" orank="16" region="0" commid="0" count="171" tid="0" op="" dtype="" >4.3702e-04 9.5367e-07 1.8120e-05</hent>
<hent key="024001000000000000000E0000000012" call="MPI_Isend" bytes="3584" orank="18" region="0" commid="0" count="126" tid="0" op="" dtype="" >1.8525e-04 9.5367e-07 2.1458e-06</hent>
<hent key="024001000000000000000E000000002B" call="MPI_Isend" bytes="3584" orank="43" region="0" commid="0" count="137" tid="0" op="" dtype="" >8.8334e-04 5.0068e-06 1.0967e-05</hent>
<hent key="024001000000000000000E00000002B3" call="MPI_Isend" bytes="3584" orank="691" region="0" commid="0" count="118" tid="0" op="" dtype="" >6.5899e-04 4.7684e-06 1.0967e-05</hent>
<hent key="0380010000000000000002800000000F" call="MPI_Irecv" bytes="640" orank="15" region="0" commid="0" count="432" tid="0" op="" dtype="" >8.5592e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000010" call="MPI_Irecv" bytes="640" orank="16" region="0" commid="0" count="348" tid="0" op="" dtype="" >1.0371e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000012" call="MPI_Irecv" bytes="640" orank="18" region="0" commid="0" count="353" tid="0" op="" dtype="" >1.4353e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000017" call="MPI_Irecv" bytes="640" orank="23" region="0" commid="0" count="413" tid="0" op="" dtype="" >1.0180e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000002B" call="MPI_Irecv" bytes="640" orank="43" region="0" commid="0" count="27" tid="0" op="" dtype="" >7.6294e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000014000000002B" call="MPI_Irecv" bytes="5120" orank="43" region="0" commid="0" count="390" tid="0" op="" dtype="" >7.5102e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.8147e-06 3.8147e-06 3.8147e-06</hent>
<hent key="038001000000000000000280000002B3" call="MPI_Irecv" bytes="640" orank="691" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.2636e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001400000002B3" call="MPI_Irecv" bytes="5120" orank="691" region="0" commid="0" count="526" tid="0" op="" dtype="" >1.2541e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000A0000000002B" call="MPI_Irecv" bytes="40960" orank="43" region="0" commid="0" count="1188" tid="0" op="" dtype="" >3.3236e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000002800000000F" call="MPI_Isend" bytes="640" orank="15" region="0" commid="0" count="424" tid="0" op="" dtype="" >4.8184e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000028000000010" call="MPI_Isend" bytes="640" orank="16" region="0" commid="0" count="336" tid="0" op="" dtype="" >1.0500e-03 1.9073e-06 1.0014e-05</hent>
<hent key="02400100000000000000028000000012" call="MPI_Isend" bytes="640" orank="18" region="0" commid="0" count="364" tid="0" op="" dtype="" >5.6839e-04 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000028000000017" call="MPI_Isend" bytes="640" orank="23" region="0" commid="0" count="417" tid="0" op="" dtype="" >4.7779e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000002800000002B" call="MPI_Isend" bytes="640" orank="43" region="0" commid="0" count="23" tid="0" op="" dtype="" >1.2016e-04 5.0068e-06 8.1062e-06</hent>
<hent key="03800100000000000000A000000002B3" call="MPI_Irecv" bytes="40960" orank="691" region="0" commid="0" count="1674" tid="0" op="" dtype="" >3.6860e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000014000000002B" call="MPI_Isend" bytes="5120" orank="43" region="0" commid="0" count="545" tid="0" op="" dtype="" >2.3351e-03 1.9073e-06 1.0014e-05</hent>
<hent key="024001000000000000000280000002B3" call="MPI_Isend" bytes="640" orank="691" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.1849e-04 3.8147e-06 6.1989e-06</hent>
<hent key="0380010000000000000080000000002B" call="MPI_Irecv" bytes="32768" orank="43" region="0" commid="0" count="11512" tid="0" op="" dtype="" >3.5732e-03 0.0000e+00 7.8678e-06</hent>
<hent key="024001000000000000001400000002B3" call="MPI_Isend" bytes="5120" orank="691" region="0" commid="0" count="471" tid="0" op="" dtype="" >1.7979e-03 9.5367e-07 1.0014e-05</hent>
<hent key="038001000000000000008000000002B3" call="MPI_Irecv" bytes="32768" orank="691" region="0" commid="0" count="11026" tid="0" op="" dtype="" >2.2757e-03 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000001400000000F" call="MPI_Irecv" bytes="320" orank="15" region="0" commid="0" count="424" tid="0" op="" dtype="" >9.9421e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000010" call="MPI_Irecv" bytes="320" orank="16" region="0" commid="0" count="590" tid="0" op="" dtype="" >1.7333e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000012" call="MPI_Irecv" bytes="320" orank="18" region="0" commid="0" count="557" tid="0" op="" dtype="" >2.2984e-04 0.0000e+00 3.8147e-06</hent>
<hent key="03800100000000000000014000000017" call="MPI_Irecv" bytes="320" orank="23" region="0" commid="0" count="458" tid="0" op="" dtype="" >1.2851e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000002B" call="MPI_Irecv" bytes="320" orank="43" region="0" commid="0" count="28" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A0000000002B" call="MPI_Isend" bytes="40960" orank="43" region="0" commid="0" count="1760" tid="0" op="" dtype="" >3.4996e-02 9.0599e-06 5.1022e-05</hent>
<hent key="038001000000000000000140000002B3" call="MPI_Irecv" bytes="320" orank="691" region="0" commid="0" count="21" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000A000000002B3" call="MPI_Isend" bytes="40960" orank="691" region="0" commid="0" count="1559" tid="0" op="" dtype="" >1.7283e-02 6.9141e-06 2.0981e-05</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >6.6895e+00 0.0000e+00 1.1075e-01</hent>
<hent key="0240010000000000000080000000002B" call="MPI_Isend" bytes="32768" orank="43" region="0" commid="0" count="10940" tid="0" op="" dtype="" >2.1532e-01 7.8678e-06 7.7009e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.0981e-05 9.5367e-07 1.9073e-05</hent>
<hent key="024001000000000000008000000002B3" call="MPI_Isend" bytes="32768" orank="691" region="0" commid="0" count="11141" tid="0" op="" dtype="" >1.2144e-01 6.9141e-06 5.7936e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 0.0000e+00 2.8610e-06</hent>
<hent key="0240010000000000000001400000000F" call="MPI_Isend" bytes="320" orank="15" region="0" commid="0" count="448" tid="0" op="" dtype="" >4.0865e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000010" call="MPI_Isend" bytes="320" orank="16" region="0" commid="0" count="599" tid="0" op="" dtype="" >1.6851e-03 1.9073e-06 1.9073e-05</hent>
<hent key="02400100000000000000014000000012" call="MPI_Isend" bytes="320" orank="18" region="0" commid="0" count="547" tid="0" op="" dtype="" >6.7759e-04 0.0000e+00 8.8215e-06</hent>
<hent key="02400100000000000000014000000017" call="MPI_Isend" bytes="320" orank="23" region="0" commid="0" count="428" tid="0" op="" dtype="" >4.2725e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000001400000002B" call="MPI_Isend" bytes="320" orank="43" region="0" commid="0" count="20" tid="0" op="" dtype="" >9.8228e-05 4.0531e-06 5.9605e-06</hent>
<hent key="024001000000000000000140000002B3" call="MPI_Isend" bytes="320" orank="691" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.3041e-04 3.8147e-06 1.0014e-05</hent>
<hent key="0380010000000000000000000000000F" call="MPI_Irecv" bytes="0" orank="15" region="0" commid="0" count="345" tid="0" op="" dtype="" >7.6532e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000010" call="MPI_Irecv" bytes="0" orank="16" region="0" commid="0" count="513" tid="0" op="" dtype="" >1.4424e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000012" call="MPI_Irecv" bytes="0" orank="18" region="0" commid="0" count="563" tid="0" op="" dtype="" >1.8406e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000017" call="MPI_Irecv" bytes="0" orank="23" region="0" commid="0" count="316" tid="0" op="" dtype="" >7.9393e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000002B" call="MPI_Irecv" bytes="0" orank="43" region="0" commid="0" count="89" tid="0" op="" dtype="" >2.3842e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002B3" call="MPI_Irecv" bytes="0" orank="691" region="0" commid="0" count="81" tid="0" op="" dtype="" >1.4067e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.1029e-04 0.0000e+00 8.1778e-05</hent>
<hent key="0380010000000000000003000000000F" call="MPI_Irecv" bytes="768" orank="15" region="0" commid="0" count="808" tid="0" op="" dtype="" >1.3375e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000030000000017" call="MPI_Irecv" bytes="768" orank="23" region="0" commid="0" count="742" tid="0" op="" dtype="" >8.8930e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000000000000000F" call="MPI_Isend" bytes="0" orank="15" region="0" commid="0" count="346" tid="0" op="" dtype="" >2.4748e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000000000000010" call="MPI_Isend" bytes="0" orank="16" region="0" commid="0" count="517" tid="0" op="" dtype="" >1.0965e-03 9.5367e-07 5.3167e-05</hent>
<hent key="02400100000000000000000000000012" call="MPI_Isend" bytes="0" orank="18" region="0" commid="0" count="530" tid="0" op="" dtype="" >4.6897e-04 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000000000000017" call="MPI_Isend" bytes="0" orank="23" region="0" commid="0" count="371" tid="0" op="" dtype="" >2.8419e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000000000000002B" call="MPI_Isend" bytes="0" orank="43" region="0" commid="0" count="84" tid="0" op="" dtype="" >3.6645e-04 9.5367e-07 1.0014e-05</hent>
<hent key="024001000000000000000000000002B3" call="MPI_Isend" bytes="0" orank="691" region="0" commid="0" count="78" tid="0" op="" dtype="" >3.2020e-04 1.9073e-06 9.0599e-06</hent>
<hent key="0380010000000000000006000000000F" call="MPI_Irecv" bytes="1536" orank="15" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.7405e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000010" call="MPI_Irecv" bytes="1536" orank="16" region="0" commid="0" count="11" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000060000000012" call="MPI_Irecv" bytes="1536" orank="18" region="0" commid="0" count="19" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000060000000017" call="MPI_Irecv" bytes="1536" orank="23" region="0" commid="0" count="65" tid="0" op="" dtype="" >2.0504e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000002B" call="MPI_Irecv" bytes="1536" orank="43" region="0" commid="0" count="77" tid="0" op="" dtype="" >3.4332e-05 0.0000e+00 7.1526e-06</hent>
<hent key="038001000000000000000600000002B3" call="MPI_Irecv" bytes="1536" orank="691" region="0" commid="0" count="70" tid="0" op="" dtype="" >1.4782e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000003000000000F" call="MPI_Isend" bytes="768" orank="15" region="0" commid="0" count="950" tid="0" op="" dtype="" >4.4322e-04 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000030000000017" call="MPI_Isend" bytes="768" orank="23" region="0" commid="0" count="762" tid="0" op="" dtype="" >4.0412e-04 0.0000e+00 8.1062e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 2.8610e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000010" call="MPI_Irecv" bytes="448" orank="16" region="0" commid="0" count="16" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000012" call="MPI_Irecv" bytes="448" orank="18" region="0" commid="0" count="26" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000006000000000F" call="MPI_Isend" bytes="1536" orank="15" region="0" commid="0" count="47" tid="0" op="" dtype="" >7.9155e-05 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000060000000010" call="MPI_Isend" bytes="1536" orank="16" region="0" commid="0" count="14" tid="0" op="" dtype="" >6.8903e-05 2.8610e-06 1.9073e-05</hent>
<hent key="02400100000000000000060000000012" call="MPI_Isend" bytes="1536" orank="18" region="0" commid="0" count="13" tid="0" op="" dtype="" >3.6001e-05 1.9073e-06 1.0014e-05</hent>
<hent key="02400100000000000000060000000017" call="MPI_Isend" bytes="1536" orank="23" region="0" commid="0" count="40" tid="0" op="" dtype="" >8.0347e-05 9.5367e-07 7.1526e-06</hent>
<hent key="0240010000000000000006000000002B" call="MPI_Isend" bytes="1536" orank="43" region="0" commid="0" count="94" tid="0" op="" dtype="" >5.3954e-04 4.7684e-06 1.2159e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002B3" call="MPI_Isend" bytes="1536" orank="691" region="0" commid="0" count="71" tid="0" op="" dtype="" >3.5501e-04 3.8147e-06 1.0014e-05</hent>
<hent key="038001000000000000000C000000002B" call="MPI_Irecv" bytes="3072" orank="43" region="0" commid="0" count="344" tid="0" op="" dtype="" >1.1086e-04 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002B3" call="MPI_Irecv" bytes="3072" orank="691" region="0" commid="0" count="341" tid="0" op="" dtype="" >1.0061e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0240010000000000000001C000000010" call="MPI_Isend" bytes="448" orank="16" region="0" commid="0" count="50" tid="0" op="" dtype="" >3.0756e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C000000012" call="MPI_Isend" bytes="448" orank="18" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.9073e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C000000000F" call="MPI_Isend" bytes="3072" orank="15" region="0" commid="0" count="1" tid="0" op="" dtype="" >6.9141e-06 6.9141e-06 6.9141e-06</hent>
<hent key="024001000000000000000C000000002B" call="MPI_Isend" bytes="3072" orank="43" region="0" commid="0" count="347" tid="0" op="" dtype="" >2.2068e-03 4.7684e-06 1.3828e-05</hent>
<hent key="024001000000000000000C00000002B3" call="MPI_Isend" bytes="3072" orank="691" region="0" commid="0" count="340" tid="0" op="" dtype="" >1.8432e-03 4.0531e-06 7.1526e-06</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.6774e-04 3.1686e-04 3.3188e-04</hent>
<hent key="0380010000000000000003800000000F" call="MPI_Irecv" bytes="896" orank="15" region="0" commid="0" count="2876" tid="0" op="" dtype="" >4.5681e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000010" call="MPI_Irecv" bytes="896" orank="16" region="0" commid="0" count="173" tid="0" op="" dtype="" >5.4359e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000012" call="MPI_Irecv" bytes="896" orank="18" region="0" commid="0" count="151" tid="0" op="" dtype="" >5.9843e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000017" call="MPI_Irecv" bytes="896" orank="23" region="0" commid="0" count="2939" tid="0" op="" dtype="" >4.5919e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000002B" call="MPI_Irecv" bytes="896" orank="43" region="0" commid="0" count="23" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002B3" call="MPI_Irecv" bytes="896" orank="691" region="0" commid="0" count="38" tid="0" op="" dtype="" >1.3113e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.3508e-04 2.3508e-04 2.3508e-04</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="0380010000000000000018000000000F" call="MPI_Irecv" bytes="6144" orank="15" region="0" commid="0" count="2982" tid="0" op="" dtype="" >3.5501e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000180000000017" call="MPI_Irecv" bytes="6144" orank="23" region="0" commid="0" count="2767" tid="0" op="" dtype="" >4.1699e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000018000000002B" call="MPI_Irecv" bytes="6144" orank="43" region="0" commid="0" count="17" tid="0" op="" dtype="" >6.6757e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000001800000002B3" call="MPI_Irecv" bytes="6144" orank="691" region="0" commid="0" count="16" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000003800000000F" call="MPI_Isend" bytes="896" orank="15" region="0" commid="0" count="2719" tid="0" op="" dtype="" >1.5604e-03 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000038000000010" call="MPI_Isend" bytes="896" orank="16" region="0" commid="0" count="174" tid="0" op="" dtype="" >5.8603e-04 1.9073e-06 5.0068e-06</hent>
<hent key="02400100000000000000038000000012" call="MPI_Isend" bytes="896" orank="18" region="0" commid="0" count="191" tid="0" op="" dtype="" >3.4857e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000038000000017" call="MPI_Isend" bytes="896" orank="23" region="0" commid="0" count="2913" tid="0" op="" dtype="" >1.6901e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000003800000002B" call="MPI_Isend" bytes="896" orank="43" region="0" commid="0" count="34" tid="0" op="" dtype="" >1.8239e-04 4.0531e-06 6.1989e-06</hent>
<hent key="024001000000000000000380000002B3" call="MPI_Isend" bytes="896" orank="691" region="0" commid="0" count="26" tid="0" op="" dtype="" >1.2326e-04 3.8147e-06 6.1989e-06</hent>
<hent key="0240010000000000000018000000000F" call="MPI_Isend" bytes="6144" orank="15" region="0" commid="0" count="3473" tid="0" op="" dtype="" >5.9705e-03 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000180000000017" call="MPI_Isend" bytes="6144" orank="23" region="0" commid="0" count="2878" tid="0" op="" dtype="" >5.5499e-03 9.5367e-07 1.5974e-05</hent>
<hent key="0240010000000000000018000000002B" call="MPI_Isend" bytes="6144" orank="43" region="0" commid="0" count="18" tid="0" op="" dtype="" >1.3065e-04 6.9141e-06 8.1062e-06</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.8174e-05 5.8174e-05 5.8174e-05</hent>
<hent key="024001000000000000001800000002B3" call="MPI_Isend" bytes="6144" orank="691" region="0" commid="0" count="19" tid="0" op="" dtype="" >1.2970e-04 5.9605e-06 1.1206e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >9.4441e+00 1.5974e-05 1.2668e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.3184e-04 8.3184e-04 8.3184e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0601e-02 1.0601e-02 1.0601e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >3.8750e-03 3.8750e-03 3.8750e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.1447e-01 3.3391e-03 2.0618e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.9295e-04 5.9295e-04 5.9295e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5747e+00 4.4584e-04 2.4739e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >3.6716e-05 0.0000e+00 1.8835e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0967e-05 1.0967e-05 1.0967e-05</hent>
<hent key="0380010000000000000007000000000F" call="MPI_Irecv" bytes="1792" orank="15" region="0" commid="0" count="24" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000010" call="MPI_Irecv" bytes="1792" orank="16" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000012" call="MPI_Irecv" bytes="1792" orank="18" region="0" commid="0" count="5" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000017" call="MPI_Irecv" bytes="1792" orank="23" region="0" commid="0" count="19" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000002B" call="MPI_Irecv" bytes="1792" orank="43" region="0" commid="0" count="118" tid="0" op="" dtype="" >5.2214e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002B3" call="MPI_Irecv" bytes="1792" orank="691" region="0" commid="0" count="113" tid="0" op="" dtype="" >2.4319e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >8.4639e-05 9.5367e-07 6.3181e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9781e+01 0.0000e+00 3.8419e+01</hent>
<hent key="038001000000000000000A000000000F" call="MPI_Irecv" bytes="2560" orank="15" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000012" call="MPI_Irecv" bytes="2560" orank="18" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000A0000000017" call="MPI_Irecv" bytes="2560" orank="23" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000002B" call="MPI_Irecv" bytes="2560" orank="43" region="0" commid="0" count="359" tid="0" op="" dtype="" >1.3113e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002B3" call="MPI_Irecv" bytes="2560" orank="691" region="0" commid="0" count="343" tid="0" op="" dtype="" >1.0681e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000007000000000F" call="MPI_Isend" bytes="1792" orank="15" region="0" commid="0" count="19" tid="0" op="" dtype="" >3.4332e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000010" call="MPI_Isend" bytes="1792" orank="16" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.1219e-05 4.0531e-06 8.1062e-06</hent>
<hent key="02400100000000000000070000000012" call="MPI_Isend" bytes="1792" orank="18" region="0" commid="0" count="3" tid="0" op="" dtype="" >7.8678e-06 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000017" call="MPI_Isend" bytes="1792" orank="23" region="0" commid="0" count="17" tid="0" op="" dtype="" >3.8624e-05 9.5367e-07 8.1062e-06</hent>
<hent key="0240010000000000000007000000002B" call="MPI_Isend" bytes="1792" orank="43" region="0" commid="0" count="113" tid="0" op="" dtype="" >6.5756e-04 4.7684e-06 7.1526e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3079e-02 8.3079e-02 8.3079e-02</hent>
<hent key="024001000000000000000700000002B3" call="MPI_Isend" bytes="1792" orank="691" region="0" commid="0" count="109" tid="0" op="" dtype="" >5.7840e-04 3.8147e-06 2.3842e-05</hent>
<hent key="024001000000000000000A0000000017" call="MPI_Isend" bytes="2560" orank="23" region="0" commid="0" count="2" tid="0" op="" dtype="" >4.2915e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000A000000002B" call="MPI_Isend" bytes="2560" orank="43" region="0" commid="0" count="353" tid="0" op="" dtype="" >2.1660e-03 5.0068e-06 1.4067e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.3297e-04 4.1008e-05 1.9693e-04</hent>
<hent key="024001000000000000000A00000002B3" call="MPI_Isend" bytes="2560" orank="691" region="0" commid="0" count="370" tid="0" op="" dtype="" >1.9617e-03 3.8147e-06 1.7881e-05</hent>
<hent key="03800100000000000000100000000010" call="MPI_Irecv" bytes="4096" orank="16" region="0" commid="0" count="12635" tid="0" op="" dtype="" >1.5280e-03 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000100000000012" call="MPI_Irecv" bytes="4096" orank="18" region="0" commid="0" count="12632" tid="0" op="" dtype="" >5.3248e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000010000000002B" call="MPI_Irecv" bytes="4096" orank="43" region="0" commid="0" count="3289" tid="0" op="" dtype="" >6.0201e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001000000002B3" call="MPI_Irecv" bytes="4096" orank="691" region="0" commid="0" count="3186" tid="0" op="" dtype="" >6.9189e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000100000000010" call="MPI_Isend" bytes="4096" orank="16" region="0" commid="0" count="12529" tid="0" op="" dtype="" >3.2341e-02 0.0000e+00 2.0027e-05</hent>
<hent key="02400100000000000000100000000012" call="MPI_Isend" bytes="4096" orank="18" region="0" commid="0" count="12574" tid="0" op="" dtype="" >2.0780e-02 0.0000e+00 2.0027e-05</hent>
<hent key="0240010000000000000010000000002B" call="MPI_Isend" bytes="4096" orank="43" region="0" commid="0" count="3143" tid="0" op="" dtype="" >1.2728e-02 1.1921e-06 2.3127e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.5230e-03 5.4598e-04 9.7704e-04</hent>
<hent key="024001000000000000001000000002B3" call="MPI_Isend" bytes="4096" orank="691" region="0" commid="0" count="3230" tid="0" op="" dtype="" >1.1516e-02 9.5367e-07 2.0027e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.6968e-05 4.6968e-05 4.6968e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.3681e-03 4.3797e-04 1.2121e-03</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.0967e-05 9.5367e-07 1.0014e-05</hent>
<hent key="0380010000000000000000040000000F" call="MPI_Irecv" bytes="4" orank="15" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.6005e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000010" call="MPI_Irecv" bytes="4" orank="16" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.4945e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000400000012" call="MPI_Irecv" bytes="4" orank="18" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.1830e-03 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000017" call="MPI_Irecv" bytes="4" orank="23" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.9186e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000002B" call="MPI_Irecv" bytes="4" orank="43" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.9523e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001C000000000F" call="MPI_Irecv" bytes="7168" orank="15" region="0" commid="0" count="9718" tid="0" op="" dtype="" >1.2891e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000001C0000000017" call="MPI_Irecv" bytes="7168" orank="23" region="0" commid="0" count="9933" tid="0" op="" dtype="" >1.4989e-03 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000004000002B3" call="MPI_Irecv" bytes="4" orank="691" region="0" commid="0" count="3400" tid="0" op="" dtype="" >7.8988e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000000040000000F" call="MPI_Isend" bytes="4" orank="15" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.4265e-03 0.0000e+00 3.4094e-05</hent>
<hent key="02400100000000000000000400000010" call="MPI_Isend" bytes="4" orank="16" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.3150e-03 0.0000e+00 5.9843e-05</hent>
<hent key="02400100000000000000000400000012" call="MPI_Isend" bytes="4" orank="18" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.2820e-03 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000400000017" call="MPI_Isend" bytes="4" orank="23" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.4334e-03 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000040000002B" call="MPI_Isend" bytes="4" orank="43" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.5514e-02 4.0531e-06 3.2783e-04</hent>
<hent key="024001000000000000001C000000000F" call="MPI_Isend" bytes="7168" orank="15" region="0" commid="0" count="9227" tid="0" op="" dtype="" >1.6818e-02 9.5367e-07 2.4080e-05</hent>
<hent key="024001000000000000001C0000000017" call="MPI_Isend" bytes="7168" orank="23" region="0" commid="0" count="9822" tid="0" op="" dtype="" >1.9480e-02 9.5367e-07 1.8120e-05</hent>
<hent key="024001000000000000001C000000002B" call="MPI_Isend" bytes="7168" orank="43" region="0" commid="0" count="1" tid="0" op="" dtype="" >7.8678e-06 7.8678e-06 7.8678e-06</hent>
<hent key="024001000000000000000004000002B3" call="MPI_Isend" bytes="4" orank="691" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.9931e-02 3.8147e-06 6.6042e-05</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.6086e-04 4.6086e-04 4.6086e-04</hent>
<hent key="03800100000000000000020000000010" call="MPI_Irecv" bytes="512" orank="16" region="0" commid="0" count="3384" tid="0" op="" dtype="" >6.2466e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000020000000012" call="MPI_Irecv" bytes="512" orank="18" region="0" commid="0" count="3374" tid="0" op="" dtype="" >5.5075e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000005000000000F" call="MPI_Irecv" bytes="1280" orank="15" region="0" commid="0" count="132" tid="0" op="" dtype="" >3.1471e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000010" call="MPI_Irecv" bytes="1280" orank="16" region="0" commid="0" count="59" tid="0" op="" dtype="" >1.6212e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000012" call="MPI_Irecv" bytes="1280" orank="18" region="0" commid="0" count="51" tid="0" op="" dtype="" >1.5259e-05 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000050000000017" call="MPI_Irecv" bytes="1280" orank="23" region="0" commid="0" count="137" tid="0" op="" dtype="" >3.7670e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000002B" call="MPI_Irecv" bytes="1280" orank="43" region="0" commid="0" count="64" tid="0" op="" dtype="" >2.1219e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4865e+01 1.0014e-05 1.8463e-01</hent>
<hent key="038001000000000000000500000002B3" call="MPI_Irecv" bytes="1280" orank="691" region="0" commid="0" count="54" tid="0" op="" dtype="" >1.0014e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000020000000010" call="MPI_Isend" bytes="512" orank="16" region="0" commid="0" count="3350" tid="0" op="" dtype="" >2.3437e-03 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000020000000012" call="MPI_Isend" bytes="512" orank="18" region="0" commid="0" count="3370" tid="0" op="" dtype="" >1.7838e-03 0.0000e+00 1.0967e-05</hent>
<hent key="0380010000000000000008000000000F" call="MPI_Irecv" bytes="2048" orank="15" region="0" commid="0" count="6" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000017" call="MPI_Irecv" bytes="2048" orank="23" region="0" commid="0" count="8" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000002B" call="MPI_Irecv" bytes="2048" orank="43" region="0" commid="0" count="144" tid="0" op="" dtype="" >4.7922e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002B3" call="MPI_Irecv" bytes="2048" orank="691" region="0" commid="0" count="148" tid="0" op="" dtype="" >3.9339e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000005000000000F" call="MPI_Isend" bytes="1280" orank="15" region="0" commid="0" count="137" tid="0" op="" dtype="" >1.9264e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000050000000010" call="MPI_Isend" bytes="1280" orank="16" region="0" commid="0" count="55" tid="0" op="" dtype="" >2.0409e-04 2.8610e-06 1.0014e-05</hent>
<hent key="02400100000000000000050000000012" call="MPI_Isend" bytes="1280" orank="18" region="0" commid="0" count="52" tid="0" op="" dtype="" >1.0538e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000017" call="MPI_Isend" bytes="1280" orank="23" region="0" commid="0" count="141" tid="0" op="" dtype="" >2.1434e-04 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000005000000002B" call="MPI_Isend" bytes="1280" orank="43" region="0" commid="0" count="44" tid="0" op="" dtype="" >2.4676e-04 4.7684e-06 6.9141e-06</hent>
<hent key="024001000000000000000500000002B3" call="MPI_Isend" bytes="1280" orank="691" region="0" commid="0" count="58" tid="0" op="" dtype="" >2.7943e-04 3.8147e-06 5.9605e-06</hent>
</hash>
<internal rank="19" log_i="1724765674.526136" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="20" mpi_size="696" stamp_init="1724765564.465364" stamp_final="1724765674.528198" username="apac4" allocationname="unknown" flags="0" pid="1612137" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10063e+02" utime="8.63542e+01" stime="1.49713e+01" mtime="7.15858e+01" gflop="0.00000e+00" gbyte="3.77819e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15858e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c315c515c6155955c615c6153f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09925e+02" utime="8.63220e+01" stime="1.49604e+01" mtime="7.15858e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15858e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1847e+09" > 8.5371e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1927e+09" > 3.6868e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9746e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9779e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9481e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3869e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5740e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.0112e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0603e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3145e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3265e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="209" >
<hent key="02400100000000000000080000000000" call="MPI_Isend" bytes="2048" orank="0" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.0266e-05 1.9073e-06 8.1062e-06</hent>
<hent key="02400100000000000000080000000010" call="MPI_Isend" bytes="2048" orank="16" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.3590e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000080000000015" call="MPI_Isend" bytes="2048" orank="21" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.5020e-05 3.0994e-06 5.0068e-06</hent>
<hent key="0240010000000000000008000000002C" call="MPI_Isend" bytes="2048" orank="44" region="0" commid="0" count="142" tid="0" op="" dtype="" >9.8610e-04 4.7684e-06 2.2888e-05</hent>
<hent key="024001000000000000000800000002B4" call="MPI_Isend" bytes="2048" orank="692" region="0" commid="0" count="157" tid="0" op="" dtype="" >8.1825e-04 3.8147e-06 1.5974e-05</hent>
<hent key="038001000000000000000E0000000015" call="MPI_Irecv" bytes="3584" orank="21" region="0" commid="0" count="106" tid="0" op="" dtype="" >3.0041e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000017" call="MPI_Irecv" bytes="3584" orank="23" region="0" commid="0" count="205" tid="0" op="" dtype="" >7.1526e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E000000002C" call="MPI_Irecv" bytes="3584" orank="44" region="0" commid="0" count="140" tid="0" op="" dtype="" >1.2374e-04 0.0000e+00 1.0967e-05</hent>
<hent key="038001000000000000000E00000002B4" call="MPI_Irecv" bytes="3584" orank="692" region="0" commid="0" count="146" tid="0" op="" dtype="" >5.4359e-05 0.0000e+00 6.9141e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="024001000000000000000E0000000015" call="MPI_Isend" bytes="3584" orank="21" region="0" commid="0" count="187" tid="0" op="" dtype="" >4.8995e-04 9.5367e-07 8.1062e-06</hent>
<hent key="024001000000000000000E0000000017" call="MPI_Isend" bytes="3584" orank="23" region="0" commid="0" count="66" tid="0" op="" dtype="" >1.0896e-04 9.5367e-07 2.1458e-06</hent>
<hent key="024001000000000000000E000000002C" call="MPI_Isend" bytes="3584" orank="44" region="0" commid="0" count="133" tid="0" op="" dtype="" >9.1410e-04 5.0068e-06 1.9073e-05</hent>
<hent key="03800100000000000000028000000000" call="MPI_Irecv" bytes="640" orank="0" region="0" commid="0" count="405" tid="0" op="" dtype="" >8.5354e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000E00000002B4" call="MPI_Isend" bytes="3584" orank="692" region="0" commid="0" count="133" tid="0" op="" dtype="" >8.2660e-04 3.8147e-06 2.1219e-05</hent>
<hent key="03800100000000000000028000000010" call="MPI_Irecv" bytes="640" orank="16" region="0" commid="0" count="415" tid="0" op="" dtype="" >7.5817e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000028000000015" call="MPI_Irecv" bytes="640" orank="21" region="0" commid="0" count="385" tid="0" op="" dtype="" >8.3208e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000017" call="MPI_Irecv" bytes="640" orank="23" region="0" commid="0" count="370" tid="0" op="" dtype="" >1.5879e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000002C" call="MPI_Irecv" bytes="640" orank="44" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.4067e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000014000000002C" call="MPI_Irecv" bytes="5120" orank="44" region="0" commid="0" count="512" tid="0" op="" dtype="" >2.3532e-04 0.0000e+00 1.2875e-05</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002B4" call="MPI_Irecv" bytes="640" orank="692" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.9789e-05 0.0000e+00 1.0014e-05</hent>
<hent key="038001000000000000001400000002B4" call="MPI_Irecv" bytes="5120" orank="692" region="0" commid="0" count="582" tid="0" op="" dtype="" >2.6083e-04 0.0000e+00 2.8849e-05</hent>
<hent key="03800100000000000000A0000000002C" call="MPI_Irecv" bytes="40960" orank="44" region="0" commid="0" count="1711" tid="0" op="" dtype="" >8.4686e-04 0.0000e+00 2.5988e-05</hent>
<hent key="02400100000000000000028000000000" call="MPI_Isend" bytes="640" orank="0" region="0" commid="0" count="436" tid="0" op="" dtype="" >5.2166e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000028000000010" call="MPI_Isend" bytes="640" orank="16" region="0" commid="0" count="386" tid="0" op="" dtype="" >4.3654e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000028000000015" call="MPI_Isend" bytes="640" orank="21" region="0" commid="0" count="377" tid="0" op="" dtype="" >1.1322e-03 1.9073e-06 1.4067e-05</hent>
<hent key="02400100000000000000028000000017" call="MPI_Isend" bytes="640" orank="23" region="0" commid="0" count="374" tid="0" op="" dtype="" >6.1727e-04 9.5367e-07 5.9605e-06</hent>
<hent key="0240010000000000000002800000002C" call="MPI_Isend" bytes="640" orank="44" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.0991e-04 3.8147e-06 1.0014e-05</hent>
<hent key="03800100000000000000A000000002B4" call="MPI_Irecv" bytes="40960" orank="692" region="0" commid="0" count="1937" tid="0" op="" dtype="" >1.0297e-03 0.0000e+00 4.1008e-05</hent>
<hent key="0240010000000000000014000000002C" call="MPI_Isend" bytes="5120" orank="44" region="0" commid="0" count="475" tid="0" op="" dtype="" >3.8064e-03 1.9073e-06 9.9897e-05</hent>
<hent key="024001000000000000000280000002B4" call="MPI_Isend" bytes="640" orank="692" region="0" commid="0" count="23" tid="0" op="" dtype="" >9.6560e-05 3.8147e-06 5.0068e-06</hent>
<hent key="0380010000000000000080000000002C" call="MPI_Irecv" bytes="32768" orank="44" region="0" commid="0" count="10989" tid="0" op="" dtype="" >5.3799e-03 0.0000e+00 6.6996e-05</hent>
<hent key="024001000000000000001400000002B4" call="MPI_Isend" bytes="5120" orank="692" region="0" commid="0" count="392" tid="0" op="" dtype="" >1.9555e-03 9.5367e-07 4.9114e-05</hent>
<hent key="038001000000000000008000000002B4" call="MPI_Irecv" bytes="32768" orank="692" region="0" commid="0" count="10763" tid="0" op="" dtype="" >5.1746e-03 0.0000e+00 6.6996e-05</hent>
<hent key="03800100000000000000014000000000" call="MPI_Irecv" bytes="320" orank="0" region="0" commid="0" count="464" tid="0" op="" dtype="" >1.0300e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000010" call="MPI_Irecv" bytes="320" orank="16" region="0" commid="0" count="457" tid="0" op="" dtype="" >8.0347e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000015" call="MPI_Irecv" bytes="320" orank="21" region="0" commid="0" count="568" tid="0" op="" dtype="" >1.2016e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000014000000017" call="MPI_Irecv" bytes="320" orank="23" region="0" commid="0" count="577" tid="0" op="" dtype="" >2.5463e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000002C" call="MPI_Irecv" bytes="320" orank="44" region="0" commid="0" count="14" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000A0000000002C" call="MPI_Isend" bytes="40960" orank="44" region="0" commid="0" count="1460" tid="0" op="" dtype="" >3.8970e-02 9.0599e-06 9.4175e-05</hent>
<hent key="038001000000000000000140000002B4" call="MPI_Irecv" bytes="320" orank="692" region="0" commid="0" count="24" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A000000002B4" call="MPI_Isend" bytes="40960" orank="692" region="0" commid="0" count="1160" tid="0" op="" dtype="" >2.4066e-02 7.8678e-06 1.0991e-04</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >5.9746e+00 0.0000e+00 1.1067e-01</hent>
<hent key="0240010000000000000080000000002C" call="MPI_Isend" bytes="32768" orank="44" region="0" commid="0" count="11240" tid="0" op="" dtype="" >2.9384e-01 8.8215e-06 1.4520e-04</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-05 0.0000e+00 1.9073e-05</hent>
<hent key="024001000000000000008000000002B4" call="MPI_Isend" bytes="32768" orank="692" region="0" commid="0" count="11540" tid="0" op="" dtype="" >2.3607e-01 7.8678e-06 1.3614e-04</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000000" call="MPI_Isend" bytes="320" orank="0" region="0" commid="0" count="430" tid="0" op="" dtype="" >4.4131e-04 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000014000000010" call="MPI_Isend" bytes="320" orank="16" region="0" commid="0" count="460" tid="0" op="" dtype="" >4.1962e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000015" call="MPI_Isend" bytes="320" orank="21" region="0" commid="0" count="553" tid="0" op="" dtype="" >1.5023e-03 1.9073e-06 1.4067e-05</hent>
<hent key="02400100000000000000014000000017" call="MPI_Isend" bytes="320" orank="23" region="0" commid="0" count="593" tid="0" op="" dtype="" >7.5555e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000001400000002C" call="MPI_Isend" bytes="320" orank="44" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.4544e-04 4.0531e-06 1.4067e-05</hent>
<hent key="024001000000000000000140000002B4" call="MPI_Isend" bytes="320" orank="692" region="0" commid="0" count="20" tid="0" op="" dtype="" >9.2506e-05 2.8610e-06 1.3113e-05</hent>
<hent key="03800100000000000000000000000000" call="MPI_Irecv" bytes="0" orank="0" region="0" commid="0" count="347" tid="0" op="" dtype="" >7.1049e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000010" call="MPI_Irecv" bytes="0" orank="16" region="0" commid="0" count="335" tid="0" op="" dtype="" >5.9605e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000015" call="MPI_Irecv" bytes="0" orank="21" region="0" commid="0" count="527" tid="0" op="" dtype="" >1.1325e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000017" call="MPI_Irecv" bytes="0" orank="23" region="0" commid="0" count="527" tid="0" op="" dtype="" >1.7166e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000002C" call="MPI_Irecv" bytes="0" orank="44" region="0" commid="0" count="88" tid="0" op="" dtype="" >2.2173e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002B4" call="MPI_Irecv" bytes="0" orank="692" region="0" commid="0" count="81" tid="0" op="" dtype="" >2.1219e-05 0.0000e+00 4.0531e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.0742e-04 0.0000e+00 8.5831e-05</hent>
<hent key="03800100000000000000030000000000" call="MPI_Irecv" bytes="768" orank="0" region="0" commid="0" count="816" tid="0" op="" dtype="" >1.4448e-04 0.0000e+00 2.6941e-05</hent>
<hent key="03800100000000000000030000000010" call="MPI_Irecv" bytes="768" orank="16" region="0" commid="0" count="776" tid="0" op="" dtype="" >1.3947e-04 0.0000e+00 3.0994e-06</hent>
<hent key="02400100000000000000000000000000" call="MPI_Isend" bytes="0" orank="0" region="0" commid="0" count="353" tid="0" op="" dtype="" >2.4509e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000010" call="MPI_Isend" bytes="0" orank="16" region="0" commid="0" count="341" tid="0" op="" dtype="" >1.9169e-04 0.0000e+00 3.0994e-06</hent>
<hent key="02400100000000000000000000000015" call="MPI_Isend" bytes="0" orank="21" region="0" commid="0" count="527" tid="0" op="" dtype="" >1.0166e-03 9.5367e-07 3.1948e-05</hent>
<hent key="02400100000000000000000000000017" call="MPI_Isend" bytes="0" orank="23" region="0" commid="0" count="507" tid="0" op="" dtype="" >4.1890e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000000000000002C" call="MPI_Isend" bytes="0" orank="44" region="0" commid="0" count="80" tid="0" op="" dtype="" >3.9244e-04 9.5367e-07 2.0981e-05</hent>
<hent key="024001000000000000000000000002B4" call="MPI_Isend" bytes="0" orank="692" region="0" commid="0" count="88" tid="0" op="" dtype="" >3.3927e-04 2.8610e-06 1.0967e-05</hent>
<hent key="03800100000000000000060000000000" call="MPI_Irecv" bytes="1536" orank="0" region="0" commid="0" count="67" tid="0" op="" dtype="" >1.8120e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000010" call="MPI_Irecv" bytes="1536" orank="16" region="0" commid="0" count="50" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000015" call="MPI_Irecv" bytes="1536" orank="21" region="0" commid="0" count="10" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000017" call="MPI_Irecv" bytes="1536" orank="23" region="0" commid="0" count="15" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000006000000002C" call="MPI_Irecv" bytes="1536" orank="44" region="0" commid="0" count="70" tid="0" op="" dtype="" >4.4584e-05 0.0000e+00 8.1062e-06</hent>
<hent key="038001000000000000000600000002B4" call="MPI_Irecv" bytes="1536" orank="692" region="0" commid="0" count="66" tid="0" op="" dtype="" >1.5020e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000030000000000" call="MPI_Isend" bytes="768" orank="0" region="0" commid="0" count="638" tid="0" op="" dtype="" >3.0541e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000030000000010" call="MPI_Isend" bytes="768" orank="16" region="0" commid="0" count="846" tid="0" op="" dtype="" >4.2892e-04 0.0000e+00 2.4080e-05</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000015" call="MPI_Irecv" bytes="448" orank="21" region="0" commid="0" count="28" tid="0" op="" dtype="" >7.3910e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001C000000017" call="MPI_Irecv" bytes="448" orank="23" region="0" commid="0" count="54" tid="0" op="" dtype="" >5.4836e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000000" call="MPI_Isend" bytes="1536" orank="0" region="0" commid="0" count="52" tid="0" op="" dtype="" >7.6056e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000010" call="MPI_Isend" bytes="1536" orank="16" region="0" commid="0" count="61" tid="0" op="" dtype="" >8.9884e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000015" call="MPI_Isend" bytes="1536" orank="21" region="0" commid="0" count="17" tid="0" op="" dtype="" >6.3419e-05 2.8610e-06 4.0531e-06</hent>
<hent key="02400100000000000000060000000017" call="MPI_Isend" bytes="1536" orank="23" region="0" commid="0" count="10" tid="0" op="" dtype="" >2.4557e-05 1.9073e-06 5.9605e-06</hent>
<hent key="0240010000000000000006000000002C" call="MPI_Isend" bytes="1536" orank="44" region="0" commid="0" count="74" tid="0" op="" dtype="" >4.7159e-04 4.0531e-06 1.6928e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002B4" call="MPI_Isend" bytes="1536" orank="692" region="0" commid="0" count="52" tid="0" op="" dtype="" >2.9325e-04 3.8147e-06 1.3113e-05</hent>
<hent key="038001000000000000000C000000002C" call="MPI_Irecv" bytes="3072" orank="44" region="0" commid="0" count="340" tid="0" op="" dtype="" >2.2078e-04 0.0000e+00 1.0967e-05</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002B4" call="MPI_Irecv" bytes="3072" orank="692" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.0777e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C000000015" call="MPI_Isend" bytes="448" orank="21" region="0" commid="0" count="48" tid="0" op="" dtype="" >2.3365e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C000000017" call="MPI_Isend" bytes="448" orank="23" region="0" commid="0" count="18" tid="0" op="" dtype="" >1.5259e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C000000002C" call="MPI_Isend" bytes="3072" orank="44" region="0" commid="0" count="371" tid="0" op="" dtype="" >2.7297e-03 4.7684e-06 2.8133e-05</hent>
<hent key="024001000000000000000C00000002B4" call="MPI_Isend" bytes="3072" orank="692" region="0" commid="0" count="362" tid="0" op="" dtype="" >2.2452e-03 3.8147e-06 2.7895e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.0173e-03 3.3212e-04 3.4714e-04</hent>
<hent key="03800100000000000000038000000000" call="MPI_Irecv" bytes="896" orank="0" region="0" commid="0" count="2829" tid="0" op="" dtype="" >4.3392e-04 0.0000e+00 1.8120e-05</hent>
<hent key="03800100000000000000038000000010" call="MPI_Irecv" bytes="896" orank="16" region="0" commid="0" count="2911" tid="0" op="" dtype="" >6.7496e-04 0.0000e+00 3.6955e-05</hent>
<hent key="03800100000000000000038000000015" call="MPI_Irecv" bytes="896" orank="21" region="0" commid="0" count="158" tid="0" op="" dtype="" >3.7193e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000017" call="MPI_Irecv" bytes="896" orank="23" region="0" commid="0" count="150" tid="0" op="" dtype="" >6.8188e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000002C" call="MPI_Irecv" bytes="896" orank="44" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.1683e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002B4" call="MPI_Irecv" bytes="896" orank="692" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.6689e-05 0.0000e+00 4.0531e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.6928e-05 1.6928e-05 1.6928e-05</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000180000000000" call="MPI_Irecv" bytes="6144" orank="0" region="0" commid="0" count="3126" tid="0" op="" dtype="" >3.4213e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000180000000010" call="MPI_Irecv" bytes="6144" orank="16" region="0" commid="0" count="2895" tid="0" op="" dtype="" >5.0354e-04 0.0000e+00 1.2159e-05</hent>
<hent key="0380010000000000000018000000002C" call="MPI_Irecv" bytes="6144" orank="44" region="0" commid="0" count="14" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001800000002B4" call="MPI_Irecv" bytes="6144" orank="692" region="0" commid="0" count="20" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000038000000000" call="MPI_Isend" bytes="896" orank="0" region="0" commid="0" count="3031" tid="0" op="" dtype="" >1.8399e-03 0.0000e+00 1.8120e-05</hent>
<hent key="02400100000000000000038000000010" call="MPI_Isend" bytes="896" orank="16" region="0" commid="0" count="2843" tid="0" op="" dtype="" >1.7049e-03 0.0000e+00 4.8161e-05</hent>
<hent key="02400100000000000000038000000015" call="MPI_Isend" bytes="896" orank="21" region="0" commid="0" count="175" tid="0" op="" dtype="" >6.2251e-04 1.9073e-06 2.1219e-05</hent>
<hent key="02400100000000000000038000000017" call="MPI_Isend" bytes="896" orank="23" region="0" commid="0" count="163" tid="0" op="" dtype="" >3.0160e-04 9.5367e-07 6.1989e-06</hent>
<hent key="0240010000000000000003800000002C" call="MPI_Isend" bytes="896" orank="44" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.5879e-04 3.8147e-06 1.3113e-05</hent>
<hent key="024001000000000000000380000002B4" call="MPI_Isend" bytes="896" orank="692" region="0" commid="0" count="26" tid="0" op="" dtype="" >1.3375e-04 2.8610e-06 1.0967e-05</hent>
<hent key="02400100000000000000180000000000" call="MPI_Isend" bytes="6144" orank="0" region="0" commid="0" count="2276" tid="0" op="" dtype="" >4.6391e-03 9.5367e-07 1.9073e-05</hent>
<hent key="02400100000000000000180000000010" call="MPI_Isend" bytes="6144" orank="16" region="0" commid="0" count="3126" tid="0" op="" dtype="" >5.7971e-03 9.5367e-07 3.6001e-05</hent>
<hent key="0240010000000000000018000000002C" call="MPI_Isend" bytes="6144" orank="44" region="0" commid="0" count="10" tid="0" op="" dtype="" >8.9407e-05 6.9141e-06 2.0981e-05</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.4094e-05 3.4094e-05 3.4094e-05</hent>
<hent key="024001000000000000001800000002B4" call="MPI_Isend" bytes="6144" orank="692" region="0" commid="0" count="7" tid="0" op="" dtype="" >5.4121e-05 5.0068e-06 1.4067e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.6758e+00 1.0014e-05 1.2664e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.8310e-04 8.8310e-04 8.8310e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0603e-02 1.0603e-02 1.0603e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >4.7221e-03 4.7221e-03 4.7221e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.0129e-01 3.3371e-03 1.9339e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >6.0391e-04 6.0391e-04 6.0391e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5740e+00 4.1604e-04 2.4732e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.9481e-03 1.9073e-06 1.2131e-03</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000070000000000" call="MPI_Irecv" bytes="1792" orank="0" region="0" commid="0" count="20" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000010" call="MPI_Irecv" bytes="1792" orank="16" region="0" commid="0" count="17" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000015" call="MPI_Irecv" bytes="1792" orank="21" region="0" commid="0" count="4" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000070000000017" call="MPI_Irecv" bytes="1792" orank="23" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="0380010000000000000007000000002C" call="MPI_Irecv" bytes="1792" orank="44" region="0" commid="0" count="121" tid="0" op="" dtype="" >7.0572e-05 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000700000002B4" call="MPI_Irecv" bytes="1792" orank="692" region="0" commid="0" count="104" tid="0" op="" dtype="" >5.9128e-05 0.0000e+00 1.6928e-05</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >9.3937e-05 0.0000e+00 7.3910e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9778e+01 0.0000e+00 3.8412e+01</hent>
<hent key="038001000000000000000A0000000000" call="MPI_Irecv" bytes="2560" orank="0" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000010" call="MPI_Irecv" bytes="2560" orank="16" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A000000002C" call="MPI_Irecv" bytes="2560" orank="44" region="0" commid="0" count="381" tid="0" op="" dtype="" >2.4557e-04 0.0000e+00 9.0599e-06</hent>
<hent key="038001000000000000000A00000002B4" call="MPI_Irecv" bytes="2560" orank="692" region="0" commid="0" count="382" tid="0" op="" dtype="" >1.4091e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000070000000000" call="MPI_Isend" bytes="1792" orank="0" region="0" commid="0" count="21" tid="0" op="" dtype="" >3.8385e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000010" call="MPI_Isend" bytes="1792" orank="16" region="0" commid="0" count="19" tid="0" op="" dtype="" >3.1233e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000015" call="MPI_Isend" bytes="1792" orank="21" region="0" commid="0" count="2" tid="0" op="" dtype="" >7.1526e-06 3.0994e-06 4.0531e-06</hent>
<hent key="02400100000000000000070000000017" call="MPI_Isend" bytes="1792" orank="23" region="0" commid="0" count="2" tid="0" op="" dtype="" >4.7684e-06 1.9073e-06 2.8610e-06</hent>
<hent key="0240010000000000000007000000002C" call="MPI_Isend" bytes="1792" orank="44" region="0" commid="0" count="116" tid="0" op="" dtype="" >8.1420e-04 4.0531e-06 3.3855e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3145e-02 8.3145e-02 8.3145e-02</hent>
<hent key="024001000000000000000700000002B4" call="MPI_Isend" bytes="1792" orank="692" region="0" commid="0" count="105" tid="0" op="" dtype="" >5.6481e-04 3.8147e-06 2.0981e-05</hent>
<hent key="024001000000000000000A0000000000" call="MPI_Isend" bytes="2560" orank="0" region="0" commid="0" count="5" tid="0" op="" dtype="" >8.8215e-06 9.5367e-07 2.1458e-06</hent>
<hent key="024001000000000000000A0000000010" call="MPI_Isend" bytes="2560" orank="16" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000A0000000017" call="MPI_Isend" bytes="2560" orank="23" region="0" commid="0" count="1" tid="0" op="" dtype="" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="024001000000000000000A000000002C" call="MPI_Isend" bytes="2560" orank="44" region="0" commid="0" count="356" tid="0" op="" dtype="" >2.6374e-03 4.7684e-06 2.5034e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.6086e-04 4.2915e-05 2.1386e-04</hent>
<hent key="024001000000000000000A00000002B4" call="MPI_Isend" bytes="2560" orank="692" region="0" commid="0" count="379" tid="0" op="" dtype="" >2.0363e-03 3.8147e-06 1.8120e-05</hent>
<hent key="03800100000000000000100000000015" call="MPI_Irecv" bytes="4096" orank="21" region="0" commid="0" count="12594" tid="0" op="" dtype="" >2.6951e-03 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000100000000017" call="MPI_Irecv" bytes="4096" orank="23" region="0" commid="0" count="12495" tid="0" op="" dtype="" >4.4186e-03 0.0000e+00 2.2173e-05</hent>
<hent key="0380010000000000000010000000002C" call="MPI_Irecv" bytes="4096" orank="44" region="0" commid="0" count="3160" tid="0" op="" dtype="" >1.2197e-03 0.0000e+00 2.9087e-05</hent>
<hent key="038001000000000000001000000002B4" call="MPI_Irecv" bytes="4096" orank="692" region="0" commid="0" count="3110" tid="0" op="" dtype="" >1.8485e-03 0.0000e+00 6.1989e-05</hent>
<hent key="02400100000000000000100000000015" call="MPI_Isend" bytes="4096" orank="21" region="0" commid="0" count="12513" tid="0" op="" dtype="" >3.3489e-02 0.0000e+00 1.8835e-05</hent>
<hent key="02400100000000000000100000000017" call="MPI_Isend" bytes="4096" orank="23" region="0" commid="0" count="12634" tid="0" op="" dtype="" >2.2514e-02 0.0000e+00 2.5034e-05</hent>
<hent key="0240010000000000000010000000002C" call="MPI_Isend" bytes="4096" orank="44" region="0" commid="0" count="3219" tid="0" op="" dtype="" >2.4208e-02 1.9073e-06 1.2302e-04</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.5938e-03 5.6887e-04 1.0250e-03</hent>
<hent key="024001000000000000001000000002B4" call="MPI_Isend" bytes="4096" orank="692" region="0" commid="0" count="3300" tid="0" op="" dtype="" >1.5984e-02 9.5367e-07 8.0109e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.1008e-05 4.1008e-05 4.1008e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.4492e-03 4.6015e-04 1.1921e-03</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.0599e-06 4.0531e-06 5.0068e-06</hent>
<hent key="03800100000000000000000400000000" call="MPI_Irecv" bytes="4" orank="0" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.8113e-04 0.0000e+00 3.8862e-05</hent>
<hent key="03800100000000000000000400000010" call="MPI_Irecv" bytes="4" orank="16" region="0" commid="0" count="3400" tid="0" op="" dtype="" >4.9496e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000400000015" call="MPI_Irecv" bytes="4" orank="21" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.4884e-04 0.0000e+00 1.5974e-05</hent>
<hent key="03800100000000000000000400000017" call="MPI_Irecv" bytes="4" orank="23" region="0" commid="0" count="3400" tid="0" op="" dtype="" >8.9169e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000040000002C" call="MPI_Irecv" bytes="4" orank="44" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.0562e-03 0.0000e+00 4.3869e-05</hent>
<hent key="038001000000000000001C0000000000" call="MPI_Irecv" bytes="7168" orank="0" region="0" commid="0" count="9574" tid="0" op="" dtype="" >1.0729e-03 0.0000e+00 1.7166e-05</hent>
<hent key="038001000000000000001C0000000010" call="MPI_Irecv" bytes="7168" orank="16" region="0" commid="0" count="9805" tid="0" op="" dtype="" >1.7288e-03 0.0000e+00 4.7207e-05</hent>
<hent key="038001000000000000000004000002B4" call="MPI_Irecv" bytes="4" orank="692" region="0" commid="0" count="3400" tid="0" op="" dtype="" >8.0156e-04 0.0000e+00 2.0027e-05</hent>
<hent key="02400100000000000000000400000000" call="MPI_Isend" bytes="4" orank="0" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.8475e-03 0.0000e+00 3.5048e-05</hent>
<hent key="02400100000000000000000400000010" call="MPI_Isend" bytes="4" orank="16" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.6022e-03 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000000400000015" call="MPI_Isend" bytes="4" orank="21" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.2017e-03 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000000400000017" call="MPI_Isend" bytes="4" orank="23" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.6768e-03 0.0000e+00 5.8889e-05</hent>
<hent key="0240010000000000000000040000002C" call="MPI_Isend" bytes="4" orank="44" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.6778e-02 4.7684e-06 4.1413e-04</hent>
<hent key="024001000000000000001C0000000000" call="MPI_Isend" bytes="7168" orank="0" region="0" commid="0" count="10424" tid="0" op="" dtype="" >2.1840e-02 9.5367e-07 3.4094e-05</hent>
<hent key="024001000000000000001C0000000010" call="MPI_Isend" bytes="7168" orank="16" region="0" commid="0" count="9574" tid="0" op="" dtype="" >1.8629e-02 9.5367e-07 6.7949e-05</hent>
<hent key="024001000000000000001C000000002C" call="MPI_Isend" bytes="7168" orank="44" region="0" commid="0" count="1" tid="0" op="" dtype="" >8.1062e-06 8.1062e-06 8.1062e-06</hent>
<hent key="024001000000000000000004000002B4" call="MPI_Isend" bytes="4" orank="692" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.6344e-02 3.8147e-06 9.6083e-05</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.8780e-04 4.8780e-04 4.8780e-04</hent>
<hent key="03800100000000000000020000000015" call="MPI_Irecv" bytes="512" orank="21" region="0" commid="0" count="3372" tid="0" op="" dtype="" >7.3957e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000020000000017" call="MPI_Irecv" bytes="512" orank="23" region="0" commid="0" count="3346" tid="0" op="" dtype="" >5.5289e-04 0.0000e+00 1.8835e-05</hent>
<hent key="03800100000000000000050000000000" call="MPI_Irecv" bytes="1280" orank="0" region="0" commid="0" count="149" tid="0" op="" dtype="" >3.0041e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000010" call="MPI_Irecv" bytes="1280" orank="16" region="0" commid="0" count="132" tid="0" op="" dtype="" >2.5034e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000015" call="MPI_Irecv" bytes="1280" orank="21" region="0" commid="0" count="48" tid="0" op="" dtype="" >1.5974e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000017" call="MPI_Irecv" bytes="1280" orank="23" region="0" commid="0" count="59" tid="0" op="" dtype="" >2.2888e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000002C" call="MPI_Irecv" bytes="1280" orank="44" region="0" commid="0" count="51" tid="0" op="" dtype="" >2.4557e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4382e+01 1.0014e-05 1.8444e-01</hent>
<hent key="038001000000000000000500000002B4" call="MPI_Irecv" bytes="1280" orank="692" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.6689e-05 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000020000000015" call="MPI_Isend" bytes="512" orank="21" region="0" commid="0" count="3352" tid="0" op="" dtype="" >2.3088e-03 0.0000e+00 1.7166e-05</hent>
<hent key="02400100000000000000020000000017" call="MPI_Isend" bytes="512" orank="23" region="0" commid="0" count="3382" tid="0" op="" dtype="" >1.9941e-03 0.0000e+00 3.6001e-05</hent>
<hent key="03800100000000000000080000000000" call="MPI_Irecv" bytes="2048" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000080000000010" call="MPI_Irecv" bytes="2048" orank="16" region="0" commid="0" count="5" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000017" call="MPI_Irecv" bytes="2048" orank="23" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000008000000002C" call="MPI_Irecv" bytes="2048" orank="44" region="0" commid="0" count="152" tid="0" op="" dtype="" >1.1563e-04 0.0000e+00 1.5020e-05</hent>
<hent key="038001000000000000000800000002B4" call="MPI_Irecv" bytes="2048" orank="692" region="0" commid="0" count="130" tid="0" op="" dtype="" >5.0306e-05 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000050000000000" call="MPI_Isend" bytes="1280" orank="0" region="0" commid="0" count="127" tid="0" op="" dtype="" >1.8907e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000050000000010" call="MPI_Isend" bytes="1280" orank="16" region="0" commid="0" count="135" tid="0" op="" dtype="" >1.9979e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000050000000015" call="MPI_Isend" bytes="1280" orank="21" region="0" commid="0" count="45" tid="0" op="" dtype="" >1.6403e-04 2.8610e-06 8.1062e-06</hent>
<hent key="02400100000000000000050000000017" call="MPI_Isend" bytes="1280" orank="23" region="0" commid="0" count="50" tid="0" op="" dtype="" >1.1396e-04 9.5367e-07 1.1921e-05</hent>
<hent key="0240010000000000000005000000002C" call="MPI_Isend" bytes="1280" orank="44" region="0" commid="0" count="45" tid="0" op="" dtype="" >3.0446e-04 4.0531e-06 2.3127e-05</hent>
<hent key="024001000000000000000500000002B4" call="MPI_Isend" bytes="1280" orank="692" region="0" commid="0" count="56" tid="0" op="" dtype="" >2.8372e-04 3.0994e-06 1.2875e-05</hent>
</hash>
<internal rank="20" log_i="1724765674.528198" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="21" mpi_size="696" stamp_init="1724765564.465386" stamp_final="1724765674.527187" username="apac4" allocationname="unknown" flags="0" pid="1612138" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10062e+02" utime="8.97262e+01" stime="1.37834e+01" mtime="7.26583e+01" gflop="0.00000e+00" gbyte="3.75187e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.26583e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000033143b5633143214af" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09925e+02" utime="8.96932e+01" stime="1.37735e+01" mtime="7.26583e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.26583e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2071e+09" > 6.0029e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1959e+09" > 2.9317e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8356e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9781e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.8161e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5740e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.3428e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0585e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3049e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4737e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="210" >
<hent key="02400100000000000000080000000001" call="MPI_Isend" bytes="2048" orank="1" region="0" commid="0" count="4" tid="0" op="" dtype="" >5.7220e-06 9.5367e-07 1.9073e-06</hent>
<hent key="02400100000000000000080000000011" call="MPI_Isend" bytes="2048" orank="17" region="0" commid="0" count="4" tid="0" op="" dtype="" >8.1062e-06 1.9073e-06 2.1458e-06</hent>
<hent key="02400100000000000000080000000016" call="MPI_Isend" bytes="2048" orank="22" region="0" commid="0" count="1" tid="0" op="" dtype="" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="0240010000000000000008000000002D" call="MPI_Isend" bytes="2048" orank="45" region="0" commid="0" count="158" tid="0" op="" dtype="" >8.4066e-04 3.8147e-06 6.9141e-06</hent>
<hent key="024001000000000000000800000002B5" call="MPI_Isend" bytes="2048" orank="693" region="0" commid="0" count="156" tid="0" op="" dtype="" >7.1025e-04 3.8147e-06 5.0068e-06</hent>
<hent key="038001000000000000000E0000000014" call="MPI_Irecv" bytes="3584" orank="20" region="0" commid="0" count="187" tid="0" op="" dtype="" >6.1989e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000016" call="MPI_Irecv" bytes="3584" orank="22" region="0" commid="0" count="216" tid="0" op="" dtype="" >3.6240e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E000000002D" call="MPI_Irecv" bytes="3584" orank="45" region="0" commid="0" count="145" tid="0" op="" dtype="" >3.6716e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E00000002B5" call="MPI_Irecv" bytes="3584" orank="693" region="0" commid="0" count="145" tid="0" op="" dtype="" >5.9366e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="024001000000000000000E0000000014" call="MPI_Isend" bytes="3584" orank="20" region="0" commid="0" count="106" tid="0" op="" dtype="" >1.6737e-04 9.5367e-07 3.0994e-06</hent>
<hent key="024001000000000000000E0000000016" call="MPI_Isend" bytes="3584" orank="22" region="0" commid="0" count="67" tid="0" op="" dtype="" >1.7715e-04 9.5367e-07 1.9073e-05</hent>
<hent key="024001000000000000000E000000002D" call="MPI_Isend" bytes="3584" orank="45" region="0" commid="0" count="160" tid="0" op="" dtype="" >9.8181e-04 4.7684e-06 1.3113e-05</hent>
<hent key="03800100000000000000028000000001" call="MPI_Irecv" bytes="640" orank="1" region="0" commid="0" count="388" tid="0" op="" dtype="" >6.6042e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000E00000002B5" call="MPI_Isend" bytes="3584" orank="693" region="0" commid="0" count="138" tid="0" op="" dtype="" >7.0429e-04 3.8147e-06 6.1989e-06</hent>
<hent key="03800100000000000000028000000011" call="MPI_Irecv" bytes="640" orank="17" region="0" commid="0" count="442" tid="0" op="" dtype="" >1.6117e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000028000000014" call="MPI_Irecv" bytes="640" orank="20" region="0" commid="0" count="377" tid="0" op="" dtype="" >1.5068e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000028000000016" call="MPI_Irecv" bytes="640" orank="22" region="0" commid="0" count="355" tid="0" op="" dtype="" >6.9857e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000002D" call="MPI_Irecv" bytes="640" orank="45" region="0" commid="0" count="18" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000014000000002D" call="MPI_Irecv" bytes="5120" orank="45" region="0" commid="0" count="615" tid="0" op="" dtype="" >9.9421e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002B5" call="MPI_Irecv" bytes="640" orank="693" region="0" commid="0" count="23" tid="0" op="" dtype="" >6.6757e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000001400000002B5" call="MPI_Irecv" bytes="5120" orank="693" region="0" commid="0" count="542" tid="0" op="" dtype="" >1.1635e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000A0000000002D" call="MPI_Irecv" bytes="40960" orank="45" region="0" commid="0" count="2170" tid="0" op="" dtype="" >9.8395e-04 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000028000000001" call="MPI_Isend" bytes="640" orank="1" region="0" commid="0" count="417" tid="0" op="" dtype="" >4.4179e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000028000000011" call="MPI_Isend" bytes="640" orank="17" region="0" commid="0" count="428" tid="0" op="" dtype="" >4.9925e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000028000000014" call="MPI_Isend" bytes="640" orank="20" region="0" commid="0" count="385" tid="0" op="" dtype="" >5.4789e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000028000000016" call="MPI_Isend" bytes="640" orank="22" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.1482e-03 1.9073e-06 1.5020e-05</hent>
<hent key="0240010000000000000002800000002D" call="MPI_Isend" bytes="640" orank="45" region="0" commid="0" count="24" tid="0" op="" dtype="" >1.0753e-04 3.8147e-06 5.0068e-06</hent>
<hent key="03800100000000000000A000000002B5" call="MPI_Irecv" bytes="40960" orank="693" region="0" commid="0" count="1836" tid="0" op="" dtype="" >3.3259e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000014000000002D" call="MPI_Isend" bytes="5120" orank="45" region="0" commid="0" count="714" tid="0" op="" dtype="" >2.8241e-03 1.9073e-06 1.5020e-05</hent>
<hent key="024001000000000000000280000002B5" call="MPI_Isend" bytes="640" orank="693" region="0" commid="0" count="21" tid="0" op="" dtype="" >8.8930e-05 3.8147e-06 5.0068e-06</hent>
<hent key="0380010000000000000080000000002D" call="MPI_Irecv" bytes="32768" orank="45" region="0" commid="0" count="10530" tid="0" op="" dtype="" >5.8227e-03 0.0000e+00 1.2159e-05</hent>
<hent key="024001000000000000001400000002B5" call="MPI_Isend" bytes="5120" orank="693" region="0" commid="0" count="793" tid="0" op="" dtype="" >2.7280e-03 9.5367e-07 9.0599e-06</hent>
<hent key="038001000000000000008000000002B5" call="MPI_Irecv" bytes="32768" orank="693" region="0" commid="0" count="10864" tid="0" op="" dtype="" >2.0680e-03 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000014000000001" call="MPI_Irecv" bytes="320" orank="1" region="0" commid="0" count="446" tid="0" op="" dtype="" >9.0837e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000011" call="MPI_Irecv" bytes="320" orank="17" region="0" commid="0" count="430" tid="0" op="" dtype="" >1.6999e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000014000000014" call="MPI_Irecv" bytes="320" orank="20" region="0" commid="0" count="553" tid="0" op="" dtype="" >1.9455e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000016" call="MPI_Irecv" bytes="320" orank="22" region="0" commid="0" count="522" tid="0" op="" dtype="" >1.2183e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000002D" call="MPI_Irecv" bytes="320" orank="45" region="0" commid="0" count="23" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A0000000002D" call="MPI_Isend" bytes="40960" orank="45" region="0" commid="0" count="2474" tid="0" op="" dtype="" >4.7427e-02 9.0599e-06 4.1962e-05</hent>
<hent key="038001000000000000000140000002B5" call="MPI_Irecv" bytes="320" orank="693" region="0" commid="0" count="27" tid="0" op="" dtype="" >6.6757e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000A000000002B5" call="MPI_Isend" bytes="40960" orank="693" region="0" commid="0" count="2729" tid="0" op="" dtype="" >2.9464e-02 6.9141e-06 1.9073e-05</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >5.8356e+00 0.0000e+00 1.1067e-01</hent>
<hent key="0240010000000000000080000000002D" call="MPI_Isend" bytes="32768" orank="45" region="0" commid="0" count="10226" tid="0" op="" dtype="" >2.0383e-01 7.8678e-06 1.0705e-04</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.0027e-05 9.5367e-07 1.8120e-05</hent>
<hent key="024001000000000000008000000002B5" call="MPI_Isend" bytes="32768" orank="693" region="0" commid="0" count="9971" tid="0" op="" dtype="" >1.0687e-01 5.9605e-06 1.2517e-04</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000014000000001" call="MPI_Isend" bytes="320" orank="1" region="0" commid="0" count="411" tid="0" op="" dtype="" >3.8791e-04 0.0000e+00 1.2159e-05</hent>
<hent key="02400100000000000000014000000011" call="MPI_Isend" bytes="320" orank="17" region="0" commid="0" count="429" tid="0" op="" dtype="" >4.0984e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000014" call="MPI_Isend" bytes="320" orank="20" region="0" commid="0" count="568" tid="0" op="" dtype="" >6.3348e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000014000000016" call="MPI_Isend" bytes="320" orank="22" region="0" commid="0" count="565" tid="0" op="" dtype="" >1.5900e-03 1.9073e-06 8.8215e-06</hent>
<hent key="0240010000000000000001400000002D" call="MPI_Isend" bytes="320" orank="45" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.2851e-04 3.0994e-06 5.0068e-06</hent>
<hent key="024001000000000000000140000002B5" call="MPI_Isend" bytes="320" orank="693" region="0" commid="0" count="22" tid="0" op="" dtype="" >9.3699e-05 3.0994e-06 5.0068e-06</hent>
<hent key="03800100000000000000000000000001" call="MPI_Irecv" bytes="0" orank="1" region="0" commid="0" count="338" tid="0" op="" dtype="" >6.3181e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000011" call="MPI_Irecv" bytes="0" orank="17" region="0" commid="0" count="324" tid="0" op="" dtype="" >9.5129e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000014" call="MPI_Irecv" bytes="0" orank="20" region="0" commid="0" count="527" tid="0" op="" dtype="" >1.6689e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000016" call="MPI_Irecv" bytes="0" orank="22" region="0" commid="0" count="574" tid="0" op="" dtype="" >1.4043e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000002D" call="MPI_Irecv" bytes="0" orank="45" region="0" commid="0" count="91" tid="0" op="" dtype="" >1.6928e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002B5" call="MPI_Irecv" bytes="0" orank="693" region="0" commid="0" count="88" tid="0" op="" dtype="" >1.9312e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.0719e-04 0.0000e+00 8.1778e-05</hent>
<hent key="03800100000000000000030000000001" call="MPI_Irecv" bytes="768" orank="1" region="0" commid="0" count="818" tid="0" op="" dtype="" >1.2064e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000030000000011" call="MPI_Irecv" bytes="768" orank="17" region="0" commid="0" count="744" tid="0" op="" dtype="" >1.5664e-04 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000002000000002B5" call="MPI_Isend" bytes="8192" orank="693" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.0599e-06 9.0599e-06 9.0599e-06</hent>
<hent key="02400100000000000000000000000001" call="MPI_Isend" bytes="0" orank="1" region="0" commid="0" count="337" tid="0" op="" dtype="" >1.9789e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000011" call="MPI_Isend" bytes="0" orank="17" region="0" commid="0" count="329" tid="0" op="" dtype="" >2.3317e-04 0.0000e+00 1.2875e-05</hent>
<hent key="02400100000000000000000000000014" call="MPI_Isend" bytes="0" orank="20" region="0" commid="0" count="527" tid="0" op="" dtype="" >3.7479e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000000000000016" call="MPI_Isend" bytes="0" orank="22" region="0" commid="0" count="535" tid="0" op="" dtype="" >1.1325e-03 9.5367e-07 5.5075e-05</hent>
<hent key="0240010000000000000000000000002D" call="MPI_Isend" bytes="0" orank="45" region="0" commid="0" count="86" tid="0" op="" dtype="" >3.2330e-04 9.5367e-07 5.0068e-06</hent>
<hent key="024001000000000000000000000002B5" call="MPI_Isend" bytes="0" orank="693" region="0" commid="0" count="86" tid="0" op="" dtype="" >3.2091e-04 1.9073e-06 5.0068e-06</hent>
<hent key="03800100000000000000060000000001" call="MPI_Irecv" bytes="1536" orank="1" region="0" commid="0" count="64" tid="0" op="" dtype="" >1.0490e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000011" call="MPI_Irecv" bytes="1536" orank="17" region="0" commid="0" count="58" tid="0" op="" dtype="" >2.0266e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000014" call="MPI_Irecv" bytes="1536" orank="20" region="0" commid="0" count="17" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000060000000016" call="MPI_Irecv" bytes="1536" orank="22" region="0" commid="0" count="15" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000006000000002D" call="MPI_Irecv" bytes="1536" orank="45" region="0" commid="0" count="63" tid="0" op="" dtype="" >1.4544e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002B5" call="MPI_Irecv" bytes="1536" orank="693" region="0" commid="0" count="70" tid="0" op="" dtype="" >2.7895e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000030000000001" call="MPI_Isend" bytes="768" orank="1" region="0" commid="0" count="694" tid="0" op="" dtype="" >3.4618e-04 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000030000000011" call="MPI_Isend" bytes="768" orank="17" region="0" commid="0" count="608" tid="0" op="" dtype="" >3.2592e-04 0.0000e+00 6.9141e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 1.9073e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C000000014" call="MPI_Irecv" bytes="448" orank="20" region="0" commid="0" count="48" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C000000016" call="MPI_Irecv" bytes="448" orank="22" region="0" commid="0" count="62" tid="0" op="" dtype="" >1.5736e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000001" call="MPI_Isend" bytes="1536" orank="1" region="0" commid="0" count="56" tid="0" op="" dtype="" >8.3923e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000011" call="MPI_Isend" bytes="1536" orank="17" region="0" commid="0" count="65" tid="0" op="" dtype="" >1.0872e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000014" call="MPI_Isend" bytes="1536" orank="20" region="0" commid="0" count="10" tid="0" op="" dtype="" >2.0504e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000016" call="MPI_Isend" bytes="1536" orank="22" region="0" commid="0" count="12" tid="0" op="" dtype="" >4.6015e-05 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000006000000002D" call="MPI_Isend" bytes="1536" orank="45" region="0" commid="0" count="77" tid="0" op="" dtype="" >3.9625e-04 4.0531e-06 6.1989e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002B5" call="MPI_Isend" bytes="1536" orank="693" region="0" commid="0" count="85" tid="0" op="" dtype="" >3.7384e-04 3.8147e-06 5.9605e-06</hent>
<hent key="038001000000000000000C000000002D" call="MPI_Irecv" bytes="3072" orank="45" region="0" commid="0" count="348" tid="0" op="" dtype="" >8.7500e-05 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002B5" call="MPI_Irecv" bytes="3072" orank="693" region="0" commid="0" count="362" tid="0" op="" dtype="" >1.5211e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0240010000000000000001C000000014" call="MPI_Isend" bytes="448" orank="20" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.6689e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C000000016" call="MPI_Isend" bytes="448" orank="22" region="0" commid="0" count="16" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C000000002D" call="MPI_Isend" bytes="3072" orank="45" region="0" commid="0" count="330" tid="0" op="" dtype="" >1.9073e-03 4.7684e-06 2.4080e-05</hent>
<hent key="024001000000000000000C00000002B5" call="MPI_Isend" bytes="3072" orank="693" region="0" commid="0" count="346" tid="0" op="" dtype="" >1.6747e-03 3.8147e-06 1.0967e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.0741e-03 3.5000e-04 3.6597e-04</hent>
<hent key="03800100000000000000038000000001" call="MPI_Irecv" bytes="896" orank="1" region="0" commid="0" count="2864" tid="0" op="" dtype="" >4.0960e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000038000000011" call="MPI_Irecv" bytes="896" orank="17" region="0" commid="0" count="2916" tid="0" op="" dtype="" >7.1359e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000014" call="MPI_Irecv" bytes="896" orank="20" region="0" commid="0" count="175" tid="0" op="" dtype="" >6.7711e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000016" call="MPI_Irecv" bytes="896" orank="22" region="0" commid="0" count="170" tid="0" op="" dtype="" >4.1485e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000002D" call="MPI_Irecv" bytes="896" orank="45" region="0" commid="0" count="30" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000380000002B5" call="MPI_Irecv" bytes="896" orank="693" region="0" commid="0" count="24" tid="0" op="" dtype="" >1.1921e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.5296e-04 2.5296e-04 2.5296e-04</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000180000000001" call="MPI_Irecv" bytes="6144" orank="1" region="0" commid="0" count="3025" tid="0" op="" dtype="" >4.7159e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000180000000011" call="MPI_Irecv" bytes="6144" orank="17" region="0" commid="0" count="2745" tid="0" op="" dtype="" >4.2415e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000018000000002D" call="MPI_Irecv" bytes="6144" orank="45" region="0" commid="0" count="10" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000001800000002B5" call="MPI_Irecv" bytes="6144" orank="693" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000038000000001" call="MPI_Isend" bytes="896" orank="1" region="0" commid="0" count="2997" tid="0" op="" dtype="" >1.6448e-03 0.0000e+00 1.3828e-05</hent>
<hent key="02400100000000000000038000000011" call="MPI_Isend" bytes="896" orank="17" region="0" commid="0" count="3065" tid="0" op="" dtype="" >1.6930e-03 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000038000000014" call="MPI_Isend" bytes="896" orank="20" region="0" commid="0" count="158" tid="0" op="" dtype="" >2.6917e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000038000000016" call="MPI_Isend" bytes="896" orank="22" region="0" commid="0" count="168" tid="0" op="" dtype="" >5.9772e-04 2.8610e-06 1.6928e-05</hent>
<hent key="0240010000000000000003800000002D" call="MPI_Isend" bytes="896" orank="45" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.1969e-04 3.8147e-06 7.1526e-06</hent>
<hent key="024001000000000000000380000002B5" call="MPI_Isend" bytes="896" orank="693" region="0" commid="0" count="15" tid="0" op="" dtype="" >6.1989e-05 3.8147e-06 5.0068e-06</hent>
<hent key="02400100000000000000180000000001" call="MPI_Isend" bytes="6144" orank="1" region="0" commid="0" count="2597" tid="0" op="" dtype="" >4.9760e-03 9.5367e-07 1.5974e-05</hent>
<hent key="02400100000000000000180000000011" call="MPI_Isend" bytes="6144" orank="17" region="0" commid="0" count="2304" tid="0" op="" dtype="" >4.2450e-03 9.5367e-07 1.8120e-05</hent>
<hent key="0240010000000000000018000000002D" call="MPI_Isend" bytes="6144" orank="45" region="0" commid="0" count="12" tid="0" op="" dtype="" >8.0585e-05 5.9605e-06 8.1062e-06</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.4121e-05 5.4121e-05 5.4121e-05</hent>
<hent key="024001000000000000001800000002B5" call="MPI_Isend" bytes="6144" orank="693" region="0" commid="0" count="14" tid="0" op="" dtype="" >8.1062e-05 5.0068e-06 6.1989e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >9.6353e+00 9.0599e-06 1.2667e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.8096e-04 8.8096e-04 8.8096e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0585e-02 1.0585e-02 1.0585e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >4.2381e-03 4.2381e-03 4.2381e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.1234e-01 3.3741e-03 2.0405e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.7507e-04 5.7507e-04 5.7507e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5740e+00 4.4394e-04 2.4729e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.5988e-05 0.0000e+00 1.0967e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0014e-05 1.0014e-05 1.0014e-05</hent>
<hent key="03800100000000000000070000000001" call="MPI_Irecv" bytes="1792" orank="1" region="0" commid="0" count="21" tid="0" op="" dtype="" >4.5300e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000011" call="MPI_Irecv" bytes="1792" orank="17" region="0" commid="0" count="10" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000014" call="MPI_Irecv" bytes="1792" orank="20" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000016" call="MPI_Irecv" bytes="1792" orank="22" region="0" commid="0" count="5" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000007000000002D" call="MPI_Irecv" bytes="1792" orank="45" region="0" commid="0" count="104" tid="0" op="" dtype="" >2.6941e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002B5" call="MPI_Irecv" bytes="1792" orank="693" region="0" commid="0" count="111" tid="0" op="" dtype="" >5.4598e-05 0.0000e+00 6.9141e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >9.8944e-05 0.0000e+00 6.3896e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9780e+01 0.0000e+00 3.8412e+01</hent>
<hent key="038001000000000000000A0000000001" call="MPI_Irecv" bytes="2560" orank="1" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000011" call="MPI_Irecv" bytes="2560" orank="17" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000016" call="MPI_Irecv" bytes="2560" orank="22" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000002D" call="MPI_Irecv" bytes="2560" orank="45" region="0" commid="0" count="407" tid="0" op="" dtype="" >1.0896e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002B5" call="MPI_Irecv" bytes="2560" orank="693" region="0" commid="0" count="376" tid="0" op="" dtype="" >1.4281e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000001" call="MPI_Isend" bytes="1792" orank="1" region="0" commid="0" count="14" tid="0" op="" dtype="" >2.7418e-05 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000070000000011" call="MPI_Isend" bytes="1792" orank="17" region="0" commid="0" count="22" tid="0" op="" dtype="" >4.1008e-05 9.5367e-07 2.8610e-06</hent>
<hent key="02400100000000000000070000000014" call="MPI_Isend" bytes="1792" orank="20" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.7752e-06 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000016" call="MPI_Isend" bytes="1792" orank="22" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.2159e-05 4.0531e-06 8.1062e-06</hent>
<hent key="0240010000000000000007000000002D" call="MPI_Isend" bytes="1792" orank="45" region="0" commid="0" count="124" tid="0" op="" dtype="" >6.4754e-04 3.8147e-06 1.1921e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3049e-02 8.3049e-02 8.3049e-02</hent>
<hent key="024001000000000000000700000002B5" call="MPI_Isend" bytes="1792" orank="693" region="0" commid="0" count="106" tid="0" op="" dtype="" >4.7708e-04 3.8147e-06 5.0068e-06</hent>
<hent key="024001000000000000000A0000000001" call="MPI_Isend" bytes="2560" orank="1" region="0" commid="0" count="2" tid="0" op="" dtype="" >3.0994e-06 9.5367e-07 2.1458e-06</hent>
<hent key="024001000000000000000A0000000011" call="MPI_Isend" bytes="2560" orank="17" region="0" commid="0" count="2" tid="0" op="" dtype="" >2.8610e-06 9.5367e-07 1.9073e-06</hent>
<hent key="024001000000000000000A000000002D" call="MPI_Isend" bytes="2560" orank="45" region="0" commid="0" count="402" tid="0" op="" dtype="" >2.1849e-03 4.7684e-06 1.1921e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.7874e-04 4.4823e-05 2.1887e-04</hent>
<hent key="024001000000000000000A00000002B5" call="MPI_Isend" bytes="2560" orank="693" region="0" commid="0" count="374" tid="0" op="" dtype="" >1.7486e-03 3.8147e-06 1.0967e-05</hent>
<hent key="03800100000000000000100000000014" call="MPI_Irecv" bytes="4096" orank="20" region="0" commid="0" count="12513" tid="0" op="" dtype="" >3.9990e-03 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000100000000016" call="MPI_Irecv" bytes="4096" orank="22" region="0" commid="0" count="12484" tid="0" op="" dtype="" >1.7707e-03 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000010000000002D" call="MPI_Irecv" bytes="4096" orank="45" region="0" commid="0" count="3045" tid="0" op="" dtype="" >6.6185e-04 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000001000000002B5" call="MPI_Irecv" bytes="4096" orank="693" region="0" commid="0" count="3133" tid="0" op="" dtype="" >6.6924e-04 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000100000000014" call="MPI_Isend" bytes="4096" orank="20" region="0" commid="0" count="12594" tid="0" op="" dtype="" >2.1577e-02 0.0000e+00 1.7166e-05</hent>
<hent key="02400100000000000000100000000016" call="MPI_Isend" bytes="4096" orank="22" region="0" commid="0" count="12633" tid="0" op="" dtype="" >3.1775e-02 0.0000e+00 2.0981e-05</hent>
<hent key="0240010000000000000010000000002D" call="MPI_Isend" bytes="4096" orank="45" region="0" commid="0" count="2922" tid="0" op="" dtype="" >1.1358e-02 9.5367e-07 2.1935e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.6689e-03 5.9390e-04 1.0750e-03</hent>
<hent key="024001000000000000001000000002B5" call="MPI_Isend" bytes="4096" orank="693" region="0" commid="0" count="2900" tid="0" op="" dtype="" >9.8913e-03 9.5367e-07 1.7166e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.6015e-05 4.6015e-05 4.6015e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.6139e-03 4.7898e-04 1.2591e-03</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.0014e-05 2.1458e-06 7.8678e-06</hent>
<hent key="03800100000000000000000400000001" call="MPI_Irecv" bytes="4" orank="1" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.1618e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000400000011" call="MPI_Irecv" bytes="4" orank="17" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.2953e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000014" call="MPI_Irecv" bytes="4" orank="20" region="0" commid="0" count="3400" tid="0" op="" dtype="" >7.4315e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000016" call="MPI_Irecv" bytes="4" orank="22" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.8651e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000002D" call="MPI_Irecv" bytes="4" orank="45" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.1750e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001C0000000001" call="MPI_Irecv" bytes="7168" orank="1" region="0" commid="0" count="9675" tid="0" op="" dtype="" >1.4091e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000001C0000000011" call="MPI_Irecv" bytes="7168" orank="17" region="0" commid="0" count="9955" tid="0" op="" dtype="" >1.6518e-03 0.0000e+00 9.0599e-06</hent>
<hent key="038001000000000000000004000002B5" call="MPI_Irecv" bytes="4" orank="693" region="0" commid="0" count="3400" tid="0" op="" dtype="" >6.3586e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000400000001" call="MPI_Isend" bytes="4" orank="1" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.9574e-03 0.0000e+00 3.0994e-05</hent>
<hent key="02400100000000000000000400000011" call="MPI_Isend" bytes="4" orank="17" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.7920e-03 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000400000014" call="MPI_Isend" bytes="4" orank="20" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.6496e-03 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000000400000016" call="MPI_Isend" bytes="4" orank="22" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.8467e-03 0.0000e+00 5.9128e-05</hent>
<hent key="0240010000000000000000040000002D" call="MPI_Isend" bytes="4" orank="45" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.4374e-02 4.7684e-06 2.5511e-04</hent>
<hent key="024001000000000000001C0000000001" call="MPI_Isend" bytes="7168" orank="1" region="0" commid="0" count="10103" tid="0" op="" dtype="" >1.9610e-02 9.5367e-07 1.8835e-05</hent>
<hent key="024001000000000000001C0000000011" call="MPI_Isend" bytes="7168" orank="17" region="0" commid="0" count="10396" tid="0" op="" dtype="" >1.9358e-02 9.5367e-07 2.1935e-05</hent>
<hent key="024001000000000000000004000002B5" call="MPI_Isend" bytes="4" orank="693" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.9389e-02 3.8147e-06 6.3896e-05</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0592e-04 5.0592e-04 5.0592e-04</hent>
<hent key="03800100000000000000020000000014" call="MPI_Irecv" bytes="512" orank="20" region="0" commid="0" count="3352" tid="0" op="" dtype="" >3.7575e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000020000000016" call="MPI_Irecv" bytes="512" orank="22" region="0" commid="0" count="3338" tid="0" op="" dtype="" >5.9199e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000001" call="MPI_Irecv" bytes="1280" orank="1" region="0" commid="0" count="153" tid="0" op="" dtype="" >3.0756e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000011" call="MPI_Irecv" bytes="1280" orank="17" region="0" commid="0" count="169" tid="0" op="" dtype="" >6.7711e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000050000000014" call="MPI_Irecv" bytes="1280" orank="20" region="0" commid="0" count="45" tid="0" op="" dtype="" >2.2173e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000016" call="MPI_Irecv" bytes="1280" orank="22" region="0" commid="0" count="56" tid="0" op="" dtype="" >1.3590e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000002D" call="MPI_Irecv" bytes="1280" orank="45" region="0" commid="0" count="36" tid="0" op="" dtype="" >1.4782e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4884e+01 1.0014e-05 1.8478e-01</hent>
<hent key="038001000000000000000500000002B5" call="MPI_Irecv" bytes="1280" orank="693" region="0" commid="0" count="38" tid="0" op="" dtype="" >1.4305e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000020000000014" call="MPI_Isend" bytes="512" orank="20" region="0" commid="0" count="3372" tid="0" op="" dtype="" >1.7469e-03 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000020000000016" call="MPI_Isend" bytes="512" orank="22" region="0" commid="0" count="3384" tid="0" op="" dtype="" >2.2202e-03 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000080000000001" call="MPI_Irecv" bytes="2048" orank="1" region="0" commid="0" count="7" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000080000000011" call="MPI_Irecv" bytes="2048" orank="17" region="0" commid="0" count="6" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000080000000014" call="MPI_Irecv" bytes="2048" orank="20" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000016" call="MPI_Irecv" bytes="2048" orank="22" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.1921e-06 1.1921e-06 1.1921e-06</hent>
<hent key="0380010000000000000008000000002D" call="MPI_Irecv" bytes="2048" orank="45" region="0" commid="0" count="165" tid="0" op="" dtype="" >4.0293e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002B5" call="MPI_Irecv" bytes="2048" orank="693" region="0" commid="0" count="154" tid="0" op="" dtype="" >6.7711e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000050000000001" call="MPI_Isend" bytes="1280" orank="1" region="0" commid="0" count="168" tid="0" op="" dtype="" >2.4223e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000050000000011" call="MPI_Isend" bytes="1280" orank="17" region="0" commid="0" count="148" tid="0" op="" dtype="" >2.0719e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000050000000014" call="MPI_Isend" bytes="1280" orank="20" region="0" commid="0" count="48" tid="0" op="" dtype="" >9.3937e-05 9.5367e-07 7.8678e-06</hent>
<hent key="02400100000000000000050000000016" call="MPI_Isend" bytes="1280" orank="22" region="0" commid="0" count="52" tid="0" op="" dtype="" >1.8334e-04 2.8610e-06 7.1526e-06</hent>
<hent key="0240010000000000000005000000002D" call="MPI_Isend" bytes="1280" orank="45" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.9050e-04 3.8147e-06 1.0967e-05</hent>
<hent key="024001000000000000000500000002B5" call="MPI_Isend" bytes="1280" orank="693" region="0" commid="0" count="43" tid="0" op="" dtype="" >1.8430e-04 3.8147e-06 5.0068e-06</hent>
</hash>
<internal rank="21" log_i="1724765674.527187" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="22" mpi_size="696" stamp_init="1724765564.465388" stamp_final="1724765674.538711" username="apac4" allocationname="unknown" flags="0" pid="1612139" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10073e+02" utime="8.71174e+01" stime="1.46395e+01" mtime="7.12052e+01" gflop="0.00000e+00" gbyte="3.77396e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.12052e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d315d3150b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09935e+02" utime="8.70834e+01" stime="1.46305e+01" mtime="7.12052e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.12052e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2288e+09" > 7.9377e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2220e+09" > 4.1945e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7138e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9762e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2286e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.7684e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5734e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.6323e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0592e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3144e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3218e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="215" >
<hent key="02400100000000000000080000000002" call="MPI_Isend" bytes="2048" orank="2" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.6689e-05 9.5367e-07 2.8610e-06</hent>
<hent key="02400100000000000000080000000012" call="MPI_Isend" bytes="2048" orank="18" region="0" commid="0" count="6" tid="0" op="" dtype="" >8.8215e-06 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000080000000015" call="MPI_Isend" bytes="2048" orank="21" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="02400100000000000000080000000017" call="MPI_Isend" bytes="2048" orank="23" region="0" commid="0" count="1" tid="0" op="" dtype="" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="0240010000000000000008000000002E" call="MPI_Isend" bytes="2048" orank="46" region="0" commid="0" count="144" tid="0" op="" dtype="" >9.7513e-04 5.0068e-06 1.7881e-05</hent>
<hent key="024001000000000000000800000002B6" call="MPI_Isend" bytes="2048" orank="694" region="0" commid="0" count="151" tid="0" op="" dtype="" >8.4662e-04 3.8147e-06 1.3113e-05</hent>
<hent key="038001000000000000000E0000000015" call="MPI_Irecv" bytes="3584" orank="21" region="0" commid="0" count="67" tid="0" op="" dtype="" >3.1471e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000017" call="MPI_Irecv" bytes="3584" orank="23" region="0" commid="0" count="259" tid="0" op="" dtype="" >5.3406e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E000000002E" call="MPI_Irecv" bytes="3584" orank="46" region="0" commid="0" count="130" tid="0" op="" dtype="" >1.0753e-04 0.0000e+00 1.1921e-05</hent>
<hent key="038001000000000000000E00000002B6" call="MPI_Irecv" bytes="3584" orank="694" region="0" commid="0" count="125" tid="0" op="" dtype="" >1.0991e-04 0.0000e+00 6.9141e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000E0000000015" call="MPI_Isend" bytes="3584" orank="21" region="0" commid="0" count="216" tid="0" op="" dtype="" >3.6073e-04 0.0000e+00 3.0994e-06</hent>
<hent key="024001000000000000000E0000000017" call="MPI_Isend" bytes="3584" orank="23" region="0" commid="0" count="295" tid="0" op="" dtype="" >7.9465e-04 9.5367e-07 1.9073e-05</hent>
<hent key="024001000000000000000E000000002E" call="MPI_Isend" bytes="3584" orank="46" region="0" commid="0" count="118" tid="0" op="" dtype="" >8.4853e-04 5.0068e-06 1.9073e-05</hent>
<hent key="03800100000000000000028000000002" call="MPI_Irecv" bytes="640" orank="2" region="0" commid="0" count="444" tid="0" op="" dtype="" >1.1802e-04 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000E00000002B6" call="MPI_Isend" bytes="3584" orank="694" region="0" commid="0" count="129" tid="0" op="" dtype="" >7.8201e-04 3.8147e-06 1.6212e-05</hent>
<hent key="03800100000000000000028000000012" call="MPI_Irecv" bytes="640" orank="18" region="0" commid="0" count="424" tid="0" op="" dtype="" >1.2636e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000015" call="MPI_Irecv" bytes="640" orank="21" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.4853e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000017" call="MPI_Irecv" bytes="640" orank="23" region="0" commid="0" count="384" tid="0" op="" dtype="" >2.3794e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000002800000002E" call="MPI_Irecv" bytes="640" orank="46" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.4544e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000014000000002E" call="MPI_Irecv" bytes="5120" orank="46" region="0" commid="0" count="1071" tid="0" op="" dtype="" >5.4455e-04 0.0000e+00 2.6941e-05</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002B6" call="MPI_Irecv" bytes="640" orank="694" region="0" commid="0" count="30" tid="0" op="" dtype="" >2.5511e-05 0.0000e+00 9.0599e-06</hent>
<hent key="038001000000000000001400000002B6" call="MPI_Irecv" bytes="5120" orank="694" region="0" commid="0" count="1057" tid="0" op="" dtype="" >4.5466e-04 0.0000e+00 1.2159e-05</hent>
<hent key="03800100000000000000A0000000002E" call="MPI_Irecv" bytes="40960" orank="46" region="0" commid="0" count="3624" tid="0" op="" dtype="" >2.7351e-03 0.0000e+00 5.6982e-05</hent>
<hent key="02400100000000000000028000000002" call="MPI_Isend" bytes="640" orank="2" region="0" commid="0" count="415" tid="0" op="" dtype="" >5.5146e-04 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000028000000012" call="MPI_Isend" bytes="640" orank="18" region="0" commid="0" count="477" tid="0" op="" dtype="" >5.7483e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000028000000015" call="MPI_Isend" bytes="640" orank="21" region="0" commid="0" count="355" tid="0" op="" dtype="" >5.6267e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000028000000017" call="MPI_Isend" bytes="640" orank="23" region="0" commid="0" count="352" tid="0" op="" dtype="" >1.0982e-03 1.9073e-06 9.7752e-06</hent>
<hent key="0240010000000000000002800000002E" call="MPI_Isend" bytes="640" orank="46" region="0" commid="0" count="23" tid="0" op="" dtype="" >1.3590e-04 4.0531e-06 1.3113e-05</hent>
<hent key="03800100000000000000A000000002B6" call="MPI_Irecv" bytes="40960" orank="694" region="0" commid="0" count="3597" tid="0" op="" dtype="" >1.1067e-03 0.0000e+00 3.7909e-05</hent>
<hent key="0240010000000000000014000000002E" call="MPI_Isend" bytes="5120" orank="46" region="0" commid="0" count="1079" tid="0" op="" dtype="" >6.0017e-03 1.9073e-06 4.6968e-05</hent>
<hent key="024001000000000000000280000002B6" call="MPI_Isend" bytes="640" orank="694" region="0" commid="0" count="20" tid="0" op="" dtype="" >8.6784e-05 3.0994e-06 5.0068e-06</hent>
<hent key="0380010000000000000080000000002E" call="MPI_Irecv" bytes="32768" orank="46" region="0" commid="0" count="9076" tid="0" op="" dtype="" >7.2761e-03 0.0000e+00 8.9884e-05</hent>
<hent key="024001000000000000001400000002B6" call="MPI_Isend" bytes="5120" orank="694" region="0" commid="0" count="1211" tid="0" op="" dtype="" >5.7895e-03 9.5367e-07 5.8174e-05</hent>
<hent key="038001000000000000008000000002B6" call="MPI_Irecv" bytes="32768" orank="694" region="0" commid="0" count="9103" tid="0" op="" dtype="" >2.7902e-03 0.0000e+00 3.6955e-05</hent>
<hent key="03800100000000000000014000000002" call="MPI_Irecv" bytes="320" orank="2" region="0" commid="0" count="435" tid="0" op="" dtype="" >1.1826e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000012" call="MPI_Irecv" bytes="320" orank="18" region="0" commid="0" count="451" tid="0" op="" dtype="" >1.2445e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000015" call="MPI_Irecv" bytes="320" orank="21" region="0" commid="0" count="565" tid="0" op="" dtype="" >2.2960e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000014000000017" call="MPI_Irecv" bytes="320" orank="23" region="0" commid="0" count="566" tid="0" op="" dtype="" >3.2425e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000001400000002E" call="MPI_Irecv" bytes="320" orank="46" region="0" commid="0" count="19" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A0000000002E" call="MPI_Isend" bytes="40960" orank="46" region="0" commid="0" count="3710" tid="0" op="" dtype="" >8.0814e-02 1.0967e-05 6.6996e-05</hent>
<hent key="038001000000000000000140000002B6" call="MPI_Irecv" bytes="320" orank="694" region="0" commid="0" count="19" tid="0" op="" dtype="" >1.5974e-05 0.0000e+00 3.0994e-06</hent>
<hent key="02400100000000000000A000000002B6" call="MPI_Isend" bytes="40960" orank="694" region="0" commid="0" count="4293" tid="0" op="" dtype="" >8.8035e-02 6.9141e-06 2.0099e-04</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >5.7138e+00 0.0000e+00 1.1067e-01</hent>
<hent key="0240010000000000000080000000002E" call="MPI_Isend" bytes="32768" orank="46" region="0" commid="0" count="8990" tid="0" op="" dtype="" >2.0793e-01 1.0967e-05 9.3937e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.7881e-05 0.0000e+00 1.7881e-05</hent>
<hent key="024001000000000000008000000002B6" call="MPI_Isend" bytes="32768" orank="694" region="0" commid="0" count="8407" tid="0" op="" dtype="" >1.6626e-01 6.9141e-06 2.0099e-04</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000002" call="MPI_Isend" bytes="320" orank="2" region="0" commid="0" count="454" tid="0" op="" dtype="" >4.7183e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000012" call="MPI_Isend" bytes="320" orank="18" region="0" commid="0" count="413" tid="0" op="" dtype="" >4.4465e-04 0.0000e+00 1.5974e-05</hent>
<hent key="02400100000000000000014000000015" call="MPI_Isend" bytes="320" orank="21" region="0" commid="0" count="522" tid="0" op="" dtype="" >6.7616e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000014000000017" call="MPI_Isend" bytes="320" orank="23" region="0" commid="0" count="562" tid="0" op="" dtype="" >1.5893e-03 1.9073e-06 1.5020e-05</hent>
<hent key="0240010000000000000001400000002E" call="MPI_Isend" bytes="320" orank="46" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.5330e-04 3.8147e-06 1.2159e-05</hent>
<hent key="03800100000000000000200000000012" call="MPI_Irecv" bytes="8192" orank="18" region="0" commid="0" count="7" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000140000002B6" call="MPI_Isend" bytes="320" orank="694" region="0" commid="0" count="26" tid="0" op="" dtype="" >1.6499e-04 3.8147e-06 2.3842e-05</hent>
<hent key="03800100000000000000000000000002" call="MPI_Irecv" bytes="0" orank="2" region="0" commid="0" count="347" tid="0" op="" dtype="" >6.9141e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000012" call="MPI_Irecv" bytes="0" orank="18" region="0" commid="0" count="350" tid="0" op="" dtype="" >8.1062e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000015" call="MPI_Irecv" bytes="0" orank="21" region="0" commid="0" count="535" tid="0" op="" dtype="" >1.8311e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000017" call="MPI_Irecv" bytes="0" orank="23" region="0" commid="0" count="529" tid="0" op="" dtype="" >2.4128e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000000000000002E" call="MPI_Irecv" bytes="0" orank="46" region="0" commid="0" count="82" tid="0" op="" dtype="" >2.7657e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000000000002B6" call="MPI_Irecv" bytes="0" orank="694" region="0" commid="0" count="87" tid="0" op="" dtype="" >2.7895e-05 0.0000e+00 2.8610e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.0409e-04 0.0000e+00 8.7976e-05</hent>
<hent key="02400100000000000000200000000012" call="MPI_Isend" bytes="8192" orank="18" region="0" commid="0" count="14" tid="0" op="" dtype="" >2.8610e-05 9.5367e-07 3.0994e-06</hent>
<hent key="03800100000000000000030000000002" call="MPI_Irecv" bytes="768" orank="2" region="0" commid="0" count="934" tid="0" op="" dtype="" >1.4591e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000030000000012" call="MPI_Irecv" bytes="768" orank="18" region="0" commid="0" count="934" tid="0" op="" dtype="" >2.0766e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000000000002" call="MPI_Isend" bytes="0" orank="2" region="0" commid="0" count="314" tid="0" op="" dtype="" >2.6488e-04 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000000000000012" call="MPI_Isend" bytes="0" orank="18" region="0" commid="0" count="365" tid="0" op="" dtype="" >2.6536e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000000000015" call="MPI_Isend" bytes="0" orank="21" region="0" commid="0" count="574" tid="0" op="" dtype="" >4.8351e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000000000000017" call="MPI_Isend" bytes="0" orank="23" region="0" commid="0" count="517" tid="0" op="" dtype="" >1.1117e-03 9.5367e-07 3.2902e-05</hent>
<hent key="0240010000000000000000000000002E" call="MPI_Isend" bytes="0" orank="46" region="0" commid="0" count="81" tid="0" op="" dtype="" >3.9840e-04 1.9073e-06 1.0967e-05</hent>
<hent key="024001000000000000000000000002B6" call="MPI_Isend" bytes="0" orank="694" region="0" commid="0" count="86" tid="0" op="" dtype="" >3.9935e-04 2.1458e-06 4.9114e-05</hent>
<hent key="03800100000000000000060000000002" call="MPI_Irecv" bytes="1536" orank="2" region="0" commid="0" count="76" tid="0" op="" dtype="" >1.9312e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000012" call="MPI_Irecv" bytes="1536" orank="18" region="0" commid="0" count="56" tid="0" op="" dtype="" >1.3113e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000015" call="MPI_Irecv" bytes="1536" orank="21" region="0" commid="0" count="12" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000060000000017" call="MPI_Irecv" bytes="1536" orank="23" region="0" commid="0" count="15" tid="0" op="" dtype="" >9.2983e-06 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000006000000002E" call="MPI_Irecv" bytes="1536" orank="46" region="0" commid="0" count="67" tid="0" op="" dtype="" >3.2425e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002B6" call="MPI_Irecv" bytes="1536" orank="694" region="0" commid="0" count="63" tid="0" op="" dtype="" >4.3631e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000030000000002" call="MPI_Isend" bytes="768" orank="2" region="0" commid="0" count="790" tid="0" op="" dtype="" >3.7527e-04 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000030000000012" call="MPI_Isend" bytes="768" orank="18" region="0" commid="0" count="950" tid="0" op="" dtype="" >4.9973e-04 0.0000e+00 5.9605e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 2.1458e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C000000015" call="MPI_Irecv" bytes="448" orank="21" region="0" commid="0" count="16" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C000000017" call="MPI_Irecv" bytes="448" orank="23" region="0" commid="0" count="64" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000002" call="MPI_Isend" bytes="1536" orank="2" region="0" commid="0" count="50" tid="0" op="" dtype="" >8.5831e-05 9.5367e-07 3.8147e-06</hent>
<hent key="02400100000000000000060000000012" call="MPI_Isend" bytes="1536" orank="18" region="0" commid="0" count="53" tid="0" op="" dtype="" >8.5592e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000015" call="MPI_Isend" bytes="1536" orank="21" region="0" commid="0" count="15" tid="0" op="" dtype="" >3.0518e-05 1.9073e-06 2.8610e-06</hent>
<hent key="02400100000000000000060000000017" call="MPI_Isend" bytes="1536" orank="23" region="0" commid="0" count="17" tid="0" op="" dtype="" >6.2704e-05 2.8610e-06 4.0531e-06</hent>
<hent key="0240010000000000000006000000002E" call="MPI_Isend" bytes="1536" orank="46" region="0" commid="0" count="93" tid="0" op="" dtype="" >5.7077e-04 4.7684e-06 1.3828e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002B6" call="MPI_Isend" bytes="1536" orank="694" region="0" commid="0" count="86" tid="0" op="" dtype="" >4.5037e-04 3.0994e-06 1.5974e-05</hent>
<hent key="038001000000000000000C000000002E" call="MPI_Irecv" bytes="3072" orank="46" region="0" commid="0" count="367" tid="0" op="" dtype="" >2.7394e-04 0.0000e+00 1.0967e-05</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002B6" call="MPI_Irecv" bytes="3072" orank="694" region="0" commid="0" count="310" tid="0" op="" dtype="" >2.6703e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0240010000000000000001C000000015" call="MPI_Isend" bytes="448" orank="21" region="0" commid="0" count="62" tid="0" op="" dtype="" >3.5763e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000001C000000017" call="MPI_Isend" bytes="448" orank="23" region="0" commid="0" count="86" tid="0" op="" dtype="" >4.9591e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C000000002E" call="MPI_Isend" bytes="3072" orank="46" region="0" commid="0" count="333" tid="0" op="" dtype="" >2.3398e-03 5.0068e-06 1.8120e-05</hent>
<hent key="024001000000000000000C00000002B6" call="MPI_Isend" bytes="3072" orank="694" region="0" commid="0" count="379" tid="0" op="" dtype="" >2.2237e-03 3.8147e-06 1.5974e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.1222e-03 3.6311e-04 3.8695e-04</hent>
<hent key="03800100000000000000038000000002" call="MPI_Irecv" bytes="896" orank="2" region="0" commid="0" count="2703" tid="0" op="" dtype="" >3.9554e-04 0.0000e+00 1.7166e-05</hent>
<hent key="03800100000000000000038000000012" call="MPI_Irecv" bytes="896" orank="18" region="0" commid="0" count="2729" tid="0" op="" dtype="" >6.5303e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000015" call="MPI_Irecv" bytes="896" orank="21" region="0" commid="0" count="168" tid="0" op="" dtype="" >7.4148e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000017" call="MPI_Irecv" bytes="896" orank="23" region="0" commid="0" count="147" tid="0" op="" dtype="" >9.9182e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000003800000002E" call="MPI_Irecv" bytes="896" orank="46" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.1921e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002B6" call="MPI_Irecv" bytes="896" orank="694" region="0" commid="0" count="32" tid="0" op="" dtype="" >2.5034e-05 0.0000e+00 2.8610e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.4901e-04 1.4901e-04 1.4901e-04</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="03800100000000000000180000000002" call="MPI_Irecv" bytes="6144" orank="2" region="0" commid="0" count="3432" tid="0" op="" dtype="" >3.8314e-04 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000180000000012" call="MPI_Irecv" bytes="6144" orank="18" region="0" commid="0" count="3453" tid="0" op="" dtype="" >6.1679e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0380010000000000000018000000002E" call="MPI_Irecv" bytes="6144" orank="46" region="0" commid="0" count="6" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000001800000002B6" call="MPI_Irecv" bytes="6144" orank="694" region="0" commid="0" count="14" tid="0" op="" dtype="" >1.6451e-05 9.5367e-07 1.9073e-06</hent>
<hent key="02400100000000000000038000000002" call="MPI_Isend" bytes="896" orank="2" region="0" commid="0" count="2906" tid="0" op="" dtype="" >1.7807e-03 0.0000e+00 1.2159e-05</hent>
<hent key="02400100000000000000038000000012" call="MPI_Isend" bytes="896" orank="18" region="0" commid="0" count="2676" tid="0" op="" dtype="" >1.6577e-03 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000038000000015" call="MPI_Isend" bytes="896" orank="21" region="0" commid="0" count="170" tid="0" op="" dtype="" >3.1996e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000038000000017" call="MPI_Isend" bytes="896" orank="23" region="0" commid="0" count="176" tid="0" op="" dtype="" >5.8913e-04 2.8610e-06 9.0599e-06</hent>
<hent key="0240010000000000000003800000002E" call="MPI_Isend" bytes="896" orank="46" region="0" commid="0" count="31" tid="0" op="" dtype="" >1.8597e-04 5.0068e-06 1.3828e-05</hent>
<hent key="024001000000000000000380000002B6" call="MPI_Isend" bytes="896" orank="694" region="0" commid="0" count="33" tid="0" op="" dtype="" >1.6665e-04 4.0531e-06 9.0599e-06</hent>
<hent key="02400100000000000000180000000002" call="MPI_Isend" bytes="6144" orank="2" region="0" commid="0" count="2922" tid="0" op="" dtype="" >6.2003e-03 9.5367e-07 1.9789e-05</hent>
<hent key="02400100000000000000180000000012" call="MPI_Isend" bytes="6144" orank="18" region="0" commid="0" count="3662" tid="0" op="" dtype="" >7.7143e-03 9.5367e-07 1.2875e-05</hent>
<hent key="0240010000000000000018000000002E" call="MPI_Isend" bytes="6144" orank="46" region="0" commid="0" count="18" tid="0" op="" dtype="" >1.5855e-04 6.9141e-06 1.6928e-05</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.6042e-05 6.6042e-05 6.6042e-05</hent>
<hent key="024001000000000000001800000002B6" call="MPI_Isend" bytes="6144" orank="694" region="0" commid="0" count="9" tid="0" op="" dtype="" >6.6280e-05 5.9605e-06 1.0967e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.8280e+00 1.3113e-05 1.2665e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.5807e-04 8.5807e-04 8.5807e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0592e-02 1.0592e-02 1.0592e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >4.6759e-03 4.6759e-03 4.6759e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.9552e-01 3.2980e-03 1.8751e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >6.0391e-04 6.0391e-04 6.0391e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5734e+00 4.1890e-04 2.4723e-01</hent>
<hent key="03800100000000000000040000000012" call="MPI_Irecv" bytes="1024" orank="18" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.2286e-03 1.9073e-06 1.1890e-03</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="03800100000000000000070000000002" call="MPI_Irecv" bytes="1792" orank="2" region="0" commid="0" count="21" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000012" call="MPI_Irecv" bytes="1792" orank="18" region="0" commid="0" count="16" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000015" call="MPI_Irecv" bytes="1792" orank="21" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000017" call="MPI_Irecv" bytes="1792" orank="23" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000007000000002E" call="MPI_Irecv" bytes="1792" orank="46" region="0" commid="0" count="113" tid="0" op="" dtype="" >6.3419e-05 0.0000e+00 3.8147e-06</hent>
<hent key="038001000000000000000700000002B6" call="MPI_Irecv" bytes="1792" orank="694" region="0" commid="0" count="131" tid="0" op="" dtype="" >1.0276e-04 0.0000e+00 7.1526e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >9.6798e-05 0.0000e+00 7.5817e-05</hent>
<hent key="02400100000000000000040000000012" call="MPI_Isend" bytes="1024" orank="18" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9761e+01 0.0000e+00 3.8415e+01</hent>
<hent key="038001000000000000000A0000000002" call="MPI_Irecv" bytes="2560" orank="2" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000012" call="MPI_Irecv" bytes="2560" orank="18" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000017" call="MPI_Irecv" bytes="2560" orank="23" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A000000002E" call="MPI_Irecv" bytes="2560" orank="46" region="0" commid="0" count="387" tid="0" op="" dtype="" >2.7180e-04 0.0000e+00 6.9141e-06</hent>
<hent key="038001000000000000000A00000002B6" call="MPI_Irecv" bytes="2560" orank="694" region="0" commid="0" count="376" tid="0" op="" dtype="" >3.1877e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000070000000002" call="MPI_Isend" bytes="1792" orank="2" region="0" commid="0" count="21" tid="0" op="" dtype="" >4.0770e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000012" call="MPI_Isend" bytes="1792" orank="18" region="0" commid="0" count="26" tid="0" op="" dtype="" >4.7445e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000015" call="MPI_Isend" bytes="1792" orank="21" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.0967e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000017" call="MPI_Isend" bytes="1792" orank="23" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.0014e-05 2.8610e-06 4.0531e-06</hent>
<hent key="0240010000000000000007000000002E" call="MPI_Isend" bytes="1792" orank="46" region="0" commid="0" count="115" tid="0" op="" dtype="" >7.5078e-04 5.0068e-06 1.7166e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3144e-02 8.3144e-02 8.3144e-02</hent>
<hent key="024001000000000000000700000002B6" call="MPI_Isend" bytes="1792" orank="694" region="0" commid="0" count="112" tid="0" op="" dtype="" >5.8389e-04 3.8147e-06 1.4067e-05</hent>
<hent key="024001000000000000000A0000000002" call="MPI_Isend" bytes="2560" orank="2" region="0" commid="0" count="2" tid="0" op="" dtype="" >4.0531e-06 9.5367e-07 3.0994e-06</hent>
<hent key="024001000000000000000A0000000012" call="MPI_Isend" bytes="2560" orank="18" region="0" commid="0" count="2" tid="0" op="" dtype="" >5.0068e-06 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000015" call="MPI_Isend" bytes="2560" orank="21" region="0" commid="0" count="2" tid="0" op="" dtype="" >5.9605e-06 2.8610e-06 3.0994e-06</hent>
<hent key="024001000000000000000A000000002E" call="MPI_Isend" bytes="2560" orank="46" region="0" commid="0" count="368" tid="0" op="" dtype="" >2.4385e-03 4.7684e-06 2.2173e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.9472e-04 4.7922e-05 2.2388e-04</hent>
<hent key="024001000000000000000A00000002B6" call="MPI_Isend" bytes="2560" orank="694" region="0" commid="0" count="358" tid="0" op="" dtype="" >2.0020e-03 3.8147e-06 1.4067e-05</hent>
<hent key="03800100000000000000100000000015" call="MPI_Irecv" bytes="4096" orank="21" region="0" commid="0" count="12633" tid="0" op="" dtype="" >5.2121e-03 0.0000e+00 2.2888e-05</hent>
<hent key="03800100000000000000100000000017" call="MPI_Irecv" bytes="4096" orank="23" region="0" commid="0" count="12441" tid="0" op="" dtype="" >3.2814e-03 0.0000e+00 7.1526e-06</hent>
<hent key="0380010000000000000010000000002E" call="MPI_Irecv" bytes="4096" orank="46" region="0" commid="0" count="2604" tid="0" op="" dtype="" >1.1008e-03 0.0000e+00 2.7180e-05</hent>
<hent key="038001000000000000001000000002B6" call="MPI_Irecv" bytes="4096" orank="694" region="0" commid="0" count="2628" tid="0" op="" dtype="" >1.2255e-03 0.0000e+00 1.6928e-05</hent>
<hent key="02400100000000000000100000000015" call="MPI_Isend" bytes="4096" orank="21" region="0" commid="0" count="12484" tid="0" op="" dtype="" >2.1862e-02 0.0000e+00 3.6001e-05</hent>
<hent key="02400100000000000000100000000017" call="MPI_Isend" bytes="4096" orank="23" region="0" commid="0" count="12405" tid="0" op="" dtype="" >3.4356e-02 0.0000e+00 2.0027e-05</hent>
<hent key="0240010000000000000010000000002E" call="MPI_Isend" bytes="4096" orank="46" region="0" commid="0" count="2620" tid="0" op="" dtype="" >1.4908e-02 1.9073e-06 6.8903e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.7312e-03 6.1607e-04 1.1151e-03</hent>
<hent key="024001000000000000001000000002B6" call="MPI_Isend" bytes="4096" orank="694" region="0" commid="0" count="2454" tid="0" op="" dtype="" >1.2073e-02 9.5367e-07 1.1683e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.4823e-05 4.4823e-05 4.4823e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.7551e-03 4.9400e-04 1.3039e-03</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.7220e-06 1.9073e-06 3.8147e-06</hent>
<hent key="03800100000000000000000400000002" call="MPI_Irecv" bytes="4" orank="2" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.8889e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000400000012" call="MPI_Irecv" bytes="4" orank="18" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.3787e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000000400000015" call="MPI_Irecv" bytes="4" orank="21" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.1535e-03 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000000400000017" call="MPI_Irecv" bytes="4" orank="23" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.3740e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000000040000002E" call="MPI_Irecv" bytes="4" orank="46" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.2259e-03 0.0000e+00 8.2970e-05</hent>
<hent key="038001000000000000001C0000000002" call="MPI_Irecv" bytes="7168" orank="2" region="0" commid="0" count="9268" tid="0" op="" dtype="" >1.1172e-03 0.0000e+00 6.9141e-06</hent>
<hent key="038001000000000000001C0000000012" call="MPI_Irecv" bytes="7168" orank="18" region="0" commid="0" count="9240" tid="0" op="" dtype="" >1.6756e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000004000002B6" call="MPI_Irecv" bytes="4" orank="694" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.1692e-03 0.0000e+00 1.6928e-05</hent>
<hent key="038001000000000000001C00000002B6" call="MPI_Irecv" bytes="7168" orank="694" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="02400100000000000000000400000002" call="MPI_Isend" bytes="4" orank="2" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.0213e-03 0.0000e+00 2.4080e-05</hent>
<hent key="02400100000000000000000400000012" call="MPI_Isend" bytes="4" orank="18" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.6298e-03 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000000400000015" call="MPI_Isend" bytes="4" orank="21" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.5700e-03 0.0000e+00 4.2200e-05</hent>
<hent key="02400100000000000000000400000017" call="MPI_Isend" bytes="4" orank="23" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.9087e-03 0.0000e+00 2.0981e-05</hent>
<hent key="0240010000000000000000040000002E" call="MPI_Isend" bytes="4" orank="46" region="0" commid="0" count="3400" tid="0" op="" dtype="" >3.0735e-02 4.7684e-06 4.2915e-05</hent>
<hent key="024001000000000000001C0000000002" call="MPI_Isend" bytes="7168" orank="2" region="0" commid="0" count="9778" tid="0" op="" dtype="" >2.0975e-02 9.5367e-07 1.8835e-05</hent>
<hent key="024001000000000000001C0000000012" call="MPI_Isend" bytes="7168" orank="18" region="0" commid="0" count="9024" tid="0" op="" dtype="" >1.9393e-02 9.5367e-07 2.2173e-05</hent>
<hent key="024001000000000000000004000002B6" call="MPI_Isend" bytes="4" orank="694" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.5499e-02 3.8147e-06 1.0991e-04</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.2810e-04 5.2810e-04 5.2810e-04</hent>
<hent key="03800100000000000000020000000015" call="MPI_Irecv" bytes="512" orank="21" region="0" commid="0" count="3384" tid="0" op="" dtype="" >6.4564e-04 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000020000000017" call="MPI_Irecv" bytes="512" orank="23" region="0" commid="0" count="3336" tid="0" op="" dtype="" >6.4993e-04 0.0000e+00 1.5974e-05</hent>
<hent key="03800100000000000000050000000002" call="MPI_Irecv" bytes="1280" orank="2" region="0" commid="0" count="127" tid="0" op="" dtype="" >2.9564e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000012" call="MPI_Irecv" bytes="1280" orank="18" region="0" commid="0" count="130" tid="0" op="" dtype="" >4.0531e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000015" call="MPI_Irecv" bytes="1280" orank="21" region="0" commid="0" count="52" tid="0" op="" dtype="" >1.6451e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000017" call="MPI_Irecv" bytes="1280" orank="23" region="0" commid="0" count="54" tid="0" op="" dtype="" >2.7180e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000005000000002E" call="MPI_Irecv" bytes="1280" orank="46" region="0" commid="0" count="51" tid="0" op="" dtype="" >3.0994e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4188e+01 9.0599e-06 1.8488e-01</hent>
<hent key="038001000000000000000500000002B6" call="MPI_Irecv" bytes="1280" orank="694" region="0" commid="0" count="45" tid="0" op="" dtype="" >3.1471e-05 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000020000000015" call="MPI_Isend" bytes="512" orank="21" region="0" commid="0" count="3338" tid="0" op="" dtype="" >1.7295e-03 0.0000e+00 1.2159e-05</hent>
<hent key="02400100000000000000020000000017" call="MPI_Isend" bytes="512" orank="23" region="0" commid="0" count="3314" tid="0" op="" dtype="" >2.2385e-03 0.0000e+00 1.1921e-05</hent>
<hent key="03800100000000000000080000000002" call="MPI_Irecv" bytes="2048" orank="2" region="0" commid="0" count="10" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000012" call="MPI_Irecv" bytes="2048" orank="18" region="0" commid="0" count="7" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000080000000015" call="MPI_Irecv" bytes="2048" orank="21" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.1921e-06 1.1921e-06 1.1921e-06</hent>
<hent key="0380010000000000000008000000002E" call="MPI_Irecv" bytes="2048" orank="46" region="0" commid="0" count="149" tid="0" op="" dtype="" >9.9421e-05 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000800000002B6" call="MPI_Irecv" bytes="2048" orank="694" region="0" commid="0" count="182" tid="0" op="" dtype="" >1.3804e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000050000000002" call="MPI_Isend" bytes="1280" orank="2" region="0" commid="0" count="140" tid="0" op="" dtype="" >2.2960e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000050000000012" call="MPI_Isend" bytes="1280" orank="18" region="0" commid="0" count="128" tid="0" op="" dtype="" >2.0719e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000015" call="MPI_Isend" bytes="1280" orank="21" region="0" commid="0" count="56" tid="0" op="" dtype="" >1.1063e-04 9.5367e-07 2.8610e-06</hent>
<hent key="02400100000000000000050000000017" call="MPI_Isend" bytes="1280" orank="23" region="0" commid="0" count="72" tid="0" op="" dtype="" >2.8324e-04 2.8610e-06 1.6212e-05</hent>
<hent key="0240010000000000000005000000002E" call="MPI_Isend" bytes="1280" orank="46" region="0" commid="0" count="49" tid="0" op="" dtype="" >2.8586e-04 5.0068e-06 1.2875e-05</hent>
<hent key="024001000000000000000500000002B6" call="MPI_Isend" bytes="1280" orank="694" region="0" commid="0" count="46" tid="0" op="" dtype="" >2.6655e-04 2.8610e-06 1.1921e-05</hent>
</hash>
<internal rank="22" log_i="1724765674.538711" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="23" mpi_size="696" stamp_init="1724765564.465381" stamp_final="1724765674.531712" username="apac4" allocationname="unknown" flags="0" pid="1612140" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >192109</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01a</host>
<perf wtime="1.10066e+02" utime="8.97860e+01" stime="1.36923e+01" mtime="7.27409e+01" gflop="0.00000e+00" gbyte="3.78010e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27409e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e515e715e8152f55e815e71534" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09931e+02" utime="8.97510e+01" stime="1.36843e+01" mtime="7.27409e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27409e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1899e+09" > 6.0998e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2055e+09" > 2.8848e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1332e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9789e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.4809e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6744e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5730e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.0011e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0595e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.3046e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4505e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="696" > </comm>
</comms>
<hash nlog="383408" nkey="212" >
<hent key="02400100000000000000080000000003" call="MPI_Isend" bytes="2048" orank="3" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.2875e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000080000000013" call="MPI_Isend" bytes="2048" orank="19" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.6212e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000080000000014" call="MPI_Isend" bytes="2048" orank="20" region="0" commid="0" count="1" tid="0" op="" dtype="" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="0240010000000000000008000000002F" call="MPI_Isend" bytes="2048" orank="47" region="0" commid="0" count="127" tid="0" op="" dtype="" >7.2503e-04 4.7684e-06 1.2159e-05</hent>
<hent key="024001000000000000000800000002B7" call="MPI_Isend" bytes="2048" orank="695" region="0" commid="0" count="161" tid="0" op="" dtype="" >8.0681e-04 3.8147e-06 6.1989e-06</hent>
<hent key="038001000000000000000E0000000014" call="MPI_Irecv" bytes="3584" orank="20" region="0" commid="0" count="66" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000E0000000016" call="MPI_Irecv" bytes="3584" orank="22" region="0" commid="0" count="295" tid="0" op="" dtype="" >1.1373e-04 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000E000000002F" call="MPI_Irecv" bytes="3584" orank="47" region="0" commid="0" count="131" tid="0" op="" dtype="" >5.9128e-05 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000000E00000002B7" call="MPI_Irecv" bytes="3584" orank="695" region="0" commid="0" count="145" tid="0" op="" dtype="" >4.7445e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000107000000002800000000" call="MPI_Bcast" bytes="40" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000E0000000014" call="MPI_Isend" bytes="3584" orank="20" region="0" commid="0" count="205" tid="0" op="" dtype="" >4.9043e-04 0.0000e+00 8.1062e-06</hent>
<hent key="024001000000000000000E0000000016" call="MPI_Isend" bytes="3584" orank="22" region="0" commid="0" count="259" tid="0" op="" dtype="" >3.8981e-04 0.0000e+00 3.0994e-06</hent>
<hent key="024001000000000000000E000000002F" call="MPI_Isend" bytes="3584" orank="47" region="0" commid="0" count="132" tid="0" op="" dtype="" >8.2326e-04 5.0068e-06 1.4067e-05</hent>
<hent key="03800100000000000000028000000003" call="MPI_Irecv" bytes="640" orank="3" region="0" commid="0" count="419" tid="0" op="" dtype="" >1.0467e-04 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000E00000002B7" call="MPI_Isend" bytes="3584" orank="695" region="0" commid="0" count="126" tid="0" op="" dtype="" >6.8808e-04 4.0531e-06 1.0967e-05</hent>
<hent key="03800100000000000000028000000013" call="MPI_Irecv" bytes="640" orank="19" region="0" commid="0" count="417" tid="0" op="" dtype="" >1.0824e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000014" call="MPI_Irecv" bytes="640" orank="20" region="0" commid="0" count="374" tid="0" op="" dtype="" >1.0610e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000016" call="MPI_Irecv" bytes="640" orank="22" region="0" commid="0" count="352" tid="0" op="" dtype="" >1.5402e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000002800000002F" call="MPI_Irecv" bytes="640" orank="47" region="0" commid="0" count="25" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000014000000002F" call="MPI_Irecv" bytes="5120" orank="47" region="0" commid="0" count="793" tid="0" op="" dtype="" >1.4877e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="038001000000000000000280000002B7" call="MPI_Irecv" bytes="640" orank="695" region="0" commid="0" count="21" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001400000002B7" call="MPI_Irecv" bytes="5120" orank="695" region="0" commid="0" count="688" tid="0" op="" dtype="" >1.3185e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000A0000000002F" call="MPI_Irecv" bytes="40960" orank="47" region="0" commid="0" count="2726" tid="0" op="" dtype="" >8.2922e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000028000000003" call="MPI_Isend" bytes="640" orank="3" region="0" commid="0" count="413" tid="0" op="" dtype="" >5.4073e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000028000000013" call="MPI_Isend" bytes="640" orank="19" region="0" commid="0" count="413" tid="0" op="" dtype="" >5.0402e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000028000000014" call="MPI_Isend" bytes="640" orank="20" region="0" commid="0" count="370" tid="0" op="" dtype="" >1.1406e-03 1.9073e-06 7.8678e-06</hent>
<hent key="02400100000000000000028000000016" call="MPI_Isend" bytes="640" orank="22" region="0" commid="0" count="384" tid="0" op="" dtype="" >5.9557e-04 9.5367e-07 1.0014e-05</hent>
<hent key="0240010000000000000002800000002F" call="MPI_Isend" bytes="640" orank="47" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.8644e-04 4.0531e-06 5.9605e-06</hent>
<hent key="03800100000000000000A000000002B7" call="MPI_Irecv" bytes="40960" orank="695" region="0" commid="0" count="2408" tid="0" op="" dtype="" >5.1785e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000014000000002F" call="MPI_Isend" bytes="5120" orank="47" region="0" commid="0" count="608" tid="0" op="" dtype="" >2.5620e-03 1.9073e-06 2.0027e-05</hent>
<hent key="024001000000000000000280000002B7" call="MPI_Isend" bytes="640" orank="695" region="0" commid="0" count="19" tid="0" op="" dtype="" >9.0361e-05 3.8147e-06 7.1526e-06</hent>
<hent key="0380010000000000000080000000002F" call="MPI_Irecv" bytes="32768" orank="47" region="0" commid="0" count="9974" tid="0" op="" dtype="" >3.1397e-03 0.0000e+00 3.4094e-05</hent>
<hent key="024001000000000000001400000002B7" call="MPI_Isend" bytes="5120" orank="695" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.4248e-03 9.5367e-07 9.0599e-06</hent>
<hent key="038001000000000000008000000002B7" call="MPI_Irecv" bytes="32768" orank="695" region="0" commid="0" count="10292" tid="0" op="" dtype="" >2.2743e-03 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000014000000003" call="MPI_Irecv" bytes="320" orank="3" region="0" commid="0" count="440" tid="0" op="" dtype="" >1.2946e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000013" call="MPI_Irecv" bytes="320" orank="19" region="0" commid="0" count="428" tid="0" op="" dtype="" >1.2779e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000014000000014" call="MPI_Irecv" bytes="320" orank="20" region="0" commid="0" count="593" tid="0" op="" dtype="" >2.1124e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000016" call="MPI_Irecv" bytes="320" orank="22" region="0" commid="0" count="562" tid="0" op="" dtype="" >2.3937e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000001400000002F" call="MPI_Irecv" bytes="320" orank="47" region="0" commid="0" count="16" tid="0" op="" dtype="" >7.3910e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A0000000002F" call="MPI_Isend" bytes="40960" orank="47" region="0" commid="0" count="1997" tid="0" op="" dtype="" >3.8421e-02 9.0599e-06 4.1962e-05</hent>
<hent key="038001000000000000000140000002B7" call="MPI_Irecv" bytes="320" orank="695" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.0252e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000A000000002B7" call="MPI_Isend" bytes="40960" orank="695" region="0" commid="0" count="1162" tid="0" op="" dtype="" >1.2937e-02 6.9141e-06 2.0027e-05</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106800" tid="0" op="" dtype="" >6.1332e+00 0.0000e+00 1.1066e-01</hent>
<hent key="0240010000000000000080000000002F" call="MPI_Isend" bytes="32768" orank="47" region="0" commid="0" count="10703" tid="0" op="" dtype="" >2.1181e-01 7.8678e-06 7.4148e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.0027e-05 9.5367e-07 1.8120e-05</hent>
<hent key="024001000000000000008000000002B7" call="MPI_Isend" bytes="32768" orank="695" region="0" commid="0" count="11538" tid="0" op="" dtype="" >1.2785e-01 6.9141e-06 1.7810e-04</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.8147e-06 0.0000e+00 2.8610e-06</hent>
<hent key="02400100000000000000014000000003" call="MPI_Isend" bytes="320" orank="3" region="0" commid="0" count="452" tid="0" op="" dtype="" >4.6659e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000014000000013" call="MPI_Isend" bytes="320" orank="19" region="0" commid="0" count="458" tid="0" op="" dtype="" >4.6706e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000014" call="MPI_Isend" bytes="320" orank="20" region="0" commid="0" count="577" tid="0" op="" dtype="" >1.5965e-03 1.9073e-06 1.4067e-05</hent>
<hent key="02400100000000000000014000000016" call="MPI_Isend" bytes="320" orank="22" region="0" commid="0" count="566" tid="0" op="" dtype="" >6.7973e-04 0.0000e+00 1.2159e-05</hent>
<hent key="0240010000000000000001400000002F" call="MPI_Isend" bytes="320" orank="47" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.2970e-04 3.8147e-06 5.9605e-06</hent>
<hent key="024001000000000000000140000002B7" call="MPI_Isend" bytes="320" orank="695" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.2445e-04 3.8147e-06 5.0068e-06</hent>
<hent key="03800100000000000000000000000003" call="MPI_Irecv" bytes="0" orank="3" region="0" commid="0" count="340" tid="0" op="" dtype="" >8.0824e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000013" call="MPI_Irecv" bytes="0" orank="19" region="0" commid="0" count="371" tid="0" op="" dtype="" >8.3923e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000014" call="MPI_Irecv" bytes="0" orank="20" region="0" commid="0" count="507" tid="0" op="" dtype="" >1.3256e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000016" call="MPI_Irecv" bytes="0" orank="22" region="0" commid="0" count="517" tid="0" op="" dtype="" >1.8001e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000002F" call="MPI_Irecv" bytes="0" orank="47" region="0" commid="0" count="85" tid="0" op="" dtype="" >1.7405e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000000000002B7" call="MPI_Irecv" bytes="0" orank="695" region="0" commid="0" count="87" tid="0" op="" dtype="" >3.2187e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.1029e-04 0.0000e+00 8.2970e-05</hent>
<hent key="03800100000000000000030000000003" call="MPI_Irecv" bytes="768" orank="3" region="0" commid="0" count="738" tid="0" op="" dtype="" >1.0777e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000030000000013" call="MPI_Irecv" bytes="768" orank="19" region="0" commid="0" count="762" tid="0" op="" dtype="" >1.3614e-04 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000002000000002B7" call="MPI_Isend" bytes="8192" orank="695" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.0014e-05 1.0014e-05 1.0014e-05</hent>
<hent key="02400100000000000000000000000003" call="MPI_Isend" bytes="0" orank="3" region="0" commid="0" count="336" tid="0" op="" dtype="" >2.3842e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000000000013" call="MPI_Isend" bytes="0" orank="19" region="0" commid="0" count="316" tid="0" op="" dtype="" >2.1243e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000000000014" call="MPI_Isend" bytes="0" orank="20" region="0" commid="0" count="527" tid="0" op="" dtype="" >1.0970e-03 0.0000e+00 5.6028e-05</hent>
<hent key="02400100000000000000000000000016" call="MPI_Isend" bytes="0" orank="22" region="0" commid="0" count="529" tid="0" op="" dtype="" >3.6550e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000000000000002F" call="MPI_Isend" bytes="0" orank="47" region="0" commid="0" count="79" tid="0" op="" dtype="" >3.2282e-04 9.5367e-07 5.9605e-06</hent>
<hent key="024001000000000000000000000002B7" call="MPI_Isend" bytes="0" orank="695" region="0" commid="0" count="85" tid="0" op="" dtype="" >3.2282e-04 1.9073e-06 5.0068e-06</hent>
<hent key="03800100000000000000060000000003" call="MPI_Irecv" bytes="1536" orank="3" region="0" commid="0" count="52" tid="0" op="" dtype="" >2.2411e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000013" call="MPI_Irecv" bytes="1536" orank="19" region="0" commid="0" count="40" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000014" call="MPI_Irecv" bytes="1536" orank="20" region="0" commid="0" count="10" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000060000000016" call="MPI_Irecv" bytes="1536" orank="22" region="0" commid="0" count="17" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000002F" call="MPI_Irecv" bytes="1536" orank="47" region="0" commid="0" count="83" tid="0" op="" dtype="" >1.9789e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002B7" call="MPI_Irecv" bytes="1536" orank="695" region="0" commid="0" count="72" tid="0" op="" dtype="" >1.5259e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000030000000003" call="MPI_Isend" bytes="768" orank="3" region="0" commid="0" count="508" tid="0" op="" dtype="" >2.5272e-04 0.0000e+00 1.2159e-05</hent>
<hent key="02400100000000000000030000000013" call="MPI_Isend" bytes="768" orank="19" region="0" commid="0" count="742" tid="0" op="" dtype="" >3.4809e-04 0.0000e+00 9.0599e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 1.9073e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C000000014" call="MPI_Irecv" bytes="448" orank="20" region="0" commid="0" count="18" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000001C000000016" call="MPI_Irecv" bytes="448" orank="22" region="0" commid="0" count="86" tid="0" op="" dtype="" >2.0027e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000003" call="MPI_Isend" bytes="1536" orank="3" region="0" commid="0" count="62" tid="0" op="" dtype="" >1.1635e-04 9.5367e-07 7.1526e-06</hent>
<hent key="02400100000000000000060000000013" call="MPI_Isend" bytes="1536" orank="19" region="0" commid="0" count="65" tid="0" op="" dtype="" >1.1539e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000014" call="MPI_Isend" bytes="1536" orank="20" region="0" commid="0" count="15" tid="0" op="" dtype="" >6.5565e-05 2.8610e-06 1.3113e-05</hent>
<hent key="02400100000000000000060000000016" call="MPI_Isend" bytes="1536" orank="22" region="0" commid="0" count="15" tid="0" op="" dtype="" >2.9802e-05 1.9073e-06 2.8610e-06</hent>
<hent key="0240010000000000000006000000002F" call="MPI_Isend" bytes="1536" orank="47" region="0" commid="0" count="78" tid="0" op="" dtype="" >4.3464e-04 4.7684e-06 1.1921e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002B7" call="MPI_Isend" bytes="1536" orank="695" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.6812e-04 3.8147e-06 6.1989e-06</hent>
<hent key="038001000000000000000C000000002F" call="MPI_Irecv" bytes="3072" orank="47" region="0" commid="0" count="370" tid="0" op="" dtype="" >1.1969e-04 0.0000e+00 2.1458e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002B7" call="MPI_Irecv" bytes="3072" orank="695" region="0" commid="0" count="331" tid="0" op="" dtype="" >1.0681e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000001C000000014" call="MPI_Isend" bytes="448" orank="20" region="0" commid="0" count="54" tid="0" op="" dtype="" >3.7193e-05 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000001C000000016" call="MPI_Isend" bytes="448" orank="22" region="0" commid="0" count="64" tid="0" op="" dtype="" >2.9087e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C000000002F" call="MPI_Isend" bytes="3072" orank="47" region="0" commid="0" count="390" tid="0" op="" dtype="" >2.3720e-03 4.7684e-06 1.2875e-05</hent>
<hent key="024001000000000000000C00000002B7" call="MPI_Isend" bytes="3072" orank="695" region="0" commid="0" count="375" tid="0" op="" dtype="" >2.0189e-03 3.8147e-06 1.5974e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.1711e-03 3.7909e-04 3.9983e-04</hent>
<hent key="03800100000000000000038000000003" call="MPI_Irecv" bytes="896" orank="3" region="0" commid="0" count="2928" tid="0" op="" dtype="" >4.7612e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000013" call="MPI_Irecv" bytes="896" orank="19" region="0" commid="0" count="2913" tid="0" op="" dtype="" >5.7220e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000014" call="MPI_Irecv" bytes="896" orank="20" region="0" commid="0" count="163" tid="0" op="" dtype="" >5.9128e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000016" call="MPI_Irecv" bytes="896" orank="22" region="0" commid="0" count="176" tid="0" op="" dtype="" >5.9605e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000002F" call="MPI_Irecv" bytes="896" orank="47" region="0" commid="0" count="35" tid="0" op="" dtype="" >1.1683e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002B7" call="MPI_Irecv" bytes="896" orank="695" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.0729e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.4700e-04 2.4700e-04 2.4700e-04</hent>
<hent key="09000100000107000000003800000000" call="MPI_Bcast" bytes="56" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="03800100000000000000180000000003" call="MPI_Irecv" bytes="6144" orank="3" region="0" commid="0" count="2771" tid="0" op="" dtype="" >2.6655e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000180000000013" call="MPI_Irecv" bytes="6144" orank="19" region="0" commid="0" count="2878" tid="0" op="" dtype="" >5.9962e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000018000000002F" call="MPI_Irecv" bytes="6144" orank="47" region="0" commid="0" count="11" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000001800000002B7" call="MPI_Irecv" bytes="6144" orank="695" region="0" commid="0" count="19" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000038000000003" call="MPI_Isend" bytes="896" orank="3" region="0" commid="0" count="3169" tid="0" op="" dtype="" >1.9164e-03 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000038000000013" call="MPI_Isend" bytes="896" orank="19" region="0" commid="0" count="2939" tid="0" op="" dtype="" >1.6766e-03 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000038000000014" call="MPI_Isend" bytes="896" orank="20" region="0" commid="0" count="150" tid="0" op="" dtype="" >5.2595e-04 2.1458e-06 1.6928e-05</hent>
<hent key="02400100000000000000038000000016" call="MPI_Isend" bytes="896" orank="22" region="0" commid="0" count="147" tid="0" op="" dtype="" >2.7275e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000003800000002F" call="MPI_Isend" bytes="896" orank="47" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.5426e-04 4.0531e-06 6.1989e-06</hent>
<hent key="024001000000000000000380000002B7" call="MPI_Isend" bytes="896" orank="695" region="0" commid="0" count="33" tid="0" op="" dtype="" >1.5354e-04 3.8147e-06 5.0068e-06</hent>
<hent key="02400100000000000000180000000003" call="MPI_Isend" bytes="6144" orank="3" region="0" commid="0" count="1854" tid="0" op="" dtype="" >4.0512e-03 9.5367e-07 1.5974e-05</hent>
<hent key="02400100000000000000180000000013" call="MPI_Isend" bytes="6144" orank="19" region="0" commid="0" count="2767" tid="0" op="" dtype="" >5.0890e-03 9.5367e-07 1.3113e-05</hent>
<hent key="0240010000000000000018000000002F" call="MPI_Isend" bytes="6144" orank="47" region="0" commid="0" count="8" tid="0" op="" dtype="" >6.1274e-05 6.9141e-06 8.1062e-06</hent>
<hent key="09000100000102000000300000000000" call="MPI_Bcast" bytes="12288" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.3896e-05 6.3896e-05 6.3896e-05</hent>
<hent key="024001000000000000001800000002B7" call="MPI_Isend" bytes="6144" orank="695" region="0" commid="0" count="9" tid="0" op="" dtype="" >6.0797e-05 5.9605e-06 7.1526e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >9.4624e+00 9.0599e-06 1.2673e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >8.5998e-04 8.5998e-04 8.5998e-04</hent>
<hent key="09000100000102000000018000000000" call="MPI_Bcast" bytes="384" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0595e-02 1.0595e-02 1.0595e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >4.7240e-03 4.7240e-03 4.7240e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.0057e-01 3.3741e-03 1.9225e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.7912e-04 5.7912e-04 5.7912e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5730e+00 4.4608e-04 2.4720e-01</hent>
<hent key="0900010000010700000000E000000000" call="MPI_Bcast" bytes="224" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >3.4809e-05 0.0000e+00 1.5974e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0967e-05 1.0967e-05 1.0967e-05</hent>
<hent key="03800100000000000000070000000003" call="MPI_Irecv" bytes="1792" orank="3" region="0" commid="0" count="29" tid="0" op="" dtype="" >6.6757e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000013" call="MPI_Irecv" bytes="1792" orank="19" region="0" commid="0" count="17" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000070000000014" call="MPI_Irecv" bytes="1792" orank="20" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000070000000016" call="MPI_Irecv" bytes="1792" orank="22" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000002F" call="MPI_Irecv" bytes="1792" orank="47" region="0" commid="0" count="114" tid="0" op="" dtype="" >4.7207e-05 0.0000e+00 6.1989e-06</hent>
<hent key="038001000000000000000700000002B7" call="MPI_Irecv" bytes="1792" orank="695" region="0" commid="0" count="134" tid="0" op="" dtype="" >3.5763e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >8.3208e-05 9.5367e-07 5.9843e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.9788e+01 0.0000e+00 3.8416e+01</hent>
<hent key="038001000000000000000A0000000003" call="MPI_Irecv" bytes="2560" orank="3" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000013" call="MPI_Irecv" bytes="2560" orank="19" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.9073e-06 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000A0000000014" call="MPI_Irecv" bytes="2560" orank="20" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A000000002F" call="MPI_Irecv" bytes="2560" orank="47" region="0" commid="0" count="364" tid="0" op="" dtype="" >1.2374e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002B7" call="MPI_Irecv" bytes="2560" orank="695" region="0" commid="0" count="363" tid="0" op="" dtype="" >1.1349e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000003" call="MPI_Isend" bytes="1792" orank="3" region="0" commid="0" count="17" tid="0" op="" dtype="" >3.0041e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000013" call="MPI_Isend" bytes="1792" orank="19" region="0" commid="0" count="19" tid="0" op="" dtype="" >3.1471e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000014" call="MPI_Isend" bytes="1792" orank="20" region="0" commid="0" count="1" tid="0" op="" dtype="" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="02400100000000000000070000000016" call="MPI_Isend" bytes="1792" orank="22" region="0" commid="0" count="4" tid="0" op="" dtype="" >7.8678e-06 1.9073e-06 2.1458e-06</hent>
<hent key="0240010000000000000007000000002F" call="MPI_Isend" bytes="1792" orank="47" region="0" commid="0" count="123" tid="0" op="" dtype="" >6.9809e-04 4.7684e-06 1.0967e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3046e-02 8.3046e-02 8.3046e-02</hent>
<hent key="024001000000000000000700000002B7" call="MPI_Isend" bytes="1792" orank="695" region="0" commid="0" count="121" tid="0" op="" dtype="" >6.0773e-04 3.8147e-06 1.0014e-05</hent>
<hent key="024001000000000000000A0000000003" call="MPI_Isend" bytes="2560" orank="3" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.0599e-06 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000013" call="MPI_Isend" bytes="2560" orank="19" region="0" commid="0" count="3" tid="0" op="" dtype="" >5.7220e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000A0000000016" call="MPI_Isend" bytes="2560" orank="22" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000A000000002F" call="MPI_Isend" bytes="2560" orank="47" region="0" commid="0" count="363" tid="0" op="" dtype="" >2.1701e-03 5.0068e-06 1.3113e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >5.1785e-04 5.0068e-05 2.3484e-04</hent>
<hent key="024001000000000000000A00000002B7" call="MPI_Isend" bytes="2560" orank="695" region="0" commid="0" count="356" tid="0" op="" dtype="" >1.8208e-03 3.8147e-06 1.0014e-05</hent>
<hent key="03800100000000000000100000000014" call="MPI_Irecv" bytes="4096" orank="20" region="0" commid="0" count="12634" tid="0" op="" dtype="" >1.4436e-03 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000100000000016" call="MPI_Irecv" bytes="4096" orank="22" region="0" commid="0" count="12405" tid="0" op="" dtype="" >4.8926e-03 0.0000e+00 6.1989e-06</hent>
<hent key="0380010000000000000010000000002F" call="MPI_Irecv" bytes="4096" orank="47" region="0" commid="0" count="2882" tid="0" op="" dtype="" >5.8246e-04 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000001000000002B7" call="MPI_Irecv" bytes="4096" orank="695" region="0" commid="0" count="2980" tid="0" op="" dtype="" >7.3957e-04 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000100000000014" call="MPI_Isend" bytes="4096" orank="20" region="0" commid="0" count="12495" tid="0" op="" dtype="" >3.1113e-02 0.0000e+00 6.4135e-05</hent>
<hent key="02400100000000000000100000000016" call="MPI_Isend" bytes="4096" orank="22" region="0" commid="0" count="12441" tid="0" op="" dtype="" >1.9739e-02 0.0000e+00 1.6928e-05</hent>
<hent key="0240010000000000000010000000002F" call="MPI_Isend" bytes="4096" orank="47" region="0" commid="0" count="3064" tid="0" op="" dtype="" >1.2602e-02 1.1921e-06 2.5034e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.8129e-03 6.4301e-04 1.1699e-03</hent>
<hent key="024001000000000000001000000002B7" call="MPI_Isend" bytes="4096" orank="695" region="0" commid="0" count="3298" tid="0" op="" dtype="" >1.1662e-02 9.5367e-07 2.2173e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-05 5.0068e-05 5.0068e-05</hent>
<hent key="0AC00100000102000001000000000000" call="MPI_Scatterv" bytes="65536" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.9420e-03 5.1308e-04 1.3959e-03</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.0967e-05 2.1458e-06 8.8215e-06</hent>
<hent key="03800100000000000000000400000003" call="MPI_Irecv" bytes="4" orank="3" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.9843e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000013" call="MPI_Irecv" bytes="4" orank="19" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.3048e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000014" call="MPI_Irecv" bytes="4" orank="20" region="0" commid="0" count="3400" tid="0" op="" dtype="" >5.8222e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000016" call="MPI_Irecv" bytes="4" orank="22" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.0519e-03 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000040000002F" call="MPI_Irecv" bytes="4" orank="47" region="0" commid="0" count="3400" tid="0" op="" dtype="" >7.5173e-04 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000001C0000000003" call="MPI_Irecv" bytes="7168" orank="3" region="0" commid="0" count="9929" tid="0" op="" dtype="" >1.0018e-03 0.0000e+00 6.1989e-06</hent>
<hent key="038001000000000000001C0000000013" call="MPI_Irecv" bytes="7168" orank="19" region="0" commid="0" count="9822" tid="0" op="" dtype="" >2.1961e-03 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000004000002B7" call="MPI_Irecv" bytes="4" orank="695" region="0" commid="0" count="3400" tid="0" op="" dtype="" >7.7534e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000400000003" call="MPI_Isend" bytes="4" orank="3" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.8990e-03 0.0000e+00 1.5020e-04</hent>
<hent key="02400100000000000000000400000013" call="MPI_Isend" bytes="4" orank="19" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.3063e-03 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000000400000014" call="MPI_Isend" bytes="4" orank="20" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.8176e-03 0.0000e+00 6.1035e-05</hent>
<hent key="02400100000000000000000400000016" call="MPI_Isend" bytes="4" orank="22" region="0" commid="0" count="3400" tid="0" op="" dtype="" >1.4431e-03 0.0000e+00 4.7684e-06</hent>
<hent key="0240010000000000000000040000002F" call="MPI_Isend" bytes="4" orank="47" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.5717e-02 4.0531e-06 3.0041e-05</hent>
<hent key="024001000000000000001C0000000003" call="MPI_Isend" bytes="7168" orank="3" region="0" commid="0" count="10846" tid="0" op="" dtype="" >2.3962e-02 9.5367e-07 1.9789e-05</hent>
<hent key="024001000000000000001C0000000013" call="MPI_Isend" bytes="7168" orank="19" region="0" commid="0" count="9933" tid="0" op="" dtype="" >1.8920e-02 9.5367e-07 3.5048e-05</hent>
<hent key="024001000000000000001C000000002F" call="MPI_Isend" bytes="7168" orank="47" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.4782e-05 6.9141e-06 7.8678e-06</hent>
<hent key="024001000000000000000004000002B7" call="MPI_Isend" bytes="4" orank="695" region="0" commid="0" count="3400" tid="0" op="" dtype="" >2.0177e-02 3.8147e-06 1.1396e-04</hent>
<hent key="024001000000000000001C00000002B7" call="MPI_Isend" bytes="7168" orank="695" region="0" commid="0" count="1" tid="0" op="" dtype="" >6.9141e-06 6.9141e-06 6.9141e-06</hent>
<hent key="0AC00100000102000000800000000000" call="MPI_Scatterv" bytes="32768" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.5504e-04 5.5504e-04 5.5504e-04</hent>
<hent key="03800100000000000000020000000014" call="MPI_Irecv" bytes="512" orank="20" region="0" commid="0" count="3382" tid="0" op="" dtype="" >6.5351e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000020000000016" call="MPI_Irecv" bytes="512" orank="22" region="0" commid="0" count="3314" tid="0" op="" dtype="" >8.2564e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000050000000003" call="MPI_Irecv" bytes="1280" orank="3" region="0" commid="0" count="145" tid="0" op="" dtype="" >3.7432e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000013" call="MPI_Irecv" bytes="1280" orank="19" region="0" commid="0" count="141" tid="0" op="" dtype="" >4.2200e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000014" call="MPI_Irecv" bytes="1280" orank="20" region="0" commid="0" count="50" tid="0" op="" dtype="" >1.2636e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000016" call="MPI_Irecv" bytes="1280" orank="22" region="0" commid="0" count="72" tid="0" op="" dtype="" >2.1935e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000002F" call="MPI_Irecv" bytes="1280" orank="47" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.7405e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.4836e+01 1.0014e-05 1.8481e-01</hent>
<hent key="038001000000000000000500000002B7" call="MPI_Irecv" bytes="1280" orank="695" region="0" commid="0" count="46" tid="0" op="" dtype="" >1.4544e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000020000000014" call="MPI_Isend" bytes="512" orank="20" region="0" commid="0" count="3346" tid="0" op="" dtype="" >2.1589e-03 0.0000e+00 1.3828e-05</hent>
<hent key="02400100000000000000020000000016" call="MPI_Isend" bytes="512" orank="22" region="0" commid="0" count="3336" tid="0" op="" dtype="" >1.6773e-03 0.0000e+00 1.4067e-05</hent>
<hent key="03800100000000000000080000000003" call="MPI_Irecv" bytes="2048" orank="3" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000013" call="MPI_Irecv" bytes="2048" orank="19" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000080000000016" call="MPI_Irecv" bytes="2048" orank="22" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000008000000002F" call="MPI_Irecv" bytes="2048" orank="47" region="0" commid="0" count="144" tid="0" op="" dtype="" >4.9114e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002B7" call="MPI_Irecv" bytes="2048" orank="695" region="0" commid="0" count="157" tid="0" op="" dtype="" >4.5776e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000050000000003" call="MPI_Isend" bytes="1280" orank="3" region="0" commid="0" count="132" tid="0" op="" dtype="" >2.2769e-04 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000050000000013" call="MPI_Isend" bytes="1280" orank="19" region="0" commid="0" count="137" tid="0" op="" dtype="" >2.1815e-04 9.5367e-07 6.1989e-06</hent>
<hent key="02400100000000000000050000000014" call="MPI_Isend" bytes="1280" orank="20" region="0" commid="0" count="59" tid="0" op="" dtype="" >2.1482e-04 2.8610e-06 8.1062e-06</hent>
<hent key="02400100000000000000050000000016" call="MPI_Isend" bytes="1280" orank="22" region="0" commid="0" count="54" tid="0" op="" dtype="" >1.0633e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000005000000002F" call="MPI_Isend" bytes="1280" orank="47" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.7977e-04 5.0068e-06 1.1921e-05</hent>
<hent key="024001000000000000000500000002B7" call="MPI_Isend" bytes="1280" orank="695" region="0" commid="0" count="48" tid="0" op="" dtype="" >2.3317e-04 4.0531e-06 5.9605e-06</hent>
</hash>
<internal rank="23" log_i="1724765674.531712" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="24" mpi_size="696" stamp_init="1724765564.532237" stamp_final="1724765674.530925" username="apac4" allocationname="unknown" flags="0" pid="107152" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.09999e+02" utime="7.70224e+01" stime="2.51590e+01" mtime="7.21216e+01" gflop="0.00000e+00" gbyte="3.87108e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21216e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e014e114e214e055e214e214af" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09870e+02" utime="7.69937e+01" stime="2.51455e+01" mtime="7.21216e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21216e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2073e+09" > 7.4708e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2161e+09" > 4.0667e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3261e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9721e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0793e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5718e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.1286e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0556e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2995e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3612e+01 </func>
</region>
</regions>
<internal rank="24" log_i="1724765674.530925" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="25" mpi_size="696" stamp_init="1724765564.531327" stamp_final="1724765674.530344" username="apac4" allocationname="unknown" flags="0" pid="107153" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.09999e+02" utime="8.96726e+01" stime="1.35034e+01" mtime="7.21117e+01" gflop="0.00000e+00" gbyte="3.76781e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21117e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008415c656841584150a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09866e+02" utime="8.96397e+01" stime="1.34934e+01" mtime="7.21117e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21117e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1971e+09" > 6.4191e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1875e+09" > 3.4402e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8603e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9712e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9028e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5725e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.7571e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0565e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2839e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4186e+01 </func>
</region>
</regions>
<internal rank="25" log_i="1724765674.530344" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="26" mpi_size="696" stamp_init="1724765564.531315" stamp_final="1724765674.535644" username="apac4" allocationname="unknown" flags="0" pid="107154" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.10004e+02" utime="8.74510e+01" stime="1.44762e+01" mtime="7.19265e+01" gflop="0.00000e+00" gbyte="3.77556e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19265e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4bd14bf14c0149155c014bf14ea" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09878e+02" utime="8.74170e+01" stime="1.44686e+01" mtime="7.19265e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19265e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1915e+09" > 7.6807e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1859e+09" > 3.8615e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3501e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9723e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.8876e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.1005e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5719e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.3672e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0597e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2977e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3369e+01 </func>
</region>
</regions>
<internal rank="26" log_i="1724765674.535644" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="27" mpi_size="696" stamp_init="1724765564.531211" stamp_final="1724765674.525767" username="apac4" allocationname="unknown" flags="0" pid="107155" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.09995e+02" utime="8.91186e+01" stime="1.40331e+01" mtime="7.26749e+01" gflop="0.00000e+00" gbyte="3.76823e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.26749e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001314645613141314c1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09869e+02" utime="8.90870e+01" stime="1.40230e+01" mtime="7.26749e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.26749e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2114e+09" > 6.7764e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2106e+09" > 3.0281e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3127e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9725e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5749e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8841e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5706e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0330e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0597e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2847e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4251e+01 </func>
</region>
</regions>
<internal rank="27" log_i="1724765674.525767" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="28" mpi_size="696" stamp_init="1724765564.532285" stamp_final="1724765674.526238" username="apac4" allocationname="unknown" flags="0" pid="107156" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.09994e+02" utime="8.49918e+01" stime="1.53118e+01" mtime="7.21438e+01" gflop="0.00000e+00" gbyte="3.77762e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21438e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ec14fb55ec14ec14e7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09865e+02" utime="8.49613e+01" stime="1.53008e+01" mtime="7.21438e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21438e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2084e+09" > 1.0594e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2019e+09" > 5.1953e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.5537e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9696e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9671e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.3201e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5707e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0109e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0596e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2981e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2104e+01 </func>
</region>
</regions>
<internal rank="28" log_i="1724765674.526238" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="29" mpi_size="696" stamp_init="1724765564.531165" stamp_final="1724765674.535756" username="apac4" allocationname="unknown" flags="0" pid="107157" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.10005e+02" utime="8.97118e+01" stime="1.37506e+01" mtime="7.22147e+01" gflop="0.00000e+00" gbyte="3.77716e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22147e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000044144414eb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09879e+02" utime="8.96811e+01" stime="1.37394e+01" mtime="7.22147e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22147e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2186e+09" > 6.3537e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2138e+09" > 3.1644e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1315e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9711e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4305e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.1562e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5703e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1225e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0546e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2835e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4027e+01 </func>
</region>
</regions>
<internal rank="29" log_i="1724765674.535756" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="30" mpi_size="696" stamp_init="1724765564.531098" stamp_final="1724765674.530564" username="apac4" allocationname="unknown" flags="0" pid="107158" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.09999e+02" utime="8.63453e+01" stime="1.49173e+01" mtime="7.24770e+01" gflop="0.00000e+00" gbyte="3.78448e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24770e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09871e+02" utime="8.63148e+01" stime="1.49053e+01" mtime="7.24770e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24770e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1987e+09" > 9.3184e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2050e+09" > 4.9199e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8484e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9727e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.0109e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.3692e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5691e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1045e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0579e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2964e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3245e+01 </func>
</region>
</regions>
<internal rank="30" log_i="1724765674.530564" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="31" mpi_size="696" stamp_init="1724765564.531298" stamp_final="1724765674.526288" username="apac4" allocationname="unknown" flags="0" pid="107159" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.09995e+02" utime="8.99484e+01" stime="1.34819e+01" mtime="7.26561e+01" gflop="0.00000e+00" gbyte="3.76434e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.26561e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c614c814c9141155c914c914b2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09870e+02" utime="8.99200e+01" stime="1.34685e+01" mtime="7.26561e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.26561e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2158e+09" > 6.2122e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2167e+09" > 3.3511e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4384e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9721e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.1782e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5709e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0532e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0579e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2842e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4165e+01 </func>
</region>
</regions>
<internal rank="31" log_i="1724765674.526288" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="32" mpi_size="696" stamp_init="1724765564.532054" stamp_final="1724765674.536194" username="apac4" allocationname="unknown" flags="0" pid="107160" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.10004e+02" utime="8.10021e+01" stime="2.02886e+01" mtime="7.23181e+01" gflop="0.00000e+00" gbyte="3.77731e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23181e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e214e414e5148155e514e414b4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09877e+02" utime="8.09681e+01" stime="2.02807e+01" mtime="7.23181e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23181e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1892e+09" > 9.1770e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1939e+09" > 4.5591e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7247e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.4863e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8244e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8358e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5687e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2316e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0590e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2480e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3226e+01 </func>
</region>
</regions>
<internal rank="32" log_i="1724765674.536194" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="33" mpi_size="696" stamp_init="1724765564.531175" stamp_final="1724765674.533304" username="apac4" allocationname="unknown" flags="0" pid="107161" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.10002e+02" utime="8.96127e+01" stime="1.37975e+01" mtime="7.30734e+01" gflop="0.00000e+00" gbyte="3.76911e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.30734e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09875e+02" utime="8.95848e+01" stime="1.37835e+01" mtime="7.30734e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.30734e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1954e+09" > 6.7160e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1974e+09" > 2.8364e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9461e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9731e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.3152e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5685e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2603e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0568e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2484e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4019e+01 </func>
</region>
</regions>
<internal rank="33" log_i="1724765674.533304" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="34" mpi_size="696" stamp_init="1724765564.531079" stamp_final="1724765674.525404" username="apac4" allocationname="unknown" flags="0" pid="107162" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.09994e+02" utime="8.68751e+01" stime="1.45785e+01" mtime="7.25967e+01" gflop="0.00000e+00" gbyte="3.76698e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25967e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000cb14f255cb14cb14db" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09866e+02" utime="8.68465e+01" stime="1.45658e+01" mtime="7.25967e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25967e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2193e+09" > 8.5989e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2157e+09" > 5.6427e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8812e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9723e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.2745e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.3543e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5670e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3976e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0576e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2875e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3398e+01 </func>
</region>
</regions>
<internal rank="34" log_i="1724765674.525404" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="35" mpi_size="696" stamp_init="1724765564.531151" stamp_final="1724765674.525768" username="apac4" allocationname="unknown" flags="0" pid="107163" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.09995e+02" utime="8.90222e+01" stime="1.43308e+01" mtime="7.24626e+01" gflop="0.00000e+00" gbyte="3.77769e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24626e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf460146214631411556314631461" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09866e+02" utime="8.89914e+01" stime="1.43194e+01" mtime="7.24626e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24626e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1920e+09" > 6.7181e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1896e+09" > 2.9125e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2246e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9728e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9177e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5664e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4731e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0584e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2858e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4131e+01 </func>
</region>
</regions>
<internal rank="35" log_i="1724765674.525768" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="36" mpi_size="696" stamp_init="1724765564.531085" stamp_final="1724765674.529874" username="apac4" allocationname="unknown" flags="0" pid="107164" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.09999e+02" utime="8.55861e+01" stime="1.52053e+01" mtime="7.22599e+01" gflop="0.00000e+00" gbyte="3.76995e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22599e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49a149b149c1430569c149c1492" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09872e+02" utime="8.55526e+01" stime="1.51970e+01" mtime="7.22599e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22599e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1931e+09" > 8.6863e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1977e+09" > 5.8012e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9391e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9714e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0300e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4160e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5664e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4599e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0563e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2455e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3005e+01 </func>
</region>
</regions>
<internal rank="36" log_i="1724765674.529874" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="37" mpi_size="696" stamp_init="1724765564.531151" stamp_final="1724765674.527887" username="apac4" allocationname="unknown" flags="0" pid="107165" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.09997e+02" utime="8.91393e+01" stime="1.42896e+01" mtime="7.23572e+01" gflop="0.00000e+00" gbyte="3.77762e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23572e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ba15f455ba15b91517" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09870e+02" utime="8.91128e+01" stime="1.42739e+01" mtime="7.23572e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23572e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1870e+09" > 6.6327e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2002e+09" > 2.9689e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9508e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9721e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1233e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8935e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5662e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5032e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0564e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2836e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4316e+01 </func>
</region>
</regions>
<internal rank="37" log_i="1724765674.527887" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="38" mpi_size="696" stamp_init="1724765564.531089" stamp_final="1724765674.520969" username="apac4" allocationname="unknown" flags="0" pid="107166" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.09990e+02" utime="8.76012e+01" stime="1.46354e+01" mtime="7.29559e+01" gflop="0.00000e+00" gbyte="3.78262e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.29559e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44e1450145114ed5551145114d8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09862e+02" utime="8.75733e+01" stime="1.46207e+01" mtime="7.29559e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.29559e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127200" bytes="1.2046e+09" > 7.2660e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2090e+09" > 3.7223e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9478e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9728e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2006e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.6370e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5658e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5160e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0573e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2873e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3840e+01 </func>
</region>
</regions>
<internal rank="38" log_i="1724765674.520969" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="39" mpi_size="696" stamp_init="1724765564.531182" stamp_final="1724765674.524221" username="apac4" allocationname="unknown" flags="0" pid="107167" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.09993e+02" utime="8.95399e+01" stime="1.38835e+01" mtime="7.29797e+01" gflop="0.00000e+00" gbyte="3.77857e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.29797e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008715861505" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09866e+02" utime="8.95102e+01" stime="1.38709e+01" mtime="7.29797e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.29797e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2141e+09" > 6.6961e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2084e+09" > 2.6645e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4669e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9720e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.6270e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5664e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4777e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0593e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2852e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4421e+01 </func>
</region>
</regions>
<internal rank="39" log_i="1724765674.524221" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="40" mpi_size="696" stamp_init="1724765564.531084" stamp_final="1724765674.540539" username="apac4" allocationname="unknown" flags="0" pid="107168" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.10009e+02" utime="8.82162e+01" stime="1.42306e+01" mtime="7.21803e+01" gflop="0.00000e+00" gbyte="3.77705e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21803e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009115911542" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09882e+02" utime="8.81837e+01" stime="1.42207e+01" mtime="7.21803e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21803e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2177e+09" > 7.3215e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2213e+09" > 4.4415e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4917e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9714e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0260e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0037e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5661e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4833e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0555e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2870e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3519e+01 </func>
</region>
</regions>
<internal rank="40" log_i="1724765674.540539" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="41" mpi_size="696" stamp_init="1724765564.531160" stamp_final="1724765674.532686" username="apac4" allocationname="unknown" flags="0" pid="107169" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.10002e+02" utime="8.91991e+01" stime="1.42248e+01" mtime="7.25582e+01" gflop="0.00000e+00" gbyte="3.76427e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25582e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007f157f153e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09873e+02" utime="8.91693e+01" stime="1.42130e+01" mtime="7.25582e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25582e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1988e+09" > 6.7682e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2047e+09" > 2.8905e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3732e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9723e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4940e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5654e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5732e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0587e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2450e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4079e+01 </func>
</region>
</regions>
<internal rank="41" log_i="1724765674.532686" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="42" mpi_size="696" stamp_init="1724765564.531206" stamp_final="1724765674.534774" username="apac4" allocationname="unknown" flags="0" pid="107170" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.10004e+02" utime="8.75469e+01" stime="1.45850e+01" mtime="7.24457e+01" gflop="0.00000e+00" gbyte="3.78296e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24457e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000fb152655fb15fb1527" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09876e+02" utime="8.75166e+01" stime="1.45729e+01" mtime="7.24457e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24457e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2139e+09" > 7.9547e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2043e+09" > 3.6394e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6064e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9722e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.1246e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.1183e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5649e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5982e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0562e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2870e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3609e+01 </func>
</region>
</regions>
<internal rank="42" log_i="1724765674.534774" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="43" mpi_size="696" stamp_init="1724765564.531885" stamp_final="1724765674.532208" username="apac4" allocationname="unknown" flags="0" pid="107171" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.10000e+02" utime="8.93102e+01" stime="1.40557e+01" mtime="7.32689e+01" gflop="0.00000e+00" gbyte="3.77502e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.32689e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09872e+02" utime="8.92764e+01" stime="1.40480e+01" mtime="7.32689e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.32689e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1906e+09" > 6.7395e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1941e+09" > 2.8004e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9110e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9729e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9539e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5649e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5917e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0601e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2826e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4249e+01 </func>
</region>
</regions>
<internal rank="43" log_i="1724765674.532208" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="44" mpi_size="696" stamp_init="1724765564.531115" stamp_final="1724765674.535247" username="apac4" allocationname="unknown" flags="0" pid="107172" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.10004e+02" utime="8.51362e+01" stime="1.54435e+01" mtime="7.19501e+01" gflop="0.00000e+00" gbyte="3.77171e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19501e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09876e+02" utime="8.51076e+01" stime="1.54300e+01" mtime="7.19501e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19501e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1908e+09" > 9.5066e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1878e+09" > 4.8850e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4852e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9713e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5953e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.3015e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5640e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7101e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0550e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2453e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3076e+01 </func>
</region>
</regions>
<internal rank="44" log_i="1724765674.535247" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="45" mpi_size="696" stamp_init="1724765564.531192" stamp_final="1724765674.532712" username="apac4" allocationname="unknown" flags="0" pid="107173" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.10002e+02" utime="8.93266e+01" stime="1.41493e+01" mtime="7.31872e+01" gflop="0.00000e+00" gbyte="3.77083e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.31872e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09875e+02" utime="8.92971e+01" stime="1.41364e+01" mtime="7.31872e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.31872e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.7752e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1997e+09" > 6.3624e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2000e+09" > 3.0697e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8074e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9715e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3140e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9202e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5642e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6570e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0578e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2826e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4322e+01 </func>
</region>
</regions>
<internal rank="45" log_i="1724765674.532712" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="46" mpi_size="696" stamp_init="1724765564.532078" stamp_final="1724765674.540604" username="apac4" allocationname="unknown" flags="0" pid="107174" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.10009e+02" utime="8.78884e+01" stime="1.44894e+01" mtime="7.21874e+01" gflop="0.00000e+00" gbyte="3.76064e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21874e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09882e+02" utime="8.78527e+01" stime="1.44828e+01" mtime="7.21874e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21874e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2221e+09" > 7.5807e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2217e+09" > 3.9032e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5251e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9720e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.2929e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2772e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5642e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6616e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0579e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2851e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3469e+01 </func>
</region>
</regions>
<internal rank="46" log_i="1724765674.540604" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="47" mpi_size="696" stamp_init="1724765564.531178" stamp_final="1724765674.524105" username="apac4" allocationname="unknown" flags="0" pid="107175" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22b</host>
<perf wtime="1.09993e+02" utime="8.96015e+01" stime="1.37654e+01" mtime="7.22684e+01" gflop="0.00000e+00" gbyte="3.77590e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22684e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c814c714a8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09866e+02" utime="8.95740e+01" stime="1.37509e+01" mtime="7.22684e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22684e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2081e+09" > 6.2832e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2056e+09" > 3.2432e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8845e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9716e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0994e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6209e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5643e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6860e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0577e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2835e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4330e+01 </func>
</region>
</regions>
<internal rank="47" log_i="1724765674.524105" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="48" mpi_size="696" stamp_init="1724765564.843282" stamp_final="1724765674.540952" username="apac4" allocationname="unknown" flags="0" pid="181907" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09698e+02" utime="6.15363e+01" stime="1.90302e+01" mtime="5.06968e+01" gflop="0.00000e+00" gbyte="3.85376e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.06968e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09567e+02" utime="6.15066e+01" stime="1.90168e+01" mtime="5.06968e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.06968e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2141e+09" > 7.7738e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2078e+09" > 3.8473e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2533e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.4691e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7040e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5642e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6879e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0573e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2834e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3443e+01 </func>
</region>
</regions>
<internal rank="48" log_i="1724765674.540952" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="49" mpi_size="696" stamp_init="1724765564.843320" stamp_final="1724765674.537266" username="apac4" allocationname="unknown" flags="0" pid="181908" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09694e+02" utime="6.47209e+01" stime="8.88010e+00" mtime="4.30175e+01" gflop="0.00000e+00" gbyte="3.78773e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.30175e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003315331514" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09559e+02" utime="6.46884e+01" stime="8.86904e+00" mtime="4.30175e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.30175e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127200" bytes="1.1886e+09" > 6.3490e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1920e+09" > 2.8075e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6387e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.9916e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.6890e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5629e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7920e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0575e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2665e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4048e+01 </func>
</region>
</regions>
<internal rank="49" log_i="1724765674.537266" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="50" mpi_size="696" stamp_init="1724765564.843274" stamp_final="1724765674.535188" username="apac4" allocationname="unknown" flags="0" pid="181909" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09692e+02" utime="5.64147e+01" stime="8.36165e+00" mtime="3.49426e+01" gflop="0.00000e+00" gbyte="3.76446e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.49426e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d415d615d715bd56d715d7153c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09562e+02" utime="5.63857e+01" stime="8.34901e+00" mtime="3.49426e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.49426e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1900e+09" > 7.1863e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1875e+09" > 4.3640e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4432e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.6526e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.6056e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.0998e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5634e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7804e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0626e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2826e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3409e+01 </func>
</region>
</regions>
<internal rank="50" log_i="1724765674.535188" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="51" mpi_size="696" stamp_init="1724765564.843314" stamp_final="1724765674.524912" username="apac4" allocationname="unknown" flags="0" pid="181910" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09682e+02" utime="5.89349e+01" stime="7.86267e+00" mtime="3.61249e+01" gflop="0.00000e+00" gbyte="3.77716e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.61249e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006e146d14e6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09551e+02" utime="5.89047e+01" stime="7.85063e+00" mtime="3.61249e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.61249e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1983e+09" > 6.2541e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2042e+09" > 3.1647e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3781e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.1865e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7643e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2837e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5639e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7292e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0568e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2673e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4228e+01 </func>
</region>
</regions>
<internal rank="51" log_i="1724765674.524912" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="52" mpi_size="696" stamp_init="1724765564.843300" stamp_final="1724765674.525190" username="apac4" allocationname="unknown" flags="0" pid="181911" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09682e+02" utime="5.29419e+01" stime="9.86504e+00" mtime="3.48566e+01" gflop="0.00000e+00" gbyte="3.78471e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.48566e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf413141414161499551614151471" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09551e+02" utime="5.29060e+01" stime="9.85805e+00" mtime="3.48566e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.48566e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2118e+09" > 1.4335e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2056e+09" > 8.4451e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4795e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.4768e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4987e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8126e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5612e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8576e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0569e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2841e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2210e+01 </func>
</region>
</regions>
<internal rank="52" log_i="1724765674.525190" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="53" mpi_size="696" stamp_init="1724765564.843338" stamp_final="1724765674.533802" username="apac4" allocationname="unknown" flags="0" pid="181912" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09690e+02" utime="5.82199e+01" stime="7.54267e+00" mtime="3.50655e+01" gflop="0.00000e+00" gbyte="3.75496e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.50655e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d6146455d614d61497" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09564e+02" utime="5.81870e+01" stime="7.53315e+00" mtime="3.50655e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.50655e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2148e+09" > 6.2727e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2202e+09" > 2.9575e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2807e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.1285e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3896e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5628e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8367e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0605e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2625e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4324e+01 </func>
</region>
</regions>
<internal rank="53" log_i="1724765674.533802" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="54" mpi_size="696" stamp_init="1724765564.844576" stamp_final="1724765674.526851" username="apac4" allocationname="unknown" flags="0" pid="181913" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09682e+02" utime="6.06949e+01" stime="1.05022e+01" mtime="4.13771e+01" gflop="0.00000e+00" gbyte="3.77888e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.13771e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09553e+02" utime="6.06616e+01" stime="1.04933e+01" mtime="4.13771e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.13771e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2133e+09" > 7.6937e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2053e+09" > 4.6387e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9683e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.4708e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.8284e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.0415e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5621e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8490e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0593e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2789e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3619e+01 </func>
</region>
</regions>
<internal rank="54" log_i="1724765674.526851" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="55" mpi_size="696" stamp_init="1724765564.843323" stamp_final="1724765674.535065" username="apac4" allocationname="unknown" flags="0" pid="181914" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09692e+02" utime="5.74389e+01" stime="7.71120e+00" mtime="3.44597e+01" gflop="0.00000e+00" gbyte="3.76862e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.44597e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf42f1531153215ab563215321543" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09565e+02" utime="5.74049e+01" stime="7.70347e+00" mtime="3.44597e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.44597e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2145e+09" > 6.3479e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2091e+09" > 2.5458e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7334e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.4706e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2159e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.0487e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5624e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8803e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0590e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2645e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3920e+01 </func>
</region>
</regions>
<internal rank="55" log_i="1724765674.535065" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="56" mpi_size="696" stamp_init="1724765564.843271" stamp_final="1724765674.531994" username="apac4" allocationname="unknown" flags="0" pid="181915" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09689e+02" utime="5.98426e+01" stime="1.04365e+01" mtime="4.10934e+01" gflop="0.00000e+00" gbyte="3.76801e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.10934e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09559e+02" utime="5.98087e+01" stime="1.04284e+01" mtime="4.10934e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.10934e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1976e+09" > 7.7910e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1957e+09" > 3.9495e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8351e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.8201e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.5450e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.6532e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5611e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9743e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0601e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2760e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3400e+01 </func>
</region>
</regions>
<internal rank="56" log_i="1724765674.531994" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="57" mpi_size="696" stamp_init="1724765564.843340" stamp_final="1724765674.528410" username="apac4" allocationname="unknown" flags="0" pid="181916" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09685e+02" utime="8.12968e+01" stime="1.22416e+01" mtime="6.32644e+01" gflop="0.00000e+00" gbyte="3.76984e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.32644e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000cb15d155cb15cb153e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09558e+02" utime="8.12651e+01" stime="1.22311e+01" mtime="6.32644e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.32644e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1987e+09" > 6.5728e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2051e+09" > 2.6971e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0014e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.9879e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5272e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.2111e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5611e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0184e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0582e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2303e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4025e+01 </func>
</region>
</regions>
<internal rank="57" log_i="1724765674.528410" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="58" mpi_size="696" stamp_init="1724765564.844380" stamp_final="1724765674.525002" username="apac4" allocationname="unknown" flags="0" pid="181917" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09681e+02" utime="5.60908e+01" stime="8.37084e+00" mtime="3.48651e+01" gflop="0.00000e+00" gbyte="3.77853e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.48651e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d114d214d414fa55d414d3147c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09548e+02" utime="5.60596e+01" stime="8.36002e+00" mtime="3.48651e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.48651e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2196e+09" > 7.5754e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2176e+09" > 3.3598e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.5145e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.3898e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.5061e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3896e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5618e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9347e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0603e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2711e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2495e+01 </func>
</region>
</regions>
<internal rank="58" log_i="1724765674.525002" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="59" mpi_size="696" stamp_init="1724765564.843310" stamp_final="1724765674.528447" username="apac4" allocationname="unknown" flags="0" pid="181918" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09685e+02" utime="6.43450e+01" stime="8.76517e+00" mtime="4.28630e+01" gflop="0.00000e+00" gbyte="3.77033e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.28630e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09557e+02" utime="6.43128e+01" stime="8.75539e+00" mtime="4.28630e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.28630e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1994e+09" > 6.2653e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1947e+09" > 2.5509e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5621e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.7716e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6689e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1119e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5616e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9623e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0575e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2638e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4198e+01 </func>
</region>
</regions>
<internal rank="59" log_i="1724765674.528447" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="60" mpi_size="696" stamp_init="1724765564.844020" stamp_final="1724765674.534749" username="apac4" allocationname="unknown" flags="0" pid="181919" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09691e+02" utime="6.49617e+01" stime="1.28065e+01" mtime="4.82261e+01" gflop="0.00000e+00" gbyte="3.76816e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.82261e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008614861477" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09563e+02" utime="6.49261e+01" stime="1.28004e+01" mtime="4.82261e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.82261e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2041e+09" > 7.5003e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1975e+09" > 3.7361e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.1702e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.8022e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3797e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7724e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5609e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0349e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0599e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2712e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2994e+01 </func>
</region>
</regions>
<internal rank="60" log_i="1724765674.534749" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="61" mpi_size="696" stamp_init="1724765564.843310" stamp_final="1724765674.532269" username="apac4" allocationname="unknown" flags="0" pid="181920" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09689e+02" utime="5.78564e+01" stime="7.64947e+00" mtime="3.46400e+01" gflop="0.00000e+00" gbyte="3.78250e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.46400e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c114c455c114c114a3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09560e+02" utime="5.78217e+01" stime="7.64171e+00" mtime="3.46400e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.46400e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2040e+09" > 6.1319e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1905e+09" > 2.5596e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2198e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.8177e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0299e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5610e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0261e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0577e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2657e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4288e+01 </func>
</region>
</regions>
<internal rank="61" log_i="1724765674.532269" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="62" mpi_size="696" stamp_init="1724765564.843289" stamp_final="1724765674.525220" username="apac4" allocationname="unknown" flags="0" pid="181921" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09682e+02" utime="7.45594e+01" stime="1.55669e+01" mtime="6.01641e+01" gflop="0.00000e+00" gbyte="3.77831e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.01641e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c715c815ca151156ca15c9152f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09554e+02" utime="7.45284e+01" stime="1.55557e+01" mtime="6.01641e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.01641e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2129e+09" > 6.7817e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2098e+09" > 3.3076e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6947e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 4.7218e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2860e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0603e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5604e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0552e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0566e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2714e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3502e+01 </func>
</region>
</regions>
<internal rank="62" log_i="1724765674.525220" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="63" mpi_size="696" stamp_init="1724765564.843351" stamp_final="1724765674.534490" username="apac4" allocationname="unknown" flags="0" pid="181922" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09691e+02" utime="6.00907e+01" stime="8.27620e+00" mtime="3.83752e+01" gflop="0.00000e+00" gbyte="3.76896e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.83752e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09561e+02" utime="6.00599e+01" stime="8.26434e+00" mtime="3.83752e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.83752e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1979e+09" > 6.1508e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2004e+09" > 2.7761e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.2754e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 4.7567e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.5800e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5597e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.1123e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0598e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2663e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4026e+01 </func>
</region>
</regions>
<internal rank="63" log_i="1724765674.534490" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="64" mpi_size="696" stamp_init="1724765564.843302" stamp_final="1724765674.535125" username="apac4" allocationname="unknown" flags="0" pid="181923" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09692e+02" utime="5.87428e+01" stime="9.24083e+00" mtime="3.74935e+01" gflop="0.00000e+00" gbyte="3.77731e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.74935e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000bd14bd145f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09562e+02" utime="5.87047e+01" stime="9.23610e+00" mtime="3.74935e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.74935e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2253e+09" > 6.9813e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2161e+09" > 3.0420e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2965e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.3510e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0768e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5588e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.2454e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0576e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2298e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3366e+01 </func>
</region>
</regions>
<internal rank="64" log_i="1724765674.535125" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="65" mpi_size="696" stamp_init="1724765564.843665" stamp_final="1724765674.528584" username="apac4" allocationname="unknown" flags="0" pid="181924" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09685e+02" utime="8.35697e+01" stime="1.34180e+01" mtime="6.61741e+01" gflop="0.00000e+00" gbyte="3.78101e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.61741e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000034143314af" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09555e+02" utime="8.35393e+01" stime="1.34058e+01" mtime="6.61741e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.61741e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2100e+09" > 6.5398e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1990e+09" > 2.6379e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5715e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.3556e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2411e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.6566e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5582e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.3122e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0569e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2609e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3691e+01 </func>
</region>
</regions>
<internal rank="65" log_i="1724765674.528584" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="66" mpi_size="696" stamp_init="1724765564.844419" stamp_final="1724765674.533370" username="apac4" allocationname="unknown" flags="0" pid="181925" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09689e+02" utime="6.24857e+01" stime="9.36038e+00" mtime="4.19262e+01" gflop="0.00000e+00" gbyte="3.77567e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.19262e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09559e+02" utime="6.24502e+01" stime="9.35397e+00" mtime="4.19262e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.19262e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2105e+09" > 7.3532e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2098e+09" > 3.7637e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4234e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.6413e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8862e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3417e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5575e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.3495e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0594e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2681e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3408e+01 </func>
</region>
</regions>
<internal rank="66" log_i="1724765674.533370" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="67" mpi_size="696" stamp_init="1724765564.843335" stamp_final="1724765674.526044" username="apac4" allocationname="unknown" flags="0" pid="181926" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09683e+02" utime="8.82541e+01" stime="1.35296e+01" mtime="7.12004e+01" gflop="0.00000e+00" gbyte="3.78151e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.12004e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09551e+02" utime="8.82194e+01" stime="1.35218e+01" mtime="7.12004e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.12004e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2033e+09" > 6.3316e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2014e+09" > 2.8287e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5940e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.8154e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0504e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.3340e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5571e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.3793e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0629e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2623e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4115e+01 </func>
</region>
</regions>
<internal rank="67" log_i="1724765674.526044" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="68" mpi_size="696" stamp_init="1724765564.843310" stamp_final="1724765674.524686" username="apac4" allocationname="unknown" flags="0" pid="181927" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09681e+02" utime="6.11854e+01" stime="9.46867e+00" mtime="4.09214e+01" gflop="0.00000e+00" gbyte="3.75053e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.09214e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09551e+02" utime="6.11511e+01" stime="9.46089e+00" mtime="4.09214e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.09214e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127200" bytes="1.1902e+09" > 8.3585e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1891e+09" > 4.2904e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9998e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.1382e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.6771e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7556e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5565e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.4434e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0624e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2670e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3221e+01 </func>
</region>
</regions>
<internal rank="68" log_i="1724765674.524686" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="69" mpi_size="696" stamp_init="1724765564.844577" stamp_final="1724765674.528344" username="apac4" allocationname="unknown" flags="0" pid="181928" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09684e+02" utime="7.38564e+01" stime="1.04302e+01" mtime="5.30340e+01" gflop="0.00000e+00" gbyte="3.77792e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.30340e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e514e614e7146d56e714e714fc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09555e+02" utime="7.38209e+01" stime="1.04235e+01" mtime="5.30340e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.30340e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1890e+09" > 6.4624e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2003e+09" > 2.8194e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1073e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.0649e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9772e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5569e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.4087e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0582e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2635e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3929e+01 </func>
</region>
</regions>
<internal rank="69" log_i="1724765674.528344" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="70" mpi_size="696" stamp_init="1724765564.844579" stamp_final="1724765674.535807" username="apac4" allocationname="unknown" flags="0" pid="181929" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09691e+02" utime="7.42886e+01" stime="1.20810e+01" mtime="5.67867e+01" gflop="0.00000e+00" gbyte="3.78246e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.67867e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000861585153d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09561e+02" utime="7.42557e+01" stime="1.20712e+01" mtime="5.67867e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.67867e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2183e+09" > 7.8152e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2233e+09" > 4.1347e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6926e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.4414e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.8889e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2392e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5563e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.4583e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0631e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2672e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3177e+01 </func>
</region>
</regions>
<internal rank="70" log_i="1724765674.535807" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="71" mpi_size="696" stamp_init="1724765564.843331" stamp_final="1724765674.536037" username="apac4" allocationname="unknown" flags="0" pid="181930" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u01b</host>
<perf wtime="1.09693e+02" utime="8.28432e+01" stime="1.20684e+01" mtime="6.39566e+01" gflop="0.00000e+00" gbyte="3.76690e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.39566e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09566e+02" utime="8.28112e+01" stime="1.20579e+01" mtime="6.39566e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.39566e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2106e+09" > 6.2970e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2111e+09" > 2.8806e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0097e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.1304e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8273e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5559e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.5090e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0575e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2651e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4310e+01 </func>
</region>
</regions>
<internal rank="71" log_i="1724765674.536037" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="72" mpi_size="696" stamp_init="1724765564.799848" stamp_final="1724765674.545200" username="apac4" allocationname="unknown" flags="0" pid="917264" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09745e+02" utime="6.52688e+01" stime="2.03521e+01" mtime="5.49859e+01" gflop="0.00000e+00" gbyte="3.84632e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.49859e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004714461457" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09619e+02" utime="6.52391e+01" stime="2.03394e+01" mtime="5.49859e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.49859e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2047e+09" > 8.7673e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2085e+09" > 3.9689e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.6541e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.2828e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0460e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.2510e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5551e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.5866e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0653e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2744e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2998e+01 </func>
</region>
</regions>
<internal rank="72" log_i="1724765674.545200" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="73" mpi_size="696" stamp_init="1724765564.800499" stamp_final="1724765674.538640" username="apac4" allocationname="unknown" flags="0" pid="917265" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09738e+02" utime="6.78125e+01" stime="9.62924e+00" mtime="4.66203e+01" gflop="0.00000e+00" gbyte="3.77693e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.66203e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf446154815491548564915491549" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09608e+02" utime="6.77792e+01" stime="9.61918e+00" mtime="4.66203e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.66203e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1820e+09" > 6.4379e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1881e+09" > 3.3298e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8180e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.3752e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7180e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.9973e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5564e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.4495e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0659e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2728e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3698e+01 </func>
</region>
</regions>
<internal rank="73" log_i="1724765674.538640" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="74" mpi_size="696" stamp_init="1724765564.799788" stamp_final="1724765674.536365" username="apac4" allocationname="unknown" flags="0" pid="917266" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09737e+02" utime="6.28549e+01" stime="9.29918e+00" mtime="4.19102e+01" gflop="0.00000e+00" gbyte="3.77411e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.19102e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09607e+02" utime="6.28250e+01" stime="9.28712e+00" mtime="4.19102e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.19102e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1839e+09" > 8.5977e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1924e+09" > 4.1068e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6287e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.9417e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2851e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.6472e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5562e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.4807e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0669e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2742e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2763e+01 </func>
</region>
</regions>
<internal rank="74" log_i="1724765674.536365" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="75" mpi_size="696" stamp_init="1724765564.799726" stamp_final="1724765674.531637" username="apac4" allocationname="unknown" flags="0" pid="917267" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09732e+02" utime="8.01498e+01" stime="1.23923e+01" mtime="6.19388e+01" gflop="0.00000e+00" gbyte="3.77796e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.19388e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09604e+02" utime="8.01141e+01" stime="1.23854e+01" mtime="6.19388e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.19388e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1980e+09" > 6.0792e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1922e+09" > 3.4753e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5575e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.8906e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.1600e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5558e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.5552e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0659e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2715e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4157e+01 </func>
</region>
</regions>
<internal rank="75" log_i="1724765674.531637" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="76" mpi_size="696" stamp_init="1724765564.800413" stamp_final="1724765674.541657" username="apac4" allocationname="unknown" flags="0" pid="917268" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09741e+02" utime="6.36165e+01" stime="1.15730e+01" mtime="4.46932e+01" gflop="0.00000e+00" gbyte="3.78067e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.46932e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf46715681569159c556915691553" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09613e+02" utime="6.35801e+01" stime="1.15670e+01" mtime="4.46932e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.46932e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2000e+09" > 8.3569e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2087e+09" > 5.0712e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7055e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.2643e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0060e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.8900e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5554e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.5406e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0692e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2736e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2102e+01 </func>
</region>
</regions>
<internal rank="76" log_i="1724765674.541657" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="77" mpi_size="696" stamp_init="1724765564.799712" stamp_final="1724765674.537977" username="apac4" allocationname="unknown" flags="0" pid="917269" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09738e+02" utime="6.44884e+01" stime="8.58414e+00" mtime="4.10283e+01" gflop="0.00000e+00" gbyte="3.76774e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.10283e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4de14e014e1141555e114e114d8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09610e+02" utime="6.44536e+01" stime="8.57707e+00" mtime="4.10283e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.10283e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2136e+09" > 6.6178e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2177e+09" > 3.4713e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.5243e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.4326e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6689e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4336e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5547e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.6355e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0664e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2711e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3700e+01 </func>
</region>
</regions>
<internal rank="77" log_i="1724765674.537977" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="78" mpi_size="696" stamp_init="1724765564.799902" stamp_final="1724765674.536805" username="apac4" allocationname="unknown" flags="0" pid="917270" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09737e+02" utime="5.66084e+01" stime="8.65075e+00" mtime="3.47138e+01" gflop="0.00000e+00" gbyte="3.77224e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.47138e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000cd15cc1540" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09609e+02" utime="5.65748e+01" stime="8.64220e+00" mtime="3.47138e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.47138e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2115e+09" > 8.2147e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2118e+09" > 4.7538e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3336e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.2695e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2988e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.0950e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5551e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.6267e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0655e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2748e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2564e+01 </func>
</region>
</regions>
<internal rank="78" log_i="1724765674.536805" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="79" mpi_size="696" stamp_init="1724765564.799720" stamp_final="1724765674.543199" username="apac4" allocationname="unknown" flags="0" pid="917271" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09743e+02" utime="7.85158e+01" stime="1.25112e+01" mtime="5.98921e+01" gflop="0.00000e+00" gbyte="3.78159e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.98921e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e414e414e1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09614e+02" utime="7.84808e+01" stime="1.25037e+01" mtime="5.98921e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.98921e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127200" bytes="1.2021e+09" > 6.5253e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2062e+09" > 2.7420e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7600e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.7359e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0192e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5551e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.6306e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0668e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2700e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3418e+01 </func>
</region>
</regions>
<internal rank="79" log_i="1724765674.543199" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="80" mpi_size="696" stamp_init="1724765564.799717" stamp_final="1724765674.528127" username="apac4" allocationname="unknown" flags="0" pid="917272" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09728e+02" utime="7.47259e+01" stime="1.67379e+01" mtime="6.14730e+01" gflop="0.00000e+00" gbyte="3.76663e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.14730e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4851486148714975587148714e5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09601e+02" utime="7.46935e+01" stime="1.67285e+01" mtime="6.14730e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.14730e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1979e+09" > 8.5468e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1982e+09" > 4.7399e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5402e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.3776e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8626e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1140e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5538e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.7637e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0670e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2736e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2341e+01 </func>
</region>
</regions>
<internal rank="80" log_i="1724765674.528127" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="81" mpi_size="696" stamp_init="1724765564.799724" stamp_final="1724765674.529184" username="apac4" allocationname="unknown" flags="0" pid="917273" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09729e+02" utime="5.98869e+01" stime="7.73786e+00" mtime="3.60349e+01" gflop="0.00000e+00" gbyte="3.78174e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.60349e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ce14cd14b2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09601e+02" utime="5.98587e+01" stime="7.72368e+00" mtime="3.60349e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.60349e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2021e+09" > 6.2824e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1983e+09" > 3.2645e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9950e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9688e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0300e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5530e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.7950e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0674e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2702e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3726e+01 </func>
</region>
</regions>
<internal rank="81" log_i="1724765674.529184" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="82" mpi_size="696" stamp_init="1724765564.800143" stamp_final="1724765674.538046" username="apac4" allocationname="unknown" flags="0" pid="917274" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09738e+02" utime="5.20832e+01" stime="9.31044e+00" mtime="3.34265e+01" gflop="0.00000e+00" gbyte="3.76427e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.34265e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007214be557214711490" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09611e+02" utime="5.20523e+01" stime="9.29974e+00" mtime="3.34265e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.34265e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2201e+09" > 2.3274e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2253e+09" > 6.8748e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4995e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.3795e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.6267e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2098e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5502e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.9547e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0651e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2748e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.1477e+01 </func>
</region>
</regions>
<internal rank="82" log_i="1724765674.538046" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="83" mpi_size="696" stamp_init="1724765564.799717" stamp_final="1724765674.528993" username="apac4" allocationname="unknown" flags="0" pid="917275" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09729e+02" utime="5.78029e+01" stime="7.48004e+00" mtime="3.46004e+01" gflop="0.00000e+00" gbyte="3.77861e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.46004e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09600e+02" utime="5.77712e+01" stime="7.46972e+00" mtime="3.46004e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.46004e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2060e+09" > 6.3629e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2105e+09" > 3.3612e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9592e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.6463e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8389e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5515e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.9925e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0683e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2705e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3650e+01 </func>
</region>
</regions>
<internal rank="83" log_i="1724765674.528993" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="84" mpi_size="696" stamp_init="1724765564.799776" stamp_final="1724765674.534359" username="apac4" allocationname="unknown" flags="0" pid="917276" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09735e+02" utime="5.49279e+01" stime="8.91378e+00" mtime="3.47884e+01" gflop="0.00000e+00" gbyte="3.76842e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.47884e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e214e214f2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09610e+02" utime="5.48938e+01" stime="8.90628e+00" mtime="3.47884e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.47884e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2060e+09" > 1.0985e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1975e+09" > 5.5787e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5722e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.2708e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.0407e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.1679e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5517e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.9415e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0647e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2716e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2612e+01 </func>
</region>
</regions>
<internal rank="84" log_i="1724765674.534359" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="85" mpi_size="696" stamp_init="1724765564.800015" stamp_final="1724765674.536066" username="apac4" allocationname="unknown" flags="0" pid="917277" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09736e+02" utime="5.85110e+01" stime="7.42428e+00" mtime="3.41223e+01" gflop="0.00000e+00" gbyte="3.76698e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.41223e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49d149e14a0148f56a0149f14de" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09608e+02" utime="5.84793e+01" stime="7.41439e+00" mtime="3.41223e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.41223e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127200" bytes="1.1949e+09" > 6.0565e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1971e+09" > 3.9347e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9969e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.2696e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2132e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5513e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.0216e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0668e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2703e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3535e+01 </func>
</region>
</regions>
<internal rank="85" log_i="1724765674.536066" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="86" mpi_size="696" stamp_init="1724765564.800406" stamp_final="1724765674.528328" username="apac4" allocationname="unknown" flags="0" pid="917278" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09728e+02" utime="5.94461e+01" stime="9.26042e+00" mtime="3.84053e+01" gflop="0.00000e+00" gbyte="3.77331e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.84053e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000057145614b9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09602e+02" utime="5.94137e+01" stime="9.25117e+00" mtime="3.84053e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.84053e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2158e+09" > 8.2173e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2165e+09" > 4.3657e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7577e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 6.7383e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1992e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2269e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5512e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.9769e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0679e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2719e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2368e+01 </func>
</region>
</regions>
<internal rank="86" log_i="1724765674.528328" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="87" mpi_size="696" stamp_init="1724765564.800480" stamp_final="1724765674.538063" username="apac4" allocationname="unknown" flags="0" pid="917279" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09738e+02" utime="6.41114e+01" stime="8.76609e+00" mtime="4.19091e+01" gflop="0.00000e+00" gbyte="3.76869e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.19091e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a114a114eb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09611e+02" utime="6.40799e+01" stime="8.75601e+00" mtime="4.19091e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.19091e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1842e+09" > 6.1694e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1965e+09" > 3.6778e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7777e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.2018e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6703e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.0351e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5508e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.0157e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0694e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2696e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3592e+01 </func>
</region>
</regions>
<internal rank="87" log_i="1724765674.538063" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="88" mpi_size="696" stamp_init="1724765564.800447" stamp_final="1724765674.535187" username="apac4" allocationname="unknown" flags="0" pid="917280" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09735e+02" utime="5.48850e+01" stime="8.23838e+00" mtime="3.28069e+01" gflop="0.00000e+00" gbyte="3.77949e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28069e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d2159b55d215d21521" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09607e+02" utime="5.48505e+01" stime="8.23073e+00" mtime="3.28069e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28069e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2140e+09" > 1.0151e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2150e+09" > 4.3673e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1434e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.3571e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.6042e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1196e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5510e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.0380e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0684e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2724e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2562e+01 </func>
</region>
</regions>
<internal rank="88" log_i="1724765674.535187" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="89" mpi_size="696" stamp_init="1724765564.799714" stamp_final="1724765674.532213" username="apac4" allocationname="unknown" flags="0" pid="917281" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09732e+02" utime="6.51372e+01" stime="8.77080e+00" mtime="4.26591e+01" gflop="0.00000e+00" gbyte="3.77514e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.26591e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000eb146055eb14eb14f4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09603e+02" utime="6.51048e+01" stime="8.76103e+00" mtime="4.26591e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.26591e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2014e+09" > 6.7386e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2132e+09" > 3.2422e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7549e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0296e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8195e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5503e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.0662e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0659e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2694e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3227e+01 </func>
</region>
</regions>
<internal rank="89" log_i="1724765674.532213" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="90" mpi_size="696" stamp_init="1724765564.799740" stamp_final="1724765674.531937" username="apac4" allocationname="unknown" flags="0" pid="917282" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09732e+02" utime="6.74018e+01" stime="1.37597e+01" mtime="5.18437e+01" gflop="0.00000e+00" gbyte="3.77579e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.18437e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4cd14cf14d0144d55d014cf149a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09604e+02" utime="6.73727e+01" stime="1.37461e+01" mtime="5.18437e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.18437e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2103e+09" > 9.7170e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2064e+09" > 4.2159e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.2230e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 7.5096e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1815e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.2852e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5500e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.1479e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0655e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2288e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2605e+01 </func>
</region>
</regions>
<internal rank="90" log_i="1724765674.531937" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="91" mpi_size="696" stamp_init="1724765564.799704" stamp_final="1724765674.548792" username="apac4" allocationname="unknown" flags="0" pid="917283" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09749e+02" utime="6.27616e+01" stime="8.42839e+00" mtime="4.02670e+01" gflop="0.00000e+00" gbyte="3.76804e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.02670e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf46b156c156d15a5556d156d1527" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09621e+02" utime="6.27246e+01" stime="8.42301e+00" mtime="4.02670e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.02670e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2127e+09" > 6.9203e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2105e+09" > 3.2930e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0307e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 7.5178e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.3400e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5497e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.1289e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0684e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2684e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3316e+01 </func>
</region>
</regions>
<internal rank="91" log_i="1724765674.548792" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="92" mpi_size="696" stamp_init="1724765564.799742" stamp_final="1724765674.536211" username="apac4" allocationname="unknown" flags="0" pid="917284" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09736e+02" utime="5.53077e+01" stime="8.34896e+00" mtime="3.34616e+01" gflop="0.00000e+00" gbyte="3.74767e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.34616e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ec14ec14c4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09610e+02" utime="5.52753e+01" stime="8.33967e+00" mtime="3.34616e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.34616e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1876e+09" > 8.4201e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1938e+09" > 4.6173e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4674e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.9519e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.8692e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8317e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5497e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.1770e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0667e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2731e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2479e+01 </func>
</region>
</regions>
<internal rank="92" log_i="1724765674.536211" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="93" mpi_size="696" stamp_init="1724765564.800247" stamp_final="1724765674.540823" username="apac4" allocationname="unknown" flags="0" pid="917285" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09741e+02" utime="6.18237e+01" stime="7.70730e+00" mtime="3.75409e+01" gflop="0.00000e+00" gbyte="3.77338e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.75409e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005b14d7565b145514bb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09614e+02" utime="6.17910e+01" stime="7.69773e+00" mtime="3.75409e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.75409e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1997e+09" > 6.1523e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1849e+09" > 3.4642e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.4574e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 5.8527e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.1783e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5492e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.1875e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0644e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2652e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3899e+01 </func>
</region>
</regions>
<internal rank="93" log_i="1724765674.540823" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="94" mpi_size="696" stamp_init="1724765564.799734" stamp_final="1724765674.539275" username="apac4" allocationname="unknown" flags="0" pid="917286" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09740e+02" utime="6.16839e+01" stime="9.60107e+00" mtime="4.08960e+01" gflop="0.00000e+00" gbyte="3.78300e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.08960e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000dd143d56dd14dd14e7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09611e+02" utime="6.16519e+01" stime="9.59023e+00" mtime="4.08960e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.08960e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2155e+09" > 9.2880e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2076e+09" > 4.1476e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9272e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.2437e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8406e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3205e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5495e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.1872e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0696e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2736e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2795e+01 </func>
</region>
</regions>
<internal rank="94" log_i="1724765674.539275" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="95" mpi_size="696" stamp_init="1724765564.799736" stamp_final="1724765674.536265" username="apac4" allocationname="unknown" flags="0" pid="917287" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="1.09737e+02" utime="6.43459e+01" stime="8.62201e+00" mtime="4.18850e+01" gflop="0.00000e+00" gbyte="3.77224e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.18850e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48f1490149114455591149114b5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09608e+02" utime="6.43037e+01" stime="8.62187e+00" mtime="4.18850e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.18850e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2154e+09" > 6.3043e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2107e+09" > 3.2561e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5738e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.2606e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.5351e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5429e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.8089e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0688e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2645e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3713e+01 </func>
</region>
</regions>
<internal rank="95" log_i="1724765674.536265" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="96" mpi_size="696" stamp_init="1724765564.775504" stamp_final="1724765674.537543" username="apac4" allocationname="unknown" flags="0" pid="1567828" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09762e+02" utime="7.72409e+01" stime="2.48054e+01" mtime="7.19595e+01" gflop="0.00000e+00" gbyte="3.85292e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19595e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44d144e144f14fa554f144f14ce" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09633e+02" utime="7.72080e+01" stime="2.47961e+01" mtime="7.19595e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19595e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2008e+09" > 6.6475e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1989e+09" > 3.2298e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7260e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9502e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2258e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6941e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5492e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.2351e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0542e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2690e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4137e+01 </func>
</region>
</regions>
<internal rank="96" log_i="1724765674.537543" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="97" mpi_size="696" stamp_init="1724765564.776021" stamp_final="1724765674.536619" username="apac4" allocationname="unknown" flags="0" pid="1567829" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09761e+02" utime="8.98991e+01" stime="1.32317e+01" mtime="7.27225e+01" gflop="0.00000e+00" gbyte="3.77666e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27225e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002d152c150e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09630e+02" utime="8.98675e+01" stime="1.32199e+01" mtime="7.27225e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27225e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1891e+09" > 6.0973e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1785e+09" > 2.6123e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1991e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9708e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.5848e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5488e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.2713e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0558e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2513e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4500e+01 </func>
</region>
</regions>
<internal rank="97" log_i="1724765674.536619" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="98" mpi_size="696" stamp_init="1724765564.775961" stamp_final="1724765674.539207" username="apac4" allocationname="unknown" flags="0" pid="1567830" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09763e+02" utime="8.74301e+01" stime="1.39574e+01" mtime="7.19807e+01" gflop="0.00000e+00" gbyte="3.77010e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19807e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006a156a1511" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09633e+02" utime="8.73932e+01" stime="1.39520e+01" mtime="7.19807e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19807e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1921e+09" > 7.7287e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1866e+09" > 3.3111e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2163e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9689e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.8399e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0035e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5495e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.2082e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0546e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2695e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3590e+01 </func>
</region>
</regions>
<internal rank="98" log_i="1724765674.539207" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="99" mpi_size="696" stamp_init="1724765564.775306" stamp_final="1724765674.534327" username="apac4" allocationname="unknown" flags="0" pid="1567831" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09759e+02" utime="8.96574e+01" stime="1.34325e+01" mtime="7.29705e+01" gflop="0.00000e+00" gbyte="3.77365e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.29705e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f8159055f815f81503" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09627e+02" utime="8.96247e+01" stime="1.34233e+01" mtime="7.29705e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.29705e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1849e+09" > 5.8506e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2001e+09" > 2.6022e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2414e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9700e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5259e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.0222e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5488e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.2789e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0570e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2489e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4738e+01 </func>
</region>
</regions>
<internal rank="99" log_i="1724765674.534327" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="100" mpi_size="696" stamp_init="1724765564.776018" stamp_final="1724765674.540980" username="apac4" allocationname="unknown" flags="0" pid="1567832" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09765e+02" utime="8.49047e+01" stime="1.52424e+01" mtime="7.15769e+01" gflop="0.00000e+00" gbyte="3.78208e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15769e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a514a5146e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09634e+02" utime="8.48701e+01" stime="1.52346e+01" mtime="7.15769e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15769e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2043e+09" > 8.7023e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2081e+09" > 4.7605e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0370e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9714e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.4135e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1430e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5490e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.2615e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0530e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2687e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3229e+01 </func>
</region>
</regions>
<internal rank="100" log_i="1724765674.540980" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="101" mpi_size="696" stamp_init="1724765564.776405" stamp_final="1724765674.536533" username="apac4" allocationname="unknown" flags="0" pid="1567833" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09760e+02" utime="8.90455e+01" stime="1.39757e+01" mtime="7.32518e+01" gflop="0.00000e+00" gbyte="3.76858e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.32518e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43e1440144114735541144014da" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09633e+02" utime="8.90112e+01" stime="1.39682e+01" mtime="7.32518e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.32518e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2077e+09" > 6.3376e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2126e+09" > 2.7043e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6942e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9700e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9123e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5483e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.3282e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0543e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2514e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4516e+01 </func>
</region>
</regions>
<internal rank="101" log_i="1724765674.536533" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="102" mpi_size="696" stamp_init="1724765564.776194" stamp_final="1724765674.539875" username="apac4" allocationname="unknown" flags="0" pid="1567834" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09764e+02" utime="8.79730e+01" stime="1.40365e+01" mtime="7.18612e+01" gflop="0.00000e+00" gbyte="3.77506e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18612e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf494149514961410569614961493" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09635e+02" utime="8.79434e+01" stime="1.40239e+01" mtime="7.18612e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18612e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2143e+09" > 6.9681e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2109e+09" > 3.5909e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8935e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9706e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.4424e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3009e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5482e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.2992e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0535e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2507e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3847e+01 </func>
</region>
</regions>
<internal rank="102" log_i="1724765674.539875" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="103" mpi_size="696" stamp_init="1724765564.775960" stamp_final="1724765674.529947" username="apac4" allocationname="unknown" flags="0" pid="1567835" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09754e+02" utime="8.95320e+01" stime="1.36251e+01" mtime="7.26796e+01" gflop="0.00000e+00" gbyte="3.78284e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.26796e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007e147d14e5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09624e+02" utime="8.95027e+01" stime="1.36121e+01" mtime="7.26796e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.26796e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2069e+09" > 6.0647e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2010e+09" > 2.5893e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1660e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9679e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3212e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5483e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.2928e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0570e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2471e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4522e+01 </func>
</region>
</regions>
<internal rank="103" log_i="1724765674.529947" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="104" mpi_size="696" stamp_init="1724765564.775890" stamp_final="1724765674.539150" username="apac4" allocationname="unknown" flags="0" pid="1567836" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09763e+02" utime="8.58584e+01" stime="1.47585e+01" mtime="7.26724e+01" gflop="0.00000e+00" gbyte="3.77094e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.26724e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006214d55662146114da" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09633e+02" utime="8.58246e+01" stime="1.47505e+01" mtime="7.26724e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.26724e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1985e+09" > 8.1371e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1979e+09" > 4.0422e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0668e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9710e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2435e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4324e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5473e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.3782e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0572e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2515e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3363e+01 </func>
</region>
</regions>
<internal rank="104" log_i="1724765674.539150" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="105" mpi_size="696" stamp_init="1724765564.776066" stamp_final="1724765674.529433" username="apac4" allocationname="unknown" flags="0" pid="1567837" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09753e+02" utime="8.99269e+01" stime="1.32502e+01" mtime="7.28649e+01" gflop="0.00000e+00" gbyte="3.77434e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28649e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09628e+02" utime="8.98912e+01" stime="1.32437e+01" mtime="7.28649e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28649e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1999e+09" > 5.9123e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1955e+09" > 2.9281e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2237e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9713e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8282e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5477e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.3925e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0540e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2470e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4626e+01 </func>
</region>
</regions>
<internal rank="105" log_i="1724765674.529433" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="106" mpi_size="696" stamp_init="1724765564.775945" stamp_final="1724765674.539090" username="apac4" allocationname="unknown" flags="0" pid="1567838" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09763e+02" utime="8.49983e+01" stime="1.53352e+01" mtime="7.20300e+01" gflop="0.00000e+00" gbyte="3.77365e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20300e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4bf15c115c2158e55c215c1150c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09633e+02" utime="8.49627e+01" stime="1.53289e+01" mtime="7.20300e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20300e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2236e+09" > 8.3350e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2141e+09" > 6.0900e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8660e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9711e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.1526e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9309e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5467e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.4251e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0537e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2523e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2879e+01 </func>
</region>
</regions>
<internal rank="106" log_i="1724765674.539090" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="107" mpi_size="696" stamp_init="1724765564.777055" stamp_final="1724765674.537136" username="apac4" allocationname="unknown" flags="0" pid="1567839" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09760e+02" utime="8.95494e+01" stime="1.33517e+01" mtime="7.32642e+01" gflop="0.00000e+00" gbyte="3.77876e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.32642e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006f146f1477" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09635e+02" utime="8.95152e+01" stime="1.33438e+01" mtime="7.32642e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.32642e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2146e+09" > 5.7741e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2093e+09" > 3.0622e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4343e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9708e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0137e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5458e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.5781e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0531e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2474e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4830e+01 </func>
</region>
</regions>
<internal rank="107" log_i="1724765674.537136" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="108" mpi_size="696" stamp_init="1724765564.775308" stamp_final="1724765674.528264" username="apac4" allocationname="unknown" flags="0" pid="1567840" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09753e+02" utime="8.77607e+01" stime="1.41853e+01" mtime="7.18620e+01" gflop="0.00000e+00" gbyte="3.77518e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18620e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09624e+02" utime="8.77270e+01" stime="1.41767e+01" mtime="7.18620e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18620e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1988e+09" > 6.7998e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2084e+09" > 3.8172e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9846e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9697e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.0127e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4534e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5457e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.5812e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0517e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2488e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3777e+01 </func>
</region>
</regions>
<internal rank="108" log_i="1724765674.528264" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="109" mpi_size="696" stamp_init="1724765564.775595" stamp_final="1724765674.528768" username="apac4" allocationname="unknown" flags="0" pid="1567841" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09753e+02" utime="8.92805e+01" stime="1.35310e+01" mtime="7.30964e+01" gflop="0.00000e+00" gbyte="3.77201e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.30964e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000039143914b9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09624e+02" utime="8.92511e+01" stime="1.35186e+01" mtime="7.30964e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.30964e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1970e+09" > 5.8502e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1933e+09" > 2.3928e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2000e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9711e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1921e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3592e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5457e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.5908e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0580e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2486e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4896e+01 </func>
</region>
</regions>
<internal rank="109" log_i="1724765674.528768" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="110" mpi_size="696" stamp_init="1724765564.775308" stamp_final="1724765674.532597" username="apac4" allocationname="unknown" flags="0" pid="1567842" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09757e+02" utime="8.64617e+01" stime="1.47151e+01" mtime="7.22048e+01" gflop="0.00000e+00" gbyte="3.76698e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22048e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4db15dd15de15d255de15de1531" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09629e+02" utime="8.64295e+01" stime="1.47057e+01" mtime="7.22048e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22048e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2208e+09" > 7.8464e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2157e+09" > 3.9808e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5940e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9717e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1206e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.5009e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5459e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.5602e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0527e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2488e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3391e+01 </func>
</region>
</regions>
<internal rank="110" log_i="1724765674.532597" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="111" mpi_size="696" stamp_init="1724765564.775966" stamp_final="1724765674.528808" username="apac4" allocationname="unknown" flags="0" pid="1567843" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09753e+02" utime="8.92415e+01" stime="1.39092e+01" mtime="7.31119e+01" gflop="0.00000e+00" gbyte="3.76778e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.31119e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d914db14dc14eb56dc14dc149a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09622e+02" utime="8.92045e+01" stime="1.39045e+01" mtime="7.31119e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.31119e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1894e+09" > 5.9420e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1900e+09" > 2.4661e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5551e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9710e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8290e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5453e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.6502e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0559e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2493e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4544e+01 </func>
</region>
</regions>
<internal rank="111" log_i="1724765674.528808" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="112" mpi_size="696" stamp_init="1724765564.776553" stamp_final="1724765674.539760" username="apac4" allocationname="unknown" flags="0" pid="1567844" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09763e+02" utime="8.71950e+01" stime="1.41376e+01" mtime="7.18877e+01" gflop="0.00000e+00" gbyte="3.75858e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18877e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09636e+02" utime="8.71559e+01" stime="1.41345e+01" mtime="7.18877e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18877e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2107e+09" > 7.6110e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2102e+09" > 3.3469e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0633e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9471e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1852e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4742e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5449e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.6541e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0522e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2090e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3663e+01 </func>
</region>
</regions>
<internal rank="112" log_i="1724765674.539760" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="113" mpi_size="696" stamp_init="1724765564.775614" stamp_final="1724765674.529571" username="apac4" allocationname="unknown" flags="0" pid="1567845" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09754e+02" utime="8.95665e+01" stime="1.36883e+01" mtime="7.31524e+01" gflop="0.00000e+00" gbyte="3.77579e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.31524e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09628e+02" utime="8.95313e+01" stime="1.36812e+01" mtime="7.31524e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.31524e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2208e+09" > 5.9509e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2068e+09" > 2.7726e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6321e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9695e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0057e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5448e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.6848e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0568e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2464e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4517e+01 </func>
</region>
</regions>
<internal rank="113" log_i="1724765674.529571" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="114" mpi_size="696" stamp_init="1724765564.775892" stamp_final="1724765674.528121" username="apac4" allocationname="unknown" flags="0" pid="1567846" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09752e+02" utime="8.83185e+01" stime="1.41099e+01" mtime="7.26183e+01" gflop="0.00000e+00" gbyte="3.77327e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.26183e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a615a5154b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09623e+02" utime="8.82827e+01" stime="1.41035e+01" mtime="7.26183e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.26183e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1969e+09" > 6.6045e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2062e+09" > 3.6140e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8806e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9700e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.1989e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9999e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5439e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.7498e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0536e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2508e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3663e+01 </func>
</region>
</regions>
<internal rank="114" log_i="1724765674.528121" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="115" mpi_size="696" stamp_init="1724765564.775761" stamp_final="1724765674.531250" username="apac4" allocationname="unknown" flags="0" pid="1567847" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09755e+02" utime="8.95516e+01" stime="1.34361e+01" mtime="7.28935e+01" gflop="0.00000e+00" gbyte="3.78235e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28935e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43614371438143f55381438147e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09629e+02" utime="8.95171e+01" stime="1.34278e+01" mtime="7.28935e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28935e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2044e+09" > 5.9677e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2182e+09" > 2.7770e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7887e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9702e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4908e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5441e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.7237e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0523e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2472e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4098e+01 </func>
</region>
</regions>
<internal rank="115" log_i="1724765674.531250" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="116" mpi_size="696" stamp_init="1724765564.775357" stamp_final="1724765674.540054" username="apac4" allocationname="unknown" flags="0" pid="1567848" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09765e+02" utime="8.66053e+01" stime="1.47481e+01" mtime="7.24053e+01" gflop="0.00000e+00" gbyte="3.78059e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24053e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf45f1461146214e7566214621483" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09636e+02" utime="8.65712e+01" stime="1.47394e+01" mtime="7.24053e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24053e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.0599e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1950e+09" > 6.9629e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1955e+09" > 3.3912e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8588e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9682e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.8549e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9790e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5437e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.7840e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0554e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2509e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3455e+01 </func>
</region>
</regions>
<internal rank="116" log_i="1724765674.540054" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="117" mpi_size="696" stamp_init="1724765564.775719" stamp_final="1724765674.535266" username="apac4" allocationname="unknown" flags="0" pid="1567849" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09760e+02" utime="8.95192e+01" stime="1.36463e+01" mtime="7.27497e+01" gflop="0.00000e+00" gbyte="3.75774e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27497e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09630e+02" utime="8.94868e+01" stime="1.36367e+01" mtime="7.27497e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27497e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1900e+09" > 5.9515e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2003e+09" > 2.5184e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1381e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9713e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0967e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.9858e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5438e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.7909e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0582e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2451e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4599e+01 </func>
</region>
</regions>
<internal rank="117" log_i="1724765674.535266" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="118" mpi_size="696" stamp_init="1724765564.775866" stamp_final="1724765674.533440" username="apac4" allocationname="unknown" flags="0" pid="1567850" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09758e+02" utime="8.69777e+01" stime="1.47270e+01" mtime="7.21455e+01" gflop="0.00000e+00" gbyte="3.76545e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21455e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003514865535143514a2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09629e+02" utime="8.69464e+01" stime="1.47165e+01" mtime="7.21455e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21455e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1905e+09" > 7.2370e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1973e+09" > 3.7109e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5031e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9715e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8862e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9296e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5433e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.8380e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0573e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2495e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3488e+01 </func>
</region>
</regions>
<internal rank="118" log_i="1724765674.533440" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="119" mpi_size="696" stamp_init="1724765564.775672" stamp_final="1724765674.536506" username="apac4" allocationname="unknown" flags="0" pid="1567851" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u06b</host>
<perf wtime="1.09761e+02" utime="8.94215e+01" stime="1.36029e+01" mtime="7.29014e+01" gflop="0.00000e+00" gbyte="3.74645e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.29014e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09634e+02" utime="8.93891e+01" stime="1.35926e+01" mtime="7.29014e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.29014e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2119e+09" > 5.9599e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2094e+09" > 2.7776e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4907e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9713e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2318e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5424e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.8971e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0587e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2459e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4393e+01 </func>
</region>
</regions>
<internal rank="119" log_i="1724765674.536506" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="120" mpi_size="696" stamp_init="1724765564.835070" stamp_final="1724765674.528409" username="apac4" allocationname="unknown" flags="0" pid="1869538" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09693e+02" utime="7.65000e+01" stime="2.55202e+01" mtime="7.15290e+01" gflop="0.00000e+00" gbyte="3.85914e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15290e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf448154a154b15b8554b154b1550" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09566e+02" utime="7.64656e+01" stime="2.55120e+01" mtime="7.15290e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15290e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1913e+09" > 6.9649e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1957e+09" > 4.1263e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0597e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9496e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1696e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4852e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5428e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.8440e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0660e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2362e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3557e+01 </func>
</region>
</regions>
<internal rank="120" log_i="1724765674.528409" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="121" mpi_size="696" stamp_init="1724765564.835148" stamp_final="1724765674.539355" username="apac4" allocationname="unknown" flags="0" pid="1869539" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09704e+02" utime="9.01616e+01" stime="1.32884e+01" mtime="7.23275e+01" gflop="0.00000e+00" gbyte="3.78517e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23275e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09574e+02" utime="9.01262e+01" stime="1.32800e+01" mtime="7.23275e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23275e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1851e+09" > 6.1094e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1948e+09" > 2.8592e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4685e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9481e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5749e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5319e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5440e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.7683e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0660e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2302e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4060e+01 </func>
</region>
</regions>
<internal rank="121" log_i="1724765674.539355" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="122" mpi_size="696" stamp_init="1724765564.836994" stamp_final="1724765674.535726" username="apac4" allocationname="unknown" flags="0" pid="1869540" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09699e+02" utime="8.53067e+01" stime="1.53142e+01" mtime="7.14975e+01" gflop="0.00000e+00" gbyte="3.77975e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.14975e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48e1590159115b5559115901552" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09573e+02" utime="8.52769e+01" stime="1.53019e+01" mtime="7.14975e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.14975e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1922e+09" > 1.0357e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1967e+09" > 6.0006e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2709e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9487e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1325e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.2775e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5433e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.7927e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0632e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2344e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2966e+01 </func>
</region>
</regions>
<internal rank="122" log_i="1724765674.535726" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="123" mpi_size="696" stamp_init="1724765564.835144" stamp_final="1724765674.526586" username="apac4" allocationname="unknown" flags="0" pid="1869541" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09691e+02" utime="9.01092e+01" stime="1.34165e+01" mtime="7.26474e+01" gflop="0.00000e+00" gbyte="3.75778e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.26474e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b514b414fb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09567e+02" utime="9.00762e+01" stime="1.34070e+01" mtime="7.26474e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.26474e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2024e+09" > 5.8323e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1874e+09" > 2.7140e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3156e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9488e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0994e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3229e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5435e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.8289e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0625e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2270e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4553e+01 </func>
</region>
</regions>
<internal rank="123" log_i="1724765674.526586" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="124" mpi_size="696" stamp_init="1724765564.836135" stamp_final="1724765674.542041" username="apac4" allocationname="unknown" flags="0" pid="1869542" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09706e+02" utime="8.66542e+01" stime="1.44775e+01" mtime="7.09013e+01" gflop="0.00000e+00" gbyte="3.78078e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.09013e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a0145455a014a014b4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09580e+02" utime="8.66189e+01" stime="1.44704e+01" mtime="7.09013e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.09013e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2133e+09" > 8.8094e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2021e+09" > 4.0946e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9129e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9490e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2362e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8235e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5431e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.8646e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0627e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2347e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2897e+01 </func>
</region>
</regions>
<internal rank="124" log_i="1724765674.542041" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="125" mpi_size="696" stamp_init="1724765564.835193" stamp_final="1724765674.530517" username="apac4" allocationname="unknown" flags="0" pid="1869543" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09695e+02" utime="8.93706e+01" stime="1.40864e+01" mtime="7.22681e+01" gflop="0.00000e+00" gbyte="3.77705e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22681e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000074158155741574154a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09568e+02" utime="8.93358e+01" stime="1.40788e+01" mtime="7.22681e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22681e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2083e+09" > 6.1194e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1994e+09" > 3.0201e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4732e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9487e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9380e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5422e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.9202e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0636e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2244e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3988e+01 </func>
</region>
</regions>
<internal rank="125" log_i="1724765674.530517" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="126" mpi_size="696" stamp_init="1724765564.835792" stamp_final="1724765674.534876" username="apac4" allocationname="unknown" flags="0" pid="1869544" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09699e+02" utime="8.80369e+01" stime="1.42891e+01" mtime="7.17477e+01" gflop="0.00000e+00" gbyte="3.74401e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17477e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09573e+02" utime="8.80011e+01" stime="1.42832e+01" mtime="7.17477e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17477e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2058e+09" > 7.1800e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2123e+09" > 3.7436e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4829e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9481e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0777e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9945e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5409e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.0462e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0665e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2341e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3350e+01 </func>
</region>
</regions>
<internal rank="126" log_i="1724765674.534876" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="127" mpi_size="696" stamp_init="1724765564.835182" stamp_final="1724765674.543020" username="apac4" allocationname="unknown" flags="0" pid="1869545" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09708e+02" utime="9.03749e+01" stime="1.31385e+01" mtime="7.18699e+01" gflop="0.00000e+00" gbyte="3.78330e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18699e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09582e+02" utime="9.03433e+01" stime="1.31284e+01" mtime="7.18699e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18699e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2080e+09" > 5.8698e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2106e+09" > 2.6874e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1065e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9494e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5749e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.6291e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5408e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.0632e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0619e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2253e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3976e+01 </func>
</region>
</regions>
<internal rank="127" log_i="1724765674.543020" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="128" mpi_size="696" stamp_init="1724765564.836456" stamp_final="1724765674.544125" username="apac4" allocationname="unknown" flags="0" pid="1869546" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09708e+02" utime="8.15057e+01" stime="1.97780e+01" mtime="7.16409e+01" gflop="0.00000e+00" gbyte="3.77716e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16409e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000bf14b056bf14bf145a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09582e+02" utime="8.14736e+01" stime="1.97679e+01" mtime="7.16409e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16409e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2034e+09" > 8.6707e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1935e+09" > 4.6368e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2719e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 6.8418e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2650e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5387e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.2429e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0613e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2329e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3290e+01 </func>
</region>
</regions>
<internal rank="128" log_i="1724765674.544125" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="129" mpi_size="696" stamp_init="1724765564.835357" stamp_final="1724765674.537642" username="apac4" allocationname="unknown" flags="0" pid="1869547" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09702e+02" utime="8.95488e+01" stime="1.38484e+01" mtime="7.25827e+01" gflop="0.00000e+00" gbyte="3.77121e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25827e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000fa145355fa14fa1495" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09579e+02" utime="8.95176e+01" stime="1.38376e+01" mtime="7.25827e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25827e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2015e+09" > 5.9753e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1970e+09" > 2.7718e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6018e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9499e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0994e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3831e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5390e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.2806e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0634e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2254e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4178e+01 </func>
</region>
</regions>
<internal rank="129" log_i="1724765674.537642" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="130" mpi_size="696" stamp_init="1724765564.836405" stamp_final="1724765674.534702" username="apac4" allocationname="unknown" flags="0" pid="1869548" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09698e+02" utime="8.86727e+01" stime="1.35204e+01" mtime="7.22960e+01" gflop="0.00000e+00" gbyte="3.76728e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22960e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c815ca15cb15ec56cb15cb151f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09572e+02" utime="8.86348e+01" stime="1.35167e+01" mtime="7.22960e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22960e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2080e+09" > 6.9503e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2070e+09" > 3.9139e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0467e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9496e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.9407e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9928e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5385e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.2950e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0622e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2318e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3341e+01 </func>
</region>
</regions>
<internal rank="130" log_i="1724765674.534702" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="131" mpi_size="696" stamp_init="1724765564.835171" stamp_final="1724765674.526904" username="apac4" allocationname="unknown" flags="0" pid="1869549" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09692e+02" utime="8.97622e+01" stime="1.36120e+01" mtime="7.24703e+01" gflop="0.00000e+00" gbyte="3.78101e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24703e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005614ed5656145614e6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09569e+02" utime="8.97304e+01" stime="1.36014e+01" mtime="7.24703e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24703e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2120e+09" > 5.7665e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2150e+09" > 3.1200e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3491e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9473e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.8890e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5384e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.3338e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0651e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2220e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4360e+01 </func>
</region>
</regions>
<internal rank="131" log_i="1724765674.526904" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="132" mpi_size="696" stamp_init="1724765564.835071" stamp_final="1724765674.535018" username="apac4" allocationname="unknown" flags="0" pid="1869550" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09700e+02" utime="8.73203e+01" stime="1.44888e+01" mtime="7.21829e+01" gflop="0.00000e+00" gbyte="3.77926e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21829e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09571e+02" utime="8.72889e+01" stime="1.44781e+01" mtime="7.21829e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21829e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.5831e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2043e+09" > 7.4513e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2024e+09" > 3.3507e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.1730e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9493e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9250e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5160e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5360e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.5469e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0611e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2320e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3059e+01 </func>
</region>
</regions>
<internal rank="132" log_i="1724765674.535018" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="133" mpi_size="696" stamp_init="1724765564.835179" stamp_final="1724765674.533752" username="apac4" allocationname="unknown" flags="0" pid="1869551" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09699e+02" utime="8.93258e+01" stime="1.41495e+01" mtime="7.29902e+01" gflop="0.00000e+00" gbyte="3.76701e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.29902e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09573e+02" utime="8.92916e+01" stime="1.41417e+01" mtime="7.29902e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.29902e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1944e+09" > 5.8688e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1961e+09" > 3.0077e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9163e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9491e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9289e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5374e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.4116e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0671e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2045e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4288e+01 </func>
</region>
</regions>
<internal rank="133" log_i="1724765674.533752" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="134" mpi_size="696" stamp_init="1724765564.837014" stamp_final="1724765674.527729" username="apac4" allocationname="unknown" flags="0" pid="1869552" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09691e+02" utime="8.76238e+01" stime="1.42299e+01" mtime="7.13692e+01" gflop="0.00000e+00" gbyte="3.77171e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.13692e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4be14bf14c014c856c014c0146b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09565e+02" utime="8.75870e+01" stime="1.42249e+01" mtime="7.13692e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.13692e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2094e+09" > 8.2761e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2108e+09" > 4.0224e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2888e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9461e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1301e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5651e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5370e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.4251e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0618e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2310e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3074e+01 </func>
</region>
</regions>
<internal rank="134" log_i="1724765674.527729" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="135" mpi_size="696" stamp_init="1724765564.835150" stamp_final="1724765674.542754" username="apac4" allocationname="unknown" flags="0" pid="1869553" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09708e+02" utime="8.91591e+01" stime="1.40942e+01" mtime="7.29294e+01" gflop="0.00000e+00" gbyte="3.75481e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.29294e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a7142055a714a61464" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09584e+02" utime="8.91247e+01" stime="1.40868e+01" mtime="7.29294e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.29294e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2017e+09" > 5.8370e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1926e+09" > 2.4877e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8939e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9490e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3283e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5374e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.4433e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0623e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2043e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4256e+01 </func>
</region>
</regions>
<internal rank="135" log_i="1724765674.542754" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="136" mpi_size="696" stamp_init="1724765564.835074" stamp_final="1724765674.538411" username="apac4" allocationname="unknown" flags="0" pid="1869554" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09703e+02" utime="8.75558e+01" stime="1.44518e+01" mtime="7.19540e+01" gflop="0.00000e+00" gbyte="3.76858e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19540e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b514d755b514b414e0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09576e+02" utime="8.75276e+01" stime="1.44380e+01" mtime="7.19540e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19540e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2095e+09" > 6.9697e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2079e+09" > 3.5322e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0578e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9496e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4615e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8142e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5362e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.5621e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0646e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2296e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2988e+01 </func>
</region>
</regions>
<internal rank="136" log_i="1724765674.538411" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="137" mpi_size="696" stamp_init="1724765564.835189" stamp_final="1724765674.538987" username="apac4" allocationname="unknown" flags="0" pid="1869555" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09704e+02" utime="9.02994e+01" stime="1.32173e+01" mtime="7.27679e+01" gflop="0.00000e+00" gbyte="3.76842e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27679e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d014d214d3142955d314d31470" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09579e+02" utime="9.02670e+01" stime="1.32076e+01" mtime="7.27679e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27679e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2104e+09" > 5.8898e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2207e+09" > 2.3816e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9396e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9479e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6941e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3410e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5366e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.5296e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0634e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2051e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4056e+01 </func>
</region>
</regions>
<internal rank="137" log_i="1724765674.538987" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="138" mpi_size="696" stamp_init="1724765564.835079" stamp_final="1724765674.541614" username="apac4" allocationname="unknown" flags="0" pid="1869556" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09707e+02" utime="8.79119e+01" stime="1.44726e+01" mtime="7.15667e+01" gflop="0.00000e+00" gbyte="3.76732e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15667e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4be14bf14c0146455c014c014bf" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09579e+02" utime="8.78873e+01" stime="1.44555e+01" mtime="7.15667e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15667e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2020e+09" > 6.6485e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2020e+09" > 3.4241e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5731e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9462e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.0306e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8598e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5362e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.5665e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0635e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2310e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3154e+01 </func>
</region>
</regions>
<internal rank="138" log_i="1724765674.541614" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="139" mpi_size="696" stamp_init="1724765564.835168" stamp_final="1724765674.531987" username="apac4" allocationname="unknown" flags="0" pid="1869557" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09697e+02" utime="8.97146e+01" stime="1.35498e+01" mtime="7.22810e+01" gflop="0.00000e+00" gbyte="3.77953e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22810e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000fa14f91458" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09569e+02" utime="8.96831e+01" stime="1.35391e+01" mtime="7.22810e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22810e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2182e+09" > 6.2274e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1997e+09" > 2.9575e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3037e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9486e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8808e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5360e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.5744e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0622e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2035e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4159e+01 </func>
</region>
</regions>
<internal rank="139" log_i="1724765674.531987" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="140" mpi_size="696" stamp_init="1724765564.835063" stamp_final="1724765674.529386" username="apac4" allocationname="unknown" flags="0" pid="1869558" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09694e+02" utime="8.57043e+01" stime="1.51420e+01" mtime="7.08762e+01" gflop="0.00000e+00" gbyte="3.77983e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.08762e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf441154315441512564415441553" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09566e+02" utime="8.56701e+01" stime="1.51335e+01" mtime="7.08762e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.08762e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1995e+09" > 9.3762e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1995e+09" > 4.2656e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.6374e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9491e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8551e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8011e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5349e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.6827e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0657e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2311e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3086e+01 </func>
</region>
</regions>
<internal rank="140" log_i="1724765674.529386" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="141" mpi_size="696" stamp_init="1724765564.835163" stamp_final="1724765674.542881" username="apac4" allocationname="unknown" flags="0" pid="1869559" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09708e+02" utime="8.91834e+01" stime="1.36195e+01" mtime="7.21493e+01" gflop="0.00000e+00" gbyte="3.77224e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21493e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf434154d155f15e5565f155a1535" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09581e+02" utime="8.91498e+01" stime="1.36111e+01" mtime="7.21493e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21493e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2000e+09" > 5.9468e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2006e+09" > 2.8164e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9769e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9474e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.7761e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5348e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.7169e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0655e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1592e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4398e+01 </func>
</region>
</regions>
<internal rank="141" log_i="1724765674.542881" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="142" mpi_size="696" stamp_init="1724765564.836504" stamp_final="1724765674.535449" username="apac4" allocationname="unknown" flags="0" pid="1869560" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09699e+02" utime="8.64142e+01" stime="1.49121e+01" mtime="7.06609e+01" gflop="0.00000e+00" gbyte="3.77720e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.06609e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006d1425566d146c1474" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09573e+02" utime="8.63806e+01" stime="1.49032e+01" mtime="7.06609e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.06609e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.0599e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1948e+09" > 8.3934e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1892e+09" > 4.3718e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7360e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9477e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1615e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8970e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5351e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.6717e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0644e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1882e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2885e+01 </func>
</region>
</regions>
<internal rank="142" log_i="1724765674.535449" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="143" mpi_size="696" stamp_init="1724765564.835159" stamp_final="1724765674.536376" username="apac4" allocationname="unknown" flags="0" pid="1869561" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="1.09701e+02" utime="8.95314e+01" stime="1.39467e+01" mtime="7.22565e+01" gflop="0.00000e+00" gbyte="3.76736e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22565e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000991498147a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09577e+02" utime="8.95014e+01" stime="1.39343e+01" mtime="7.22565e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22565e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2028e+09" > 5.9012e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2058e+09" > 2.9364e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6242e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9486e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4319e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9003e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5344e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.7532e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0613e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2035e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3849e+01 </func>
</region>
</regions>
<internal rank="143" log_i="1724765674.536376" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="144" mpi_size="696" stamp_init="1724765564.669586" stamp_final="1724765674.542547" username="apac4" allocationname="unknown" flags="0" pid="2530619" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09873e+02" utime="7.51814e+01" stime="2.59975e+01" mtime="7.17109e+01" gflop="0.00000e+00" gbyte="3.85242e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17109e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b415b4151e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09744e+02" utime="7.51491e+01" stime="2.59881e+01" mtime="7.17109e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17109e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1934e+09" > 7.4224e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1878e+09" > 3.8748e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0916e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9463e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.7641e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.6209e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5340e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.7717e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0612e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1627e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3690e+01 </func>
</region>
</regions>
<internal rank="144" log_i="1724765674.542547" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="145" mpi_size="696" stamp_init="1724765564.669445" stamp_final="1724765674.532281" username="apac4" allocationname="unknown" flags="0" pid="2530620" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09863e+02" utime="8.99014e+01" stime="1.29975e+01" mtime="7.20959e+01" gflop="0.00000e+00" gbyte="3.75252e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20959e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09730e+02" utime="8.98666e+01" stime="1.29889e+01" mtime="7.20959e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20959e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2033e+09" > 6.6273e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2028e+09" > 2.6039e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2421e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9462e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2880e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5348e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.6839e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0574e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1861e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4022e+01 </func>
</region>
</regions>
<internal rank="145" log_i="1724765674.532281" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="146" mpi_size="696" stamp_init="1724765564.669615" stamp_final="1724765674.528659" username="apac4" allocationname="unknown" flags="0" pid="2530621" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09859e+02" utime="8.69639e+01" stime="1.45033e+01" mtime="7.15261e+01" gflop="0.00000e+00" gbyte="3.78143e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15261e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b015b01540" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09727e+02" utime="8.69335e+01" stime="1.44916e+01" mtime="7.15261e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15261e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2042e+09" > 7.5382e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1979e+09" > 3.9161e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2909e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9466e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1301e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.7359e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5350e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.6552e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0564e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2020e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3297e+01 </func>
</region>
</regions>
<internal rank="146" log_i="1724765674.528659" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="147" mpi_size="696" stamp_init="1724765564.669426" stamp_final="1724765674.531726" username="apac4" allocationname="unknown" flags="0" pid="2530622" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09862e+02" utime="8.93886e+01" stime="1.35773e+01" mtime="7.27570e+01" gflop="0.00000e+00" gbyte="3.76667e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27570e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09735e+02" utime="8.93598e+01" stime="1.35640e+01" mtime="7.27570e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27570e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2043e+09" > 6.2229e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2030e+09" > 2.7153e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4706e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9476e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.9481e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5340e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.7764e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0562e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1858e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4482e+01 </func>
</region>
</regions>
<internal rank="147" log_i="1724765674.531726" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="148" mpi_size="696" stamp_init="1724765564.669524" stamp_final="1724765674.527825" username="apac4" allocationname="unknown" flags="0" pid="2530623" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09858e+02" utime="8.50228e+01" stime="1.53344e+01" mtime="7.16486e+01" gflop="0.00000e+00" gbyte="3.75370e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16486e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48b148c148d1474558d148d1491" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09729e+02" utime="8.49949e+01" stime="1.53202e+01" mtime="7.16486e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16486e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1206e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1980e+09" > 9.0347e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2057e+09" > 4.8073e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4622e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9466e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.6060e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.6574e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5346e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.7075e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0552e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1980e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3084e+01 </func>
</region>
</regions>
<internal rank="148" log_i="1724765674.527825" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="149" mpi_size="696" stamp_init="1724765564.669400" stamp_final="1724765674.531791" username="apac4" allocationname="unknown" flags="0" pid="2530624" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09862e+02" utime="8.92957e+01" stime="1.36355e+01" mtime="7.24296e+01" gflop="0.00000e+00" gbyte="3.76965e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24296e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09734e+02" utime="8.92586e+01" stime="1.36306e+01" mtime="7.24296e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24296e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1975e+09" > 6.1777e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2032e+09" > 3.0223e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6007e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9460e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6772e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5336e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.8184e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0555e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1838e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4044e+01 </func>
</region>
</regions>
<internal rank="149" log_i="1724765674.531791" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="150" mpi_size="696" stamp_init="1724765564.669621" stamp_final="1724765674.537807" username="apac4" allocationname="unknown" flags="0" pid="2530625" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09868e+02" utime="8.61558e+01" stime="1.46814e+01" mtime="7.17347e+01" gflop="0.00000e+00" gbyte="3.77205e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17347e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001c141c14e4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09740e+02" utime="8.61234e+01" stime="1.46716e+01" mtime="7.17347e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17347e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.7752e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2091e+09" > 9.0632e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2038e+09" > 4.7499e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0285e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9466e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.4639e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.6772e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5326e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.8472e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0567e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1977e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3606e+01 </func>
</region>
</regions>
<internal rank="150" log_i="1724765674.537807" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="151" mpi_size="696" stamp_init="1724765564.669409" stamp_final="1724765674.538450" username="apac4" allocationname="unknown" flags="0" pid="2530626" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09869e+02" utime="8.91452e+01" stime="1.38327e+01" mtime="7.25634e+01" gflop="0.00000e+00" gbyte="3.76560e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25634e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007714771468" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09742e+02" utime="8.91148e+01" stime="1.38212e+01" mtime="7.25634e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25634e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2114e+09" > 6.1370e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2185e+09" > 2.5185e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8014e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9466e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.5073e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5332e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.8463e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0591e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1838e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3975e+01 </func>
</region>
</regions>
<internal rank="151" log_i="1724765674.538450" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="152" mpi_size="696" stamp_init="1724765564.669554" stamp_final="1724765674.536956" username="apac4" allocationname="unknown" flags="0" pid="2530627" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09867e+02" utime="8.68896e+01" stime="1.42975e+01" mtime="7.25096e+01" gflop="0.00000e+00" gbyte="3.76453e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25096e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4971499149a1447559a14991472" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09739e+02" utime="8.68543e+01" stime="1.42910e+01" mtime="7.25096e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25096e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1942e+09" > 7.3257e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2036e+09" > 3.5912e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0218e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9466e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0099e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.9628e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5328e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.8417e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0603e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1946e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3572e+01 </func>
</region>
</regions>
<internal rank="152" log_i="1724765674.536956" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="153" mpi_size="696" stamp_init="1724765564.669451" stamp_final="1724765674.539535" username="apac4" allocationname="unknown" flags="0" pid="2530628" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09870e+02" utime="8.85393e+01" stime="1.44047e+01" mtime="7.33348e+01" gflop="0.00000e+00" gbyte="3.76644e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.33348e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000046145f55461445145e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09743e+02" utime="8.85086e+01" stime="1.43935e+01" mtime="7.33348e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.33348e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1987e+09" > 6.1481e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1972e+09" > 2.8123e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.2393e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9471e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1272e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5323e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.9011e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0550e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1831e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4303e+01 </func>
</region>
</regions>
<internal rank="153" log_i="1724765674.539535" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="154" mpi_size="696" stamp_init="1724765564.669551" stamp_final="1724765674.537851" username="apac4" allocationname="unknown" flags="0" pid="2530629" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09868e+02" utime="8.68767e+01" stime="1.45207e+01" mtime="7.18375e+01" gflop="0.00000e+00" gbyte="3.77903e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18375e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006714225567146714c7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09738e+02" utime="8.68452e+01" stime="1.45100e+01" mtime="7.18375e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18375e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1945e+09" > 6.7387e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1941e+09" > 3.5649e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7995e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9461e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0777e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.9962e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5321e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.8947e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0590e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1955e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3187e+01 </func>
</region>
</regions>
<internal rank="154" log_i="1724765674.537851" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="155" mpi_size="696" stamp_init="1724765564.669465" stamp_final="1724765674.542965" username="apac4" allocationname="unknown" flags="0" pid="2530630" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09873e+02" utime="8.97272e+01" stime="1.32781e+01" mtime="7.23363e+01" gflop="0.00000e+00" gbyte="3.77155e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23363e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ba15b91553" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09745e+02" utime="8.96984e+01" stime="1.32646e+01" mtime="7.23363e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23363e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2179e+09" > 6.0929e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2112e+09" > 2.8632e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0117e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9473e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7643e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.3069e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5319e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.9921e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0605e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1827e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4536e+01 </func>
</region>
</regions>
<internal rank="155" log_i="1724765674.542965" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="156" mpi_size="696" stamp_init="1724765564.669609" stamp_final="1724765674.537401" username="apac4" allocationname="unknown" flags="0" pid="2530631" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09868e+02" utime="8.56443e+01" stime="1.48466e+01" mtime="7.12497e+01" gflop="0.00000e+00" gbyte="3.77056e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.12497e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000020142014a1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09738e+02" utime="8.56095e+01" stime="1.48395e+01" mtime="7.12497e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.12497e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127200" bytes="1.2004e+09" > 9.7301e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2011e+09" > 4.2669e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0196e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9468e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7118e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.6671e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5313e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.0082e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0566e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1959e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3064e+01 </func>
</region>
</regions>
<internal rank="156" log_i="1724765674.537401" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="157" mpi_size="696" stamp_init="1724765564.669419" stamp_final="1724765674.528632" username="apac4" allocationname="unknown" flags="0" pid="2530632" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09859e+02" utime="8.89813e+01" stime="1.39291e+01" mtime="7.24107e+01" gflop="0.00000e+00" gbyte="3.77567e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24107e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002514241460" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09732e+02" utime="8.89490e+01" stime="1.39184e+01" mtime="7.24107e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24107e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1978e+09" > 6.1072e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1969e+09" > 2.9565e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5110e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9466e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8300e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5319e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 4.9911e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0583e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1825e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4113e+01 </func>
</region>
</regions>
<internal rank="157" log_i="1724765674.528632" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="158" mpi_size="696" stamp_init="1724765564.669592" stamp_final="1724765674.535818" username="apac4" allocationname="unknown" flags="0" pid="2530633" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09866e+02" utime="8.72467e+01" stime="1.42325e+01" mtime="7.23041e+01" gflop="0.00000e+00" gbyte="3.76587e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23041e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ed149156ed14ed14b2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09737e+02" utime="8.72204e+01" stime="1.42170e+01" mtime="7.23041e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23041e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1949e+09" > 7.4352e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2108e+09" > 3.4605e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8420e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9451e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3704e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7002e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5308e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.0771e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0561e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1925e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3549e+01 </func>
</region>
</regions>
<internal rank="158" log_i="1724765674.535818" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="159" mpi_size="696" stamp_init="1724765564.669461" stamp_final="1724765674.539068" username="apac4" allocationname="unknown" flags="0" pid="2530634" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09870e+02" utime="8.96876e+01" stime="1.33302e+01" mtime="7.26103e+01" gflop="0.00000e+00" gbyte="3.77800e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.26103e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09741e+02" utime="8.96576e+01" stime="1.33180e+01" mtime="7.26103e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.26103e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2013e+09" > 5.9553e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2037e+09" > 2.8174e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4354e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9463e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7078e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5311e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.0633e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0592e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1812e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4407e+01 </func>
</region>
</regions>
<internal rank="159" log_i="1724765674.539068" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="160" mpi_size="696" stamp_init="1724765564.669572" stamp_final="1724765674.539975" username="apac4" allocationname="unknown" flags="0" pid="2530635" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09870e+02" utime="8.60872e+01" stime="1.46891e+01" mtime="7.14389e+01" gflop="0.00000e+00" gbyte="3.76099e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.14389e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005b15cc555b155a1540" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09740e+02" utime="8.60536e+01" stime="1.46804e+01" mtime="7.14389e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.14389e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2090e+09" > 8.9838e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2048e+09" > 4.4032e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1388e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9386e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.9761e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9234e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5298e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.1854e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0559e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1923e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3219e+01 </func>
</region>
</regions>
<internal rank="160" log_i="1724765674.539975" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="161" mpi_size="696" stamp_init="1724765564.669455" stamp_final="1724765674.539131" username="apac4" allocationname="unknown" flags="0" pid="2530636" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09870e+02" utime="8.90902e+01" stime="1.38231e+01" mtime="7.26054e+01" gflop="0.00000e+00" gbyte="3.74649e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.26054e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d314d214e5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09742e+02" utime="8.90640e+01" stime="1.38073e+01" mtime="7.26054e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.26054e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2160e+09" > 6.1834e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2145e+09" > 2.4289e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7283e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9455e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0822e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5297e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.2040e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0594e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1785e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4097e+01 </func>
</region>
</regions>
<internal rank="161" log_i="1724765674.539131" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="162" mpi_size="696" stamp_init="1724765564.669607" stamp_final="1724765674.540905" username="apac4" allocationname="unknown" flags="0" pid="2530637" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09871e+02" utime="8.69336e+01" stime="1.44850e+01" mtime="7.10726e+01" gflop="0.00000e+00" gbyte="3.77026e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.10726e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000931493146f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09743e+02" utime="8.68999e+01" stime="1.44764e+01" mtime="7.10726e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.10726e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2158e+09" > 7.2341e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2140e+09" > 4.2548e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8832e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9461e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5392e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.9056e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5287e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.2183e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0567e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1879e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3282e+01 </func>
</region>
</regions>
<internal rank="162" log_i="1724765674.540905" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="163" mpi_size="696" stamp_init="1724765564.669447" stamp_final="1724765674.531690" username="apac4" allocationname="unknown" flags="0" pid="2530638" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09862e+02" utime="8.85770e+01" stime="1.42905e+01" mtime="7.28116e+01" gflop="0.00000e+00" gbyte="3.76587e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28116e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000281534562815281501" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09735e+02" utime="8.85413e+01" stime="1.42838e+01" mtime="7.28116e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28116e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2105e+09" > 6.2684e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2140e+09" > 3.0143e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0498e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9456e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.9008e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5286e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.2730e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0617e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1591e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3968e+01 </func>
</region>
</regions>
<internal rank="163" log_i="1724765674.531690" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="164" mpi_size="696" stamp_init="1724765564.669574" stamp_final="1724765674.527546" username="apac4" allocationname="unknown" flags="0" pid="2530639" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09858e+02" utime="8.55883e+01" stime="1.47809e+01" mtime="7.19270e+01" gflop="0.00000e+00" gbyte="3.76556e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19270e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09729e+02" utime="8.55526e+01" stime="1.47748e+01" mtime="7.19270e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19270e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2033e+09" > 7.9435e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2051e+09" > 4.6876e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7787e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9445e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.7070e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.1089e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5289e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.2830e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0588e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1869e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3173e+01 </func>
</region>
</regions>
<internal rank="164" log_i="1724765674.527546" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="165" mpi_size="696" stamp_init="1724765564.669439" stamp_final="1724765674.538855" username="apac4" allocationname="unknown" flags="0" pid="2530640" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09869e+02" utime="8.93815e+01" stime="1.35460e+01" mtime="7.25526e+01" gflop="0.00000e+00" gbyte="3.74859e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25526e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09742e+02" utime="8.93452e+01" stime="1.35404e+01" mtime="7.25526e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25526e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2098e+09" > 5.9590e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2079e+09" > 2.6912e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5836e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9473e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.9615e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5285e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.3222e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0599e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1588e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4194e+01 </func>
</region>
</regions>
<internal rank="165" log_i="1724765674.538855" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="166" mpi_size="696" stamp_init="1724765564.669579" stamp_final="1724765674.537873" username="apac4" allocationname="unknown" flags="0" pid="2530641" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09868e+02" utime="8.70117e+01" stime="1.44452e+01" mtime="7.19720e+01" gflop="0.00000e+00" gbyte="3.76812e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19720e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09738e+02" utime="8.69762e+01" stime="1.44389e+01" mtime="7.19720e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19720e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2102e+09" > 7.0839e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2074e+09" > 4.0331e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8065e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9460e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1730e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0757e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5278e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.3911e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0614e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1863e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3275e+01 </func>
</region>
</regions>
<internal rank="166" log_i="1724765674.537873" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="167" mpi_size="696" stamp_init="1724765564.669405" stamp_final="1724765674.532656" username="apac4" allocationname="unknown" flags="0" pid="2530642" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u05a</host>
<perf wtime="1.09863e+02" utime="8.92637e+01" stime="1.36165e+01" mtime="7.20670e+01" gflop="0.00000e+00" gbyte="3.77213e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20670e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09736e+02" utime="8.92362e+01" stime="1.36014e+01" mtime="7.20670e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20670e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2050e+09" > 6.2460e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2002e+09" > 2.7218e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4120e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9472e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8410e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5274e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.4321e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0554e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1582e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3851e+01 </func>
</region>
</regions>
<internal rank="167" log_i="1724765674.532656" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="168" mpi_size="696" stamp_init="1724765564.868096" stamp_final="1724765674.540265" username="apac4" allocationname="unknown" flags="0" pid="168163" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09672e+02" utime="7.62153e+01" stime="2.55918e+01" mtime="7.16412e+01" gflop="0.00000e+00" gbyte="3.86425e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16412e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000038142655381437145c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09546e+02" utime="7.61852e+01" stime="2.55799e+01" mtime="7.16412e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16412e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1923e+09" > 7.0514e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1890e+09" > 3.7246e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4555e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9374e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.3663e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9269e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5269e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.4778e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0652e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1597e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3350e+01 </func>
</region>
</regions>
<internal rank="168" log_i="1724765674.540265" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="169" mpi_size="696" stamp_init="1724765564.869205" stamp_final="1724765674.628268" username="apac4" allocationname="unknown" flags="0" pid="168164" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09759e+02" utime="9.01513e+01" stime="1.31830e+01" mtime="7.18942e+01" gflop="0.00000e+00" gbyte="3.77422e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18942e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4231425142614725526142514bd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09631e+02" utime="9.01170e+01" stime="1.31741e+01" mtime="7.18942e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18942e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2152e+09" > 6.0464e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2093e+09" > 3.1942e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8320e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9392e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.2915e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9789e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5284e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.3521e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0633e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1506e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4320e+01 </func>
</region>
</regions>
<internal rank="169" log_i="1724765674.628268" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="170" mpi_size="696" stamp_init="1724765564.868529" stamp_final="1724765674.540201" username="apac4" allocationname="unknown" flags="0" pid="168165" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09672e+02" utime="8.58947e+01" stime="1.52702e+01" mtime="7.16786e+01" gflop="0.00000e+00" gbyte="3.77338e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16786e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000de14dd14fa" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09543e+02" utime="8.58642e+01" stime="1.52589e+01" mtime="7.16786e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16786e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2061e+09" > 8.5028e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2027e+09" > 4.6730e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3079e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9385e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8660e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9538e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5273e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.3416e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0641e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1587e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3373e+01 </func>
</region>
</regions>
<internal rank="170" log_i="1724765674.540201" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="171" mpi_size="696" stamp_init="1724765564.867683" stamp_final="1724765674.520477" username="apac4" allocationname="unknown" flags="0" pid="168166" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09653e+02" utime="8.95034e+01" stime="1.38378e+01" mtime="7.20479e+01" gflop="0.00000e+00" gbyte="3.78048e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20479e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000068146814eb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09526e+02" utime="8.94714e+01" stime="1.38274e+01" mtime="7.20479e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20479e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2063e+09" > 5.9484e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2133e+09" > 3.0126e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9157e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9386e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9536e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5283e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.3631e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0617e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1492e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4408e+01 </func>
</region>
</regions>
<internal rank="171" log_i="1724765674.520477" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="172" mpi_size="696" stamp_init="1724765564.867674" stamp_final="1724765674.540124" username="apac4" allocationname="unknown" flags="0" pid="168167" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09672e+02" utime="8.62869e+01" stime="1.49515e+01" mtime="7.10462e+01" gflop="0.00000e+00" gbyte="3.76488e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.10462e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c414c414bd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09546e+02" utime="8.62570e+01" stime="1.49396e+01" mtime="7.10462e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.10462e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2021e+09" > 7.6102e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2023e+09" > 4.5515e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0830e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9388e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.9623e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9332e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5273e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.4359e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0621e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1580e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3052e+01 </func>
</region>
</regions>
<internal rank="172" log_i="1724765674.540124" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="173" mpi_size="696" stamp_init="1724765564.867735" stamp_final="1724765674.539793" username="apac4" allocationname="unknown" flags="0" pid="168168" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09672e+02" utime="8.98587e+01" stime="1.34110e+01" mtime="7.18568e+01" gflop="0.00000e+00" gbyte="3.76640e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18568e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09547e+02" utime="8.98272e+01" stime="1.34004e+01" mtime="7.18568e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18568e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2003e+09" > 5.7396e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1969e+09" > 3.2704e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7473e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9414e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8610e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3229e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5274e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.4613e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0665e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1491e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4412e+01 </func>
</region>
</regions>
<internal rank="173" log_i="1724765674.539793" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="174" mpi_size="696" stamp_init="1724765564.867672" stamp_final="1724765674.542597" username="apac4" allocationname="unknown" flags="0" pid="168169" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09675e+02" utime="8.79971e+01" stime="1.40776e+01" mtime="7.17303e+01" gflop="0.00000e+00" gbyte="3.78174e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17303e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09548e+02" utime="8.79629e+01" stime="1.40694e+01" mtime="7.17303e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17303e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2100e+09" > 7.0156e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2101e+09" > 3.5174e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8708e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9383e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8037e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1966e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5270e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.4813e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0648e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1593e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3020e+01 </func>
</region>
</regions>
<internal rank="174" log_i="1724765674.542597" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="175" mpi_size="696" stamp_init="1724765564.867695" stamp_final="1724765674.540548" username="apac4" allocationname="unknown" flags="0" pid="168170" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09673e+02" utime="8.96008e+01" stime="1.37753e+01" mtime="7.26887e+01" gflop="0.00000e+00" gbyte="3.77670e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.26887e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09549e+02" utime="8.95725e+01" stime="1.37614e+01" mtime="7.26887e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.26887e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2188e+09" > 6.0756e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2186e+09" > 2.5957e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8739e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9413e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3291e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5270e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.4982e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0605e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1449e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4092e+01 </func>
</region>
</regions>
<internal rank="175" log_i="1724765674.540548" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="176" mpi_size="696" stamp_init="1724765564.867849" stamp_final="1724765674.532394" username="apac4" allocationname="unknown" flags="0" pid="168171" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09665e+02" utime="8.72527e+01" stime="1.47046e+01" mtime="7.16289e+01" gflop="0.00000e+00" gbyte="3.77014e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16289e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f015d555f015f01530" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09538e+02" utime="8.72232e+01" stime="1.46922e+01" mtime="7.16289e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16289e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1997e+09" > 7.2828e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1980e+09" > 4.0027e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0843e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9377e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.7500e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9260e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5251e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.6488e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0644e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1586e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2686e+01 </func>
</region>
</regions>
<internal rank="176" log_i="1724765674.532394" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="177" mpi_size="696" stamp_init="1724765564.867707" stamp_final="1724765674.521126" username="apac4" allocationname="unknown" flags="0" pid="168172" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09653e+02" utime="9.03476e+01" stime="1.30516e+01" mtime="7.27235e+01" gflop="0.00000e+00" gbyte="3.78162e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27235e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09528e+02" utime="9.03125e+01" stime="1.30443e+01" mtime="7.27235e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27235e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1938e+09" > 5.9990e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1918e+09" > 3.0468e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3826e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9408e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0770e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9438e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5252e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.6841e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0638e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1480e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4589e+01 </func>
</region>
</regions>
<internal rank="177" log_i="1724765674.521126" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="178" mpi_size="696" stamp_init="1724765564.869248" stamp_final="1724765674.525059" username="apac4" allocationname="unknown" flags="0" pid="168173" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09656e+02" utime="8.59931e+01" stime="1.47283e+01" mtime="7.12590e+01" gflop="0.00000e+00" gbyte="3.77522e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.12590e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4871489148a146c558a148a1458" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09530e+02" utime="8.59569e+01" stime="1.47228e+01" mtime="7.12590e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.12590e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1804e+09" > 8.7194e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1969e+09" > 5.5272e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0147e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9398e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2159e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9916e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5246e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.6881e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0641e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1569e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3206e+01 </func>
</region>
</regions>
<internal rank="178" log_i="1724765674.525059" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="179" mpi_size="696" stamp_init="1724765564.867844" stamp_final="1724765674.534160" username="apac4" allocationname="unknown" flags="0" pid="168174" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09666e+02" utime="8.97385e+01" stime="1.35396e+01" mtime="7.18980e+01" gflop="0.00000e+00" gbyte="3.77102e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18980e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09543e+02" utime="8.97047e+01" stime="1.35311e+01" mtime="7.18980e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18980e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2113e+09" > 5.6292e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2115e+09" > 2.6879e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.5531e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9385e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.4094e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1863e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5250e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.6761e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0654e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1436e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4654e+01 </func>
</region>
</regions>
<internal rank="179" log_i="1724765674.534160" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="180" mpi_size="696" stamp_init="1724765564.869254" stamp_final="1724765674.535954" username="apac4" allocationname="unknown" flags="0" pid="168175" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09667e+02" utime="8.59168e+01" stime="1.49966e+01" mtime="7.17786e+01" gflop="0.00000e+00" gbyte="3.76717e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17786e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09541e+02" utime="8.58779e+01" stime="1.49940e+01" mtime="7.17786e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17786e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2034e+09" > 8.1668e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1984e+09" > 5.0041e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9628e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9409e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6278e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9391e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5235e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.8586e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0614e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1554e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2823e+01 </func>
</region>
</regions>
<internal rank="180" log_i="1724765674.535954" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="181" mpi_size="696" stamp_init="1724765564.868766" stamp_final="1724765674.529285" username="apac4" allocationname="unknown" flags="0" pid="168176" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09661e+02" utime="8.95345e+01" stime="1.38273e+01" mtime="7.22486e+01" gflop="0.00000e+00" gbyte="3.76423e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22486e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000071147114fd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09536e+02" utime="8.95045e+01" stime="1.38160e+01" mtime="7.22486e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22486e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2008e+09" > 5.6997e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2009e+09" > 3.0832e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3259e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9400e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0322e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5231e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.8986e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0634e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1473e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4208e+01 </func>
</region>
</regions>
<internal rank="181" log_i="1724765674.529285" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="182" mpi_size="696" stamp_init="1724765564.867861" stamp_final="1724765674.532281" username="apac4" allocationname="unknown" flags="0" pid="168177" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09664e+02" utime="8.69241e+01" stime="1.48918e+01" mtime="7.17733e+01" gflop="0.00000e+00" gbyte="3.76499e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17733e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf439143a143b1460553b143b147c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09537e+02" utime="8.68866e+01" stime="1.48879e+01" mtime="7.17733e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17733e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2095e+09" > 7.5270e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2035e+09" > 4.0788e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6703e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9382e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7786e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9385e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5238e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.7847e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0637e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1559e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3214e+01 </func>
</region>
</regions>
<internal rank="182" log_i="1724765674.532281" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="183" mpi_size="696" stamp_init="1724765564.867701" stamp_final="1724765674.540189" username="apac4" allocationname="unknown" flags="0" pid="168178" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09672e+02" utime="8.89848e+01" stime="1.34732e+01" mtime="7.23682e+01" gflop="0.00000e+00" gbyte="3.75301e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23682e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000941594153a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09546e+02" utime="8.89540e+01" stime="1.34617e+01" mtime="7.23682e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23682e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2022e+09" > 6.1577e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2056e+09" > 2.8248e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4129e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9446e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9485e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5226e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.9412e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0665e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1461e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4191e+01 </func>
</region>
</regions>
<internal rank="183" log_i="1724765674.540189" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="184" mpi_size="696" stamp_init="1724765564.867671" stamp_final="1724765674.535812" username="apac4" allocationname="unknown" flags="0" pid="168179" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09668e+02" utime="8.71698e+01" stime="1.46345e+01" mtime="7.10117e+01" gflop="0.00000e+00" gbyte="3.76602e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.10117e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09542e+02" utime="8.71355e+01" stime="1.46268e+01" mtime="7.10117e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.10117e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1953e+09" > 7.3285e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2081e+09" > 3.4050e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3429e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9383e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0734e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9285e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5230e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.9041e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0606e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1561e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2804e+01 </func>
</region>
</regions>
<internal rank="184" log_i="1724765674.535812" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="185" mpi_size="696" stamp_init="1724765564.867905" stamp_final="1724765674.522996" username="apac4" allocationname="unknown" flags="0" pid="168180" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09655e+02" utime="8.97209e+01" stime="1.37088e+01" mtime="7.17448e+01" gflop="0.00000e+00" gbyte="3.76835e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17448e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ee15ee1540" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09528e+02" utime="8.96875e+01" stime="1.37003e+01" mtime="7.17448e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17448e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2095e+09" > 6.1041e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2123e+09" > 3.1036e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9540e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9399e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0041e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0881e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5225e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.9453e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0617e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0930e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4036e+01 </func>
</region>
</regions>
<internal rank="185" log_i="1724765674.522996" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="186" mpi_size="696" stamp_init="1724765564.867674" stamp_final="1724765674.537775" username="apac4" allocationname="unknown" flags="0" pid="168181" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09670e+02" utime="8.66894e+01" stime="1.48312e+01" mtime="7.12950e+01" gflop="0.00000e+00" gbyte="3.76495e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.12950e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a114a114ef" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09544e+02" utime="8.66543e+01" stime="1.48244e+01" mtime="7.12950e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.12950e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2250e+09" > 8.3819e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2221e+09" > 3.9846e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4369e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9385e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6510e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9910e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5221e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.9344e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0635e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1510e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2880e+01 </func>
</region>
</regions>
<internal rank="186" log_i="1724765674.537775" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="187" mpi_size="696" stamp_init="1724765564.867824" stamp_final="1724765674.559595" username="apac4" allocationname="unknown" flags="0" pid="168182" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09692e+02" utime="8.99797e+01" stime="1.33593e+01" mtime="7.24170e+01" gflop="0.00000e+00" gbyte="3.76759e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24170e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000591554150f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09567e+02" utime="8.99482e+01" stime="1.33488e+01" mtime="7.24170e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24170e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2087e+09" > 6.0673e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2087e+09" > 3.3465e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3150e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9399e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5749e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9911e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5221e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 5.9970e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0608e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1455e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4349e+01 </func>
</region>
</regions>
<internal rank="187" log_i="1724765674.559595" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="188" mpi_size="696" stamp_init="1724765564.868885" stamp_final="1724765674.556525" username="apac4" allocationname="unknown" flags="0" pid="168183" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09688e+02" utime="8.55528e+01" stime="1.50947e+01" mtime="7.08623e+01" gflop="0.00000e+00" gbyte="3.76717e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.08623e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09563e+02" utime="8.55205e+01" stime="1.50851e+01" mtime="7.08623e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.08623e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2092e+09" > 8.3267e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2133e+09" > 4.8404e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8990e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9399e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1386e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9293e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5213e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.0615e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0666e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1542e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2970e+01 </func>
</region>
</regions>
<internal rank="188" log_i="1724765674.556525" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="189" mpi_size="696" stamp_init="1724765564.867692" stamp_final="1724765674.538552" username="apac4" allocationname="unknown" flags="0" pid="168184" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09671e+02" utime="8.92814e+01" stime="1.32759e+01" mtime="7.21759e+01" gflop="0.00000e+00" gbyte="3.76591e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21759e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf47514761477144e5577147714f7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09548e+02" utime="8.92492e+01" stime="1.32663e+01" mtime="7.21759e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21759e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2086e+09" > 5.7471e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2048e+09" > 3.0993e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6736e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9383e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0291e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5208e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.0995e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0618e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1471e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3800e+01 </func>
</region>
</regions>
<internal rank="189" log_i="1724765674.538552" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="190" mpi_size="696" stamp_init="1724765564.868941" stamp_final="1724765674.542368" username="apac4" allocationname="unknown" flags="0" pid="168185" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09673e+02" utime="8.72759e+01" stime="1.43695e+01" mtime="7.13469e+01" gflop="0.00000e+00" gbyte="3.75072e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.13469e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09548e+02" utime="8.72406e+01" stime="1.43625e+01" mtime="7.13469e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.13469e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2169e+09" > 7.5408e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2187e+09" > 4.1603e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1836e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9398e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.6784e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9769e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5208e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.1341e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0631e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1496e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3256e+01 </func>
</region>
</regions>
<internal rank="190" log_i="1724765674.542368" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="191" mpi_size="696" stamp_init="1724765564.867687" stamp_final="1724765674.526374" username="apac4" allocationname="unknown" flags="0" pid="168186" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="1.09659e+02" utime="8.93063e+01" stime="1.32414e+01" mtime="7.23406e+01" gflop="0.00000e+00" gbyte="3.76633e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23406e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf41b141d141e143c551e141e1489" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09534e+02" utime="8.92683e+01" stime="1.32371e+01" mtime="7.23406e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23406e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1988e+09" > 6.1078e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2002e+09" > 2.8310e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3324e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9386e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9899e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5207e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.1467e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0616e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1505e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4269e+01 </func>
</region>
</regions>
<internal rank="191" log_i="1724765674.526374" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="192" mpi_size="696" stamp_init="1724765564.792728" stamp_final="1724765674.539366" username="apac4" allocationname="unknown" flags="0" pid="784488" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09747e+02" utime="7.54893e+01" stime="2.57938e+01" mtime="7.20278e+01" gflop="0.00000e+00" gbyte="3.85738e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20278e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09622e+02" utime="7.54610e+01" stime="2.57803e+01" mtime="7.20278e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20278e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1891e+09" > 6.7801e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1923e+09" > 5.4356e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7614e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 6.8523e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2508e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5196e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.2178e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0553e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1471e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3500e+01 </func>
</region>
</regions>
<internal rank="192" log_i="1724765674.539366" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="193" mpi_size="696" stamp_init="1724765564.792452" stamp_final="1724765674.532243" username="apac4" allocationname="unknown" flags="0" pid="784489" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09740e+02" utime="8.95825e+01" stime="1.33332e+01" mtime="7.23074e+01" gflop="0.00000e+00" gbyte="3.77548e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23074e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44c1465147714835577147214df" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09612e+02" utime="8.95537e+01" stime="1.33189e+01" mtime="7.23074e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23074e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2172e+09" > 5.9801e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2177e+09" > 3.6762e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5571e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9335e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0926e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5206e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.1651e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0557e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0601e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4096e+01 </func>
</region>
</regions>
<internal rank="193" log_i="1724765674.532243" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="194" mpi_size="696" stamp_init="1724765564.794201" stamp_final="1724765674.544595" username="apac4" allocationname="unknown" flags="0" pid="784490" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09750e+02" utime="8.67022e+01" stime="1.47425e+01" mtime="7.16863e+01" gflop="0.00000e+00" gbyte="3.78075e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16863e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a814a755a814a314e5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09627e+02" utime="8.66768e+01" stime="1.47258e+01" mtime="7.16863e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16863e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2014e+09" > 7.1460e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2092e+09" > 4.1296e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5977e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9355e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5679e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7291e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5207e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.1396e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0598e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1444e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3292e+01 </func>
</region>
</regions>
<internal rank="194" log_i="1724765674.544595" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="195" mpi_size="696" stamp_init="1724765564.792888" stamp_final="1724765674.540594" username="apac4" allocationname="unknown" flags="0" pid="784491" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09748e+02" utime="8.95732e+01" stime="1.31978e+01" mtime="7.22875e+01" gflop="0.00000e+00" gbyte="3.76793e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22875e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48b148c148d14f7558d148d147e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09623e+02" utime="8.95367e+01" stime="1.31926e+01" mtime="7.22875e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22875e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2121e+09" > 6.1679e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2130e+09" > 3.2101e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2766e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9351e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6941e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7496e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5202e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.2074e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0594e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1087e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4329e+01 </func>
</region>
</regions>
<internal rank="195" log_i="1724765674.540594" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="196" mpi_size="696" stamp_init="1724765564.792221" stamp_final="1724765674.530045" username="apac4" allocationname="unknown" flags="0" pid="784492" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09738e+02" utime="8.70090e+01" stime="1.43265e+01" mtime="7.16803e+01" gflop="0.00000e+00" gbyte="3.76915e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16803e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003f153f1542" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09611e+02" utime="8.69766e+01" stime="1.43170e+01" mtime="7.16803e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16803e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2057e+09" > 7.1269e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2041e+09" > 4.8060e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7713e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9348e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3508e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.6981e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5195e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.2449e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0579e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1408e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3119e+01 </func>
</region>
</regions>
<internal rank="196" log_i="1724765674.530045" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="197" mpi_size="696" stamp_init="1724765564.792647" stamp_final="1724765674.541798" username="apac4" allocationname="unknown" flags="0" pid="784493" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09749e+02" utime="8.93100e+01" stime="1.36081e+01" mtime="7.20160e+01" gflop="0.00000e+00" gbyte="3.74954e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20160e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ec14ed14ef143156ef14ee1492" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09625e+02" utime="8.92770e+01" stime="1.35994e+01" mtime="7.20160e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20160e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2025e+09" > 6.3606e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2035e+09" > 3.2687e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3098e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9355e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3855e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7150e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5194e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.2919e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0588e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0573e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4001e+01 </func>
</region>
</regions>
<internal rank="197" log_i="1724765674.541798" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="198" mpi_size="696" stamp_init="1724765564.792240" stamp_final="1724765674.532959" username="apac4" allocationname="unknown" flags="0" pid="784494" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09741e+02" utime="8.67196e+01" stime="1.45074e+01" mtime="7.11492e+01" gflop="0.00000e+00" gbyte="3.77121e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.11492e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b314b514b6149b56b614b514b2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09615e+02" utime="8.66881e+01" stime="1.44972e+01" mtime="7.11492e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.11492e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2105e+09" > 6.9819e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2177e+09" > 5.2520e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2766e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9353e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.8651e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8890e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5185e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.3012e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0549e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1149e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3088e+01 </func>
</region>
</regions>
<internal rank="198" log_i="1724765674.532959" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="199" mpi_size="696" stamp_init="1724765564.792951" stamp_final="1724765674.536021" username="apac4" allocationname="unknown" flags="0" pid="784495" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09743e+02" utime="8.93678e+01" stime="1.36063e+01" mtime="7.18059e+01" gflop="0.00000e+00" gbyte="3.77476e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18059e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4cf14d114d2141656d214d114e2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09623e+02" utime="8.93353e+01" stime="1.35971e+01" mtime="7.18059e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18059e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2177e+09" > 6.1907e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2160e+09" > 3.4506e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2606e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9357e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.1177e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5191e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.2852e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0573e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1074e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3853e+01 </func>
</region>
</regions>
<internal rank="199" log_i="1724765674.536021" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="200" mpi_size="696" stamp_init="1724765564.793242" stamp_final="1724765674.532879" username="apac4" allocationname="unknown" flags="0" pid="784496" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09740e+02" utime="8.39340e+01" stime="1.57884e+01" mtime="7.09412e+01" gflop="0.00000e+00" gbyte="3.76640e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.09412e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4fe141a1413145e551314e914d5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09614e+02" utime="8.39016e+01" stime="1.57796e+01" mtime="7.09412e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.09412e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1980e+09" > 7.4824e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2025e+09" > 4.6171e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.6726e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9349e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.1284e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7164e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5187e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.3303e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0609e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1152e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.1444e+01 </func>
</region>
</regions>
<internal rank="200" log_i="1724765674.532879" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="201" mpi_size="696" stamp_init="1724765564.792118" stamp_final="1724765674.532307" username="apac4" allocationname="unknown" flags="0" pid="784497" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09740e+02" utime="8.92572e+01" stime="1.36025e+01" mtime="7.23129e+01" gflop="0.00000e+00" gbyte="3.78216e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23129e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ef14f014f2149055f214f11475" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09617e+02" utime="8.92227e+01" stime="1.35949e+01" mtime="7.23129e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23129e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1930e+09" > 5.9014e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1983e+09" > 2.9519e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4184e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9353e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7176e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5172e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.4792e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0551e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0977e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4240e+01 </func>
</region>
</regions>
<internal rank="201" log_i="1724765674.532307" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="202" mpi_size="696" stamp_init="1724765564.794015" stamp_final="1724765674.528543" username="apac4" allocationname="unknown" flags="0" pid="784498" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09735e+02" utime="8.56455e+01" stime="1.51937e+01" mtime="7.12625e+01" gflop="0.00000e+00" gbyte="3.77140e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.12625e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09610e+02" utime="8.56164e+01" stime="1.51811e+01" mtime="7.12625e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.12625e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2008e+09" > 8.1819e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1973e+09" > 5.3211e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4044e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9352e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1300e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.1391e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5173e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.4552e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0569e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1150e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2950e+01 </func>
</region>
</regions>
<internal rank="202" log_i="1724765674.528543" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="203" mpi_size="696" stamp_init="1724765564.792320" stamp_final="1724765674.536920" username="apac4" allocationname="unknown" flags="0" pid="784499" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09745e+02" utime="8.88674e+01" stime="1.40390e+01" mtime="7.23780e+01" gflop="0.00000e+00" gbyte="3.77747e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23780e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43614381439149056391439149a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09624e+02" utime="8.88347e+01" stime="1.40295e+01" mtime="7.23780e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23780e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2005e+09" > 6.5099e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2074e+09" > 2.6386e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8627e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9324e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0531e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5183e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.4008e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0614e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0981e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3831e+01 </func>
</region>
</regions>
<internal rank="203" log_i="1724765674.536920" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="204" mpi_size="696" stamp_init="1724765564.792860" stamp_final="1724765674.528636" username="apac4" allocationname="unknown" flags="0" pid="784500" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09736e+02" utime="8.63804e+01" stime="1.48558e+01" mtime="7.18543e+01" gflop="0.00000e+00" gbyte="3.76717e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18543e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007114f35571147114ac" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09611e+02" utime="8.63489e+01" stime="1.48460e+01" mtime="7.18543e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18543e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2025e+09" > 6.9802e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1983e+09" > 4.7749e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0697e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9353e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5233e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6700e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5175e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.4856e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0580e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1136e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3000e+01 </func>
</region>
</regions>
<internal rank="204" log_i="1724765674.528636" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="205" mpi_size="696" stamp_init="1724765564.793037" stamp_final="1724765674.535390" username="apac4" allocationname="unknown" flags="0" pid="784501" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09742e+02" utime="8.89175e+01" stime="1.40304e+01" mtime="7.18606e+01" gflop="0.00000e+00" gbyte="3.74702e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18606e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4601462146314f25663146214fb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09618e+02" utime="8.88821e+01" stime="1.40242e+01" mtime="7.18606e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18606e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2090e+09" > 6.1690e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2041e+09" > 3.3655e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4449e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9336e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5497e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7079e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5173e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.5091e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0575e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0983e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3746e+01 </func>
</region>
</regions>
<internal rank="205" log_i="1724765674.535390" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="206" mpi_size="696" stamp_init="1724765564.792759" stamp_final="1724765674.532047" username="apac4" allocationname="unknown" flags="0" pid="784502" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09739e+02" utime="8.68173e+01" stime="1.45687e+01" mtime="7.15031e+01" gflop="0.00000e+00" gbyte="3.77750e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15031e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09614e+02" utime="8.67855e+01" stime="1.45587e+01" mtime="7.15031e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15031e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2149e+09" > 7.2589e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2033e+09" > 4.7519e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5861e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9356e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0777e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6714e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5170e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.5052e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0571e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1126e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3104e+01 </func>
</region>
</regions>
<internal rank="206" log_i="1724765674.532047" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="207" mpi_size="696" stamp_init="1724765564.792847" stamp_final="1724765674.543289" username="apac4" allocationname="unknown" flags="0" pid="784503" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09750e+02" utime="8.89185e+01" stime="1.36479e+01" mtime="7.20939e+01" gflop="0.00000e+00" gbyte="3.76781e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20939e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000015145856151415147c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09631e+02" utime="8.88884e+01" stime="1.36362e+01" mtime="7.20939e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20939e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2033e+09" > 5.7884e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2033e+09" > 3.6017e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2756e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9351e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6912e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5167e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.5360e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0576e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0951e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4169e+01 </func>
</region>
</regions>
<internal rank="207" log_i="1724765674.543289" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="208" mpi_size="696" stamp_init="1724765564.792600" stamp_final="1724765674.529334" username="apac4" allocationname="unknown" flags="0" pid="784504" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09737e+02" utime="8.08678e+01" stime="1.97962e+01" mtime="7.09833e+01" gflop="0.00000e+00" gbyte="3.77724e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.09833e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003715361516" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09610e+02" utime="8.08402e+01" stime="1.97818e+01" mtime="7.09833e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.09833e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2017e+09" > 8.8379e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1928e+09" > 5.6696e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4538e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 7.3404e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2011e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.3218e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5168e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.5361e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0553e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1142e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2563e+01 </func>
</region>
</regions>
<internal rank="208" log_i="1724765674.529334" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="209" mpi_size="696" stamp_init="1724765564.792133" stamp_final="1724765674.526727" username="apac4" allocationname="unknown" flags="0" pid="784505" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09735e+02" utime="8.89164e+01" stime="1.37262e+01" mtime="7.21024e+01" gflop="0.00000e+00" gbyte="3.77747e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21024e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000cd14cc14c1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09613e+02" utime="8.88876e+01" stime="1.37119e+01" mtime="7.21024e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21024e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2047e+09" > 5.9850e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2094e+09" > 3.6654e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4665e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9345e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4080e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7249e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5164e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.6130e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0589e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0964e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3972e+01 </func>
</region>
</regions>
<internal rank="209" log_i="1724765674.526727" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="210" mpi_size="696" stamp_init="1724765564.792124" stamp_final="1724765674.529792" username="apac4" allocationname="unknown" flags="0" pid="784506" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09738e+02" utime="8.51388e+01" stime="1.48076e+01" mtime="7.15611e+01" gflop="0.00000e+00" gbyte="3.77556e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15611e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4fc14fe14ff14d755ff14ff14d0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09612e+02" utime="8.51048e+01" stime="1.47991e+01" mtime="7.15611e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15611e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2269e+09" > 8.0646e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2280e+09" > 5.0062e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0281e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9362e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7715e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.5330e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5150e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.6851e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0596e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1132e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2632e+01 </func>
</region>
</regions>
<internal rank="210" log_i="1724765674.529792" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="211" mpi_size="696" stamp_init="1724765564.792816" stamp_final="1724765674.537015" username="apac4" allocationname="unknown" flags="0" pid="784507" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09744e+02" utime="8.95786e+01" stime="1.34221e+01" mtime="7.24946e+01" gflop="0.00000e+00" gbyte="3.77831e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24946e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000bf14ed55bf14bf148f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09621e+02" utime="8.95488e+01" stime="1.34099e+01" mtime="7.24946e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24946e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1948e+09" > 6.0209e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2049e+09" > 3.1516e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8250e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9323e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2650e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0526e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5159e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.6521e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0562e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0442e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4029e+01 </func>
</region>
</regions>
<internal rank="211" log_i="1724765674.537015" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="212" mpi_size="696" stamp_init="1724765564.792412" stamp_final="1724765674.533458" username="apac4" allocationname="unknown" flags="0" pid="784508" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09741e+02" utime="8.49692e+01" stime="1.53082e+01" mtime="7.10063e+01" gflop="0.00000e+00" gbyte="3.75191e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.10063e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009615de559615961501" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09616e+02" utime="8.49361e+01" stime="1.52983e+01" mtime="7.10063e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.10063e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2195e+09" > 8.0484e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2099e+09" > 4.9806e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1987e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9352e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8102e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.5390e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5155e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.6867e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0610e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1132e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2915e+01 </func>
</region>
</regions>
<internal rank="212" log_i="1724765674.533458" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="213" mpi_size="696" stamp_init="1724765564.792811" stamp_final="1724765674.538649" username="apac4" allocationname="unknown" flags="0" pid="784509" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09746e+02" utime="8.86346e+01" stime="1.32888e+01" mtime="7.24042e+01" gflop="0.00000e+00" gbyte="3.77048e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24042e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4cc14ce14cf146e55cf14ce14a1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09621e+02" utime="8.86022e+01" stime="1.32789e+01" mtime="7.24042e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24042e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2062e+09" > 6.2514e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1982e+09" > 3.3709e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8188e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9355e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.6110e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5152e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.7199e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0553e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0926e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3891e+01 </func>
</region>
</regions>
<internal rank="213" log_i="1724765674.538649" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="214" mpi_size="696" stamp_init="1724765564.792730" stamp_final="1724765674.533628" username="apac4" allocationname="unknown" flags="0" pid="784510" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09741e+02" utime="8.66937e+01" stime="1.45677e+01" mtime="7.07898e+01" gflop="0.00000e+00" gbyte="3.77743e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.07898e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000691469147d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09616e+02" utime="8.66561e+01" stime="1.45628e+01" mtime="7.07898e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.07898e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2132e+09" > 7.6705e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2190e+09" > 4.4068e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9640e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9349e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.2200e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.5443e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5153e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.7122e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0571e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.1120e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2985e+01 </func>
</region>
</regions>
<internal rank="214" log_i="1724765674.533628" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="215" mpi_size="696" stamp_init="1724765564.793403" stamp_final="1724765674.538789" username="apac4" allocationname="unknown" flags="0" pid="784511" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u17b</host>
<perf wtime="1.09745e+02" utime="8.84677e+01" stime="1.34532e+01" mtime="7.23959e+01" gflop="0.00000e+00" gbyte="3.77434e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23959e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b4148456b414af1490" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09622e+02" utime="8.84317e+01" stime="1.34471e+01" mtime="7.23959e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23959e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1906e+09" > 6.1014e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1955e+09" > 3.4167e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7847e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9333e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1921e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.3791e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5150e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.7453e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0586e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0940e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3950e+01 </func>
</region>
</regions>
<internal rank="215" log_i="1724765674.538789" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="216" mpi_size="696" stamp_init="1724765564.711932" stamp_final="1724765674.539387" username="apac4" allocationname="unknown" flags="0" pid="126440" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09827e+02" utime="7.28094e+01" stime="2.46873e+01" mtime="6.74756e+01" gflop="0.00000e+00" gbyte="3.85548e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.74756e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000961596150f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09700e+02" utime="7.27787e+01" stime="2.46764e+01" mtime="6.74756e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.74756e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1844e+09" > 7.5099e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1908e+09" > 3.3241e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6331e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 7.3320e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7635e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.4816e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5137e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.8588e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0431e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0888e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3417e+01 </func>
</region>
</regions>
<internal rank="216" log_i="1724765674.539387" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="217" mpi_size="696" stamp_init="1724765564.711730" stamp_final="1724765674.536383" username="apac4" allocationname="unknown" flags="0" pid="126441" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09825e+02" utime="6.43098e+01" stime="8.73766e+00" mtime="4.23231e+01" gflop="0.00000e+00" gbyte="3.77068e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.23231e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009e159e1539" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09693e+02" utime="6.42762e+01" stime="8.72810e+00" mtime="4.23231e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.23231e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127200" bytes="1.2230e+09" > 6.4015e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2211e+09" > 2.8615e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5128e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.2585e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7180e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2060e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5148e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.7323e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0435e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0254e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4210e+01 </func>
</region>
</regions>
<internal rank="217" log_i="1724765674.536383" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="218" mpi_size="696" stamp_init="1724765564.711828" stamp_final="1724765674.528318" username="apac4" allocationname="unknown" flags="0" pid="126442" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09816e+02" utime="7.92394e+01" stime="1.35413e+01" mtime="6.38438e+01" gflop="0.00000e+00" gbyte="3.75443e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.38438e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001214121463" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09690e+02" utime="7.92058e+01" stime="1.35326e+01" mtime="6.38438e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.38438e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2042e+09" > 8.7809e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2055e+09" > 3.8326e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6817e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.1108e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.2718e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.4223e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5141e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.8000e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0284e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0891e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3463e+01 </func>
</region>
</regions>
<internal rank="218" log_i="1724765674.528318" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="219" mpi_size="696" stamp_init="1724765564.711724" stamp_final="1724765674.535545" username="apac4" allocationname="unknown" flags="0" pid="126443" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09824e+02" utime="8.80741e+01" stime="1.34663e+01" mtime="7.11476e+01" gflop="0.00000e+00" gbyte="3.77480e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.11476e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005c145c146b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09696e+02" utime="8.80413e+01" stime="1.34568e+01" mtime="7.11476e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.11476e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2105e+09" > 6.7595e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2119e+09" > 2.6555e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0571e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.7779e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4796e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.8978e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5137e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.8474e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0398e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0203e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3935e+01 </func>
</region>
</regions>
<internal rank="219" log_i="1724765674.535545" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="220" mpi_size="696" stamp_init="1724765564.711947" stamp_final="1724765674.534089" username="apac4" allocationname="unknown" flags="0" pid="126444" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09822e+02" utime="6.25026e+01" stime="9.99980e+00" mtime="4.28404e+01" gflop="0.00000e+00" gbyte="3.77869e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.28404e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001514ee55151414145a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09693e+02" utime="6.24682e+01" stime="9.99209e+00" mtime="4.28404e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.28404e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1992e+09" > 7.5985e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2012e+09" > 4.1581e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5931e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 7.3513e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7878e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.9894e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5141e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.8476e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0348e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0862e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3632e+01 </func>
</region>
</regions>
<internal rank="220" log_i="1724765674.534089" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="221" mpi_size="696" stamp_init="1724765564.711894" stamp_final="1724765674.533632" username="apac4" allocationname="unknown" flags="0" pid="126445" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09822e+02" utime="6.40597e+01" stime="8.53015e+00" mtime="4.18397e+01" gflop="0.00000e+00" gbyte="3.76076e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.18397e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006d1491556d146c1493" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09695e+02" utime="6.40286e+01" stime="8.51874e+00" mtime="4.18397e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.18397e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2029e+09" > 6.1776e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2051e+09" > 3.3106e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4833e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.1705e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0674e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5140e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.8198e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0313e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0211e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3854e+01 </func>
</region>
</regions>
<internal rank="221" log_i="1724765674.533632" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="222" mpi_size="696" stamp_init="1724765564.711824" stamp_final="1724765674.533319" username="apac4" allocationname="unknown" flags="0" pid="126446" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09821e+02" utime="6.29066e+01" stime="9.60643e+00" mtime="4.21935e+01" gflop="0.00000e+00" gbyte="3.76579e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.21935e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e214e114e3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09694e+02" utime="6.28764e+01" stime="9.59484e+00" mtime="4.21935e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.21935e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2105e+09" > 7.8641e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2090e+09" > 3.6657e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4466e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 7.3338e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6151e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0204e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5131e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 6.9232e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0316e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0377e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3301e+01 </func>
</region>
</regions>
<internal rank="222" log_i="1724765674.533319" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="223" mpi_size="696" stamp_init="1724765564.711720" stamp_final="1724765674.528391" username="apac4" allocationname="unknown" flags="0" pid="126447" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09817e+02" utime="6.18475e+01" stime="8.26510e+00" mtime="4.03436e+01" gflop="0.00000e+00" gbyte="3.77972e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.03436e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf467146814691423556914691455" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09688e+02" utime="6.18170e+01" stime="8.25324e+00" mtime="4.03436e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.03436e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2145e+09" > 6.6155e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2125e+09" > 2.7356e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0213e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 7.3424e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.2827e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5125e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.0020e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0246e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0215e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3617e+01 </func>
</region>
</regions>
<internal rank="223" log_i="1724765674.528391" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="224" mpi_size="696" stamp_init="1724765564.711839" stamp_final="1724765674.540170" username="apac4" allocationname="unknown" flags="0" pid="126448" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09828e+02" utime="5.74355e+01" stime="1.10125e+01" mtime="4.12126e+01" gflop="0.00000e+00" gbyte="3.77197e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.12126e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09700e+02" utime="5.73976e+01" stime="1.10087e+01" mtime="4.12126e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.12126e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2117e+09" > 1.9974e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2083e+09" > 8.9252e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0739e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 6.8499e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2761e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.2717e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5080e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.3376e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0430e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0773e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2251e+01 </func>
</region>
</regions>
<internal rank="224" log_i="1724765674.540170" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="225" mpi_size="696" stamp_init="1724765564.711873" stamp_final="1724765674.539056" username="apac4" allocationname="unknown" flags="0" pid="126449" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09827e+02" utime="7.45725e+01" stime="1.03660e+01" mtime="5.40202e+01" gflop="0.00000e+00" gbyte="3.77525e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.40202e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09701e+02" utime="7.45391e+01" stime="1.03572e+01" mtime="5.40202e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.40202e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2028e+09" > 6.2301e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2026e+09" > 2.6494e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2011e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.1232e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.4091e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5113e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.1027e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0324e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0212e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4257e+01 </func>
</region>
</regions>
<internal rank="225" log_i="1724765674.539056" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="226" mpi_size="696" stamp_init="1724765564.711895" stamp_final="1724765674.533256" username="apac4" allocationname="unknown" flags="0" pid="126450" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09821e+02" utime="7.36305e+01" stime="1.21392e+01" mtime="5.61923e+01" gflop="0.00000e+00" gbyte="3.76659e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.61923e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09692e+02" utime="7.36036e+01" stime="1.21246e+01" mtime="5.61923e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.61923e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2148e+09" > 7.5440e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2048e+09" > 4.1353e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5323e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.0107e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3247e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8392e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5109e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.1615e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0265e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0775e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3758e+01 </func>
</region>
</regions>
<internal rank="226" log_i="1724765674.533256" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="227" mpi_size="696" stamp_init="1724765564.711880" stamp_final="1724765674.528250" username="apac4" allocationname="unknown" flags="0" pid="126451" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09816e+02" utime="7.27634e+01" stime="1.00236e+01" mtime="5.33212e+01" gflop="0.00000e+00" gbyte="3.77480e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.33212e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4211423142414b85524142314b2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09688e+02" utime="7.27320e+01" stime="1.00132e+01" mtime="5.33212e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.33212e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2011e+09" > 6.1687e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2020e+09" > 2.7399e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7639e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.0101e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.3685e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5108e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.1491e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0284e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0214e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4139e+01 </func>
</region>
</regions>
<internal rank="227" log_i="1724765674.528250" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="228" mpi_size="696" stamp_init="1724765564.711821" stamp_final="1724765674.542768" username="apac4" allocationname="unknown" flags="0" pid="126452" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09831e+02" utime="8.28311e+01" stime="1.88591e+01" mtime="7.24472e+01" gflop="0.00000e+00" gbyte="3.76987e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24472e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a814a81473" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09704e+02" utime="8.27979e+01" stime="1.88503e+01" mtime="7.24472e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24472e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1953e+09" > 7.7154e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2043e+09" > 3.4616e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.4915e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.8690e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0608e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2503e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5106e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.1890e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0296e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0708e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2998e+01 </func>
</region>
</regions>
<internal rank="228" log_i="1724765674.542768" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="229" mpi_size="696" stamp_init="1724765564.711809" stamp_final="1724765674.539008" username="apac4" allocationname="unknown" flags="0" pid="126453" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09827e+02" utime="8.10487e+01" stime="1.22458e+01" mtime="6.29167e+01" gflop="0.00000e+00" gbyte="3.78468e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.29167e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c714c914ca14fc56ca14c91474" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09699e+02" utime="8.10134e+01" stime="1.22379e+01" mtime="6.29167e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.29167e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2049e+09" > 6.1319e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2100e+09" > 2.9377e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8902e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.9621e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.3470e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5086e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.4105e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0439e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0208e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4080e+01 </func>
</region>
</regions>
<internal rank="229" log_i="1724765674.539008" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="230" mpi_size="696" stamp_init="1724765564.711830" stamp_final="1724765674.528136" username="apac4" allocationname="unknown" flags="0" pid="126454" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09816e+02" utime="6.21817e+01" stime="9.21110e+00" mtime="4.21436e+01" gflop="0.00000e+00" gbyte="3.76896e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.21436e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09688e+02" utime="6.21517e+01" stime="9.19934e+00" mtime="4.21436e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.21436e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2034e+09" > 6.7798e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2126e+09" > 2.8089e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.3852e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.8831e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.6253e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2934e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5102e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.2376e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0322e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0658e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3494e+01 </func>
</region>
</regions>
<internal rank="230" log_i="1724765674.528136" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="231" mpi_size="696" stamp_init="1724765564.711733" stamp_final="1724765674.538742" username="apac4" allocationname="unknown" flags="0" pid="126455" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09827e+02" utime="6.48317e+01" stime="9.35157e+00" mtime="4.41183e+01" gflop="0.00000e+00" gbyte="3.78323e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.41183e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c8143655c814c8148b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09698e+02" utime="6.48045e+01" stime="9.33659e+00" mtime="4.41183e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.41183e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2008e+09" > 6.5529e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2011e+09" > 2.5841e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.5099e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0726e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.2248e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5099e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.2648e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0303e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0179e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3528e+01 </func>
</region>
</regions>
<internal rank="231" log_i="1724765674.538742" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="232" mpi_size="696" stamp_init="1724765564.711815" stamp_final="1724765674.539871" username="apac4" allocationname="unknown" flags="0" pid="126456" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09828e+02" utime="6.16913e+01" stime="9.14113e+00" mtime="4.08852e+01" gflop="0.00000e+00" gbyte="3.74851e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.08852e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09700e+02" utime="6.16595e+01" stime="9.13077e+00" mtime="4.08852e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.08852e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1962e+09" > 6.9201e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1937e+09" > 4.2068e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9038e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 6.8512e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5057e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.0541e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5092e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.3417e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0251e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0463e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3217e+01 </func>
</region>
</regions>
<internal rank="232" log_i="1724765674.539871" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="233" mpi_size="696" stamp_init="1724765564.711842" stamp_final="1724765674.533275" username="apac4" allocationname="unknown" flags="0" pid="126457" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09821e+02" utime="8.70674e+01" stime="1.37558e+01" mtime="7.01188e+01" gflop="0.00000e+00" gbyte="3.75233e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.01188e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002214a65522142214b2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09694e+02" utime="8.70367e+01" stime="1.37441e+01" mtime="7.01188e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.01188e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2083e+09" > 6.2777e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1958e+09" > 2.3410e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.3283e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.7084e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9325e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2310e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5093e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.3339e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0254e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0184e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3381e+01 </func>
</region>
</regions>
<internal rank="233" log_i="1724765674.533275" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="234" mpi_size="696" stamp_init="1724765564.711872" stamp_final="1724765674.534439" username="apac4" allocationname="unknown" flags="0" pid="126458" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09823e+02" utime="8.36056e+01" stime="1.84045e+01" mtime="7.15613e+01" gflop="0.00000e+00" gbyte="3.78025e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15613e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09694e+02" utime="8.35775e+01" stime="1.83902e+01" mtime="7.15613e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15613e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.0729e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2239e+09" > 7.1566e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2316e+09" > 3.7117e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5709e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0231e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9257e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.5893e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5079e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.4820e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0263e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0467e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3067e+01 </func>
</region>
</regions>
<internal rank="234" log_i="1724765674.534439" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="235" mpi_size="696" stamp_init="1724765564.711828" stamp_final="1724765674.530737" username="apac4" allocationname="unknown" flags="0" pid="126459" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09819e+02" utime="6.47397e+01" stime="9.27673e+00" mtime="4.34542e+01" gflop="0.00000e+00" gbyte="3.74737e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.34542e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf41c141d141e14fb561e141e146c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09690e+02" utime="6.47084e+01" stime="9.26625e+00" mtime="4.34542e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.34542e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2000e+09" > 6.0988e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1903e+09" > 2.7705e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9336e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0218e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4305e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.6510e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5084e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.4280e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0310e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0196e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3983e+01 </func>
</region>
</regions>
<internal rank="235" log_i="1724765674.530737" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="236" mpi_size="696" stamp_init="1724765564.712064" stamp_final="1724765674.535343" username="apac4" allocationname="unknown" flags="0" pid="126460" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09823e+02" utime="6.88048e+01" stime="1.45820e+01" mtime="5.44006e+01" gflop="0.00000e+00" gbyte="3.74893e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.44006e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a614a714a814f955a814a814f2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09695e+02" utime="6.87695e+01" stime="1.45757e+01" mtime="5.44006e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.44006e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2103e+09" > 9.3516e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2190e+09" > 4.8330e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2610e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 6.8406e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5151e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.2914e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5078e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.4352e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0270e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0299e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3489e+01 </func>
</region>
</regions>
<internal rank="236" log_i="1724765674.535343" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="237" mpi_size="696" stamp_init="1724765564.711763" stamp_final="1724765674.538918" username="apac4" allocationname="unknown" flags="0" pid="126461" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09827e+02" utime="6.42545e+01" stime="8.89131e+00" mtime="4.31743e+01" gflop="0.00000e+00" gbyte="3.75648e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.31743e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000082148756821482146a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09699e+02" utime="6.42268e+01" stime="8.87685e+00" mtime="4.31743e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.31743e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1971e+09" > 6.6723e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2026e+09" > 2.6192e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5724e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0193e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8048e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5080e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.4729e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0277e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0172e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4036e+01 </func>
</region>
</regions>
<internal rank="237" log_i="1724765674.538918" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="238" mpi_size="696" stamp_init="1724765564.711852" stamp_final="1724765674.534014" username="apac4" allocationname="unknown" flags="0" pid="126462" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09822e+02" utime="5.85781e+01" stime="9.68122e+00" mtime="3.95011e+01" gflop="0.00000e+00" gbyte="3.78040e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.95011e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000058145814b9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09694e+02" utime="5.85457e+01" stime="9.67131e+00" mtime="3.95011e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.95011e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2206e+09" > 8.7052e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2165e+09" > 4.8161e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0258e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 6.8466e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.6757e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4976e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5081e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.4408e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0369e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0287e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3036e+01 </func>
</region>
</regions>
<internal rank="238" log_i="1724765674.534014" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="239" mpi_size="696" stamp_init="1724765564.711737" stamp_final="1724765674.546811" username="apac4" allocationname="unknown" flags="0" pid="126463" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="1.09835e+02" utime="6.97269e+01" stime="9.78341e+00" mtime="4.84263e+01" gflop="0.00000e+00" gbyte="3.77621e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.84263e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c815c71527" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09708e+02" utime="6.96954e+01" stime="9.77244e+00" mtime="4.84263e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.84263e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127200" bytes="1.1905e+09" > 6.6925e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1929e+09" > 3.1737e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0880e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.5758e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.5899e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5074e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.4901e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0342e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0188e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4206e+01 </func>
</region>
</regions>
<internal rank="239" log_i="1724765674.546811" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="240" mpi_size="696" stamp_init="1724765564.908430" stamp_final="1724765674.535445" username="apac4" allocationname="unknown" flags="0" pid="2254516" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09627e+02" utime="7.59265e+01" stime="2.59233e+01" mtime="7.17427e+01" gflop="0.00000e+00" gbyte="3.85719e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17427e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000099141c5699149914a7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09496e+02" utime="7.58929e+01" stime="2.59148e+01" mtime="7.17427e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17427e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1889e+09" > 6.9156e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1819e+09" > 3.5062e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4627e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9549e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.3911e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2952e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5073e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.5368e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0554e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 7.9745e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3319e+01 </func>
</region>
</regions>
<internal rank="240" log_i="1724765674.535445" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="241" mpi_size="696" stamp_init="1724765564.908505" stamp_final="1724765674.543832" username="apac4" allocationname="unknown" flags="0" pid="2254517" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09635e+02" utime="8.98094e+01" stime="1.34724e+01" mtime="7.24392e+01" gflop="0.00000e+00" gbyte="3.77335e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24392e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ae14b014b114b456b114b114d7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09501e+02" utime="8.97757e+01" stime="1.34627e+01" mtime="7.24392e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24392e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2229e+09" > 6.1953e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2246e+09" > 3.0672e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4087e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9529e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8147e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1276e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5077e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.4816e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0520e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 7.9674e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4167e+01 </func>
</region>
</regions>
<internal rank="241" log_i="1724765674.543832" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="242" mpi_size="696" stamp_init="1724765564.908426" stamp_final="1724765674.531253" username="apac4" allocationname="unknown" flags="0" pid="2254518" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09623e+02" utime="8.46761e+01" stime="1.55632e+01" mtime="7.13130e+01" gflop="0.00000e+00" gbyte="3.76770e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.13130e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000027142714ee" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09494e+02" utime="8.46472e+01" stime="1.55503e+01" mtime="7.13130e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.13130e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2013e+09" > 1.1206e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2007e+09" > 4.9733e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0216e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9526e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8061e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.6299e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5063e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.5254e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0525e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0267e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2912e+01 </func>
</region>
</regions>
<internal rank="242" log_i="1724765674.531253" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="243" mpi_size="696" stamp_init="1724765564.908489" stamp_final="1724765674.532975" username="apac4" allocationname="unknown" flags="0" pid="2254519" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09624e+02" utime="8.96766e+01" stime="1.35433e+01" mtime="7.25636e+01" gflop="0.00000e+00" gbyte="3.77827e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25636e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09495e+02" utime="8.96434e+01" stime="1.35338e+01" mtime="7.25636e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25636e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2099e+09" > 5.6759e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2063e+09" > 2.8623e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4626e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9549e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9802e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.6499e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5073e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.5283e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0528e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 7.9570e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4275e+01 </func>
</region>
</regions>
<internal rank="243" log_i="1724765674.532975" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="244" mpi_size="696" stamp_init="1724765564.910332" stamp_final="1724765674.539092" username="apac4" allocationname="unknown" flags="0" pid="2254520" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09629e+02" utime="8.62357e+01" stime="1.45106e+01" mtime="7.12216e+01" gflop="0.00000e+00" gbyte="3.78124e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.12216e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48f1590159215b655921591152d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09498e+02" utime="8.62029e+01" stime="1.45012e+01" mtime="7.12216e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.12216e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1971e+09" > 9.7222e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1924e+09" > 5.1659e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9212e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9551e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.9770e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.4900e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5068e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.5409e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0569e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0275e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3041e+01 </func>
</region>
</regions>
<internal rank="244" log_i="1724765674.539092" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="245" mpi_size="696" stamp_init="1724765564.908481" stamp_final="1724765674.534341" username="apac4" allocationname="unknown" flags="0" pid="2254521" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09626e+02" utime="8.97779e+01" stime="1.36028e+01" mtime="7.23454e+01" gflop="0.00000e+00" gbyte="3.77934e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23454e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09495e+02" utime="8.97486e+01" stime="1.35893e+01" mtime="7.23454e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23454e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2087e+09" > 6.1274e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2019e+09" > 2.9709e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2002e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9534e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7895e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1274e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5061e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.6465e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0574e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 7.9558e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4284e+01 </func>
</region>
</regions>
<internal rank="245" log_i="1724765674.534341" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="246" mpi_size="696" stamp_init="1724765564.908451" stamp_final="1724765674.532446" username="apac4" allocationname="unknown" flags="0" pid="2254522" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09624e+02" utime="8.72177e+01" stime="1.47370e+01" mtime="7.13529e+01" gflop="0.00000e+00" gbyte="3.77914e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.13529e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e414e41482" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09493e+02" utime="8.71904e+01" stime="1.47213e+01" mtime="7.13529e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.13529e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2075e+09" > 7.0054e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2064e+09" > 3.5421e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4264e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9515e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1981e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.5458e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5056e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.6800e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0545e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0260e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2995e+01 </func>
</region>
</regions>
<internal rank="246" log_i="1724765674.532446" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="247" mpi_size="696" stamp_init="1724765564.908538" stamp_final="1724765674.531064" username="apac4" allocationname="unknown" flags="0" pid="2254523" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09623e+02" utime="8.94322e+01" stime="1.37192e+01" mtime="7.28827e+01" gflop="0.00000e+00" gbyte="3.77071e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28827e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09494e+02" utime="8.93987e+01" stime="1.37103e+01" mtime="7.28827e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28827e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2098e+09" > 6.0651e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2076e+09" > 3.0840e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6421e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9546e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.5479e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5056e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.7017e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0578e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 7.9526e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4378e+01 </func>
</region>
</regions>
<internal rank="247" log_i="1724765674.531064" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="248" mpi_size="696" stamp_init="1724765564.908443" stamp_final="1724765674.537893" username="apac4" allocationname="unknown" flags="0" pid="2254524" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09629e+02" utime="8.56657e+01" stime="1.51487e+01" mtime="7.15726e+01" gflop="0.00000e+00" gbyte="3.78063e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15726e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008e158d151f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09496e+02" utime="8.56378e+01" stime="1.51341e+01" mtime="7.15726e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15726e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2154e+09" > 8.6505e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2204e+09" > 4.5664e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7909e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9545e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.4789e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3760e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5047e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.7538e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0544e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0137e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2642e+01 </func>
</region>
</regions>
<internal rank="248" log_i="1724765674.537893" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="249" mpi_size="696" stamp_init="1724765564.910363" stamp_final="1724765674.534026" username="apac4" allocationname="unknown" flags="0" pid="2254525" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09624e+02" utime="8.92671e+01" stime="1.40634e+01" mtime="7.31383e+01" gflop="0.00000e+00" gbyte="3.78391e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.31383e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09493e+02" utime="8.92408e+01" stime="1.40469e+01" mtime="7.31383e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.31383e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2055e+09" > 6.0940e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2012e+09" > 2.5805e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6862e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9546e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.7193e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.4676e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5053e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.7343e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0559e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 7.9326e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4592e+01 </func>
</region>
</regions>
<internal rank="249" log_i="1724765674.534026" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="250" mpi_size="696" stamp_init="1724765564.908464" stamp_final="1724765674.532709" username="apac4" allocationname="unknown" flags="0" pid="2254526" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09624e+02" utime="8.76448e+01" stime="1.42373e+01" mtime="7.17838e+01" gflop="0.00000e+00" gbyte="3.78326e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17838e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09493e+02" utime="8.76097e+01" stime="1.42289e+01" mtime="7.17838e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17838e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2134e+09" > 6.9128e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2252e+09" > 3.3645e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8435e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9548e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0324e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.6316e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5045e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.7709e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0554e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0120e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2988e+01 </func>
</region>
</regions>
<internal rank="250" log_i="1724765674.532709" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="251" mpi_size="696" stamp_init="1724765564.908795" stamp_final="1724765674.530390" username="apac4" allocationname="unknown" flags="0" pid="2254527" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09622e+02" utime="8.93314e+01" stime="1.38678e+01" mtime="7.28923e+01" gflop="0.00000e+00" gbyte="3.77625e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28923e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d014d214d3142356d314d21461" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09492e+02" utime="8.92936e+01" stime="1.38637e+01" mtime="7.28923e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28923e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2058e+09" > 6.0741e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2005e+09" > 2.4679e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6852e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9556e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2187e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.7203e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5050e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.7594e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0563e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 7.9335e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4339e+01 </func>
</region>
</regions>
<internal rank="251" log_i="1724765674.530390" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="252" mpi_size="696" stamp_init="1724765564.908434" stamp_final="1724765674.537751" username="apac4" allocationname="unknown" flags="0" pid="2254528" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09629e+02" utime="8.51019e+01" stime="1.54864e+01" mtime="7.19761e+01" gflop="0.00000e+00" gbyte="3.77254e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19761e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d5142c56d514d514c7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09498e+02" utime="8.50676e+01" stime="1.54785e+01" mtime="7.19761e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19761e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2046e+09" > 8.2660e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2009e+09" > 4.7574e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0430e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9552e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2040e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3400e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5048e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.6842e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0561e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.0047e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2828e+01 </func>
</region>
</regions>
<internal rank="252" log_i="1724765674.537751" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="253" mpi_size="696" stamp_init="1724765564.908526" stamp_final="1724765674.538233" username="apac4" allocationname="unknown" flags="0" pid="2254529" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09630e+02" utime="8.95059e+01" stime="1.37669e+01" mtime="7.30475e+01" gflop="0.00000e+00" gbyte="3.77392e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.30475e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09499e+02" utime="8.94748e+01" stime="1.37561e+01" mtime="7.30475e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.30475e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2068e+09" > 5.9302e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2080e+09" > 2.8132e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7532e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9551e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8862e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.6788e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5044e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.8264e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0558e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 7.9308e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4443e+01 </func>
</region>
</regions>
<internal rank="253" log_i="1724765674.538233" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="254" mpi_size="696" stamp_init="1724765564.908423" stamp_final="1724765674.526808" username="apac4" allocationname="unknown" flags="0" pid="2254530" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09618e+02" utime="8.76368e+01" stime="1.43002e+01" mtime="7.20518e+01" gflop="0.00000e+00" gbyte="3.76774e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20518e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ac14ab1477" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09487e+02" utime="8.76075e+01" stime="1.42877e+01" mtime="7.20518e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20518e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.0599e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2125e+09" > 6.8543e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2115e+09" > 3.4950e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0751e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9557e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.9857e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8094e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5038e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.8638e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0540e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 7.9960e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3018e+01 </func>
</region>
</regions>
<internal rank="254" log_i="1724765674.526808" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="255" mpi_size="696" stamp_init="1724765564.908497" stamp_final="1724765674.532917" username="apac4" allocationname="unknown" flags="0" pid="2254531" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09624e+02" utime="8.90173e+01" stime="1.40828e+01" mtime="7.24000e+01" gflop="0.00000e+00" gbyte="3.75202e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24000e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09495e+02" utime="8.89880e+01" stime="1.40700e+01" mtime="7.24000e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24000e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2057e+09" > 6.0540e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1961e+09" > 3.2001e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4207e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9543e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0054e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.7440e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5044e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 7.7907e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0541e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 7.9254e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4118e+01 </func>
</region>
</regions>
<internal rank="255" log_i="1724765674.532917" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="256" mpi_size="696" stamp_init="1724765564.909269" stamp_final="1724765674.532379" username="apac4" allocationname="unknown" flags="0" pid="2254532" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09623e+02" utime="8.19026e+01" stime="1.94358e+01" mtime="7.09446e+01" gflop="0.00000e+00" gbyte="3.77441e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.09446e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002414241462" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09492e+02" utime="8.18700e+01" stime="1.94263e+01" mtime="7.09446e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.09446e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1876e+09" > 7.5749e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1951e+09" > 4.0962e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1987e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.4526e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.7083e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5005e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.2089e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0546e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 7.9880e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2739e+01 </func>
</region>
</regions>
<internal rank="256" log_i="1724765674.532379" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="257" mpi_size="696" stamp_init="1724765564.908526" stamp_final="1724765674.538033" username="apac4" allocationname="unknown" flags="0" pid="2254533" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09630e+02" utime="8.89394e+01" stime="1.42154e+01" mtime="7.29952e+01" gflop="0.00000e+00" gbyte="3.74557e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.29952e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b615b715b815ff55b815b8152b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09500e+02" utime="8.89055e+01" stime="1.42073e+01" mtime="7.29952e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.29952e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1877e+09" > 6.0235e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1976e+09" > 3.0195e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9403e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9547e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3855e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.2543e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5002e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.2078e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0545e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 7.9247e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4194e+01 </func>
</region>
</regions>
<internal rank="257" log_i="1724765674.538033" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="258" mpi_size="696" stamp_init="1724765564.908440" stamp_final="1724765674.528141" username="apac4" allocationname="unknown" flags="0" pid="2254534" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09620e+02" utime="8.66035e+01" stime="1.46005e+01" mtime="7.15912e+01" gflop="0.00000e+00" gbyte="3.77357e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15912e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000fa14fa14ed" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09488e+02" utime="8.65676e+01" stime="1.45938e+01" mtime="7.15912e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15912e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2300e+09" > 7.6971e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2184e+09" > 3.7166e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4598e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9526e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.7922e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0225e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5002e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.2362e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0548e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 7.9768e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3116e+01 </func>
</region>
</regions>
<internal rank="258" log_i="1724765674.528141" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="259" mpi_size="696" stamp_init="1724765564.908540" stamp_final="1724765674.532721" username="apac4" allocationname="unknown" flags="0" pid="2254535" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09624e+02" utime="8.93979e+01" stime="1.38642e+01" mtime="7.23919e+01" gflop="0.00000e+00" gbyte="3.76999e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23919e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e414e41463" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09496e+02" utime="8.93661e+01" stime="1.38536e+01" mtime="7.23919e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23919e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1922e+09" > 5.9432e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1926e+09" > 2.4389e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5757e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9537e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4080e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1277e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5002e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.2473e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0566e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 7.9155e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3977e+01 </func>
</region>
</regions>
<internal rank="259" log_i="1724765674.532721" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="260" mpi_size="696" stamp_init="1724765564.908456" stamp_final="1724765674.540117" username="apac4" allocationname="unknown" flags="0" pid="2254536" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09632e+02" utime="8.76888e+01" stime="1.40780e+01" mtime="7.12935e+01" gflop="0.00000e+00" gbyte="3.77960e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.12935e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000079147814a3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09501e+02" utime="8.76566e+01" stime="1.40678e+01" mtime="7.12935e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.12935e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2107e+09" > 7.5972e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2122e+09" > 3.2840e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0966e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9546e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.2074e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.5439e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4996e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.3055e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0551e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 7.9766e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3179e+01 </func>
</region>
</regions>
<internal rank="260" log_i="1724765674.540117" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="261" mpi_size="696" stamp_init="1724765564.908522" stamp_final="1724765674.537508" username="apac4" allocationname="unknown" flags="0" pid="2254537" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09629e+02" utime="8.89709e+01" stime="1.37310e+01" mtime="7.26000e+01" gflop="0.00000e+00" gbyte="3.78212e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.26000e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09498e+02" utime="8.89375e+01" stime="1.37219e+01" mtime="7.26000e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.26000e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1948e+09" > 6.0342e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1938e+09" > 2.5852e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0209e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9551e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0868e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4994e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.2965e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0560e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 7.9019e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3716e+01 </func>
</region>
</regions>
<internal rank="261" log_i="1724765674.537508" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="262" mpi_size="696" stamp_init="1724765564.908472" stamp_final="1724765674.546605" username="apac4" allocationname="unknown" flags="0" pid="2254538" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09638e+02" utime="8.75613e+01" stime="1.43428e+01" mtime="7.21742e+01" gflop="0.00000e+00" gbyte="3.76865e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21742e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf489158b158c15c7558c158c153e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09506e+02" utime="8.75315e+01" stime="1.43305e+01" mtime="7.21742e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21742e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2170e+09" > 7.4692e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2227e+09" > 3.9071e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9352e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9551e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.8228e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.5641e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4993e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.3386e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0529e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 7.9147e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3223e+01 </func>
</region>
</regions>
<internal rank="262" log_i="1724765674.546605" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="263" mpi_size="696" stamp_init="1724765564.908511" stamp_final="1724765674.534185" username="apac4" allocationname="unknown" flags="0" pid="2254539" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="1.09626e+02" utime="8.97278e+01" stime="1.34799e+01" mtime="7.22670e+01" gflop="0.00000e+00" gbyte="3.77644e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22670e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4931595159615f6569615951527" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09496e+02" utime="8.96903e+01" stime="1.34736e+01" mtime="7.22670e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22670e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1935e+09" > 5.8390e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1905e+09" > 2.8913e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1044e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9545e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0046e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4987e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.3704e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0523e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8321e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4344e+01 </func>
</region>
</regions>
<internal rank="263" log_i="1724765674.534185" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="264" mpi_size="696" stamp_init="1724765564.881708" stamp_final="1724765674.530894" username="apac4" allocationname="unknown" flags="0" pid="1403872" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09649e+02" utime="7.49844e+01" stime="2.61739e+01" mtime="7.16939e+01" gflop="0.00000e+00" gbyte="3.85693e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16939e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44f1451145214de5552145214e9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09519e+02" utime="7.49515e+01" stime="2.61648e+01" mtime="7.16939e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16939e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1848e+09" > 8.0433e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1854e+09" > 6.3969e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9860e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9704e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2482e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0715e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4985e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.4047e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0559e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8313e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3470e+01 </func>
</region>
</regions>
<internal rank="264" log_i="1724765674.530894" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="265" mpi_size="696" stamp_init="1724765564.881699" stamp_final="1724765674.522868" username="apac4" allocationname="unknown" flags="0" pid="1403873" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09641e+02" utime="8.89081e+01" stime="1.35711e+01" mtime="7.28297e+01" gflop="0.00000e+00" gbyte="3.76823e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28297e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004515441500" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09504e+02" utime="8.88736e+01" stime="1.35618e+01" mtime="7.28297e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28297e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2242e+09" > 6.1520e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2237e+09" > 4.2338e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4457e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9717e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6689e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.0058e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4991e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.3337e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0605e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7730e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4353e+01 </func>
</region>
</regions>
<internal rank="265" log_i="1724765674.522868" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="266" mpi_size="696" stamp_init="1724765564.881683" stamp_final="1724765674.540997" username="apac4" allocationname="unknown" flags="0" pid="1403874" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09659e+02" utime="8.64736e+01" stime="1.38994e+01" mtime="7.22051e+01" gflop="0.00000e+00" gbyte="3.75584e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22051e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b715c056b715b61510" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09530e+02" utime="8.64434e+01" stime="1.38871e+01" mtime="7.22051e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22051e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2064e+09" > 7.5122e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1902e+09" > 5.8632e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9675e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9712e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.4387e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0774e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4986e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.3651e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0558e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8311e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3054e+01 </func>
</region>
</regions>
<internal rank="266" log_i="1724765674.540997" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="267" mpi_size="696" stamp_init="1724765564.881668" stamp_final="1724765674.522760" username="apac4" allocationname="unknown" flags="0" pid="1403875" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09641e+02" utime="8.95753e+01" stime="1.37166e+01" mtime="7.28615e+01" gflop="0.00000e+00" gbyte="3.77865e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28615e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008c15be558c158c1549" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09510e+02" utime="8.95468e+01" stime="1.37029e+01" mtime="7.28615e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28615e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2076e+09" > 5.6582e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2034e+09" > 3.8635e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2736e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9713e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4080e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.7991e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4987e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.3425e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0553e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8220e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4618e+01 </func>
</region>
</regions>
<internal rank="267" log_i="1724765674.522760" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="268" mpi_size="696" stamp_init="1724765564.881665" stamp_final="1724765674.536149" username="apac4" allocationname="unknown" flags="0" pid="1403876" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09654e+02" utime="8.72234e+01" stime="1.43936e+01" mtime="7.27963e+01" gflop="0.00000e+00" gbyte="3.76785e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27963e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09523e+02" utime="8.71926e+01" stime="1.43823e+01" mtime="7.27963e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27963e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1874e+09" > 6.9942e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1899e+09" > 5.5515e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.1403e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9706e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2826e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0825e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4978e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.4271e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0581e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8319e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3520e+01 </func>
</region>
</regions>
<internal rank="268" log_i="1724765674.536149" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="269" mpi_size="696" stamp_init="1724765564.881686" stamp_final="1724765674.536283" username="apac4" allocationname="unknown" flags="0" pid="1403877" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09655e+02" utime="8.90519e+01" stime="1.41817e+01" mtime="7.30812e+01" gflop="0.00000e+00" gbyte="3.77426e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.30812e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09524e+02" utime="8.90190e+01" stime="1.41717e+01" mtime="7.30812e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.30812e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2014e+09" > 5.6737e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2139e+09" > 3.5696e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6021e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9721e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.1048e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4978e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.4671e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0551e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7626e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4502e+01 </func>
</region>
</regions>
<internal rank="269" log_i="1724765674.536283" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="270" mpi_size="696" stamp_init="1724765564.881706" stamp_final="1724765674.538592" username="apac4" allocationname="unknown" flags="0" pid="1403878" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09657e+02" utime="8.64270e+01" stime="1.40567e+01" mtime="7.24096e+01" gflop="0.00000e+00" gbyte="3.78010e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24096e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09525e+02" utime="8.63962e+01" stime="1.40453e+01" mtime="7.24096e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24096e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2073e+09" > 6.8135e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2055e+09" > 4.8121e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0479e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9692e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0395e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0833e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4975e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.4880e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0569e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8326e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3278e+01 </func>
</region>
</regions>
<internal rank="270" log_i="1724765674.538592" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="271" mpi_size="696" stamp_init="1724765564.881687" stamp_final="1724765674.542115" username="apac4" allocationname="unknown" flags="0" pid="1403879" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09660e+02" utime="8.91784e+01" stime="1.40955e+01" mtime="7.30207e+01" gflop="0.00000e+00" gbyte="3.77453e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.30207e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4fc14fe14ff14a355ff14ff14ec" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09531e+02" utime="8.91485e+01" stime="1.40834e+01" mtime="7.30207e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.30207e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2072e+09" > 5.6392e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2053e+09" > 3.5153e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6340e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9716e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6689e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9307e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4971e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.5380e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0566e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8193e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4418e+01 </func>
</region>
</regions>
<internal rank="271" log_i="1724765674.542115" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="272" mpi_size="696" stamp_init="1724765564.881696" stamp_final="1724765674.531482" username="apac4" allocationname="unknown" flags="0" pid="1403880" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09650e+02" utime="8.77192e+01" stime="1.39231e+01" mtime="7.23395e+01" gflop="0.00000e+00" gbyte="3.76354e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23395e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000cb15cb1505" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09520e+02" utime="8.76891e+01" stime="1.39114e+01" mtime="7.23395e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23395e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2164e+09" > 8.0945e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2144e+09" > 4.5387e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0856e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9712e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9299e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0470e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4957e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.6122e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0610e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8331e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3023e+01 </func>
</region>
</regions>
<internal rank="272" log_i="1724765674.531482" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="273" mpi_size="696" stamp_init="1724765564.881709" stamp_final="1724765674.536674" username="apac4" allocationname="unknown" flags="0" pid="1403881" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09655e+02" utime="8.96879e+01" stime="1.36030e+01" mtime="7.27280e+01" gflop="0.00000e+00" gbyte="3.77514e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27280e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f615f61524" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09525e+02" utime="8.96615e+01" stime="1.35869e+01" mtime="7.27280e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27280e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2034e+09" > 5.6045e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1985e+09" > 3.8825e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9518e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9705e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2159e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.7773e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4959e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.6606e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0602e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8200e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4816e+01 </func>
</region>
</regions>
<internal rank="273" log_i="1724765674.536674" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="274" mpi_size="696" stamp_init="1724765564.881736" stamp_final="1724765674.538242" username="apac4" allocationname="unknown" flags="0" pid="1403882" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09657e+02" utime="8.46141e+01" stime="1.53171e+01" mtime="7.20802e+01" gflop="0.00000e+00" gbyte="3.76530e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20802e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000fd14fd1455" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09524e+02" utime="8.45805e+01" stime="1.53092e+01" mtime="7.20802e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20802e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2264e+09" > 8.9618e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2184e+09" > 7.4217e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8454e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9715e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.4671e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0844e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4957e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.6801e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0561e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8325e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2884e+01 </func>
</region>
</regions>
<internal rank="274" log_i="1724765674.538242" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="275" mpi_size="696" stamp_init="1724765564.881698" stamp_final="1724765674.534685" username="apac4" allocationname="unknown" flags="0" pid="1403883" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09653e+02" utime="8.93317e+01" stime="1.39273e+01" mtime="7.25739e+01" gflop="0.00000e+00" gbyte="3.77079e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25739e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e914e81475" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09522e+02" utime="8.93021e+01" stime="1.39148e+01" mtime="7.25739e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25739e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2048e+09" > 5.6125e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2082e+09" > 4.3012e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0640e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9716e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0821e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4948e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.7320e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0615e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8196e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4527e+01 </func>
</region>
</regions>
<internal rank="275" log_i="1724765674.534685" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="276" mpi_size="696" stamp_init="1724765564.881666" stamp_final="1724765674.535499" username="apac4" allocationname="unknown" flags="0" pid="1403884" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09654e+02" utime="8.63617e+01" stime="1.49380e+01" mtime="7.21378e+01" gflop="0.00000e+00" gbyte="3.76652e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21378e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008b148b14f1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09523e+02" utime="8.63305e+01" stime="1.49274e+01" mtime="7.21378e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21378e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1956e+09" > 7.2398e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2054e+09" > 6.0975e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9192e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9717e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9111e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0686e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4946e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.7617e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0559e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8333e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3055e+01 </func>
</region>
</regions>
<internal rank="276" log_i="1724765674.535499" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="277" mpi_size="696" stamp_init="1724765564.881670" stamp_final="1724765674.541881" username="apac4" allocationname="unknown" flags="0" pid="1403885" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09660e+02" utime="8.95014e+01" stime="1.37223e+01" mtime="7.30228e+01" gflop="0.00000e+00" gbyte="3.76442e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.30228e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf451155315541593555415541546" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09531e+02" utime="8.94712e+01" stime="1.37105e+01" mtime="7.30228e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.30228e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127200" bytes="1.2126e+09" > 5.6439e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2089e+09" > 4.5790e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3305e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9722e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0753e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4943e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.8300e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0582e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8187e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4698e+01 </func>
</region>
</regions>
<internal rank="277" log_i="1724765674.541881" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="278" mpi_size="696" stamp_init="1724765564.881618" stamp_final="1724765674.529036" username="apac4" allocationname="unknown" flags="0" pid="1403886" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09647e+02" utime="8.66932e+01" stime="1.44604e+01" mtime="7.18707e+01" gflop="0.00000e+00" gbyte="3.76755e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18707e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4201522152315c2552315231536" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09516e+02" utime="8.66574e+01" stime="1.44544e+01" mtime="7.18707e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18707e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2130e+09" > 7.3249e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2137e+09" > 6.5093e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9080e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9700e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4400e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0736e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4944e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.8037e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0550e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8328e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2803e+01 </func>
</region>
</regions>
<internal rank="278" log_i="1724765674.529036" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="279" mpi_size="696" stamp_init="1724765564.881925" stamp_final="1724765674.521260" username="apac4" allocationname="unknown" flags="0" pid="1403887" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09639e+02" utime="8.93711e+01" stime="1.38466e+01" mtime="7.29281e+01" gflop="0.00000e+00" gbyte="3.78208e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.29281e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ad14ad14ce" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09509e+02" utime="8.93465e+01" stime="1.38290e+01" mtime="7.29281e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.29281e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1945e+09" > 5.7448e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2072e+09" > 3.6074e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6386e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9715e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.2711e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4945e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.8076e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0577e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8188e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4309e+01 </func>
</region>
</regions>
<internal rank="279" log_i="1724765674.521260" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="280" mpi_size="696" stamp_init="1724765564.881728" stamp_final="1724765674.536390" username="apac4" allocationname="unknown" flags="0" pid="1403888" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09655e+02" utime="8.54262e+01" stime="1.49970e+01" mtime="7.22254e+01" gflop="0.00000e+00" gbyte="3.77544e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22254e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000491549154d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09523e+02" utime="8.53981e+01" stime="1.49828e+01" mtime="7.22254e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22254e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1837e+09" > 7.6955e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1860e+09" > 6.5218e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.3911e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9713e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4496e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0560e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4925e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.9468e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0588e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8337e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2625e+01 </func>
</region>
</regions>
<internal rank="280" log_i="1724765674.536390" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="281" mpi_size="696" stamp_init="1724765564.881724" stamp_final="1724765674.531126" username="apac4" allocationname="unknown" flags="0" pid="1403889" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09649e+02" utime="8.93810e+01" stime="1.38929e+01" mtime="7.31298e+01" gflop="0.00000e+00" gbyte="3.76545e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.31298e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09520e+02" utime="8.93458e+01" stime="1.38858e+01" mtime="7.31298e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.31298e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1904e+09" > 5.7390e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1959e+09" > 3.4494e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7499e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9707e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1189e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4934e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.8892e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0553e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8189e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4402e+01 </func>
</region>
</regions>
<internal rank="281" log_i="1724765674.531126" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="282" mpi_size="696" stamp_init="1724765564.881685" stamp_final="1724765674.532363" username="apac4" allocationname="unknown" flags="0" pid="1403890" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09651e+02" utime="8.71428e+01" stime="1.43370e+01" mtime="7.14886e+01" gflop="0.00000e+00" gbyte="3.76366e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.14886e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f514ae56f514f514d2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09521e+02" utime="8.71096e+01" stime="1.43273e+01" mtime="7.14886e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.14886e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2150e+09" > 7.2337e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2192e+09" > 5.7230e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3572e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9693e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.5790e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0598e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4929e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.9226e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0570e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8307e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2996e+01 </func>
</region>
</regions>
<internal rank="282" log_i="1724765674.532363" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="283" mpi_size="696" stamp_init="1724765564.881935" stamp_final="1724765674.536599" username="apac4" allocationname="unknown" flags="0" pid="1403891" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09655e+02" utime="8.90922e+01" stime="1.42009e+01" mtime="7.28649e+01" gflop="0.00000e+00" gbyte="3.77983e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28649e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4731474147514985575147514e0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09525e+02" utime="8.90569e+01" stime="1.41936e+01" mtime="7.28649e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28649e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1953e+09" > 5.8533e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1936e+09" > 5.1447e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4675e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9715e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0282e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4924e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.9910e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0610e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8195e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4385e+01 </func>
</region>
</regions>
<internal rank="283" log_i="1724765674.536599" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="284" mpi_size="696" stamp_init="1724765564.881692" stamp_final="1724765674.524124" username="apac4" allocationname="unknown" flags="0" pid="1403892" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09642e+02" utime="8.62378e+01" stime="1.46591e+01" mtime="7.18770e+01" gflop="0.00000e+00" gbyte="3.77934e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18770e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09511e+02" utime="8.62039e+01" stime="1.46506e+01" mtime="7.18770e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18770e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2131e+09" > 7.8801e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2086e+09" > 5.4566e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6544e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9713e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0943e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0527e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4922e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.0384e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0582e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8297e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3005e+01 </func>
</region>
</regions>
<internal rank="284" log_i="1724765674.524124" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="285" mpi_size="696" stamp_init="1724765564.881916" stamp_final="1724765674.534466" username="apac4" allocationname="unknown" flags="0" pid="1403893" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09653e+02" utime="8.96889e+01" stime="1.35654e+01" mtime="7.22907e+01" gflop="0.00000e+00" gbyte="3.78414e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22907e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c515c51505" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09523e+02" utime="8.96604e+01" stime="1.35521e+01" mtime="7.22907e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22907e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1902e+09" > 5.7267e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1887e+09" > 4.2886e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0971e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9716e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1921e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0796e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4927e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 8.9942e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0576e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8203e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4200e+01 </func>
</region>
</regions>
<internal rank="285" log_i="1724765674.534466" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="286" mpi_size="696" stamp_init="1724765564.881677" stamp_final="1724765674.529296" username="apac4" allocationname="unknown" flags="0" pid="1403894" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09648e+02" utime="8.81191e+01" stime="1.34496e+01" mtime="7.19782e+01" gflop="0.00000e+00" gbyte="3.77724e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19782e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b815ba15bb15a156bb15ba1549" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09516e+02" utime="8.80893e+01" stime="1.34371e+01" mtime="7.19782e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19782e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2188e+09" > 7.3744e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2163e+09" > 6.4249e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5133e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9672e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0531e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0431e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4919e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.0303e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0573e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8295e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3339e+01 </func>
</region>
</regions>
<internal rank="286" log_i="1724765674.529296" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="287" mpi_size="696" stamp_init="1724765564.881712" stamp_final="1724765674.536632" username="apac4" allocationname="unknown" flags="0" pid="1403895" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u07b</host>
<perf wtime="1.09655e+02" utime="8.89599e+01" stime="1.42947e+01" mtime="7.29867e+01" gflop="0.00000e+00" gbyte="3.76656e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.29867e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000079147914a6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09524e+02" utime="8.89289e+01" stime="1.42837e+01" mtime="7.29867e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.29867e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1908e+09" > 5.5417e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1876e+09" > 3.5480e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5047e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9719e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2159e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9381e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4922e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.0355e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0552e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8182e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4520e+01 </func>
</region>
</regions>
<internal rank="287" log_i="1724765674.536632" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="288" mpi_size="696" stamp_init="1724765564.846117" stamp_final="1724765674.527408" username="apac4" allocationname="unknown" flags="0" pid="2579697" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09681e+02" utime="5.58758e+01" stime="1.67086e+01" mtime="4.26016e+01" gflop="0.00000e+00" gbyte="3.87077e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.26016e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009c1496569c149b148d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09553e+02" utime="5.58442e+01" stime="1.66971e+01" mtime="4.26016e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.26016e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1919e+09" > 6.7386e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1870e+09" > 3.4811e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8439e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.7715e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5768e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.2691e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4913e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.1545e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0551e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8152e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4049e+01 </func>
</region>
</regions>
<internal rank="288" log_i="1724765674.527408" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="289" mpi_size="696" stamp_init="1724765564.846141" stamp_final="1724765674.524475" username="apac4" allocationname="unknown" flags="0" pid="2579698" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09678e+02" utime="7.41747e+01" stime="1.08205e+01" mtime="5.50284e+01" gflop="0.00000e+00" gbyte="3.77472e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.50284e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf42c15451557155e555715521500" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09546e+02" utime="7.41446e+01" stime="1.08073e+01" mtime="5.50284e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.50284e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2233e+09" > 6.2851e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2259e+09" > 2.9941e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2321e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.1844e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8372e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.1495e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4915e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.1509e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0552e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8019e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4641e+01 </func>
</region>
</regions>
<internal rank="289" log_i="1724765674.524475" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="290" mpi_size="696" stamp_init="1724765564.847295" stamp_final="1724765674.534613" username="apac4" allocationname="unknown" flags="0" pid="2579699" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09687e+02" utime="6.12388e+01" stime="9.70824e+00" mtime="4.12596e+01" gflop="0.00000e+00" gbyte="3.75473e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.12596e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09557e+02" utime="6.12059e+01" stime="9.69902e+00" mtime="4.12596e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.12596e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1955e+09" > 8.7966e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2061e+09" > 5.6197e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.1037e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.8181e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0770e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3131e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4917e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.1237e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0582e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8147e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3748e+01 </func>
</region>
</regions>
<internal rank="290" log_i="1724765674.534613" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="291" mpi_size="696" stamp_init="1724765564.846154" stamp_final="1724765674.529401" username="apac4" allocationname="unknown" flags="0" pid="2579700" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09683e+02" utime="6.46316e+01" stime="8.93333e+00" mtime="4.34188e+01" gflop="0.00000e+00" gbyte="3.78063e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.34188e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09552e+02" utime="6.46003e+01" stime="8.92226e+00" mtime="4.34188e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.34188e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127200" bytes="1.1994e+09" > 5.9742e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2052e+09" > 2.5558e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2509e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0096e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.9209e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4913e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.1638e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0591e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7998e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4789e+01 </func>
</region>
</regions>
<internal rank="291" log_i="1724765674.529401" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="292" mpi_size="696" stamp_init="1724765564.846143" stamp_final="1724765674.528914" username="apac4" allocationname="unknown" flags="0" pid="2579701" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09683e+02" utime="8.30907e+01" stime="1.85098e+01" mtime="7.17852e+01" gflop="0.00000e+00" gbyte="3.76507e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17852e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44b144d144e14a0554e144e1457" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09553e+02" utime="8.30608e+01" stime="1.84981e+01" mtime="7.17852e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17852e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1843e+09" > 7.6739e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1846e+09" > 4.0125e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8153e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.7540e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9988e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8144e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4915e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.1479e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0590e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8113e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3761e+01 </func>
</region>
</regions>
<internal rank="292" log_i="1724765674.528914" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="293" mpi_size="696" stamp_init="1724765564.846142" stamp_final="1724765674.525856" username="apac4" allocationname="unknown" flags="0" pid="2579702" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09680e+02" utime="6.42679e+01" stime="8.89043e+00" mtime="4.33316e+01" gflop="0.00000e+00" gbyte="3.76427e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.33316e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000bf14bf14db" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09551e+02" utime="6.42358e+01" stime="8.88015e+00" mtime="4.33316e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.33316e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2131e+09" > 6.4193e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2004e+09" > 2.6524e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5178e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.7539e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8588e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4908e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.1878e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0579e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8002e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4732e+01 </func>
</region>
</regions>
<internal rank="293" log_i="1724765674.525856" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="294" mpi_size="696" stamp_init="1724765564.846153" stamp_final="1724765674.537194" username="apac4" allocationname="unknown" flags="0" pid="2579703" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09691e+02" utime="7.60430e+01" stime="1.47767e+01" mtime="6.13647e+01" gflop="0.00000e+00" gbyte="3.78098e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.13647e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f1157156f115f11523" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09562e+02" utime="7.60149e+01" stime="1.47624e+01" mtime="6.13647e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.13647e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2125e+09" > 8.4177e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2027e+09" > 5.2708e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1058e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.6660e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2771e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7677e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4904e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.2194e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0585e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8085e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3282e+01 </func>
</region>
</regions>
<internal rank="294" log_i="1724765674.537194" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="295" mpi_size="696" stamp_init="1724765564.846174" stamp_final="1724765674.531578" username="apac4" allocationname="unknown" flags="0" pid="2579704" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09685e+02" utime="7.00607e+01" stime="1.00494e+01" mtime="4.98407e+01" gflop="0.00000e+00" gbyte="3.78387e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.98407e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4851486148814955688148714aa" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09554e+02" utime="7.00289e+01" stime="1.00386e+01" mtime="4.98407e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.98407e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2103e+09" > 6.1426e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2019e+09" > 2.9129e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2746e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.6653e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0276e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4903e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.2774e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0561e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7995e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4617e+01 </func>
</region>
</regions>
<internal rank="295" log_i="1724765674.531578" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="296" mpi_size="696" stamp_init="1724765564.846098" stamp_final="1724765674.537685" username="apac4" allocationname="unknown" flags="0" pid="2579705" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09692e+02" utime="8.06521e+01" stime="1.97880e+01" mtime="7.16928e+01" gflop="0.00000e+00" gbyte="3.77151e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16928e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d114d314d4148c55d414d3148b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09561e+02" utime="8.06202e+01" stime="1.97779e+01" mtime="7.16928e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16928e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2156e+09" > 9.4184e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2209e+09" > 6.1589e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8467e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.4681e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0287e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0247e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4893e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.2620e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0550e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8084e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3436e+01 </func>
</region>
</regions>
<internal rank="296" log_i="1724765674.537685" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="297" mpi_size="696" stamp_init="1724765564.846159" stamp_final="1724765674.533617" username="apac4" allocationname="unknown" flags="0" pid="2579706" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09687e+02" utime="7.99618e+01" stime="1.20536e+01" mtime="6.18172e+01" gflop="0.00000e+00" gbyte="3.77281e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.18172e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000081148114a6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09557e+02" utime="7.99324e+01" stime="1.20408e+01" mtime="6.18172e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.18172e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1988e+09" > 6.0343e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2004e+09" > 2.6189e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1179e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.8606e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1188e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4895e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.3631e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0579e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7506e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4812e+01 </func>
</region>
</regions>
<internal rank="297" log_i="1724765674.533617" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="298" mpi_size="696" stamp_init="1724765564.846144" stamp_final="1724765674.534590" username="apac4" allocationname="unknown" flags="0" pid="2579707" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09688e+02" utime="6.62141e+01" stime="1.08409e+01" mtime="4.73182e+01" gflop="0.00000e+00" gbyte="3.77266e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.73182e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000df14bb55df14df1457" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09558e+02" utime="6.61850e+01" stime="1.08279e+01" mtime="4.73182e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.73182e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2247e+09" > 6.6764e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2190e+09" > 4.3206e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8632e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0213e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.7009e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9482e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4893e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.3777e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0585e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8053e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4174e+01 </func>
</region>
</regions>
<internal rank="298" log_i="1724765674.534590" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="299" mpi_size="696" stamp_init="1724765564.846289" stamp_final="1724765674.532932" username="apac4" allocationname="unknown" flags="0" pid="2579708" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09687e+02" utime="6.48594e+01" stime="8.81764e+00" mtime="4.37146e+01" gflop="0.00000e+00" gbyte="3.78056e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.37146e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09554e+02" utime="6.48223e+01" stime="8.81240e+00" mtime="4.37146e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.37146e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2083e+09" > 6.1542e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2150e+09" > 2.7887e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4240e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0232e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3842e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9580e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4887e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.3999e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0568e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7996e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4760e+01 </func>
</region>
</regions>
<internal rank="299" log_i="1724765674.532932" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="300" mpi_size="696" stamp_init="1724765564.846105" stamp_final="1724765674.525174" username="apac4" allocationname="unknown" flags="0" pid="2579709" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09679e+02" utime="6.34567e+01" stime="1.13438e+01" mtime="4.62318e+01" gflop="0.00000e+00" gbyte="3.77598e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.62318e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09549e+02" utime="6.34265e+01" stime="1.13317e+01" mtime="4.62318e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.62318e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2006e+09" > 8.9331e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1928e+09" > 4.4572e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3303e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.4701e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0540e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0424e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4886e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.4194e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0596e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8058e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3787e+01 </func>
</region>
</regions>
<internal rank="300" log_i="1724765674.525174" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="301" mpi_size="696" stamp_init="1724765564.846594" stamp_final="1724765674.531072" username="apac4" allocationname="unknown" flags="0" pid="2579710" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09684e+02" utime="6.60050e+01" stime="9.21099e+00" mtime="4.50169e+01" gflop="0.00000e+00" gbyte="3.77972e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.50169e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09553e+02" utime="6.59740e+01" stime="9.19989e+00" mtime="4.50169e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.50169e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2076e+09" > 6.3685e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2186e+09" > 2.8685e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2126e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.1768e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.2786e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4888e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.4212e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0583e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7990e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4718e+01 </func>
</region>
</regions>
<internal rank="301" log_i="1724765674.531072" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="302" mpi_size="696" stamp_init="1724765564.846104" stamp_final="1724765674.524841" username="apac4" allocationname="unknown" flags="0" pid="2579711" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09679e+02" utime="8.16921e+01" stime="1.91112e+01" mtime="7.10556e+01" gflop="0.00000e+00" gbyte="3.76915e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.10556e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ef14ef14ae" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09550e+02" utime="8.16639e+01" stime="1.90975e+01" mtime="7.10556e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.10556e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2133e+09" > 7.8183e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2141e+09" > 4.3830e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.6188e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.4680e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0065e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0352e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4875e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.5390e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0595e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8033e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3425e+01 </func>
</region>
</regions>
<internal rank="302" log_i="1724765674.524841" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="303" mpi_size="696" stamp_init="1724765564.846169" stamp_final="1724765674.530872" username="apac4" allocationname="unknown" flags="0" pid="2579712" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09685e+02" utime="6.38924e+01" stime="8.62050e+00" mtime="4.19987e+01" gflop="0.00000e+00" gbyte="3.77945e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.19987e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007614615576147614be" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09556e+02" utime="6.38626e+01" stime="8.60777e+00" mtime="4.19987e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.19987e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2011e+09" > 6.0319e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2017e+09" > 2.5892e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7722e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.4603e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0676e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4875e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.5595e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0586e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7989e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4484e+01 </func>
</region>
</regions>
<internal rank="303" log_i="1724765674.530872" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="304" mpi_size="696" stamp_init="1724765564.846116" stamp_final="1724765674.534746" username="apac4" allocationname="unknown" flags="0" pid="2579713" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09689e+02" utime="7.17222e+01" stime="1.45121e+01" mtime="5.69875e+01" gflop="0.00000e+00" gbyte="3.76911e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.69875e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000060144e5560145f14e0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09559e+02" utime="7.16975e+01" stime="1.44949e+01" mtime="5.69875e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.69875e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127200" bytes="1.1880e+09" > 7.8135e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1833e+09" > 3.7957e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1721e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.7670e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5850e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5718e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4869e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.6008e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0558e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8038e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3727e+01 </func>
</region>
</regions>
<internal rank="304" log_i="1724765674.534746" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="305" mpi_size="696" stamp_init="1724765564.846167" stamp_final="1724765674.533605" username="apac4" allocationname="unknown" flags="0" pid="2579714" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09687e+02" utime="6.37098e+01" stime="8.71466e+00" mtime="4.18411e+01" gflop="0.00000e+00" gbyte="3.78208e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.18411e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005f1456555f145f148d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09557e+02" utime="6.36824e+01" stime="8.69945e+00" mtime="4.18411e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.18411e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2065e+09" > 6.1292e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2017e+09" > 3.3545e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9354e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.0040e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.2684e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4878e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.5313e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0550e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7997e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4603e+01 </func>
</region>
</regions>
<internal rank="305" log_i="1724765674.533605" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="306" mpi_size="696" stamp_init="1724765564.846159" stamp_final="1724765674.534949" username="apac4" allocationname="unknown" flags="0" pid="2579715" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09689e+02" utime="8.13031e+01" stime="1.98670e+01" mtime="7.14836e+01" gflop="0.00000e+00" gbyte="3.76862e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.14836e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000057146a5557145714b0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09560e+02" utime="8.12717e+01" stime="1.98558e+01" mtime="7.14836e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.14836e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2106e+09" > 7.5910e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2080e+09" > 3.6277e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7982e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.7868e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0784e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8794e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4871e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.5901e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0554e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8036e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3667e+01 </func>
</region>
</regions>
<internal rank="306" log_i="1724765674.534949" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="307" mpi_size="696" stamp_init="1724765564.846181" stamp_final="1724765674.524617" username="apac4" allocationname="unknown" flags="0" pid="2579716" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09678e+02" utime="6.32254e+01" stime="8.82806e+00" mtime="4.16504e+01" gflop="0.00000e+00" gbyte="3.77514e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.16504e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f814fa14fb143e56fb14fa14bc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09547e+02" utime="6.31887e+01" stime="8.82260e+00" mtime="4.16504e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.16504e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1969e+09" > 6.2075e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2068e+09" > 2.6834e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9244e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.7792e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.0987e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4873e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.5880e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0559e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7977e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4647e+01 </func>
</region>
</regions>
<internal rank="307" log_i="1724765674.524617" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="308" mpi_size="696" stamp_init="1724765564.846138" stamp_final="1724765674.525694" username="apac4" allocationname="unknown" flags="0" pid="2579717" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09680e+02" utime="7.14544e+01" stime="1.12755e+01" mtime="5.30997e+01" gflop="0.00000e+00" gbyte="3.76827e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.30997e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001014fc1468" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09549e+02" utime="7.14210e+01" stime="1.12669e+01" mtime="5.30997e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.30997e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127200" bytes="1.2129e+09" > 7.8181e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2196e+09" > 3.6867e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8443e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.9776e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0880e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6328e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4867e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.6352e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0576e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8032e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3911e+01 </func>
</region>
</regions>
<internal rank="308" log_i="1724765674.525694" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="309" mpi_size="696" stamp_init="1724765564.846136" stamp_final="1724765674.532334" username="apac4" allocationname="unknown" flags="0" pid="2579718" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09686e+02" utime="8.89918e+01" stime="1.40916e+01" mtime="7.24209e+01" gflop="0.00000e+00" gbyte="3.78319e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24209e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001d148d551d141d14fe" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09556e+02" utime="8.89651e+01" stime="1.40752e+01" mtime="7.24209e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24209e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.0252e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1852e+09" > 6.0182e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1933e+09" > 3.0107e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7591e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9752e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.2660e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4857e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.7574e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0568e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7977e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4625e+01 </func>
</region>
</regions>
<internal rank="309" log_i="1724765674.532334" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="310" mpi_size="696" stamp_init="1724765564.846132" stamp_final="1724765674.531534" username="apac4" allocationname="unknown" flags="0" pid="2579719" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09685e+02" utime="8.04570e+01" stime="1.54380e+01" mtime="6.61210e+01" gflop="0.00000e+00" gbyte="3.77972e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.61210e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4eb14ec14ee14eb56ee14ed14fa" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09554e+02" utime="8.04272e+01" stime="1.54254e+01" mtime="6.61210e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.61210e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2111e+09" > 7.9028e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2101e+09" > 4.0465e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8165e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.9795e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4221e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6824e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4861e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.6632e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0572e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8033e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3796e+01 </func>
</region>
</regions>
<internal rank="310" log_i="1724765674.531534" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="311" mpi_size="696" stamp_init="1724765564.846174" stamp_final="1724765674.529570" username="apac4" allocationname="unknown" flags="0" pid="2579720" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="1.09683e+02" utime="7.27784e+01" stime="1.04579e+01" mtime="5.25088e+01" gflop="0.00000e+00" gbyte="3.76850e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.25088e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09552e+02" utime="7.27419e+01" stime="1.04522e+01" mtime="5.25088e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.25088e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1828e+09" > 5.9792e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1895e+09" > 2.7265e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9756e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.9781e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6900e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4849e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.8212e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0559e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7982e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4465e+01 </func>
</region>
</regions>
<internal rank="311" log_i="1724765674.529570" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="312" mpi_size="696" stamp_init="1724765564.745793" stamp_final="1724765674.526886" username="apac4" allocationname="unknown" flags="0" pid="51213" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09781e+02" utime="7.44487e+01" stime="2.66828e+01" mtime="7.14343e+01" gflop="0.00000e+00" gbyte="3.85166e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.14343e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009d14e1559d149c145b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09655e+02" utime="7.44142e+01" stime="2.66749e+01" mtime="7.14343e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.14343e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1941e+09" > 1.1274e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1963e+09" > 4.7748e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.2785e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9560e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9800e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1761e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4842e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.8392e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0573e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8046e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3762e+01 </func>
</region>
</regions>
<internal rank="312" log_i="1724765674.526886" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="313" mpi_size="696" stamp_init="1724765564.743559" stamp_final="1724765674.530500" username="apac4" allocationname="unknown" flags="0" pid="51214" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09787e+02" utime="8.96946e+01" stime="1.37047e+01" mtime="7.28661e+01" gflop="0.00000e+00" gbyte="3.75919e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28661e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000039143814e4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09654e+02" utime="8.96637e+01" stime="1.36920e+01" mtime="7.28661e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28661e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2256e+09" > 7.1572e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2256e+09" > 3.0836e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4073e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9546e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8849e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3950e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4859e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.6950e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0621e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8058e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4510e+01 </func>
</region>
</regions>
<internal rank="313" log_i="1724765674.530500" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="314" mpi_size="696" stamp_init="1724765564.743534" stamp_final="1724765674.537350" username="apac4" allocationname="unknown" flags="0" pid="51215" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09794e+02" utime="8.79288e+01" stime="1.40852e+01" mtime="7.23154e+01" gflop="0.00000e+00" gbyte="3.76877e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23154e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000901411559014901490" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09666e+02" utime="8.78988e+01" stime="1.40733e+01" mtime="7.23154e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23154e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1991e+09" > 8.3401e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1967e+09" > 4.1373e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4648e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9555e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4448e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.6419e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4858e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.7429e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0612e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8130e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3762e+01 </func>
</region>
</regions>
<internal rank="314" log_i="1724765674.537350" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="315" mpi_size="696" stamp_init="1724765564.743537" stamp_final="1724765674.535683" username="apac4" allocationname="unknown" flags="0" pid="51216" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09792e+02" utime="8.93422e+01" stime="1.37373e+01" mtime="7.27419e+01" gflop="0.00000e+00" gbyte="3.77136e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27419e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09665e+02" utime="8.93100e+01" stime="1.37274e+01" mtime="7.27419e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27419e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1982e+09" > 6.7889e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1947e+09" > 2.7408e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1326e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9559e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.7179e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4855e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.7397e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0607e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8066e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4685e+01 </func>
</region>
</regions>
<internal rank="315" log_i="1724765674.535683" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="316" mpi_size="696" stamp_init="1724765564.744164" stamp_final="1724765674.538173" username="apac4" allocationname="unknown" flags="0" pid="51217" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09794e+02" utime="8.67477e+01" stime="1.46844e+01" mtime="7.13712e+01" gflop="0.00000e+00" gbyte="3.77316e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.13712e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000094149314d5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09667e+02" utime="8.67159e+01" stime="1.46741e+01" mtime="7.13712e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.13712e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1875e+09" > 9.2811e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1907e+09" > 4.6413e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.4270e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9559e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2898e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.2553e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4848e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.7999e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0548e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8074e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3755e+01 </func>
</region>
</regions>
<internal rank="316" log_i="1724765674.538173" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="317" mpi_size="696" stamp_init="1724765564.743530" stamp_final="1724765674.540403" username="apac4" allocationname="unknown" flags="0" pid="51218" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09797e+02" utime="8.94933e+01" stime="1.33301e+01" mtime="7.23995e+01" gflop="0.00000e+00" gbyte="3.75317e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23995e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d214d156d214d214b1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09669e+02" utime="8.94597e+01" stime="1.33213e+01" mtime="7.23995e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23995e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2002e+09" > 5.9801e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2103e+09" > 2.7205e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7952e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9562e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4917e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4852e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.8046e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0594e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8083e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4760e+01 </func>
</region>
</regions>
<internal rank="317" log_i="1724765674.540403" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="318" mpi_size="696" stamp_init="1724765564.743533" stamp_final="1724765674.526445" username="apac4" allocationname="unknown" flags="0" pid="51219" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09783e+02" utime="8.54287e+01" stime="1.52859e+01" mtime="7.17656e+01" gflop="0.00000e+00" gbyte="3.76896e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17656e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009514a5559514941492" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09652e+02" utime="8.53988e+01" stime="1.52739e+01" mtime="7.17656e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17656e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1931e+09" > 9.5071e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2089e+09" > 5.9105e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1713e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9559e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2663e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8479e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4808e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0189e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0550e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8079e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3368e+01 </func>
</region>
</regions>
<internal rank="318" log_i="1724765674.526445" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="319" mpi_size="696" stamp_init="1724765564.743593" stamp_final="1724765674.529798" username="apac4" allocationname="unknown" flags="0" pid="51220" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09786e+02" utime="8.93269e+01" stime="1.40051e+01" mtime="7.25400e+01" gflop="0.00000e+00" gbyte="3.78159e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25400e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b4153655b415b4154a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09657e+02" utime="8.92963e+01" stime="1.39933e+01" mtime="7.25400e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25400e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1922e+09" > 6.6301e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2079e+09" > 2.9691e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8915e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9572e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0041e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8022e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4842e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 9.8766e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0626e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8083e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4726e+01 </func>
</region>
</regions>
<internal rank="319" log_i="1724765674.529798" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="320" mpi_size="696" stamp_init="1724765564.743776" stamp_final="1724765674.537994" username="apac4" allocationname="unknown" flags="0" pid="51221" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09794e+02" utime="8.64556e+01" stime="1.45839e+01" mtime="7.20512e+01" gflop="0.00000e+00" gbyte="3.78044e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20512e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004914d7554914481498" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09667e+02" utime="8.64254e+01" stime="1.45717e+01" mtime="7.20512e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20512e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2306e+09" > 8.5229e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2170e+09" > 3.6807e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4151e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9546e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4360e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8881e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4826e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0061e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0577e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8088e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3544e+01 </func>
</region>
</regions>
<internal rank="320" log_i="1724765674.537994" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="321" mpi_size="696" stamp_init="1724765564.743554" stamp_final="1724765674.536786" username="apac4" allocationname="unknown" flags="0" pid="51222" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09793e+02" utime="8.92466e+01" stime="1.38147e+01" mtime="7.31234e+01" gflop="0.00000e+00" gbyte="3.78193e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.31234e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf42d142e143014145530142f14c8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09664e+02" utime="8.92120e+01" stime="1.38071e+01" mtime="7.31234e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.31234e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1954e+09" > 6.8027e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1932e+09" > 2.8926e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3909e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9555e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7895e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.0927e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4823e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0099e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0581e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8063e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4809e+01 </func>
</region>
</regions>
<internal rank="321" log_i="1724765674.536786" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="322" mpi_size="696" stamp_init="1724765564.744849" stamp_final="1724765674.532259" username="apac4" allocationname="unknown" flags="0" pid="51223" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09787e+02" utime="8.69813e+01" stime="1.45894e+01" mtime="7.20428e+01" gflop="0.00000e+00" gbyte="3.78117e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20428e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09662e+02" utime="8.69487e+01" stime="1.45798e+01" mtime="7.20428e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20428e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2081e+09" > 8.0280e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2172e+09" > 3.4322e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0549e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9562e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0638e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.0728e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4817e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0150e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0550e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8044e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3930e+01 </func>
</region>
</regions>
<internal rank="322" log_i="1724765674.532259" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="323" mpi_size="696" stamp_init="1724765564.745121" stamp_final="1724765674.538055" username="apac4" allocationname="unknown" flags="0" pid="51224" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09793e+02" utime="8.97262e+01" stime="1.36458e+01" mtime="7.27285e+01" gflop="0.00000e+00" gbyte="3.78029e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27285e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000dc14dc14c7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09667e+02" utime="8.96952e+01" stime="1.36349e+01" mtime="7.27285e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27285e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2160e+09" > 6.6070e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2120e+09" > 2.8903e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0006e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9552e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0518e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.0790e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4819e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0143e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0543e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8027e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4828e+01 </func>
</region>
</regions>
<internal rank="323" log_i="1724765674.538055" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="324" mpi_size="696" stamp_init="1724765564.743519" stamp_final="1724765674.545701" username="apac4" allocationname="unknown" flags="0" pid="51225" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09802e+02" utime="8.59158e+01" stime="1.49789e+01" mtime="7.15391e+01" gflop="0.00000e+00" gbyte="3.77522e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15391e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004f144f146b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09675e+02" utime="8.58804e+01" stime="1.49715e+01" mtime="7.15391e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15391e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1938e+09" > 8.0108e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1886e+09" > 4.5986e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8973e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9549e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3494e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3490e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4809e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0179e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0604e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8044e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3590e+01 </func>
</region>
</regions>
<internal rank="324" log_i="1724765674.545701" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="325" mpi_size="696" stamp_init="1724765564.743778" stamp_final="1724765674.540930" username="apac4" allocationname="unknown" flags="0" pid="51226" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09797e+02" utime="8.93039e+01" stime="1.37513e+01" mtime="7.23056e+01" gflop="0.00000e+00" gbyte="3.78216e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23056e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ca14c914e0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09669e+02" utime="8.92705e+01" stime="1.37423e+01" mtime="7.23056e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23056e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2186e+09" > 6.5500e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2163e+09" > 2.6216e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.6410e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9564e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4331e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4808e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0213e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0626e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8024e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4762e+01 </func>
</region>
</regions>
<internal rank="325" log_i="1724765674.540930" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="326" mpi_size="696" stamp_init="1724765564.743520" stamp_final="1724765674.533513" username="apac4" allocationname="unknown" flags="0" pid="51227" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09790e+02" utime="8.68164e+01" stime="1.41408e+01" mtime="7.11859e+01" gflop="0.00000e+00" gbyte="3.77266e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.11859e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf459145b145c1420555c145c1470" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09663e+02" utime="8.67807e+01" stime="1.41345e+01" mtime="7.11859e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.11859e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2059e+09" > 9.0325e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1967e+09" > 4.2850e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.3528e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9559e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7095e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.2979e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4802e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0289e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0622e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8041e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3671e+01 </func>
</region>
</regions>
<internal rank="326" log_i="1724765674.533513" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="327" mpi_size="696" stamp_init="1724765564.743561" stamp_final="1724765674.533624" username="apac4" allocationname="unknown" flags="0" pid="51228" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09790e+02" utime="8.92318e+01" stime="1.41237e+01" mtime="7.30201e+01" gflop="0.00000e+00" gbyte="3.75057e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.30201e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48014821483144f56831482148a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09662e+02" utime="8.91993e+01" stime="1.41140e+01" mtime="7.30201e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.30201e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2033e+09" > 6.8042e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1986e+09" > 2.7296e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4412e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9539e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.7193e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.2419e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4805e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0285e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0599e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8017e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4675e+01 </func>
</region>
</regions>
<internal rank="327" log_i="1724765674.533624" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="328" mpi_size="696" stamp_init="1724765564.743618" stamp_final="1724765674.527351" username="apac4" allocationname="unknown" flags="0" pid="51229" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09784e+02" utime="8.70748e+01" stime="1.46894e+01" mtime="7.19914e+01" gflop="0.00000e+00" gbyte="3.78101e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19914e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ce14d014d114eb55d114d01471" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09656e+02" utime="8.70473e+01" stime="1.46740e+01" mtime="7.19914e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19914e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.0599e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1884e+09" > 8.5526e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1952e+09" > 4.1021e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4387e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9548e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2302e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9341e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4804e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0294e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0606e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8047e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3452e+01 </func>
</region>
</regions>
<internal rank="328" log_i="1724765674.527351" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="329" mpi_size="696" stamp_init="1724765564.745086" stamp_final="1724765674.531072" username="apac4" allocationname="unknown" flags="0" pid="51230" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09786e+02" utime="8.98512e+01" stime="1.31453e+01" mtime="7.22763e+01" gflop="0.00000e+00" gbyte="3.76938e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22763e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09660e+02" utime="8.98211e+01" stime="1.31334e+01" mtime="7.22763e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22763e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2112e+09" > 6.6998e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2127e+09" > 3.0649e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.5253e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9562e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.2073e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4802e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0315e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0545e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8009e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4832e+01 </func>
</region>
</regions>
<internal rank="329" log_i="1724765674.531072" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="330" mpi_size="696" stamp_init="1724765564.743548" stamp_final="1724765674.533728" username="apac4" allocationname="unknown" flags="0" pid="51231" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09790e+02" utime="8.64686e+01" stime="1.48502e+01" mtime="7.15087e+01" gflop="0.00000e+00" gbyte="3.74779e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15087e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e014e214e3145555e314e2146d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09663e+02" utime="8.64367e+01" stime="1.48401e+01" mtime="7.15087e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15087e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1983e+09" > 8.6123e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1972e+09" > 4.3026e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8735e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9534e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.0320e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.7918e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4793e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0356e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0569e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8074e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3538e+01 </func>
</region>
</regions>
<internal rank="330" log_i="1724765674.533728" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="331" mpi_size="696" stamp_init="1724765564.743552" stamp_final="1724765674.533612" username="apac4" allocationname="unknown" flags="0" pid="51232" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09790e+02" utime="8.92884e+01" stime="1.37243e+01" mtime="7.28567e+01" gflop="0.00000e+00" gbyte="3.76743e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28567e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002714df562714271477" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09662e+02" utime="8.92575e+01" stime="1.37135e+01" mtime="7.28567e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28567e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2100e+09" > 6.5730e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2101e+09" > 2.9330e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4112e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9561e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.9220e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4796e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0376e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0606e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8002e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4539e+01 </func>
</region>
</regions>
<internal rank="331" log_i="1724765674.533612" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="332" mpi_size="696" stamp_init="1724765564.745737" stamp_final="1724765674.538282" username="apac4" allocationname="unknown" flags="0" pid="51233" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09793e+02" utime="8.64543e+01" stime="1.47593e+01" mtime="7.24814e+01" gflop="0.00000e+00" gbyte="3.78418e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24814e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09666e+02" utime="8.64201e+01" stime="1.47510e+01" mtime="7.24814e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24814e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2193e+09" > 9.3912e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2163e+09" > 4.4065e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6762e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9558e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4329e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.2210e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4793e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0408e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0622e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8074e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3608e+01 </func>
</region>
</regions>
<internal rank="332" log_i="1724765674.538282" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="333" mpi_size="696" stamp_init="1724765564.743551" stamp_final="1724765674.539218" username="apac4" allocationname="unknown" flags="0" pid="51234" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09796e+02" utime="8.90908e+01" stime="1.41572e+01" mtime="7.21936e+01" gflop="0.00000e+00" gbyte="3.78365e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21936e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09666e+02" utime="8.90553e+01" stime="1.41503e+01" mtime="7.21936e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21936e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1943e+09" > 6.7500e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1923e+09" > 2.6464e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8448e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9556e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.8344e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4785e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0452e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0568e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8003e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4435e+01 </func>
</region>
</regions>
<internal rank="333" log_i="1724765674.539218" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="334" mpi_size="696" stamp_init="1724765564.743533" stamp_final="1724765674.539756" username="apac4" allocationname="unknown" flags="0" pid="51235" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09796e+02" utime="8.72931e+01" stime="1.40872e+01" mtime="7.18247e+01" gflop="0.00000e+00" gbyte="3.76511e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18247e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009514941462" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09670e+02" utime="8.72576e+01" stime="1.40811e+01" mtime="7.18247e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18247e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1986e+09" > 8.7664e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2005e+09" > 3.9452e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8504e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9561e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.8161e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1594e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4787e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0472e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0575e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8042e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3840e+01 </func>
</region>
</regions>
<internal rank="334" log_i="1724765674.539756" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="335" mpi_size="696" stamp_init="1724765564.743769" stamp_final="1724765674.536755" username="apac4" allocationname="unknown" flags="0" pid="51236" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u16b</host>
<perf wtime="1.09793e+02" utime="8.98429e+01" stime="1.36025e+01" mtime="7.19618e+01" gflop="0.00000e+00" gbyte="3.78124e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19618e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007a1420557a14791487" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09666e+02" utime="8.98116e+01" stime="1.35914e+01" mtime="7.19618e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19618e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1893e+09" > 6.8929e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1823e+09" > 2.7493e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.6834e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9554e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.1149e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4779e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0546e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0581e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7999e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4348e+01 </func>
</region>
</regions>
<internal rank="335" log_i="1724765674.536755" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="336" mpi_size="696" stamp_init="1724765564.680967" stamp_final="1724765674.532390" username="apac4" allocationname="unknown" flags="0" pid="3304939" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09851e+02" utime="7.57942e+01" stime="2.56425e+01" mtime="7.16656e+01" gflop="0.00000e+00" gbyte="3.86913e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16656e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09727e+02" utime="7.57654e+01" stime="2.56292e+01" mtime="7.16656e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16656e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2001e+09" > 7.0963e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1950e+09" > 3.9533e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7037e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9679e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9578e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0850e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4778e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0572e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0532e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8006e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3877e+01 </func>
</region>
</regions>
<internal rank="336" log_i="1724765674.532390" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="337" mpi_size="696" stamp_init="1724765564.680886" stamp_final="1724765674.523678" username="apac4" allocationname="unknown" flags="0" pid="3304940" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09843e+02" utime="9.02522e+01" stime="1.28596e+01" mtime="7.24021e+01" gflop="0.00000e+00" gbyte="3.78147e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24021e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf47a147b147c1435557c147c14ef" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09714e+02" utime="9.02153e+01" stime="1.28535e+01" mtime="7.24021e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24021e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2209e+09" > 6.1143e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2179e+09" > 2.5111e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8728e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9680e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3167e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4787e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0468e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0549e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7399e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4556e+01 </func>
</region>
</regions>
<internal rank="337" log_i="1724765674.523678" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="338" mpi_size="696" stamp_init="1724765564.680904" stamp_final="1724765674.529275" username="apac4" allocationname="unknown" flags="0" pid="3304941" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09848e+02" utime="8.66992e+01" stime="1.45043e+01" mtime="7.19473e+01" gflop="0.00000e+00" gbyte="3.74813e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19473e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4501452145314215553145214db" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09724e+02" utime="8.66664e+01" stime="1.44955e+01" mtime="7.19473e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19473e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2012e+09" > 7.8470e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1970e+09" > 4.0572e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8985e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9682e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.1199e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.7289e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4769e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0620e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0557e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7997e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3883e+01 </func>
</region>
</regions>
<internal rank="338" log_i="1724765674.529275" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="339" mpi_size="696" stamp_init="1724765564.680920" stamp_final="1724765674.532030" username="apac4" allocationname="unknown" flags="0" pid="3304942" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09851e+02" utime="8.95479e+01" stime="1.36248e+01" mtime="7.28589e+01" gflop="0.00000e+00" gbyte="3.78334e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28589e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004e144d1493" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09727e+02" utime="8.95149e+01" stime="1.36155e+01" mtime="7.28589e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28589e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.5831e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1864e+09" > 5.6334e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1928e+09" > 2.7738e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9786e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9670e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.7139e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4783e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0510e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0524e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7892e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4965e+01 </func>
</region>
</regions>
<internal rank="339" log_i="1724765674.532030" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="340" mpi_size="696" stamp_init="1724765564.680949" stamp_final="1724765674.529297" username="apac4" allocationname="unknown" flags="0" pid="3304943" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09848e+02" utime="8.72304e+01" stime="1.43815e+01" mtime="7.18827e+01" gflop="0.00000e+00" gbyte="3.76785e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18827e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000e915c256e915e9153f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09727e+02" utime="8.72017e+01" stime="1.43687e+01" mtime="7.18827e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18827e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1981e+09" > 7.3487e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1894e+09" > 3.9080e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9004e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9684e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.0405e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1311e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4778e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0569e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0546e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8009e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3871e+01 </func>
</region>
</regions>
<internal rank="340" log_i="1724765674.529297" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="341" mpi_size="696" stamp_init="1724765564.681862" stamp_final="1724765674.525465" username="apac4" allocationname="unknown" flags="0" pid="3304944" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09844e+02" utime="8.98979e+01" stime="1.30359e+01" mtime="7.26678e+01" gflop="0.00000e+00" gbyte="3.76762e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.26678e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001215111540" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09723e+02" utime="8.98620e+01" stime="1.30302e+01" mtime="7.26678e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.26678e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2068e+09" > 5.6057e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1986e+09" > 2.6255e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0561e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9684e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5259e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1621e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4774e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0610e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0535e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7862e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4688e+01 </func>
</region>
</regions>
<internal rank="341" log_i="1724765674.525465" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="342" mpi_size="696" stamp_init="1724765564.680911" stamp_final="1724765674.541435" username="apac4" allocationname="unknown" flags="0" pid="3304945" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09861e+02" utime="8.55187e+01" stime="1.49143e+01" mtime="7.15929e+01" gflop="0.00000e+00" gbyte="3.76602e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15929e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001e1534561e151e1513" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09737e+02" utime="8.54867e+01" stime="1.49050e+01" mtime="7.15929e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15929e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2017e+09" > 8.9008e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1986e+09" > 5.3889e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7052e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9665e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0483e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1709e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4760e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0698e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0521e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8008e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3626e+01 </func>
</region>
</regions>
<internal rank="342" log_i="1724765674.541435" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="343" mpi_size="696" stamp_init="1724765564.681333" stamp_final="1724765674.534605" username="apac4" allocationname="unknown" flags="0" pid="3304946" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09853e+02" utime="8.98426e+01" stime="1.32113e+01" mtime="7.23077e+01" gflop="0.00000e+00" gbyte="3.76854e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23077e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004914491466" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09731e+02" utime="8.98106e+01" stime="1.32019e+01" mtime="7.23077e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23077e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2003e+09" > 5.6280e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1957e+09" > 2.7139e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.5028e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9688e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1261e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4768e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0670e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0525e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7859e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4874e+01 </func>
</region>
</regions>
<internal rank="343" log_i="1724765674.534605" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="344" mpi_size="696" stamp_init="1724765564.680959" stamp_final="1724765674.525656" username="apac4" allocationname="unknown" flags="0" pid="3304947" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09845e+02" utime="8.70446e+01" stime="1.46444e+01" mtime="7.24086e+01" gflop="0.00000e+00" gbyte="3.76747e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24086e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ed14ed1483" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09720e+02" utime="8.70103e+01" stime="1.46373e+01" mtime="7.24086e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24086e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.0490e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2192e+09" > 7.0922e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2281e+09" > 3.6448e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0875e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9693e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6813e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.3897e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4758e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0734e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0510e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.8011e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3227e+01 </func>
</region>
</regions>
<internal rank="344" log_i="1724765674.525656" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="345" mpi_size="696" stamp_init="1724765564.680870" stamp_final="1724765674.534883" username="apac4" allocationname="unknown" flags="0" pid="3304948" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09854e+02" utime="8.94984e+01" stime="1.35915e+01" mtime="7.29538e+01" gflop="0.00000e+00" gbyte="3.77338e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.29538e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003a153a1541" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09733e+02" utime="8.94660e+01" stime="1.35826e+01" mtime="7.29538e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.29538e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1938e+09" > 5.5818e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1947e+09" > 2.9209e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0167e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9670e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.1617e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4760e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0755e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0539e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7861e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.5025e+01 </func>
</region>
</regions>
<internal rank="345" log_i="1724765674.534883" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="346" mpi_size="696" stamp_init="1724765564.681428" stamp_final="1724765674.533536" username="apac4" allocationname="unknown" flags="0" pid="3304949" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09852e+02" utime="8.79050e+01" stime="1.40087e+01" mtime="7.15715e+01" gflop="0.00000e+00" gbyte="3.78036e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15715e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4fa14fc14fd14b555fd14fd1487" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09727e+02" utime="8.78786e+01" stime="1.39935e+01" mtime="7.15715e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15715e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2082e+09" > 6.7747e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2025e+09" > 4.2210e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.2138e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9698e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.5327e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4617e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4755e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0808e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0531e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7998e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4286e+01 </func>
</region>
</regions>
<internal rank="346" log_i="1724765674.533536" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="347" mpi_size="696" stamp_init="1724765564.680867" stamp_final="1724765674.528205" username="apac4" allocationname="unknown" flags="0" pid="3304950" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09847e+02" utime="8.97228e+01" stime="1.34174e+01" mtime="7.26724e+01" gflop="0.00000e+00" gbyte="3.77869e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.26724e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002314ff1485" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09723e+02" utime="8.96889e+01" stime="1.34093e+01" mtime="7.26724e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.26724e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2146e+09" > 5.4999e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2099e+09" > 2.3010e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7020e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9687e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.6542e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4752e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0829e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0519e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7830e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.5054e+01 </func>
</region>
</regions>
<internal rank="347" log_i="1724765674.528205" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="348" mpi_size="696" stamp_init="1724765564.680950" stamp_final="1724765674.533370" username="apac4" allocationname="unknown" flags="0" pid="3304951" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09852e+02" utime="8.69075e+01" stime="1.42935e+01" mtime="7.15406e+01" gflop="0.00000e+00" gbyte="3.77914e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15406e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09729e+02" utime="8.68721e+01" stime="1.42870e+01" mtime="7.15406e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15406e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1819e+09" > 7.5527e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1902e+09" > 4.4157e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.3811e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9671e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5405e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4188e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4746e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0868e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0516e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7987e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4036e+01 </func>
</region>
</regions>
<internal rank="348" log_i="1724765674.533370" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="349" mpi_size="696" stamp_init="1724765564.682046" stamp_final="1724765674.531795" username="apac4" allocationname="unknown" flags="0" pid="3304952" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09850e+02" utime="8.98451e+01" stime="1.32979e+01" mtime="7.28707e+01" gflop="0.00000e+00" gbyte="3.77762e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28707e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b015b0153c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09729e+02" utime="8.98109e+01" stime="1.32903e+01" mtime="7.28707e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28707e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2263e+09" > 5.7583e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2095e+09" > 2.6849e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2309e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9665e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9167e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4750e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0847e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0531e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7824e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4716e+01 </func>
</region>
</regions>
<internal rank="349" log_i="1724765674.531795" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="350" mpi_size="696" stamp_init="1724765564.680913" stamp_final="1724765674.533250" username="apac4" allocationname="unknown" flags="0" pid="3304953" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09852e+02" utime="8.70632e+01" stime="1.43423e+01" mtime="7.23808e+01" gflop="0.00000e+00" gbyte="3.77666e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23808e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005a15ef555a155a1500" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09727e+02" utime="8.70319e+01" stime="1.43320e+01" mtime="7.23808e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23808e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1934e+09" > 7.8175e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1950e+09" > 4.1461e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1283e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9693e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.5611e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4122e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4741e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0911e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0522e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7966e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4077e+01 </func>
</region>
</regions>
<internal rank="350" log_i="1724765674.533250" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="351" mpi_size="696" stamp_init="1724765564.681887" stamp_final="1724765674.529135" username="apac4" allocationname="unknown" flags="0" pid="3304954" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09847e+02" utime="8.95662e+01" stime="1.35623e+01" mtime="7.27481e+01" gflop="0.00000e+00" gbyte="3.77769e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27481e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43e153f154015d1554015401541" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09725e+02" utime="8.95312e+01" stime="1.35552e+01" mtime="7.27481e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27481e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1935e+09" > 5.5117e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2003e+09" > 2.5197e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1001e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9695e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8597e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.3838e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4741e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.0907e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0531e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7809e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4723e+01 </func>
</region>
</regions>
<internal rank="351" log_i="1724765674.529135" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="352" mpi_size="696" stamp_init="1724765564.680957" stamp_final="1724765674.532868" username="apac4" allocationname="unknown" flags="0" pid="3304955" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09852e+02" utime="8.61962e+01" stime="1.44386e+01" mtime="7.15089e+01" gflop="0.00000e+00" gbyte="3.78338e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15089e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09729e+02" utime="8.61602e+01" stime="1.44327e+01" mtime="7.15089e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15089e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.3113e-05 </func>
<func name="MPI_Isend" count="127200" bytes="1.2064e+09" > 8.2108e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1906e+09" > 4.3868e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7385e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9672e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.4475e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.6617e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4724e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1084e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0552e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7437e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3581e+01 </func>
</region>
</regions>
<internal rank="352" log_i="1724765674.532868" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="353" mpi_size="696" stamp_init="1724765564.680908" stamp_final="1724765674.533437" username="apac4" allocationname="unknown" flags="0" pid="3304956" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09853e+02" utime="8.98913e+01" stime="1.33121e+01" mtime="7.28487e+01" gflop="0.00000e+00" gbyte="3.78086e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28487e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48c158e158f15d6558f158e1552" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09729e+02" utime="8.98582e+01" stime="1.33024e+01" mtime="7.28487e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28487e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2162e+09" > 5.6832e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2149e+09" > 2.6688e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3487e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9685e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.1100e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4725e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1064e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0527e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7810e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4567e+01 </func>
</region>
</regions>
<internal rank="353" log_i="1724765674.533437" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="354" mpi_size="696" stamp_init="1724765564.680905" stamp_final="1724765674.533412" username="apac4" allocationname="unknown" flags="0" pid="3304957" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09853e+02" utime="8.63027e+01" stime="1.47717e+01" mtime="7.17225e+01" gflop="0.00000e+00" gbyte="3.76003e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17225e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004715bd554715461550" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09728e+02" utime="8.62727e+01" stime="1.47593e+01" mtime="7.17225e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17225e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1930e+09" > 8.0177e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2002e+09" > 4.4452e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8677e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9685e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0770e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.3390e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4725e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1063e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0563e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7891e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3671e+01 </func>
</region>
</regions>
<internal rank="354" log_i="1724765674.533412" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="355" mpi_size="696" stamp_init="1724765564.680892" stamp_final="1724765674.525463" username="apac4" allocationname="unknown" flags="0" pid="3304958" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09845e+02" utime="8.98300e+01" stime="1.33672e+01" mtime="7.25477e+01" gflop="0.00000e+00" gbyte="3.76556e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25477e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c714c814c914d856c914c91490" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09722e+02" utime="8.97931e+01" stime="1.33623e+01" mtime="7.25477e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25477e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2212e+09" > 5.6746e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2149e+09" > 3.1535e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7069e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9686e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3122e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4718e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1140e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0567e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7814e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4899e+01 </func>
</region>
</regions>
<internal rank="355" log_i="1724765674.525463" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="356" mpi_size="696" stamp_init="1724765564.680933" stamp_final="1724765674.527030" username="apac4" allocationname="unknown" flags="0" pid="3304959" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09846e+02" utime="8.63611e+01" stime="1.46511e+01" mtime="7.20372e+01" gflop="0.00000e+00" gbyte="3.76266e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20372e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005614561457" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09722e+02" utime="8.63316e+01" stime="1.46382e+01" mtime="7.20372e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20372e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2248e+09" > 7.7146e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2130e+09" > 3.9161e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2716e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9689e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.4407e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0850e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4721e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1116e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0534e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7903e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3613e+01 </func>
</region>
</regions>
<internal rank="356" log_i="1724765674.527030" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="357" mpi_size="696" stamp_init="1724765564.680899" stamp_final="1724765674.528020" username="apac4" allocationname="unknown" flags="0" pid="3304960" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09847e+02" utime="8.95085e+01" stime="1.36859e+01" mtime="7.25382e+01" gflop="0.00000e+00" gbyte="3.77548e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25382e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f11410141114c75611141114f3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09723e+02" utime="8.94757e+01" stime="1.36768e+01" mtime="7.25382e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25382e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1942e+09" > 5.7510e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1943e+09" > 3.0084e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1019e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9681e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.9427e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4716e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1169e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0557e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7794e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4493e+01 </func>
</region>
</regions>
<internal rank="357" log_i="1724765674.528020" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="358" mpi_size="696" stamp_init="1724765564.680921" stamp_final="1724765674.528007" username="apac4" allocationname="unknown" flags="0" pid="3304961" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09847e+02" utime="8.82215e+01" stime="1.39351e+01" mtime="7.15337e+01" gflop="0.00000e+00" gbyte="3.77605e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15337e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf42514271428146055281428146b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09724e+02" utime="8.81889e+01" stime="1.39254e+01" mtime="7.15337e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15337e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2015e+09" > 6.9922e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1911e+09" > 3.5843e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.4531e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9684e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1289e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0719e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4713e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1228e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0516e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7880e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4007e+01 </func>
</region>
</regions>
<internal rank="358" log_i="1724765674.528007" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="359" mpi_size="696" stamp_init="1724765564.680883" stamp_final="1724765674.533742" username="apac4" allocationname="unknown" flags="0" pid="3304962" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u18b</host>
<perf wtime="1.09853e+02" utime="8.94093e+01" stime="1.34642e+01" mtime="7.24359e+01" gflop="0.00000e+00" gbyte="3.76556e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24359e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf474158d159f1555569f159a153a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09730e+02" utime="8.93773e+01" stime="1.34541e+01" mtime="7.24359e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24359e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1832e+09" > 6.3073e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1921e+09" > 2.8784e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2101e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9668e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0839e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4704e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1322e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0556e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7793e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4245e+01 </func>
</region>
</regions>
<internal rank="359" log_i="1724765674.533742" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="360" mpi_size="696" stamp_init="1724765564.881810" stamp_final="1724765674.529935" username="apac4" allocationname="unknown" flags="0" pid="2238517" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09648e+02" utime="7.54715e+01" stime="2.55600e+01" mtime="7.14426e+01" gflop="0.00000e+00" gbyte="3.86688e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.14426e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000951495147c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09522e+02" utime="7.54398e+01" stime="2.55497e+01" mtime="7.14426e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.14426e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1946e+09" > 7.6625e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2053e+09" > 4.1313e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8521e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9755e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.4898e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.5530e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4707e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1248e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0638e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7253e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3367e+01 </func>
</region>
</regions>
<internal rank="360" log_i="1724765674.529935" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="361" mpi_size="696" stamp_init="1724765564.880923" stamp_final="1724765674.523829" username="apac4" allocationname="unknown" flags="0" pid="2238518" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09643e+02" utime="8.98858e+01" stime="1.36126e+01" mtime="7.26840e+01" gflop="0.00000e+00" gbyte="3.77586e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.26840e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09514e+02" utime="8.98509e+01" stime="1.36044e+01" mtime="7.26840e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.26840e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2123e+09" > 5.8452e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2127e+09" > 3.4412e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5363e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9751e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6200e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4715e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1130e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0661e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7579e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4124e+01 </func>
</region>
</regions>
<internal rank="361" log_i="1724765674.523829" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="362" mpi_size="696" stamp_init="1724765564.880668" stamp_final="1724765674.527066" username="apac4" allocationname="unknown" flags="0" pid="2238519" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09646e+02" utime="8.80583e+01" stime="1.36675e+01" mtime="7.19131e+01" gflop="0.00000e+00" gbyte="3.78090e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19131e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4db15f41510159b55101510150c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09520e+02" utime="8.80256e+01" stime="1.36583e+01" mtime="7.19131e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19131e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1927e+09" > 7.1971e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2074e+09" > 4.1286e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2155e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9753e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.6042e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6729e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4713e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1182e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0622e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7671e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3529e+01 </func>
</region>
</regions>
<internal rank="362" log_i="1724765674.527066" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="363" mpi_size="696" stamp_init="1724765564.880830" stamp_final="1724765674.535380" username="apac4" allocationname="unknown" flags="0" pid="2238520" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09655e+02" utime="9.00498e+01" stime="1.34733e+01" mtime="7.27941e+01" gflop="0.00000e+00" gbyte="3.77926e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27941e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006315c4556315631526" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09529e+02" utime="9.00169e+01" stime="1.34642e+01" mtime="7.27941e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27941e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1874e+09" > 5.9807e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1900e+09" > 2.7578e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4161e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9758e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7895e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8358e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4709e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1227e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0640e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7572e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4337e+01 </func>
</region>
</regions>
<internal rank="363" log_i="1724765674.535380" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="364" mpi_size="696" stamp_init="1724765564.881251" stamp_final="1724765674.536498" username="apac4" allocationname="unknown" flags="0" pid="2238521" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09655e+02" utime="8.73330e+01" stime="1.41483e+01" mtime="7.20503e+01" gflop="0.00000e+00" gbyte="3.78124e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20503e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf42e1530153115ba563115301516" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09526e+02" utime="8.73036e+01" stime="1.41358e+01" mtime="7.20503e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20503e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1934e+09" > 7.6402e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1984e+09" > 4.9338e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4239e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9756e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4104e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6073e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4700e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1321e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0613e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7676e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3402e+01 </func>
</region>
</regions>
<internal rank="364" log_i="1724765674.536498" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="365" mpi_size="696" stamp_init="1724765564.880702" stamp_final="1724765674.524843" username="apac4" allocationname="unknown" flags="0" pid="2238522" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09644e+02" utime="9.00175e+01" stime="1.36171e+01" mtime="7.23942e+01" gflop="0.00000e+00" gbyte="3.77686e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23942e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a1147855a114a01490" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09518e+02" utime="8.99868e+01" stime="1.36056e+01" mtime="7.23942e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23942e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1976e+09" > 5.8431e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2022e+09" > 3.3051e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8397e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9746e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.8658e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4701e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1281e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0665e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7568e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4537e+01 </func>
</region>
</regions>
<internal rank="365" log_i="1724765674.524843" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="366" mpi_size="696" stamp_init="1724765564.880648" stamp_final="1724765674.537078" username="apac4" allocationname="unknown" flags="0" pid="2238523" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09656e+02" utime="8.62028e+01" stime="1.51944e+01" mtime="7.22247e+01" gflop="0.00000e+00" gbyte="3.76751e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22247e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ad14ad14e1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09530e+02" utime="8.61729e+01" stime="1.51827e+01" mtime="7.22247e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22247e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1991e+09" > 8.3281e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2039e+09" > 4.5102e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4513e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9735e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2016e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6550e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4687e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1447e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0630e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7667e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3506e+01 </func>
</region>
</regions>
<internal rank="366" log_i="1724765674.537078" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="367" mpi_size="696" stamp_init="1724765564.880684" stamp_final="1724765674.531373" username="apac4" allocationname="unknown" flags="0" pid="2238524" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09651e+02" utime="8.99184e+01" stime="1.32997e+01" mtime="7.28458e+01" gflop="0.00000e+00" gbyte="3.77514e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28458e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008415831509" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09522e+02" utime="8.98847e+01" stime="1.32915e+01" mtime="7.28458e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28458e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2022e+09" > 5.5585e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2083e+09" > 2.6106e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3730e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9741e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1518e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4700e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1331e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0607e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7567e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4494e+01 </func>
</region>
</regions>
<internal rank="367" log_i="1724765674.531373" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="368" mpi_size="696" stamp_init="1724765564.880649" stamp_final="1724765674.521397" username="apac4" allocationname="unknown" flags="0" pid="2238525" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09641e+02" utime="8.49408e+01" stime="1.55332e+01" mtime="7.21239e+01" gflop="0.00000e+00" gbyte="3.77029e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21239e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003314bd5533143314dc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09515e+02" utime="8.49008e+01" stime="1.55312e+01" mtime="7.21239e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21239e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2257e+09" > 8.7926e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2225e+09" > 5.7090e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9258e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9750e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5461e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6238e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4663e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1590e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0650e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7166e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2856e+01 </func>
</region>
</regions>
<internal rank="368" log_i="1724765674.521397" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="369" mpi_size="696" stamp_init="1724765564.880693" stamp_final="1724765674.534118" username="apac4" allocationname="unknown" flags="0" pid="2238526" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09653e+02" utime="8.98401e+01" stime="1.36231e+01" mtime="7.28507e+01" gflop="0.00000e+00" gbyte="3.76732e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28507e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000af14af14fe" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09527e+02" utime="8.98057e+01" stime="1.36160e+01" mtime="7.28507e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28507e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2003e+09" > 5.9986e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1955e+09" > 2.6351e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3034e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9751e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6941e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.5571e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4673e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1598e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0659e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7577e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4514e+01 </func>
</region>
</regions>
<internal rank="369" log_i="1724765674.534118" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="370" mpi_size="696" stamp_init="1724765564.880887" stamp_final="1724765674.532816" username="apac4" allocationname="unknown" flags="0" pid="2238527" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09652e+02" utime="8.82779e+01" stime="1.41088e+01" mtime="7.24221e+01" gflop="0.00000e+00" gbyte="3.76583e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24221e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09526e+02" utime="8.82417e+01" stime="1.41025e+01" mtime="7.24221e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24221e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2024e+09" > 6.9390e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1930e+09" > 3.7095e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3450e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9763e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1206e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6681e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4670e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1589e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0621e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7681e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3930e+01 </func>
</region>
</regions>
<internal rank="370" log_i="1724765674.532816" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="371" mpi_size="696" stamp_init="1724765564.880667" stamp_final="1724765674.527259" username="apac4" allocationname="unknown" flags="0" pid="2238528" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09647e+02" utime="8.97626e+01" stime="1.33757e+01" mtime="7.28401e+01" gflop="0.00000e+00" gbyte="3.77834e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28401e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003814205538143314c5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09517e+02" utime="8.97280e+01" stime="1.33683e+01" mtime="7.28401e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28401e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2078e+09" > 5.6629e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2061e+09" > 2.8630e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0232e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9765e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9789e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6551e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4669e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1643e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0620e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7569e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4800e+01 </func>
</region>
</regions>
<internal rank="371" log_i="1724765674.527259" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="372" mpi_size="696" stamp_init="1724765564.880653" stamp_final="1724765674.521348" username="apac4" allocationname="unknown" flags="0" pid="2238529" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09641e+02" utime="8.75378e+01" stime="1.47173e+01" mtime="7.21852e+01" gflop="0.00000e+00" gbyte="3.76804e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21852e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf46d146f147014b55570147014ad" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09515e+02" utime="8.75085e+01" stime="1.47050e+01" mtime="7.21852e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21852e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1885e+09" > 6.9996e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1809e+09" > 3.7520e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3050e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9746e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.0759e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6309e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4669e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1606e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0655e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7668e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3738e+01 </func>
</region>
</regions>
<internal rank="372" log_i="1724765674.521348" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="373" mpi_size="696" stamp_init="1724765564.880683" stamp_final="1724765674.529128" username="apac4" allocationname="unknown" flags="0" pid="2238530" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09648e+02" utime="9.00979e+01" stime="1.34165e+01" mtime="7.24790e+01" gflop="0.00000e+00" gbyte="3.77537e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24790e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09522e+02" utime="9.00652e+01" stime="1.34069e+01" mtime="7.24790e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24790e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2044e+09" > 5.7131e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2195e+09" > 2.7828e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9138e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9733e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.2058e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4654e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1808e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0656e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7556e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4578e+01 </func>
</region>
</regions>
<internal rank="373" log_i="1724765674.529128" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="374" mpi_size="696" stamp_init="1724765564.880642" stamp_final="1724765674.532293" username="apac4" allocationname="unknown" flags="0" pid="2238531" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09652e+02" utime="8.81316e+01" stime="1.40506e+01" mtime="7.21447e+01" gflop="0.00000e+00" gbyte="3.77590e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21447e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b914b9149a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09524e+02" utime="8.80978e+01" stime="1.40420e+01" mtime="7.21447e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21447e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.0599e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1917e+09" > 7.2085e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2001e+09" > 3.4916e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9843e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9737e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1621e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6529e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4647e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1884e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0608e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7651e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4012e+01 </func>
</region>
</regions>
<internal rank="374" log_i="1724765674.532293" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="375" mpi_size="696" stamp_init="1724765564.881766" stamp_final="1724765674.532343" username="apac4" allocationname="unknown" flags="0" pid="2238532" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09651e+02" utime="8.96607e+01" stime="1.38933e+01" mtime="7.27133e+01" gflop="0.00000e+00" gbyte="3.76213e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27133e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4361437143914975539143814bb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09524e+02" utime="8.96278e+01" stime="1.38844e+01" mtime="7.27133e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27133e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1937e+09" > 5.5500e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1926e+09" > 2.6250e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1649e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9764e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6801e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4658e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1746e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0655e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7552e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4550e+01 </func>
</region>
</regions>
<internal rank="375" log_i="1724765674.532343" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="376" mpi_size="696" stamp_init="1724765564.881319" stamp_final="1724765674.537994" username="apac4" allocationname="unknown" flags="0" pid="2238533" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09657e+02" utime="8.79167e+01" stime="1.39268e+01" mtime="7.18645e+01" gflop="0.00000e+00" gbyte="3.78208e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18645e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ab14e456ab14ab14b1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09530e+02" utime="8.78891e+01" stime="1.39121e+01" mtime="7.18645e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18645e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1961e+09" > 7.2517e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2062e+09" > 3.9080e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8535e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9753e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5892e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6400e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4655e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1765e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0654e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7626e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2840e+01 </func>
</region>
</regions>
<internal rank="376" log_i="1724765674.537994" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="377" mpi_size="696" stamp_init="1724765564.880768" stamp_final="1724765674.530619" username="apac4" allocationname="unknown" flags="0" pid="2238534" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09650e+02" utime="9.03471e+01" stime="1.33264e+01" mtime="7.28490e+01" gflop="0.00000e+00" gbyte="3.76900e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28490e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f914b956f914f81497" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09524e+02" utime="9.03105e+01" stime="1.33210e+01" mtime="7.28490e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28490e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2192e+09" > 5.6755e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2139e+09" > 3.0235e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3015e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9747e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7895e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3012e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4650e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1798e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0628e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7545e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4545e+01 </func>
</region>
</regions>
<internal rank="377" log_i="1724765674.530619" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="378" mpi_size="696" stamp_init="1724765564.880669" stamp_final="1724765674.526405" username="apac4" allocationname="unknown" flags="0" pid="2238535" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09646e+02" utime="8.73246e+01" stime="1.45771e+01" mtime="7.21697e+01" gflop="0.00000e+00" gbyte="3.77602e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21697e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000096149614cb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09519e+02" utime="8.72862e+01" stime="1.45732e+01" mtime="7.21697e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21697e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2041e+09" > 6.6137e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2067e+09" > 5.3359e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8742e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9727e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.6982e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6410e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4648e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1809e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0609e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7632e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3200e+01 </func>
</region>
</regions>
<internal rank="378" log_i="1724765674.526405" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="379" mpi_size="696" stamp_init="1724765564.880718" stamp_final="1724765674.534164" username="apac4" allocationname="unknown" flags="0" pid="2238536" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09653e+02" utime="9.01878e+01" stime="1.33223e+01" mtime="7.27928e+01" gflop="0.00000e+00" gbyte="3.76507e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27928e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003714321459" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09526e+02" utime="9.01571e+01" stime="1.33115e+01" mtime="7.27928e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27928e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2179e+09" > 5.6637e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2233e+09" > 2.7900e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6345e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9752e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3855e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.1421e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4641e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1923e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0638e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7505e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4158e+01 </func>
</region>
</regions>
<internal rank="379" log_i="1724765674.534164" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="380" mpi_size="696" stamp_init="1724765564.880650" stamp_final="1724765674.530808" username="apac4" allocationname="unknown" flags="0" pid="2238537" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09650e+02" utime="8.65660e+01" stime="1.49036e+01" mtime="7.25275e+01" gflop="0.00000e+00" gbyte="3.74836e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25275e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009214b2559214921464" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09523e+02" utime="8.65343e+01" stime="1.48936e+01" mtime="7.25275e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25275e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2095e+09" > 7.9707e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2245e+09" > 4.6407e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7171e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9754e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0085e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8960e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4640e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1934e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0617e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7613e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3557e+01 </func>
</region>
</regions>
<internal rank="380" log_i="1724765674.530808" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="381" mpi_size="696" stamp_init="1724765564.880951" stamp_final="1724765674.539740" username="apac4" allocationname="unknown" flags="0" pid="2238538" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09659e+02" utime="9.05092e+01" stime="1.29508e+01" mtime="7.25065e+01" gflop="0.00000e+00" gbyte="3.75469e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25065e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf468146a146b1448566b146a14a3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09532e+02" utime="9.04780e+01" stime="1.29397e+01" mtime="7.25065e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25065e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1976e+09" > 6.4154e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1869e+09" > 2.6307e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3815e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9735e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6648e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4628e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2071e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0618e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7489e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4065e+01 </func>
</region>
</regions>
<internal rank="381" log_i="1724765674.539740" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="382" mpi_size="696" stamp_init="1724765564.881665" stamp_final="1724765674.531781" username="apac4" allocationname="unknown" flags="0" pid="2238539" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09650e+02" utime="8.73587e+01" stime="1.46964e+01" mtime="7.17056e+01" gflop="0.00000e+00" gbyte="3.76930e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17056e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09525e+02" utime="8.73244e+01" stime="1.46888e+01" mtime="7.17056e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17056e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1880e+09" > 7.9292e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2012e+09" > 4.0863e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1161e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9757e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4360e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9983e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4638e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1951e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0621e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7029e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3343e+01 </func>
</region>
</regions>
<internal rank="382" log_i="1724765674.531781" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="383" mpi_size="696" stamp_init="1724765564.880692" stamp_final="1724765674.537367" username="apac4" allocationname="unknown" flags="0" pid="2238540" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u03a</host>
<perf wtime="1.09657e+02" utime="9.07389e+01" stime="1.28825e+01" mtime="7.23463e+01" gflop="0.00000e+00" gbyte="3.77934e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23463e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09528e+02" utime="9.07019e+01" stime="1.28772e+01" mtime="7.23463e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23463e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1967e+09" > 5.9577e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1838e+09" > 2.7285e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1153e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9744e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6226e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0240e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4636e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.1942e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0660e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7264e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4209e+01 </func>
</region>
</regions>
<internal rank="383" log_i="1724765674.537367" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="384" mpi_size="696" stamp_init="1724765564.882359" stamp_final="1724765674.520763" username="apac4" allocationname="unknown" flags="0" pid="141873" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09638e+02" utime="7.55782e+01" stime="2.57359e+01" mtime="7.20974e+01" gflop="0.00000e+00" gbyte="3.85323e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20974e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f714f714fd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09517e+02" utime="7.55452e+01" stime="2.57275e+01" mtime="7.20974e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20974e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2050e+09" > 6.9343e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2061e+09" > 4.2592e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5229e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.4757e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6906e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.8133e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4628e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2109e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0603e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7289e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3805e+01 </func>
</region>
</regions>
<internal rank="384" log_i="1724765674.520763" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="385" mpi_size="696" stamp_init="1724765564.883185" stamp_final="1724765674.534970" username="apac4" allocationname="unknown" flags="0" pid="141874" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09652e+02" utime="8.98769e+01" stime="1.29982e+01" mtime="7.27064e+01" gflop="0.00000e+00" gbyte="3.78666e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27064e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09522e+02" utime="8.98456e+01" stime="1.29859e+01" mtime="7.27064e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27064e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.5831e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2040e+09" > 5.8557e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2083e+09" > 3.1259e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9216e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9383e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3209e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4635e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2037e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0623e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6865e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4130e+01 </func>
</region>
</regions>
<internal rank="385" log_i="1724765674.534970" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="386" mpi_size="696" stamp_init="1724765564.882143" stamp_final="1724765674.532926" username="apac4" allocationname="unknown" flags="0" pid="141875" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09651e+02" utime="8.75025e+01" stime="1.38163e+01" mtime="7.14319e+01" gflop="0.00000e+00" gbyte="3.76534e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.14319e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009a14a1559a14991455" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09524e+02" utime="8.74712e+01" stime="1.38061e+01" mtime="7.14319e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.14319e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2027e+09" > 7.7344e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1993e+09" > 3.5887e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1053e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9372e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5048e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.2158e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4632e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2064e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0589e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6733e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3490e+01 </func>
</region>
</regions>
<internal rank="386" log_i="1724765674.532926" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="387" mpi_size="696" stamp_init="1724765564.882115" stamp_final="1724765674.527544" username="apac4" allocationname="unknown" flags="0" pid="141876" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09645e+02" utime="8.89312e+01" stime="1.38457e+01" mtime="7.24963e+01" gflop="0.00000e+00" gbyte="3.76862e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24963e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009114735591148c148a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09520e+02" utime="8.88977e+01" stime="1.38375e+01" mtime="7.24963e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24963e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1895e+09" > 5.7218e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1873e+09" > 3.0728e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6588e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9376e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.2130e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4630e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2064e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0592e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6873e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4203e+01 </func>
</region>
</regions>
<internal rank="387" log_i="1724765674.527544" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="388" mpi_size="696" stamp_init="1724765564.882148" stamp_final="1724765674.541070" username="apac4" allocationname="unknown" flags="0" pid="141877" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09659e+02" utime="8.69309e+01" stime="1.44369e+01" mtime="7.20811e+01" gflop="0.00000e+00" gbyte="3.77037e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20811e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007d157d151b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09535e+02" utime="8.68952e+01" stime="1.44308e+01" mtime="7.20811e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20811e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1971e+09" > 7.6819e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1929e+09" > 4.0701e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7224e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9352e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1528e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.2902e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4625e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2101e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0586e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7279e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3540e+01 </func>
</region>
</regions>
<internal rank="388" log_i="1724765674.541070" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="389" mpi_size="696" stamp_init="1724765564.883915" stamp_final="1724765674.526452" username="apac4" allocationname="unknown" flags="0" pid="141878" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09643e+02" utime="8.84501e+01" stime="1.39956e+01" mtime="7.28231e+01" gflop="0.00000e+00" gbyte="3.77560e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28231e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d0147555d014cf1461" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09521e+02" utime="8.84155e+01" stime="1.39887e+01" mtime="7.28231e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28231e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2020e+09" > 5.8148e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2014e+09" > 3.0480e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0047e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9380e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2636e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.2978e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4626e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2133e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0589e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6867e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4172e+01 </func>
</region>
</regions>
<internal rank="389" log_i="1724765674.526452" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="390" mpi_size="696" stamp_init="1724765564.882138" stamp_final="1724765674.524885" username="apac4" allocationname="unknown" flags="0" pid="141879" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09643e+02" utime="8.56263e+01" stime="1.51507e+01" mtime="7.13529e+01" gflop="0.00000e+00" gbyte="3.77918e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.13529e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09517e+02" utime="8.55901e+01" stime="1.51446e+01" mtime="7.13529e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.13529e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2049e+09" > 8.2424e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2023e+09" > 3.9405e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0993e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9364e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6689e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5357e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4609e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2269e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0614e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7254e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3371e+01 </func>
</region>
</regions>
<internal rank="390" log_i="1724765674.524885" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="391" mpi_size="696" stamp_init="1724765564.883875" stamp_final="1724765674.526378" username="apac4" allocationname="unknown" flags="0" pid="141880" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09643e+02" utime="8.91975e+01" stime="1.36829e+01" mtime="7.25257e+01" gflop="0.00000e+00" gbyte="3.77239e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25257e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000171422551714171497" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09519e+02" utime="8.91575e+01" stime="1.36811e+01" mtime="7.25257e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25257e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2172e+09" > 5.7603e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2045e+09" > 2.9002e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8867e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9387e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2159e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.5030e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4622e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2181e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0602e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6874e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3994e+01 </func>
</region>
</regions>
<internal rank="391" log_i="1724765674.526378" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="392" mpi_size="696" stamp_init="1724765564.882365" stamp_final="1724765674.539465" username="apac4" allocationname="unknown" flags="0" pid="141881" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09657e+02" utime="8.41061e+01" stime="1.54529e+01" mtime="7.18284e+01" gflop="0.00000e+00" gbyte="3.76713e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18284e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09534e+02" utime="8.40703e+01" stime="1.54474e+01" mtime="7.18284e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18284e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2204e+09" > 9.2483e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2201e+09" > 5.6096e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5678e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9354e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3089e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5353e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4615e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2242e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0619e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7230e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3271e+01 </func>
</region>
</regions>
<internal rank="392" log_i="1724765674.539465" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="393" mpi_size="696" stamp_init="1724765564.883639" stamp_final="1724765674.529246" username="apac4" allocationname="unknown" flags="0" pid="141882" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09646e+02" utime="8.94268e+01" stime="1.33854e+01" mtime="7.23447e+01" gflop="0.00000e+00" gbyte="3.77323e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23447e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ca14cb14cd141055cd14cc1473" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09522e+02" utime="8.93913e+01" stime="1.33792e+01" mtime="7.23447e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23447e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1966e+09" > 5.7503e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2039e+09" > 3.0194e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4203e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9378e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1992e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4619e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2214e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0634e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6839e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4284e+01 </func>
</region>
</regions>
<internal rank="393" log_i="1724765674.529246" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="394" mpi_size="696" stamp_init="1724765564.883606" stamp_final="1724765674.524660" username="apac4" allocationname="unknown" flags="0" pid="141883" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09641e+02" utime="8.67582e+01" stime="1.46064e+01" mtime="7.19936e+01" gflop="0.00000e+00" gbyte="3.77628e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19936e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ca14cc14cd148256cd14cc14c2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09518e+02" utime="8.67234e+01" stime="1.45994e+01" mtime="7.19936e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19936e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1863e+09" > 7.1358e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1976e+09" > 3.7091e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8224e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9363e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.5061e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9370e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4616e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2229e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0589e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7178e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3402e+01 </func>
</region>
</regions>
<internal rank="394" log_i="1724765674.524660" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="395" mpi_size="696" stamp_init="1724765564.883816" stamp_final="1724765674.526442" username="apac4" allocationname="unknown" flags="0" pid="141884" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09643e+02" utime="8.92068e+01" stime="1.35827e+01" mtime="7.25880e+01" gflop="0.00000e+00" gbyte="3.77380e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25880e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09520e+02" utime="8.91724e+01" stime="1.35755e+01" mtime="7.25880e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25880e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2000e+09" > 5.7262e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1965e+09" > 3.0138e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6097e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9377e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1990e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4606e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2304e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0622e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6840e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4342e+01 </func>
</region>
</regions>
<internal rank="395" log_i="1724765674.526442" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="396" mpi_size="696" stamp_init="1724765564.882463" stamp_final="1724765674.535315" username="apac4" allocationname="unknown" flags="0" pid="141885" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09653e+02" utime="8.71590e+01" stime="1.43828e+01" mtime="7.16070e+01" gflop="0.00000e+00" gbyte="3.78010e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16070e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000fe15fd1532" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09529e+02" utime="8.71330e+01" stime="1.43668e+01" mtime="7.16070e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16070e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1829e+09" > 6.5738e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1879e+09" > 3.6262e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5045e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9370e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8382e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.6049e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4604e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2295e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0603e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7174e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3384e+01 </func>
</region>
</regions>
<internal rank="396" log_i="1724765674.535315" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="397" mpi_size="696" stamp_init="1724765564.882416" stamp_final="1724765674.528607" username="apac4" allocationname="unknown" flags="0" pid="141886" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09646e+02" utime="8.90661e+01" stime="1.36796e+01" mtime="7.28268e+01" gflop="0.00000e+00" gbyte="3.76568e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28268e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000bb14bb146d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09523e+02" utime="8.90354e+01" stime="1.36690e+01" mtime="7.28268e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28268e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2166e+09" > 5.7404e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2045e+09" > 2.7505e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8504e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9371e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1921e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9320e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4600e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2405e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0625e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6841e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4348e+01 </func>
</region>
</regions>
<internal rank="397" log_i="1724765674.528607" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="398" mpi_size="696" stamp_init="1724765564.882327" stamp_final="1724765674.535211" username="apac4" allocationname="unknown" flags="0" pid="141887" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09653e+02" utime="8.75733e+01" stime="1.39733e+01" mtime="7.19776e+01" gflop="0.00000e+00" gbyte="3.77121e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19776e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf441154215431557554315431533" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09531e+02" utime="8.75391e+01" stime="1.39661e+01" mtime="7.19776e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19776e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2009e+09" > 7.2027e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2022e+09" > 3.2741e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4807e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9380e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4877e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8242e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4595e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2430e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0594e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7160e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3708e+01 </func>
</region>
</regions>
<internal rank="398" log_i="1724765674.535211" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="399" mpi_size="696" stamp_init="1724765564.882114" stamp_final="1724765674.532980" username="apac4" allocationname="unknown" flags="0" pid="141888" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09651e+02" utime="8.93139e+01" stime="1.34795e+01" mtime="7.24067e+01" gflop="0.00000e+00" gbyte="3.76160e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24067e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09527e+02" utime="8.92802e+01" stime="1.34716e+01" mtime="7.24067e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24067e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127200" bytes="1.1956e+09" > 5.9549e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1820e+09" > 2.6612e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7195e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9382e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2159e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3581e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4596e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2441e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0617e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6825e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4027e+01 </func>
</region>
</regions>
<internal rank="399" log_i="1724765674.532980" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="400" mpi_size="696" stamp_init="1724765564.882482" stamp_final="1724765674.520881" username="apac4" allocationname="unknown" flags="0" pid="141889" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09638e+02" utime="7.93540e+01" stime="2.02004e+01" mtime="7.09879e+01" gflop="0.00000e+00" gbyte="3.76747e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.09879e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f114f314f4142955f414f314f4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09514e+02" utime="7.93186e+01" stime="2.01940e+01" mtime="7.09879e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.09879e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2051e+09" > 1.3907e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2068e+09" > 7.7435e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.6663e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.4648e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6906e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.8113e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4591e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2432e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0617e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7180e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2831e+01 </func>
</region>
</regions>
<internal rank="400" log_i="1724765674.520881" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="401" mpi_size="696" stamp_init="1724765564.882132" stamp_final="1724765674.530863" username="apac4" allocationname="unknown" flags="0" pid="141890" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09649e+02" utime="8.96464e+01" stime="1.32278e+01" mtime="7.24109e+01" gflop="0.00000e+00" gbyte="3.76751e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24109e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008d14f9558d148c1472" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09524e+02" utime="8.96100e+01" stime="1.32227e+01" mtime="7.24109e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24109e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2242e+09" > 5.7587e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2155e+09" > 2.8055e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8444e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9373e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1206e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5319e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4594e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2471e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0612e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6785e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3935e+01 </func>
</region>
</regions>
<internal rank="401" log_i="1724765674.530863" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="402" mpi_size="696" stamp_init="1724765564.882453" stamp_final="1724765674.522773" username="apac4" allocationname="unknown" flags="0" pid="141891" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09640e+02" utime="8.63877e+01" stime="1.46345e+01" mtime="7.16261e+01" gflop="0.00000e+00" gbyte="3.77514e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16261e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005314bf565314531491" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09519e+02" utime="8.63549e+01" stime="1.46255e+01" mtime="7.16261e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16261e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2194e+09" > 7.2606e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2118e+09" > 3.3673e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5535e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9372e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0204e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5582e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4589e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2474e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0616e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7134e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3286e+01 </func>
</region>
</regions>
<internal rank="402" log_i="1724765674.522773" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="403" mpi_size="696" stamp_init="1724765564.882675" stamp_final="1724765674.529755" username="apac4" allocationname="unknown" flags="0" pid="141892" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09647e+02" utime="8.92164e+01" stime="1.36085e+01" mtime="7.20544e+01" gflop="0.00000e+00" gbyte="3.78326e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20544e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09522e+02" utime="8.91894e+01" stime="1.35926e+01" mtime="7.20544e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20544e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2198e+09" > 5.8148e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2229e+09" > 2.7633e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5370e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9363e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1528e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4581e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2557e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0604e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6784e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3890e+01 </func>
</region>
</regions>
<internal rank="403" log_i="1724765674.529755" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="404" mpi_size="696" stamp_init="1724765564.882191" stamp_final="1724765674.529642" username="apac4" allocationname="unknown" flags="0" pid="141893" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09647e+02" utime="8.69359e+01" stime="1.44239e+01" mtime="7.17833e+01" gflop="0.00000e+00" gbyte="3.74992e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17833e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09524e+02" utime="8.68993e+01" stime="1.44178e+01" mtime="7.17833e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17833e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2206e+09" > 7.2895e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2137e+09" > 3.9066e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6168e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9376e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7071e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.1211e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4585e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2561e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0607e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.7087e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3368e+01 </func>
</region>
</regions>
<internal rank="404" log_i="1724765674.529642" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="405" mpi_size="696" stamp_init="1724765564.882114" stamp_final="1724765674.535687" username="apac4" allocationname="unknown" flags="0" pid="141894" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09654e+02" utime="8.89623e+01" stime="1.37142e+01" mtime="7.22993e+01" gflop="0.00000e+00" gbyte="3.77735e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22993e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e914eb14ec149655ec14ec14b5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09529e+02" utime="8.89280e+01" stime="1.37062e+01" mtime="7.22993e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22993e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1851e+09" > 5.6666e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1925e+09" > 2.5525e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6277e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9371e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3399e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4580e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2600e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0629e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6779e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4051e+01 </func>
</region>
</regions>
<internal rank="405" log_i="1724765674.535687" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="406" mpi_size="696" stamp_init="1724765564.882138" stamp_final="1724765674.532917" username="apac4" allocationname="unknown" flags="0" pid="141895" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09651e+02" utime="8.65799e+01" stime="1.47039e+01" mtime="7.19853e+01" gflop="0.00000e+00" gbyte="3.77605e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19853e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000be14be14a3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09528e+02" utime="8.65489e+01" stime="1.46921e+01" mtime="7.19853e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19853e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1988e+09" > 7.6017e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1983e+09" > 3.6620e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7512e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9362e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4221e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.3829e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4574e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2678e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0590e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6879e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3417e+01 </func>
</region>
</regions>
<internal rank="406" log_i="1724765674.532917" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="407" mpi_size="696" stamp_init="1724765564.882126" stamp_final="1724765674.537288" username="apac4" allocationname="unknown" flags="0" pid="141896" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u18a</host>
<perf wtime="1.09655e+02" utime="8.92130e+01" stime="1.35594e+01" mtime="7.29411e+01" gflop="0.00000e+00" gbyte="3.78002e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.29411e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007a1441557a14741461" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09530e+02" utime="8.91824e+01" stime="1.35476e+01" mtime="7.29411e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.29411e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1867e+09" > 5.6345e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1914e+09" > 2.4680e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.5308e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9349e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4063e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4573e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2682e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0632e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6778e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3818e+01 </func>
</region>
</regions>
<internal rank="407" log_i="1724765674.537288" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="408" mpi_size="696" stamp_init="1724765564.733200" stamp_final="1724765674.523968" username="apac4" allocationname="unknown" flags="0" pid="3361266" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09791e+02" utime="5.47995e+01" stime="1.58971e+01" mtime="4.08905e+01" gflop="0.00000e+00" gbyte="3.86650e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.08905e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4831485148614d956861486147d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09663e+02" utime="5.47634e+01" stime="1.58909e+01" mtime="4.08905e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.08905e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2107e+09" > 6.7028e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2026e+09" > 3.7500e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0907e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.4854e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.8834e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.2605e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4573e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2663e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0563e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6769e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3072e+01 </func>
</region>
</regions>
<internal rank="408" log_i="1724765674.523968" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="409" mpi_size="696" stamp_init="1724765564.733163" stamp_final="1724765674.533698" username="apac4" allocationname="unknown" flags="0" pid="3361267" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09801e+02" utime="6.70876e+01" stime="9.13584e+00" mtime="4.56605e+01" gflop="0.00000e+00" gbyte="3.77861e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.56605e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b5151a55b515b4152b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09668e+02" utime="6.70569e+01" stime="9.12340e+00" mtime="4.56605e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.56605e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1935e+09" > 6.1908e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1905e+09" > 3.2536e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9195e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.2762e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6941e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.8614e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4579e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2593e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0575e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6388e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3676e+01 </func>
</region>
</regions>
<internal rank="409" log_i="1724765674.533698" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="410" mpi_size="696" stamp_init="1724765564.733393" stamp_final="1724765674.534350" username="apac4" allocationname="unknown" flags="0" pid="3361268" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09801e+02" utime="8.41987e+01" stime="1.67348e+01" mtime="7.13451e+01" gflop="0.00000e+00" gbyte="3.76553e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.13451e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002d142c14f4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09674e+02" utime="8.41630e+01" stime="1.67286e+01" mtime="7.13451e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.13451e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2050e+09" > 7.1393e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1954e+09" > 3.4425e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.2066e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.2570e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6070e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.3814e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4576e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2619e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0571e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6723e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3098e+01 </func>
</region>
</regions>
<internal rank="410" log_i="1724765674.534350" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="411" mpi_size="696" stamp_init="1724765564.733151" stamp_final="1724765674.538326" username="apac4" allocationname="unknown" flags="0" pid="3361269" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09805e+02" utime="7.52555e+01" stime="1.09312e+01" mtime="5.56463e+01" gflop="0.00000e+00" gbyte="3.76637e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.56463e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002115ea552115211500" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09675e+02" utime="7.52275e+01" stime="1.09176e+01" mtime="5.56463e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.56463e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1934e+09" > 6.0925e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1907e+09" > 2.9185e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9473e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.2574e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.9298e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4568e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2670e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0577e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6369e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3835e+01 </func>
</region>
</regions>
<internal rank="411" log_i="1724765674.538326" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="412" mpi_size="696" stamp_init="1724765564.733210" stamp_final="1724765674.523384" username="apac4" allocationname="unknown" flags="0" pid="3361270" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09790e+02" utime="6.03332e+01" stime="1.10394e+01" mtime="4.24106e+01" gflop="0.00000e+00" gbyte="3.75076e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.24106e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09661e+02" utime="6.03003e+01" stime="1.10306e+01" mtime="4.24106e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.24106e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1902e+09" > 7.7916e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1948e+09" > 4.9069e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6920e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.4714e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.7060e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.8900e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4564e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2720e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0548e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6719e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3062e+01 </func>
</region>
</regions>
<internal rank="412" log_i="1724765674.523384" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="413" mpi_size="696" stamp_init="1724765564.733894" stamp_final="1724765674.540789" username="apac4" allocationname="unknown" flags="0" pid="3361271" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09807e+02" utime="7.20226e+01" stime="1.02386e+01" mtime="5.18386e+01" gflop="0.00000e+00" gbyte="3.77045e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.18386e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004714c65547144214ac" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09678e+02" utime="7.19907e+01" stime="1.02285e+01" mtime="5.18386e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.18386e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2011e+09" > 6.2237e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2005e+09" > 3.0031e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8350e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.8878e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.0930e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4563e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2724e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0575e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6372e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3822e+01 </func>
</region>
</regions>
<internal rank="413" log_i="1724765674.540789" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="414" mpi_size="696" stamp_init="1724765564.733218" stamp_final="1724765674.528996" username="apac4" allocationname="unknown" flags="0" pid="3361272" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09796e+02" utime="7.99282e+01" stime="1.83003e+01" mtime="6.86512e+01" gflop="0.00000e+00" gbyte="3.77502e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.86512e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf449144a144b148a564b144b14e6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09670e+02" utime="7.99006e+01" stime="1.82860e+01" mtime="6.86512e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.86512e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2037e+09" > 6.7183e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2002e+09" > 3.7421e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0829e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.4795e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3383e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.6682e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4561e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2728e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0574e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6714e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3345e+01 </func>
</region>
</regions>
<internal rank="414" log_i="1724765674.528996" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="415" mpi_size="696" stamp_init="1724765564.733146" stamp_final="1724765674.529665" username="apac4" allocationname="unknown" flags="0" pid="3361273" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09797e+02" utime="5.77562e+01" stime="8.08504e+00" mtime="3.55840e+01" gflop="0.00000e+00" gbyte="3.78201e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.55840e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002d14b3552d142c1498" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09666e+02" utime="5.77224e+01" stime="8.07671e+00" mtime="3.55840e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.55840e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2118e+09" > 6.1264e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2170e+09" > 2.5549e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0147e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.4779e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4979e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4559e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2800e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0549e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6376e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3802e+01 </func>
</region>
</regions>
<internal rank="415" log_i="1724765674.529665" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="416" mpi_size="696" stamp_init="1724765564.733343" stamp_final="1724765674.534390" username="apac4" allocationname="unknown" flags="0" pid="3361274" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09801e+02" utime="8.22670e+01" stime="1.96217e+01" mtime="7.19428e+01" gflop="0.00000e+00" gbyte="3.76152e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19428e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000171451551714161471" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09675e+02" utime="8.22407e+01" stime="1.96060e+01" mtime="7.19428e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19428e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2116e+09" > 6.6227e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2160e+09" > 4.1925e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4116e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.8218e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6771e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1582e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4540e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2922e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0552e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6627e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3583e+01 </func>
</region>
</regions>
<internal rank="416" log_i="1724765674.534390" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="417" mpi_size="696" stamp_init="1724765564.733152" stamp_final="1724765674.527520" username="apac4" allocationname="unknown" flags="0" pid="3361275" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09794e+02" utime="7.25372e+01" stime="1.07734e+01" mtime="5.32618e+01" gflop="0.00000e+00" gbyte="3.77506e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.32618e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008515a955851585152c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09665e+02" utime="7.24976e+01" stime="1.07708e+01" mtime="5.32618e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.32618e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2023e+09" > 6.0885e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2035e+09" > 2.6259e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.3118e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.9681e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1206e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.1076e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4543e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2962e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0580e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6375e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3982e+01 </func>
</region>
</regions>
<internal rank="417" log_i="1724765674.527520" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="418" mpi_size="696" stamp_init="1724765564.734092" stamp_final="1724765674.529119" username="apac4" allocationname="unknown" flags="0" pid="3361276" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09795e+02" utime="6.15970e+01" stime="1.08252e+01" mtime="4.41852e+01" gflop="0.00000e+00" gbyte="3.77842e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.41852e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000821581151b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09671e+02" utime="6.15676e+01" stime="1.08129e+01" mtime="4.41852e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.41852e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1988e+09" > 8.5789e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1959e+09" > 4.9622e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.8363e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.1130e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.4359e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.7909e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4541e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2952e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0537e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6628e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2661e+01 </func>
</region>
</regions>
<internal rank="418" log_i="1724765674.529119" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="419" mpi_size="696" stamp_init="1724765564.733145" stamp_final="1724765674.521663" username="apac4" allocationname="unknown" flags="0" pid="3361277" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09789e+02" utime="8.01690e+01" stime="1.21489e+01" mtime="6.22450e+01" gflop="0.00000e+00" gbyte="3.77361e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.22450e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48f1490149114f25591149114fa" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09659e+02" utime="8.01354e+01" stime="1.21409e+01" mtime="6.22450e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.22450e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127200" bytes="1.1838e+09" > 6.0680e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1910e+09" > 2.4272e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0345e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.8820e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.2105e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4540e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.2997e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0584e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6368e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4108e+01 </func>
</region>
</regions>
<internal rank="419" log_i="1724765674.521663" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="420" mpi_size="696" stamp_init="1724765564.733232" stamp_final="1724765674.534138" username="apac4" allocationname="unknown" flags="0" pid="3361278" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09801e+02" utime="5.92511e+01" stime="1.12899e+01" mtime="4.25227e+01" gflop="0.00000e+00" gbyte="3.76427e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.25227e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008114ac558114801488" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09675e+02" utime="5.92278e+01" stime="1.12719e+01" mtime="4.25227e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.25227e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1869e+09" > 7.7722e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1817e+09" > 7.1060e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0738e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 4.3474e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.5323e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0059e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4537e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3024e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0538e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6495e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3069e+01 </func>
</region>
</regions>
<internal rank="420" log_i="1724765674.534138" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="421" mpi_size="696" stamp_init="1724765564.733156" stamp_final="1724765674.527989" username="apac4" allocationname="unknown" flags="0" pid="3361279" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09795e+02" utime="5.98078e+01" stime="8.16325e+00" mtime="3.74211e+01" gflop="0.00000e+00" gbyte="3.76492e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.74211e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b315b415b5153c55b515b5150a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09665e+02" utime="5.97681e+01" stime="8.16146e+00" mtime="3.74211e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.74211e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2001e+09" > 6.1392e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2083e+09" > 2.8510e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9342e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 4.3408e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5272e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0540e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4531e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3049e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0587e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6350e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3852e+01 </func>
</region>
</regions>
<internal rank="421" log_i="1724765674.527989" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="422" mpi_size="696" stamp_init="1724765564.733199" stamp_final="1724765674.534040" username="apac4" allocationname="unknown" flags="0" pid="3361280" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09801e+02" utime="8.29099e+01" stime="1.87935e+01" mtime="7.20953e+01" gflop="0.00000e+00" gbyte="3.74588e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20953e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d614d614f1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09677e+02" utime="8.28752e+01" stime="1.87869e+01" mtime="7.20953e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20953e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2073e+09" > 7.0282e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2083e+09" > 6.1868e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1098e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.9606e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0634e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.4570e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4532e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3072e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0566e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6476e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3968e+01 </func>
</region>
</regions>
<internal rank="422" log_i="1724765674.534040" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="423" mpi_size="696" stamp_init="1724765564.733160" stamp_final="1724765674.534663" username="apac4" allocationname="unknown" flags="0" pid="3361281" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09802e+02" utime="6.37907e+01" stime="8.81747e+00" mtime="4.21359e+01" gflop="0.00000e+00" gbyte="3.76102e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.21359e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003b15a4553b153a1519" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09671e+02" utime="6.37605e+01" stime="8.80520e+00" mtime="4.21359e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.21359e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1807e+09" > 6.0754e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1872e+09" > 2.7125e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0581e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.9403e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.4959e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4526e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3131e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0600e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6340e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3845e+01 </func>
</region>
</regions>
<internal rank="423" log_i="1724765674.534663" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="424" mpi_size="696" stamp_init="1724765564.733485" stamp_final="1724765674.529099" username="apac4" allocationname="unknown" flags="0" pid="3361282" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09796e+02" utime="6.11443e+01" stime="1.03509e+01" mtime="4.14588e+01" gflop="0.00000e+00" gbyte="3.77972e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.14588e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e114e114d4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09670e+02" utime="6.11090e+01" stime="1.03440e+01" mtime="4.14588e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.14588e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2110e+09" > 6.7363e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2145e+09" > 4.1038e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7472e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.8183e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.3179e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1539e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4518e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3175e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0584e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6483e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3209e+01 </func>
</region>
</regions>
<internal rank="424" log_i="1724765674.529099" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="425" mpi_size="696" stamp_init="1724765564.734079" stamp_final="1724765674.526291" username="apac4" allocationname="unknown" flags="0" pid="3361283" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09792e+02" utime="7.10701e+01" stime="1.02985e+01" mtime="5.03913e+01" gflop="0.00000e+00" gbyte="3.77895e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.03913e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09664e+02" utime="7.10386e+01" stime="1.02876e+01" mtime="5.03913e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.03913e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2155e+09" > 6.2151e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2227e+09" > 3.1797e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7185e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.7990e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.7184e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4511e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3253e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0547e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6348e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3375e+01 </func>
</region>
</regions>
<internal rank="425" log_i="1724765674.526291" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="426" mpi_size="696" stamp_init="1724765564.733330" stamp_final="1724765674.528699" username="apac4" allocationname="unknown" flags="0" pid="3361284" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09795e+02" utime="5.61550e+01" stime="8.64560e+00" mtime="3.51122e+01" gflop="0.00000e+00" gbyte="3.76945e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.51122e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000dd14d814e4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09667e+02" utime="5.61256e+01" stime="8.63344e+00" mtime="3.51122e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.51122e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2230e+09" > 7.3593e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2208e+09" > 4.1231e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8234e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.8208e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1086e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1971e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4519e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3209e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0537e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6464e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3039e+01 </func>
</region>
</regions>
<internal rank="426" log_i="1724765674.528699" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="427" mpi_size="696" stamp_init="1724765564.733759" stamp_final="1724765674.534507" username="apac4" allocationname="unknown" flags="0" pid="3361285" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09801e+02" utime="8.37173e+01" stime="1.32416e+01" mtime="6.66165e+01" gflop="0.00000e+00" gbyte="3.77773e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.66165e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4371438143914b9553914391456" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09670e+02" utime="8.36872e+01" stime="1.32297e+01" mtime="6.66165e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.66165e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.5763e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2265e+09" > 6.1483e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2212e+09" > 2.5444e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.3322e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.3357e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9789e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.0047e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4512e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3276e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0592e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6352e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3635e+01 </func>
</region>
</regions>
<internal rank="427" log_i="1724765674.534507" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="428" mpi_size="696" stamp_init="1724765564.733195" stamp_final="1724765674.538837" username="apac4" allocationname="unknown" flags="0" pid="3361286" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09806e+02" utime="6.01466e+01" stime="1.00069e+01" mtime="4.02541e+01" gflop="0.00000e+00" gbyte="3.77205e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.02541e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09678e+02" utime="6.01127e+01" stime="9.99781e+00" mtime="4.02541e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.02541e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2176e+09" > 7.0807e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2194e+09" > 3.6494e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9461e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.6654e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.4031e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.6502e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4516e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3234e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0550e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6459e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2844e+01 </func>
</region>
</regions>
<internal rank="428" log_i="1724765674.538837" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="429" mpi_size="696" stamp_init="1724765564.733172" stamp_final="1724765674.532730" username="apac4" allocationname="unknown" flags="0" pid="3361287" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09800e+02" utime="6.09533e+01" stime="8.37977e+00" mtime="3.83152e+01" gflop="0.00000e+00" gbyte="3.77571e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.83152e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09671e+02" utime="6.09197e+01" stime="8.37113e+00" mtime="3.83152e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.83152e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1947e+09" > 6.1531e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1916e+09" > 3.0656e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8210e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 5.7481e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0040e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4510e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3292e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0565e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6337e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3448e+01 </func>
</region>
</regions>
<internal rank="429" log_i="1724765674.532730" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="430" mpi_size="696" stamp_init="1724765564.733198" stamp_final="1724765674.534466" username="apac4" allocationname="unknown" flags="0" pid="3361288" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09801e+02" utime="5.68570e+01" stime="9.07035e+00" mtime="3.62912e+01" gflop="0.00000e+00" gbyte="3.77663e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.62912e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004a14ab564a144914f9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09672e+02" utime="5.68248e+01" stime="9.06014e+00" mtime="3.62912e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.62912e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2038e+09" > 7.9680e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1985e+09" > 4.5699e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7091e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.6644e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5532e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.0303e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4507e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3292e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0559e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6412e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3069e+01 </func>
</region>
</regions>
<internal rank="430" log_i="1724765674.534466" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="431" mpi_size="696" stamp_init="1724765564.733175" stamp_final="1724765674.534468" username="apac4" allocationname="unknown" flags="0" pid="3361289" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09a</host>
<perf wtime="1.09801e+02" utime="5.91228e+01" stime="8.17925e+00" mtime="3.68285e+01" gflop="0.00000e+00" gbyte="3.77346e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.68285e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f314f314cd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09672e+02" utime="5.90874e+01" stime="8.17271e+00" mtime="3.68285e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.68285e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1912e+09" > 6.5555e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1907e+09" > 3.0895e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0839e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.6474e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5286e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4505e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3355e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0571e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6344e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3759e+01 </func>
</region>
</regions>
<internal rank="431" log_i="1724765674.534468" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="432" mpi_size="696" stamp_init="1724765564.734926" stamp_final="1724765674.530159" username="apac4" allocationname="unknown" flags="0" pid="1021703" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09795e+02" utime="6.39166e+01" stime="2.13867e+01" mtime="5.58551e+01" gflop="0.00000e+00" gbyte="3.86200e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.58551e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004d154c153a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09669e+02" utime="6.38787e+01" stime="2.13827e+01" mtime="5.58551e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.58551e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2022e+09" > 8.1575e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2052e+09" > 5.2727e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5196e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.3673e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5637e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6774e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4499e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3396e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0570e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6310e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2809e+01 </func>
</region>
</regions>
<internal rank="432" log_i="1724765674.530159" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="433" mpi_size="696" stamp_init="1724765564.734827" stamp_final="1724765674.523019" username="apac4" allocationname="unknown" flags="0" pid="1021704" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09788e+02" utime="7.75189e+01" stime="1.11052e+01" mtime="5.78317e+01" gflop="0.00000e+00" gbyte="3.77041e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.78317e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d615a755d615d61521" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09655e+02" utime="7.74832e+01" stime="1.10978e+01" mtime="5.78317e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.78317e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1811e+09" > 6.0328e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1899e+09" > 3.0645e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.1377e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.4978e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8985e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4508e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3315e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0632e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6112e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3430e+01 </func>
</region>
</regions>
<internal rank="433" log_i="1724765674.523019" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="434" mpi_size="696" stamp_init="1724765564.734898" stamp_final="1724765674.533998" username="apac4" allocationname="unknown" flags="0" pid="1021705" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09799e+02" utime="6.12682e+01" stime="9.26467e+00" mtime="4.05690e+01" gflop="0.00000e+00" gbyte="3.76568e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.05690e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d514d51494" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09671e+02" utime="6.12364e+01" stime="9.25488e+00" mtime="4.05690e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.05690e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1965e+09" > 6.8622e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2064e+09" > 4.1760e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6362e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.3714e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.2956e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.7368e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4515e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3275e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0581e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6299e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3179e+01 </func>
</region>
</regions>
<internal rank="434" log_i="1724765674.533998" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="435" mpi_size="696" stamp_init="1724765564.734855" stamp_final="1724765674.534564" username="apac4" allocationname="unknown" flags="0" pid="1021706" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09800e+02" utime="7.79686e+01" stime="1.18771e+01" mtime="5.87446e+01" gflop="0.00000e+00" gbyte="3.76644e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.87446e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000029142914c9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09670e+02" utime="7.79311e+01" stime="1.18728e+01" mtime="5.87446e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.87446e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1925e+09" > 6.3355e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1954e+09" > 3.3107e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5800e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.6084e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.4196e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4501e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3385e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0611e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6088e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3763e+01 </func>
</region>
</regions>
<internal rank="435" log_i="1724765674.534564" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="436" mpi_size="696" stamp_init="1724765564.734907" stamp_final="1724765674.535062" username="apac4" allocationname="unknown" flags="0" pid="1021707" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09800e+02" utime="6.80724e+01" stime="1.04781e+01" mtime="4.81925e+01" gflop="0.00000e+00" gbyte="3.74840e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.81925e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4be15bf15c015e655c015c0154d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09674e+02" utime="6.80359e+01" stime="1.04726e+01" mtime="4.81925e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.81925e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2013e+09" > 7.5996e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1963e+09" > 4.8460e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2251e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.6211e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.7990e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4213e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4503e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3392e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0617e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6276e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3296e+01 </func>
</region>
</regions>
<internal rank="436" log_i="1724765674.535062" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="437" mpi_size="696" stamp_init="1724765564.734843" stamp_final="1724765674.534852" username="apac4" allocationname="unknown" flags="0" pid="1021708" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09800e+02" utime="8.35050e+01" stime="1.31620e+01" mtime="6.63721e+01" gflop="0.00000e+00" gbyte="3.78086e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.63721e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007315731530" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09673e+02" utime="8.34697e+01" stime="1.31547e+01" mtime="6.63721e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.63721e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2004e+09" > 6.5276e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2004e+09" > 3.5530e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0828e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.2966e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0014e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1420e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4502e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3400e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0609e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6087e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3983e+01 </func>
</region>
</regions>
<internal rank="437" log_i="1724765674.534852" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="438" mpi_size="696" stamp_init="1724765564.734875" stamp_final="1724765674.540203" username="apac4" allocationname="unknown" flags="0" pid="1021709" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09805e+02" utime="8.44012e+01" stime="1.67590e+01" mtime="7.17503e+01" gflop="0.00000e+00" gbyte="3.77026e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17503e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a414a414bb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09676e+02" utime="8.43659e+01" stime="1.67521e+01" mtime="7.17503e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17503e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2006e+09" > 7.2532e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1977e+09" > 4.7967e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6800e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.6749e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2718e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0442e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4500e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3424e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0590e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6274e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3178e+01 </func>
</region>
</regions>
<internal rank="438" log_i="1724765674.540203" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="439" mpi_size="696" stamp_init="1724765564.734852" stamp_final="1724765674.530195" username="apac4" allocationname="unknown" flags="0" pid="1021710" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09795e+02" utime="7.83667e+01" stime="1.20235e+01" mtime="5.95105e+01" gflop="0.00000e+00" gbyte="3.78155e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.95105e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000bf14bf14eb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09666e+02" utime="7.83278e+01" stime="1.20201e+01" mtime="5.95105e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.95105e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2173e+09" > 6.3700e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2175e+09" > 3.3660e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6769e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.6725e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.5878e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4500e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3393e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0624e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6076e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3787e+01 </func>
</region>
</regions>
<internal rank="439" log_i="1724765674.530195" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="440" mpi_size="696" stamp_init="1724765564.734894" stamp_final="1724765674.531848" username="apac4" allocationname="unknown" flags="0" pid="1021711" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09797e+02" utime="6.35694e+01" stime="1.01720e+01" mtime="4.39458e+01" gflop="0.00000e+00" gbyte="3.78269e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.39458e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09668e+02" utime="6.35342e+01" stime="1.01648e+01" mtime="4.39458e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.39458e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2125e+09" > 7.1185e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2030e+09" > 4.3402e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7034e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.8218e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6472e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0011e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4497e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3471e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0579e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6252e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3366e+01 </func>
</region>
</regions>
<internal rank="440" log_i="1724765674.531848" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="441" mpi_size="696" stamp_init="1724765564.734824" stamp_final="1724765674.523386" username="apac4" allocationname="unknown" flags="0" pid="1021712" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09789e+02" utime="7.53426e+01" stime="1.11851e+01" mtime="5.58244e+01" gflop="0.00000e+00" gbyte="3.77777e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.58244e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09661e+02" utime="7.53053e+01" stime="1.11801e+01" mtime="5.58244e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.58244e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2038e+09" > 6.5584e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2014e+09" > 3.2140e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3366e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.2839e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.0081e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4491e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3533e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0573e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6048e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4310e+01 </func>
</region>
</regions>
<internal rank="441" log_i="1724765674.523386" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="442" mpi_size="696" stamp_init="1724765564.734871" stamp_final="1724765674.530531" username="apac4" allocationname="unknown" flags="0" pid="1021713" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09796e+02" utime="5.93581e+01" stime="1.05909e+01" mtime="4.18857e+01" gflop="0.00000e+00" gbyte="3.77670e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.18857e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09668e+02" utime="5.93252e+01" stime="1.05818e+01" mtime="4.18857e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.18857e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2048e+09" > 1.0918e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2053e+09" > 7.7142e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3142e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.8208e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.8876e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1418e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4457e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3652e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0575e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6222e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2932e+01 </func>
</region>
</regions>
<internal rank="442" log_i="1724765674.530531" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="443" mpi_size="696" stamp_init="1724765564.734843" stamp_final="1724765674.523978" username="apac4" allocationname="unknown" flags="0" pid="1021714" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09789e+02" utime="6.79895e+01" stime="1.01222e+01" mtime="4.78739e+01" gflop="0.00000e+00" gbyte="3.76251e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.78739e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09659e+02" utime="6.79570e+01" stime="1.01122e+01" mtime="4.78739e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.78739e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1838e+09" > 5.9196e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1851e+09" > 3.4307e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.1431e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.4305e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.9904e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4482e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3573e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0599e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5962e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4148e+01 </func>
</region>
</regions>
<internal rank="443" log_i="1724765674.523978" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="444" mpi_size="696" stamp_init="1724765564.734909" stamp_final="1724765674.537692" username="apac4" allocationname="unknown" flags="0" pid="1021715" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09803e+02" utime="6.19609e+01" stime="1.14453e+01" mtime="4.47785e+01" gflop="0.00000e+00" gbyte="3.77937e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.47785e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09673e+02" utime="6.19306e+01" stime="1.14335e+01" mtime="4.47785e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.47785e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1806e+09" > 8.5070e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1880e+09" > 4.6883e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4701e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0041e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5310e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.0342e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4469e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3745e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0574e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6201e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3182e+01 </func>
</region>
</regions>
<internal rank="444" log_i="1724765674.537692" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="445" mpi_size="696" stamp_init="1724765564.734836" stamp_final="1724765674.523526" username="apac4" allocationname="unknown" flags="0" pid="1021716" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09789e+02" utime="8.71402e+01" stime="1.39108e+01" mtime="7.10388e+01" gflop="0.00000e+00" gbyte="3.77907e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.10388e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09659e+02" utime="8.71087e+01" stime="1.39004e+01" mtime="7.10388e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.10388e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1966e+09" > 5.9822e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1947e+09" > 3.0609e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.2488e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.7395e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3430e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4477e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3650e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0598e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5956e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4115e+01 </func>
</region>
</regions>
<internal rank="445" log_i="1724765674.523526" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="446" mpi_size="696" stamp_init="1724765564.734861" stamp_final="1724765674.532540" username="apac4" allocationname="unknown" flags="0" pid="1021717" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09798e+02" utime="6.31265e+01" stime="9.42417e+00" mtime="4.24260e+01" gflop="0.00000e+00" gbyte="3.77754e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.24260e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008a14c6558a14891460" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09669e+02" utime="6.30869e+01" stime="9.42163e+00" mtime="4.24260e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.24260e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2103e+09" > 7.2425e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1989e+09" > 3.6629e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1718e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0040e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.1975e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.0452e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4475e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3681e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0586e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6200e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3797e+01 </func>
</region>
</regions>
<internal rank="446" log_i="1724765674.532540" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="447" mpi_size="696" stamp_init="1724765564.734803" stamp_final="1724765674.529104" username="apac4" allocationname="unknown" flags="0" pid="1021718" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09794e+02" utime="8.84079e+01" stime="1.43070e+01" mtime="7.20948e+01" gflop="0.00000e+00" gbyte="3.78120e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20948e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000331515553315331521" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09664e+02" utime="8.83752e+01" stime="1.42973e+01" mtime="7.20948e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20948e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1886e+09" > 6.1985e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1834e+09" > 3.2362e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8714e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9031e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1926e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4469e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3702e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0600e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5949e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3888e+01 </func>
</region>
</regions>
<internal rank="447" log_i="1724765674.529104" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="448" mpi_size="696" stamp_init="1724765564.734924" stamp_final="1724765674.530348" username="apac4" allocationname="unknown" flags="0" pid="1021719" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09795e+02" utime="8.16333e+01" stime="2.02035e+01" mtime="7.14853e+01" gflop="0.00000e+00" gbyte="3.77808e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.14853e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008514595685148514c2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09666e+02" utime="8.16015e+01" stime="2.01930e+01" mtime="7.14853e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.14853e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.7752e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2163e+09" > 7.0705e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2121e+09" > 3.7757e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6646e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 4.1279e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5333e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.8995e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4441e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3988e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0605e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6204e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2963e+01 </func>
</region>
</regions>
<internal rank="448" log_i="1724765674.530348" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="449" mpi_size="696" stamp_init="1724765564.734822" stamp_final="1724765674.534402" username="apac4" allocationname="unknown" flags="0" pid="1021720" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09800e+02" utime="7.07334e+01" stime="1.05271e+01" mtime="5.06890e+01" gflop="0.00000e+00" gbyte="3.77033e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.06890e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a414a41496" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09672e+02" utime="7.07002e+01" stime="1.05183e+01" mtime="5.06890e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.06890e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2086e+09" > 6.4693e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2164e+09" > 3.2317e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.1908e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.7533e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1642e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4451e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3932e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0624e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5450e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3635e+01 </func>
</region>
</regions>
<internal rank="449" log_i="1724765674.534402" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="450" mpi_size="696" stamp_init="1724765564.734906" stamp_final="1724765674.537635" username="apac4" allocationname="unknown" flags="0" pid="1021721" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09803e+02" utime="7.62578e+01" stime="1.44212e+01" mtime="6.03428e+01" gflop="0.00000e+00" gbyte="3.75698e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.03428e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ad15ac150e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09678e+02" utime="7.62241e+01" stime="1.44121e+01" mtime="6.03428e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.03428e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.5831e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2133e+09" > 7.4364e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2246e+09" > 4.8244e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6869e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.0295e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.0408e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.5389e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4445e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3951e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0631e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6194e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2875e+01 </func>
</region>
</regions>
<internal rank="450" log_i="1724765674.537635" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="451" mpi_size="696" stamp_init="1724765564.734831" stamp_final="1724765674.538178" username="apac4" allocationname="unknown" flags="0" pid="1021722" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09803e+02" utime="7.28801e+01" stime="1.11633e+01" mtime="5.34229e+01" gflop="0.00000e+00" gbyte="3.77983e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.34229e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000036143614e0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09675e+02" utime="7.28494e+01" stime="1.11512e+01" mtime="5.34229e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.34229e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2230e+09" > 5.8863e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2260e+09" > 3.3180e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.1896e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.0274e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6040e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4443e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.3969e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0633e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5960e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3685e+01 </func>
</region>
</regions>
<internal rank="451" log_i="1724765674.538178" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="452" mpi_size="696" stamp_init="1724765564.734931" stamp_final="1724765674.530878" username="apac4" allocationname="unknown" flags="0" pid="1021723" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09796e+02" utime="6.14024e+01" stime="1.01013e+01" mtime="4.19404e+01" gflop="0.00000e+00" gbyte="3.76709e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.19404e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf429142a142b1443562b142b1496" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09668e+02" utime="6.13712e+01" stime="1.00906e+01" mtime="4.19404e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.19404e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2163e+09" > 7.4603e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2167e+09" > 5.7496e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0790e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.6149e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.0081e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.6645e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4439e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4012e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0586e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6190e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2792e+01 </func>
</region>
</regions>
<internal rank="452" log_i="1724765674.530878" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="453" mpi_size="696" stamp_init="1724765564.734818" stamp_final="1724765674.533803" username="apac4" allocationname="unknown" flags="0" pid="1021724" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09799e+02" utime="7.37724e+01" stime="1.14088e+01" mtime="5.45609e+01" gflop="0.00000e+00" gbyte="3.76987e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.45609e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000076152855761576152c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09672e+02" utime="7.37422e+01" stime="1.13962e+01" mtime="5.45609e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.45609e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1951e+09" > 6.4416e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2000e+09" > 3.1251e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.2160e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.1614e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7895e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1308e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4437e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4039e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0639e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5946e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3405e+01 </func>
</region>
</regions>
<internal rank="453" log_i="1724765674.533803" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="454" mpi_size="696" stamp_init="1724765564.735146" stamp_final="1724765674.534286" username="apac4" allocationname="unknown" flags="0" pid="1021725" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09799e+02" utime="6.57628e+01" stime="1.14757e+01" mtime="4.69100e+01" gflop="0.00000e+00" gbyte="3.76736e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.69100e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000581583555815581538" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09671e+02" utime="6.57316e+01" stime="1.14651e+01" mtime="4.69100e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.69100e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1979e+09" > 7.7279e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2026e+09" > 4.7245e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3136e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.6527e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.5027e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.6580e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4437e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4064e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0579e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.6123e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2965e+01 </func>
</region>
</regions>
<internal rank="454" log_i="1724765674.534286" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="455" mpi_size="696" stamp_init="1724765564.734813" stamp_final="1724765674.525440" username="apac4" allocationname="unknown" flags="0" pid="1021726" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u12b</host>
<perf wtime="1.09791e+02" utime="6.40152e+01" stime="9.45183e+00" mtime="4.28366e+01" gflop="0.00000e+00" gbyte="3.75980e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.28366e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a714a614c0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09661e+02" utime="6.39801e+01" stime="9.44442e+00" mtime="4.28366e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.28366e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.0252e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1960e+09" > 6.8629e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1946e+09" > 3.4597e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0597e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.6590e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.6923e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4435e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4087e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0633e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5966e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3741e+01 </func>
</region>
</regions>
<internal rank="455" log_i="1724765674.525440" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="456" mpi_size="696" stamp_init="1724765564.822695" stamp_final="1724765674.527239" username="apac4" allocationname="unknown" flags="0" pid="897231" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09705e+02" utime="7.34946e+01" stime="2.61917e+01" mtime="6.87989e+01" gflop="0.00000e+00" gbyte="3.86616e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.87989e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009214835692149214f3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09581e+02" utime="7.34622e+01" stime="2.61822e+01" mtime="6.87989e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.87989e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2002e+09" > 7.6749e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1985e+09" > 3.4804e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4015e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.4309e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8858e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1005e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4430e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4133e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0620e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5893e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2655e+01 </func>
</region>
</regions>
<internal rank="456" log_i="1724765674.527239" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="457" mpi_size="696" stamp_init="1724765564.821389" stamp_final="1724765674.529964" username="apac4" allocationname="unknown" flags="0" pid="897232" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09709e+02" utime="6.32497e+01" stime="9.20094e+00" mtime="4.19268e+01" gflop="0.00000e+00" gbyte="3.77937e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.19268e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a715ed55a715a61511" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09579e+02" utime="6.32153e+01" stime="9.19202e+00" mtime="4.19268e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.19268e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1966e+09" > 7.1391e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1922e+09" > 2.9135e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.8337e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.4283e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9789e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1990e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4434e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4080e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0568e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5638e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3270e+01 </func>
</region>
</regions>
<internal rank="457" log_i="1724765674.529964" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="458" mpi_size="696" stamp_init="1724765564.822899" stamp_final="1724765674.527919" username="apac4" allocationname="unknown" flags="0" pid="897233" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09705e+02" utime="6.21479e+01" stime="9.92433e+00" mtime="4.07819e+01" gflop="0.00000e+00" gbyte="3.76209e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.07819e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09580e+02" utime="6.21156e+01" stime="9.91444e+00" mtime="4.07819e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.07819e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2042e+09" > 7.7896e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2109e+09" > 3.5006e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3795e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.8945e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.1082e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5389e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4428e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4106e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0612e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5888e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2327e+01 </func>
</region>
</regions>
<internal rank="458" log_i="1724765674.527919" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="459" mpi_size="696" stamp_init="1724765564.822247" stamp_final="1724765674.536167" username="apac4" allocationname="unknown" flags="0" pid="897234" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09714e+02" utime="6.34840e+01" stime="9.44281e+00" mtime="4.17192e+01" gflop="0.00000e+00" gbyte="3.78086e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.17192e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09588e+02" utime="6.34527e+01" stime="9.43175e+00" mtime="4.17192e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.17192e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1971e+09" > 6.3985e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1921e+09" > 2.5011e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.2923e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.9039e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5487e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4432e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4103e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0615e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5582e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3202e+01 </func>
</region>
</regions>
<internal rank="459" log_i="1724765674.536167" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="460" mpi_size="696" stamp_init="1724765564.823328" stamp_final="1724765674.527769" username="apac4" allocationname="unknown" flags="0" pid="897235" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09704e+02" utime="7.20244e+01" stime="1.27463e+01" mtime="5.45086e+01" gflop="0.00000e+00" gbyte="3.78395e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.45086e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09579e+02" utime="7.19943e+01" stime="1.27341e+01" mtime="5.45086e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.45086e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2067e+09" > 7.4924e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2130e+09" > 3.7776e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.1875e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.9493e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9118e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4203e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4420e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4225e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0579e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5852e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2478e+01 </func>
</region>
</regions>
<internal rank="460" log_i="1724765674.527769" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="461" mpi_size="696" stamp_init="1724765564.821694" stamp_final="1724765674.534217" username="apac4" allocationname="unknown" flags="0" pid="897236" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09713e+02" utime="7.89986e+01" stime="1.18431e+01" mtime="6.03941e+01" gflop="0.00000e+00" gbyte="3.76835e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.03941e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004a1549153b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09584e+02" utime="7.89661e+01" stime="1.18342e+01" mtime="6.03941e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.03941e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2083e+09" > 5.8785e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2017e+09" > 2.6560e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.4744e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.7097e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1149e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4420e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4223e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0566e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5581e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3547e+01 </func>
</region>
</regions>
<internal rank="461" log_i="1724765674.534217" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="462" mpi_size="696" stamp_init="1724765564.822202" stamp_final="1724765674.527303" username="apac4" allocationname="unknown" flags="0" pid="897237" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09705e+02" utime="6.96971e+01" stime="1.19766e+01" mtime="5.17824e+01" gflop="0.00000e+00" gbyte="3.77098e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.17824e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09582e+02" utime="6.96609e+01" stime="1.19710e+01" mtime="5.17824e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.17824e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1981e+09" > 7.3207e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2033e+09" > 3.8218e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.3985e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.9484e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.5790e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.8614e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4418e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4203e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0568e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5807e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2479e+01 </func>
</region>
</regions>
<internal rank="462" log_i="1724765674.527303" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="463" mpi_size="696" stamp_init="1724765564.823574" stamp_final="1724765674.537607" username="apac4" allocationname="unknown" flags="0" pid="897238" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09714e+02" utime="7.77170e+01" stime="1.16177e+01" mtime="5.78839e+01" gflop="0.00000e+00" gbyte="3.77449e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.78839e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009514951477" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09588e+02" utime="7.76850e+01" stime="1.16074e+01" mtime="5.78839e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.78839e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2139e+09" > 6.2014e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2152e+09" > 2.7237e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6525e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.5395e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.3361e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4415e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4246e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0580e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5376e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3538e+01 </func>
</region>
</regions>
<internal rank="463" log_i="1724765674.537607" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="464" mpi_size="696" stamp_init="1724765564.822058" stamp_final="1724765674.530966" username="apac4" allocationname="unknown" flags="0" pid="897239" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09709e+02" utime="5.52129e+01" stime="9.64015e+00" mtime="3.71901e+01" gflop="0.00000e+00" gbyte="3.76614e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.71901e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003b15b8553b153b1537" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09583e+02" utime="5.51832e+01" stime="9.62810e+00" mtime="3.71901e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.71901e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.7752e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1923e+09" > 9.5824e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1992e+09" > 4.6550e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.2947e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 4.1220e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.0136e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.2214e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4389e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4440e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0569e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5810e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2217e+01 </func>
</region>
</regions>
<internal rank="464" log_i="1724765674.530966" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="465" mpi_size="696" stamp_init="1724765564.823619" stamp_final="1724765674.526493" username="apac4" allocationname="unknown" flags="0" pid="897240" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09703e+02" utime="7.81082e+01" stime="1.21934e+01" mtime="5.91523e+01" gflop="0.00000e+00" gbyte="3.77209e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.91523e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000dd14c255dd14dd14f0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09578e+02" utime="7.80710e+01" stime="1.21891e+01" mtime="5.91523e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.91523e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127200" bytes="1.1989e+09" > 6.4413e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2030e+09" > 2.3945e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9800e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.6293e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7394e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4401e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4430e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0623e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5348e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3560e+01 </func>
</region>
</regions>
<internal rank="465" log_i="1724765674.526493" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="466" mpi_size="696" stamp_init="1724765564.823657" stamp_final="1724765674.530890" username="apac4" allocationname="unknown" flags="0" pid="897241" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09707e+02" utime="8.30913e+01" stime="1.91635e+01" mtime="7.18761e+01" gflop="0.00000e+00" gbyte="3.77502e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18761e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09582e+02" utime="8.30575e+01" stime="1.91554e+01" mtime="7.18761e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18761e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2095e+09" > 7.0095e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2055e+09" > 3.4061e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9254e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0054e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9584e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9911e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4396e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4476e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0583e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5799e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2926e+01 </func>
</region>
</regions>
<internal rank="466" log_i="1724765674.530890" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="467" mpi_size="696" stamp_init="1724765564.821375" stamp_final="1724765674.527026" username="apac4" allocationname="unknown" flags="0" pid="897242" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09706e+02" utime="6.50224e+01" stime="9.10915e+00" mtime="4.28121e+01" gflop="0.00000e+00" gbyte="3.76644e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.28121e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000181518153d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09579e+02" utime="6.49912e+01" stime="9.09879e+00" mtime="4.28121e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.28121e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1865e+09" > 5.9616e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1905e+09" > 2.8667e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7641e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0057e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4080e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3893e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4392e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4478e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0593e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5320e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3715e+01 </func>
</region>
</regions>
<internal rank="467" log_i="1724765674.527026" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="468" mpi_size="696" stamp_init="1724765564.821377" stamp_final="1724765674.534941" username="apac4" allocationname="unknown" flags="0" pid="897243" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09714e+02" utime="7.52109e+01" stime="1.65335e+01" mtime="6.24083e+01" gflop="0.00000e+00" gbyte="3.76995e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.24083e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09592e+02" utime="7.51788e+01" stime="1.65237e+01" mtime="6.24083e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.24083e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1897e+09" > 7.5440e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1878e+09" > 3.7297e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6420e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 4.1407e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6366e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5695e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4398e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4453e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0613e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5796e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2816e+01 </func>
</region>
</regions>
<internal rank="468" log_i="1724765674.534941" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="469" mpi_size="696" stamp_init="1724765564.822100" stamp_final="1724765674.531053" username="apac4" allocationname="unknown" flags="0" pid="897244" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09709e+02" utime="5.97260e+01" stime="8.35946e+00" mtime="3.72187e+01" gflop="0.00000e+00" gbyte="3.77903e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.72187e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09582e+02" utime="5.96953e+01" stime="8.34761e+00" mtime="3.72187e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.72187e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1936e+09" > 6.2988e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1929e+09" > 2.5648e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.2513e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 4.1361e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4796e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8579e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4397e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4461e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0625e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5316e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3525e+01 </func>
</region>
</regions>
<internal rank="469" log_i="1724765674.531053" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="470" mpi_size="696" stamp_init="1724765564.821796" stamp_final="1724765674.530798" username="apac4" allocationname="unknown" flags="0" pid="897245" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09709e+02" utime="6.73271e+01" stime="1.24418e+01" mtime="5.01529e+01" gflop="0.00000e+00" gbyte="3.76820e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.01529e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09587e+02" utime="6.72914e+01" stime="1.24355e+01" mtime="5.01529e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.01529e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1919e+09" > 7.8850e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1962e+09" > 3.5892e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.4069e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.3910e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.9509e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0312e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4386e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4538e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0578e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5785e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2925e+01 </func>
</region>
</regions>
<internal rank="470" log_i="1724765674.530798" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="471" mpi_size="696" stamp_init="1724765564.823605" stamp_final="1724765674.533243" username="apac4" allocationname="unknown" flags="0" pid="897246" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09710e+02" utime="6.42739e+01" stime="9.13154e+00" mtime="4.31117e+01" gflop="0.00000e+00" gbyte="3.77094e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.31117e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000be15c456be15be1552" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09584e+02" utime="6.42391e+01" stime="9.12460e+00" mtime="4.31117e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.31117e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1868e+09" > 6.0868e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1934e+09" > 2.3235e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.7805e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.3601e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0689e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4386e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4571e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0608e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5262e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3685e+01 </func>
</region>
</regions>
<internal rank="471" log_i="1724765674.533243" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="472" mpi_size="696" stamp_init="1724765564.823375" stamp_final="1724765674.528398" username="apac4" allocationname="unknown" flags="0" pid="897247" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09705e+02" utime="5.69686e+01" stime="9.16967e+00" mtime="3.66125e+01" gflop="0.00000e+00" gbyte="3.78059e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.66125e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09578e+02" utime="5.69348e+01" stime="9.16133e+00" mtime="3.66125e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.66125e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2118e+09" > 8.0341e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2135e+09" > 4.2704e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.5307e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 4.4922e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5463e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.1590e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4381e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4620e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0620e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5794e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2092e+01 </func>
</region>
</regions>
<internal rank="472" log_i="1724765674.528398" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="473" mpi_size="696" stamp_init="1724765564.823542" stamp_final="1724765674.533324" username="apac4" allocationname="unknown" flags="0" pid="897248" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09710e+02" utime="6.49208e+01" stime="9.43084e+00" mtime="4.34762e+01" gflop="0.00000e+00" gbyte="3.74466e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.34762e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004f144a148d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09585e+02" utime="6.48872e+01" stime="9.42289e+00" mtime="4.34762e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.34762e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2115e+09" > 6.4300e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2022e+09" > 3.0231e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 8.1354e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0304e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0041e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3839e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4379e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4646e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0617e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5049e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2711e+01 </func>
</region>
</regions>
<internal rank="473" log_i="1724765674.533324" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="474" mpi_size="696" stamp_init="1724765564.821375" stamp_final="1724765674.531128" username="apac4" allocationname="unknown" flags="0" pid="897249" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09710e+02" utime="5.83785e+01" stime="9.54357e+00" mtime="3.80129e+01" gflop="0.00000e+00" gbyte="3.77213e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.80129e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4371538153915d6553915391504" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09589e+02" utime="5.83480e+01" stime="9.53199e+00" mtime="3.80129e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.80129e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2186e+09" > 8.4709e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2088e+09" > 5.1064e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.2198e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 6.1992e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.1553e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0161e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4375e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4651e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0589e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5724e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2045e+01 </func>
</region>
</regions>
<internal rank="474" log_i="1724765674.531128" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="475" mpi_size="696" stamp_init="1724765564.823495" stamp_final="1724765674.531062" username="apac4" allocationname="unknown" flags="0" pid="897250" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09708e+02" utime="7.01329e+01" stime="1.01477e+01" mtime="4.90479e+01" gflop="0.00000e+00" gbyte="3.77636e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.90479e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c214c414c514a355c514c51470" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09582e+02" utime="7.00988e+01" stime="1.01393e+01" mtime="4.90479e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.90479e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2207e+09" > 6.1175e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2200e+09" > 2.9907e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0750e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.6402e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5272e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.6669e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4364e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4755e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0592e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5046e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3279e+01 </func>
</region>
</regions>
<internal rank="475" log_i="1724765674.531062" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="476" mpi_size="696" stamp_init="1724765564.821378" stamp_final="1724765674.528419" username="apac4" allocationname="unknown" flags="0" pid="897251" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09707e+02" utime="7.80514e+01" stime="1.71014e+01" mtime="6.48609e+01" gflop="0.00000e+00" gbyte="3.77586e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.48609e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004d145c554d144d149b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09585e+02" utime="7.80186e+01" stime="1.70921e+01" mtime="6.48609e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.48609e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2118e+09" > 8.6749e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2102e+09" > 4.0044e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.3011e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.2249e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4509e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.0603e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4362e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4812e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0607e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5686e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2261e+01 </func>
</region>
</regions>
<internal rank="476" log_i="1724765674.528419" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="477" mpi_size="696" stamp_init="1724765564.823159" stamp_final="1724765674.540931" username="apac4" allocationname="unknown" flags="0" pid="897252" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09718e+02" utime="6.74782e+01" stime="9.36945e+00" mtime="4.51069e+01" gflop="0.00000e+00" gbyte="3.78395e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.51069e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b114d355b114b11455" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09591e+02" utime="6.74447e+01" stime="9.36068e+00" mtime="4.51069e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.51069e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2015e+09" > 6.4991e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2008e+09" > 2.8087e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9420e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.2932e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7895e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3705e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4365e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4753e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0602e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5035e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2905e+01 </func>
</region>
</regions>
<internal rank="477" log_i="1724765674.540931" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="478" mpi_size="696" stamp_init="1724765564.821375" stamp_final="1724765674.534306" username="apac4" allocationname="unknown" flags="0" pid="897253" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09713e+02" utime="6.08551e+01" stime="9.61550e+00" mtime="3.99378e+01" gflop="0.00000e+00" gbyte="3.75046e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.99378e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09589e+02" utime="6.08252e+01" stime="9.60312e+00" mtime="3.99378e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.99378e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2006e+09" > 8.5983e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1928e+09" > 4.2064e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.3633e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.2306e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.5075e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.1094e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4365e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4771e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0580e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5682e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.1784e+01 </func>
</region>
</regions>
<internal rank="478" log_i="1724765674.534306" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="479" mpi_size="696" stamp_init="1724765564.821787" stamp_final="1724765674.537532" username="apac4" allocationname="unknown" flags="0" pid="897254" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u19a</host>
<perf wtime="1.09716e+02" utime="8.98184e+01" stime="1.36148e+01" mtime="7.20586e+01" gflop="0.00000e+00" gbyte="3.75610e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20586e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09588e+02" utime="8.97888e+01" stime="1.36021e+01" mtime="7.20586e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20586e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1977e+09" > 6.4916e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1954e+09" > 2.8755e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5273e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9539e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.4094e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0850e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4352e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4874e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0574e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5020e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3664e+01 </func>
</region>
</regions>
<internal rank="479" log_i="1724765674.537532" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="480" mpi_size="696" stamp_init="1724765564.851254" stamp_final="1724765674.534525" username="apac4" allocationname="unknown" flags="0" pid="1908095" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09683e+02" utime="7.56953e+01" stime="2.56853e+01" mtime="7.11954e+01" gflop="0.00000e+00" gbyte="3.86246e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.11954e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4951497149814aa55981498147b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09558e+02" utime="7.56609e+01" stime="2.56778e+01" mtime="7.11954e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.11954e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1982e+09" > 6.5209e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1901e+09" > 4.2546e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3187e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.0500e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0447e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.2964e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4371e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4751e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0539e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4941e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3031e+01 </func>
</region>
</regions>
<internal rank="480" log_i="1724765674.534525" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="481" mpi_size="696" stamp_init="1724765564.851296" stamp_final="1724765674.525560" username="apac4" allocationname="unknown" flags="0" pid="1908096" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09674e+02" utime="8.94456e+01" stime="1.33668e+01" mtime="7.24063e+01" gflop="0.00000e+00" gbyte="3.75965e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24063e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09543e+02" utime="8.94194e+01" stime="1.33495e+01" mtime="7.24063e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24063e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2021e+09" > 5.9594e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2031e+09" > 3.2539e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0229e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9495e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0585e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4372e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4732e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0574e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3466e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3600e+01 </func>
</region>
</regions>
<internal rank="481" log_i="1724765674.525560" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="482" mpi_size="696" stamp_init="1724765564.851226" stamp_final="1724765674.524236" username="apac4" allocationname="unknown" flags="0" pid="1908097" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09673e+02" utime="8.79643e+01" stime="1.37512e+01" mtime="7.16089e+01" gflop="0.00000e+00" gbyte="3.77247e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16089e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09549e+02" utime="8.79353e+01" stime="1.37381e+01" mtime="7.16089e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16089e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2173e+09" > 6.9546e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2098e+09" > 3.7664e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.1195e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9482e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.7459e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0738e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4358e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4801e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0579e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4876e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2614e+01 </func>
</region>
</regions>
<internal rank="482" log_i="1724765674.524236" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="483" mpi_size="696" stamp_init="1724765564.851275" stamp_final="1724765674.535657" username="apac4" allocationname="unknown" flags="0" pid="1908098" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09684e+02" utime="8.91288e+01" stime="1.36874e+01" mtime="7.29818e+01" gflop="0.00000e+00" gbyte="3.77819e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.29818e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4821484148514b9558514841477" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09559e+02" utime="8.90956e+01" stime="1.36786e+01" mtime="7.29818e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.29818e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1965e+09" > 5.6845e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1975e+09" > 3.1752e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.4962e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9502e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1289e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4367e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4787e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0556e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3456e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3733e+01 </func>
</region>
</regions>
<internal rank="483" log_i="1724765674.535657" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="484" mpi_size="696" stamp_init="1724765564.852237" stamp_final="1724765674.530224" username="apac4" allocationname="unknown" flags="0" pid="1908099" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09678e+02" utime="8.63867e+01" stime="1.45067e+01" mtime="7.16780e+01" gflop="0.00000e+00" gbyte="3.77312e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16780e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000e3151655e315e31530" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09558e+02" utime="8.63517e+01" stime="1.45002e+01" mtime="7.16780e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16780e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.0252e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2159e+09" > 6.9535e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2153e+09" > 4.3327e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7703e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9462e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.3728e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0603e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4365e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4809e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0564e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4677e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3042e+01 </func>
</region>
</regions>
<internal rank="484" log_i="1724765674.530224" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="485" mpi_size="696" stamp_init="1724765564.851289" stamp_final="1724765674.528299" username="apac4" allocationname="unknown" flags="0" pid="1908100" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09677e+02" utime="8.90373e+01" stime="1.39048e+01" mtime="7.23927e+01" gflop="0.00000e+00" gbyte="3.77190e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23927e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09552e+02" utime="8.90073e+01" stime="1.38922e+01" mtime="7.23927e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23927e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1968e+09" > 5.8207e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2175e+09" > 3.1890e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7246e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9488e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7643e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0767e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4354e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4878e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0597e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3405e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3907e+01 </func>
</region>
</regions>
<internal rank="485" log_i="1724765674.528299" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="486" mpi_size="696" stamp_init="1724765564.851865" stamp_final="1724765674.530306" username="apac4" allocationname="unknown" flags="0" pid="1908101" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09678e+02" utime="8.60709e+01" stime="1.44283e+01" mtime="7.10225e+01" gflop="0.00000e+00" gbyte="3.76411e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.10225e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09555e+02" utime="8.60367e+01" stime="1.44200e+01" mtime="7.10225e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.10225e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.4067e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2004e+09" > 7.6364e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1967e+09" > 4.5169e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2411e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9494e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9705e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0752e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4348e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4959e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0530e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4678e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2817e+01 </func>
</region>
</regions>
<internal rank="486" log_i="1724765674.530306" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="487" mpi_size="696" stamp_init="1724765564.851306" stamp_final="1724765674.528746" username="apac4" allocationname="unknown" flags="0" pid="1908102" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09677e+02" utime="8.92286e+01" stime="1.37455e+01" mtime="7.23878e+01" gflop="0.00000e+00" gbyte="3.77888e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23878e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007e157e1504" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09551e+02" utime="8.91989e+01" stime="1.37334e+01" mtime="7.23878e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23878e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2176e+09" > 6.0098e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2143e+09" > 3.5979e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7390e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9497e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0641e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4347e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4983e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0558e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3411e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3856e+01 </func>
</region>
</regions>
<internal rank="487" log_i="1724765674.528746" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="488" mpi_size="696" stamp_init="1724765564.851212" stamp_final="1724765674.534404" username="apac4" allocationname="unknown" flags="0" pid="1908103" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09683e+02" utime="8.44577e+01" stime="1.53801e+01" mtime="7.16173e+01" gflop="0.00000e+00" gbyte="3.77388e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16173e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49e14a014a114f956a114a0149c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09559e+02" utime="8.44240e+01" stime="1.53720e+01" mtime="7.16173e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16173e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1892e+09" > 8.6583e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1889e+09" > 4.1426e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.1120e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9490e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6849e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0615e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4349e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.4953e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0525e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4537e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2446e+01 </func>
</region>
</regions>
<internal rank="488" log_i="1724765674.534404" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="489" mpi_size="696" stamp_init="1724765564.851267" stamp_final="1724765674.536477" username="apac4" allocationname="unknown" flags="0" pid="1908104" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09685e+02" utime="8.94769e+01" stime="1.34084e+01" mtime="7.22987e+01" gflop="0.00000e+00" gbyte="3.75313e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22987e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003f143e1474" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09559e+02" utime="8.94459e+01" stime="1.33976e+01" mtime="7.22987e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22987e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2028e+09" > 6.3610e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1993e+09" > 3.3071e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2286e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9495e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0469e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4345e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5003e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0536e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3418e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4256e+01 </func>
</region>
</regions>
<internal rank="489" log_i="1724765674.536477" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="490" mpi_size="696" stamp_init="1724765564.851244" stamp_final="1724765674.534643" username="apac4" allocationname="unknown" flags="0" pid="1908105" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09683e+02" utime="8.65376e+01" stime="1.48052e+01" mtime="7.17032e+01" gflop="0.00000e+00" gbyte="3.76659e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17032e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09561e+02" utime="8.65101e+01" stime="1.47911e+01" mtime="7.17032e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17032e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2083e+09" > 6.7106e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2078e+09" > 3.7680e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7279e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9492e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6885e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0630e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4341e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5034e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0549e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4535e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3098e+01 </func>
</region>
</regions>
<internal rank="490" log_i="1724765674.534643" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="491" mpi_size="696" stamp_init="1724765564.851272" stamp_final="1724765674.522357" username="apac4" allocationname="unknown" flags="0" pid="1908106" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09671e+02" utime="8.91618e+01" stime="1.38030e+01" mtime="7.24225e+01" gflop="0.00000e+00" gbyte="3.75072e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24225e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c815e115f315eb55f315ee1533" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09545e+02" utime="8.91286e+01" stime="1.37947e+01" mtime="7.24225e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24225e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1960e+09" > 5.9491e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1883e+09" > 3.3091e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4879e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9485e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1206e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0607e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4343e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5030e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0562e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3404e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4162e+01 </func>
</region>
</regions>
<internal rank="491" log_i="1724765674.522357" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="492" mpi_size="696" stamp_init="1724765564.851208" stamp_final="1724765674.531392" username="apac4" allocationname="unknown" flags="0" pid="1908107" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09680e+02" utime="8.68133e+01" stime="1.44941e+01" mtime="7.19480e+01" gflop="0.00000e+00" gbyte="3.77052e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19480e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000074147414dc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09559e+02" utime="8.67802e+01" stime="1.44855e+01" mtime="7.19480e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19480e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1955e+09" > 6.4452e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1882e+09" > 4.0053e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.1019e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9501e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.7167e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0724e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4339e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5064e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0535e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4422e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2993e+01 </func>
</region>
</regions>
<internal rank="492" log_i="1724765674.531392" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="493" mpi_size="696" stamp_init="1724765564.851267" stamp_final="1724765674.529848" username="apac4" allocationname="unknown" flags="0" pid="1908108" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09679e+02" utime="8.91852e+01" stime="1.38077e+01" mtime="7.26984e+01" gflop="0.00000e+00" gbyte="3.76907e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.26984e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000046144614ff" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09553e+02" utime="8.91550e+01" stime="1.37959e+01" mtime="7.26984e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.26984e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1985e+09" > 5.8950e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2001e+09" > 3.4608e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6665e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9495e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1037e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4337e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5096e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0546e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3408e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4253e+01 </func>
</region>
</regions>
<internal rank="493" log_i="1724765674.529848" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="494" mpi_size="696" stamp_init="1724765564.851476" stamp_final="1724765674.523792" username="apac4" allocationname="unknown" flags="0" pid="1908109" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09672e+02" utime="8.61687e+01" stime="1.48989e+01" mtime="7.17535e+01" gflop="0.00000e+00" gbyte="3.76499e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17535e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09547e+02" utime="8.61398e+01" stime="1.48863e+01" mtime="7.17535e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17535e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1889e+09" > 6.7628e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1824e+09" > 3.7884e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6074e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9477e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1195e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0732e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4330e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5157e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0545e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4416e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3294e+01 </func>
</region>
</regions>
<internal rank="494" log_i="1724765674.523792" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="495" mpi_size="696" stamp_init="1724765564.851280" stamp_final="1724765674.528496" username="apac4" allocationname="unknown" flags="0" pid="1908110" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09677e+02" utime="8.87268e+01" stime="1.42183e+01" mtime="7.31669e+01" gflop="0.00000e+00" gbyte="3.78242e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.31669e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000931523559315931554" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09550e+02" utime="8.86938e+01" stime="1.42093e+01" mtime="7.31669e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.31669e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1952e+09" > 5.7229e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1947e+09" > 3.4381e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.1664e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9492e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0763e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4322e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5241e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0587e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3398e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4242e+01 </func>
</region>
</regions>
<internal rank="495" log_i="1724765674.528496" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="496" mpi_size="696" stamp_init="1724765564.852041" stamp_final="1724765674.524005" username="apac4" allocationname="unknown" flags="0" pid="1908111" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09672e+02" utime="7.98801e+01" stime="2.00897e+01" mtime="7.12552e+01" gflop="0.00000e+00" gbyte="3.77327e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.12552e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48f1590159115c0559115911523" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09549e+02" utime="7.98488e+01" stime="2.00790e+01" mtime="7.12552e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.12552e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2075e+09" > 8.4331e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2131e+09" > 4.5051e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8203e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.0462e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0446e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.3567e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4326e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5177e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0549e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3254e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2403e+01 </func>
</region>
</regions>
<internal rank="496" log_i="1724765674.524005" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="497" mpi_size="696" stamp_init="1724765564.851307" stamp_final="1724765674.524206" username="apac4" allocationname="unknown" flags="0" pid="1908112" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09673e+02" utime="8.87251e+01" stime="1.41493e+01" mtime="7.24057e+01" gflop="0.00000e+00" gbyte="3.76957e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24057e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000341434146c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09547e+02" utime="8.86859e+01" stime="1.41467e+01" mtime="7.24057e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24057e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2015e+09" > 6.0112e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2088e+09" > 4.5837e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.9215e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9481e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1565e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4326e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5205e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0531e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3445e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3696e+01 </func>
</region>
</regions>
<internal rank="497" log_i="1724765674.524206" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="498" mpi_size="696" stamp_init="1724765564.851235" stamp_final="1724765674.530597" username="apac4" allocationname="unknown" flags="0" pid="1908113" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09679e+02" utime="8.52254e+01" stime="1.44974e+01" mtime="7.12290e+01" gflop="0.00000e+00" gbyte="3.77296e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.12290e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09555e+02" utime="8.51888e+01" stime="1.44916e+01" mtime="7.12290e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.12290e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2063e+09" > 7.3565e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2037e+09" > 4.4493e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8347e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9484e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.1032e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.6686e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4324e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5176e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0571e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3764e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2473e+01 </func>
</region>
</regions>
<internal rank="498" log_i="1724765674.530597" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="499" mpi_size="696" stamp_init="1724765564.852327" stamp_final="1724765674.528480" username="apac4" allocationname="unknown" flags="0" pid="1908114" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09676e+02" utime="8.89571e+01" stime="1.39003e+01" mtime="7.25524e+01" gflop="0.00000e+00" gbyte="3.75557e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25524e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09552e+02" utime="8.89251e+01" stime="1.38903e+01" mtime="7.25524e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25524e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2128e+09" > 5.8324e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2132e+09" > 3.5558e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.1701e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9494e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1206e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.8116e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4322e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5256e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0574e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3409e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3620e+01 </func>
</region>
</regions>
<internal rank="499" log_i="1724765674.528480" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="500" mpi_size="696" stamp_init="1724765564.851228" stamp_final="1724765674.529822" username="apac4" allocationname="unknown" flags="0" pid="1908115" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09679e+02" utime="8.46014e+01" stime="1.52142e+01" mtime="7.11915e+01" gflop="0.00000e+00" gbyte="3.76839e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.11915e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09554e+02" utime="8.45673e+01" stime="1.52066e+01" mtime="7.11915e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.11915e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2048e+09" > 8.0129e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1986e+09" > 5.2334e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7242e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9494e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.7140e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0651e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4308e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5377e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0564e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3774e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2455e+01 </func>
</region>
</regions>
<internal rank="500" log_i="1724765674.529822" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="501" mpi_size="696" stamp_init="1724765564.851294" stamp_final="1724765674.533386" username="apac4" allocationname="unknown" flags="0" pid="1908116" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09682e+02" utime="8.89052e+01" stime="1.41090e+01" mtime="7.24330e+01" gflop="0.00000e+00" gbyte="3.77655e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24330e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09556e+02" utime="8.88779e+01" stime="1.40944e+01" mtime="7.24330e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24330e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2122e+09" > 6.4025e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2104e+09" > 3.6253e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.1256e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9492e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8509e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4304e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5389e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0599e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3383e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3487e+01 </func>
</region>
</regions>
<internal rank="501" log_i="1724765674.533386" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="502" mpi_size="696" stamp_init="1724765564.851249" stamp_final="1724765674.526141" username="apac4" allocationname="unknown" flags="0" pid="1908117" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09675e+02" utime="8.72826e+01" stime="1.44097e+01" mtime="7.12714e+01" gflop="0.00000e+00" gbyte="3.75858e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.12714e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000052145f5652145214e2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09552e+02" utime="8.72508e+01" stime="1.43986e+01" mtime="7.12714e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.12714e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1963e+09" > 6.9239e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1903e+09" > 3.5467e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.2344e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9493e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5511e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0669e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4307e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5363e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0531e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3614e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2157e+01 </func>
</region>
</regions>
<internal rank="502" log_i="1724765674.526141" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="503" mpi_size="696" stamp_init="1724765564.851309" stamp_final="1724765674.530553" username="apac4" allocationname="unknown" flags="0" pid="1908118" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u18a</host>
<perf wtime="1.09679e+02" utime="8.90489e+01" stime="1.38796e+01" mtime="7.27685e+01" gflop="0.00000e+00" gbyte="3.77476e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27685e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e414e314d7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09553e+02" utime="8.90181e+01" stime="1.38684e+01" mtime="7.27685e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27685e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1914e+09" > 5.6718e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1951e+09" > 3.0130e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.3362e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9486e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2159e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0707e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4305e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5419e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0592e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3387e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3689e+01 </func>
</region>
</regions>
<internal rank="503" log_i="1724765674.530553" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="504" mpi_size="696" stamp_init="1724765564.814563" stamp_final="1724765674.530238" username="apac4" allocationname="unknown" flags="0" pid="917740" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09716e+02" utime="5.90557e+01" stime="1.78093e+01" mtime="4.59248e+01" gflop="0.00000e+00" gbyte="3.86707e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.59248e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007815771501" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09585e+02" utime="5.90242e+01" stime="1.77982e+01" mtime="4.59248e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.59248e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1852e+09" > 6.6877e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1983e+09" > 4.7755e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.5164e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.0409e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.0943e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.3581e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4312e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5360e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0641e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2916e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3905e+01 </func>
</region>
</regions>
<internal rank="504" log_i="1724765674.530238" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="505" mpi_size="696" stamp_init="1724765564.815094" stamp_final="1724765674.531526" username="apac4" allocationname="unknown" flags="0" pid="917741" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09716e+02" utime="6.52181e+01" stime="9.14921e+00" mtime="4.27931e+01" gflop="0.00000e+00" gbyte="3.78231e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.27931e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09581e+02" utime="6.51846e+01" stime="9.13897e+00" mtime="4.27931e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.27931e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2059e+09" > 5.9953e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2022e+09" > 3.1917e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1393e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0372e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7418e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3678e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4304e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5435e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0638e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2663e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4002e+01 </func>
</region>
</regions>
<internal rank="505" log_i="1724765674.531526" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="506" mpi_size="696" stamp_init="1724765564.814554" stamp_final="1724765674.526444" username="apac4" allocationname="unknown" flags="0" pid="917742" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09712e+02" utime="6.15672e+01" stime="9.38023e+00" mtime="4.15235e+01" gflop="0.00000e+00" gbyte="3.78365e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.15235e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c7144e55c714c71463" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09582e+02" utime="6.15375e+01" stime="9.36692e+00" mtime="4.15235e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.15235e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2134e+09" > 7.4228e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2134e+09" > 4.9733e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0432e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.0557e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1919e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4122e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4312e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5316e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0632e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3411e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3662e+01 </func>
</region>
</regions>
<internal rank="506" log_i="1724765674.526444" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="507" mpi_size="696" stamp_init="1724765564.814814" stamp_final="1724765674.528096" username="apac4" allocationname="unknown" flags="0" pid="917743" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09713e+02" utime="6.42797e+01" stime="8.84121e+00" mtime="4.19812e+01" gflop="0.00000e+00" gbyte="3.77644e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.19812e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09581e+02" utime="6.42491e+01" stime="8.82950e+00" mtime="4.19812e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.19812e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1977e+09" > 5.8562e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1983e+09" > 3.6433e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2719e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.0468e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4180e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4295e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5545e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0635e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2634e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4389e+01 </func>
</region>
</regions>
<internal rank="507" log_i="1724765674.528096" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="508" mpi_size="696" stamp_init="1724765564.814552" stamp_final="1724765674.536249" username="apac4" allocationname="unknown" flags="0" pid="917744" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09722e+02" utime="8.42015e+01" stime="1.77207e+01" mtime="7.15191e+01" gflop="0.00000e+00" gbyte="3.76980e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15191e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000057145714ae" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09592e+02" utime="8.41701e+01" stime="1.77093e+01" mtime="7.15191e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15191e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2225e+09" > 8.0921e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2184e+09" > 4.5007e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0000e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.8155e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1297e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8794e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4309e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5391e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0621e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3313e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3563e+01 </func>
</region>
</regions>
<internal rank="508" log_i="1724765674.536249" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="509" mpi_size="696" stamp_init="1724765564.814570" stamp_final="1724765674.521723" username="apac4" allocationname="unknown" flags="0" pid="917745" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09707e+02" utime="7.14715e+01" stime="1.06260e+01" mtime="5.11719e+01" gflop="0.00000e+00" gbyte="3.77781e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.11719e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4191432144414495644143f14c6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09576e+02" utime="7.14347e+01" stime="1.06199e+01" mtime="5.11719e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.11719e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.3447e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2114e+09" > 6.2800e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1959e+09" > 3.1609e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2799e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.8168e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5259e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.5708e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4304e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5432e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0626e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2436e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4416e+01 </func>
</region>
</regions>
<internal rank="509" log_i="1724765674.521723" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="510" mpi_size="696" stamp_init="1724765564.815264" stamp_final="1724765674.533976" username="apac4" allocationname="unknown" flags="0" pid="917746" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09719e+02" utime="7.53136e+01" stime="1.23297e+01" mtime="5.70307e+01" gflop="0.00000e+00" gbyte="3.75763e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.70307e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09590e+02" utime="7.52795e+01" stime="1.23216e+01" mtime="5.70307e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.70307e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1927e+09" > 7.4079e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1952e+09" > 5.0144e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.5870e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.5254e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0293e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.5872e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4300e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5477e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0630e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2391e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3751e+01 </func>
</region>
</regions>
<internal rank="510" log_i="1724765674.533976" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="511" mpi_size="696" stamp_init="1724765564.815299" stamp_final="1724765674.539969" username="apac4" allocationname="unknown" flags="0" pid="917747" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09725e+02" utime="8.00073e+01" stime="1.20928e+01" mtime="6.12015e+01" gflop="0.00000e+00" gbyte="3.77903e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.12015e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000271425552714261477" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09596e+02" utime="7.99724e+01" stime="1.20853e+01" mtime="6.12015e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.12015e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2110e+09" > 6.1494e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2191e+09" > 3.3471e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2897e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.8163e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6226e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.1314e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4291e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5536e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0650e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2436e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4452e+01 </func>
</region>
</regions>
<internal rank="511" log_i="1724765674.539969" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="512" mpi_size="696" stamp_init="1724765564.816302" stamp_final="1724765674.534665" username="apac4" allocationname="unknown" flags="0" pid="917748" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09718e+02" utime="7.91495e+01" stime="2.11625e+01" mtime="7.12846e+01" gflop="0.00000e+00" gbyte="3.77876e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.12846e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49a1422143414f85534142f14a6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09589e+02" utime="7.91203e+01" stime="2.11495e+01" mtime="7.12846e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.12846e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1934e+09" > 1.2307e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1945e+09" > 8.1931e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.4963e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.5729e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8240e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4234e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6079e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0665e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2681e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3015e+01 </func>
</region>
</regions>
<internal rank="512" log_i="1724765674.534665" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="513" mpi_size="696" stamp_init="1724765564.814700" stamp_final="1724765674.539888" username="apac4" allocationname="unknown" flags="0" pid="917749" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09725e+02" utime="6.39810e+01" stime="8.66272e+00" mtime="4.16436e+01" gflop="0.00000e+00" gbyte="3.77956e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.16436e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09594e+02" utime="6.39476e+01" stime="8.65354e+00" mtime="4.16436e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.16436e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2021e+09" > 5.8290e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1971e+09" > 3.6487e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9116e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.6628e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6941e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2434e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4244e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6040e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0648e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1916e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4801e+01 </func>
</region>
</regions>
<internal rank="513" log_i="1724765674.539888" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="514" mpi_size="696" stamp_init="1724765564.815723" stamp_final="1724765674.529924" username="apac4" allocationname="unknown" flags="0" pid="917750" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09714e+02" utime="8.18134e+01" stime="1.86013e+01" mtime="7.01682e+01" gflop="0.00000e+00" gbyte="3.76305e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.01682e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003114301480" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09586e+02" utime="8.17802e+01" stime="1.85929e+01" mtime="7.01682e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.01682e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2084e+09" > 7.0865e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2074e+09" > 4.3046e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8591e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0152e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7824e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7129e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4229e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6161e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0659e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2695e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3924e+01 </func>
</region>
</regions>
<internal rank="514" log_i="1724765674.529924" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="515" mpi_size="696" stamp_init="1724765564.816641" stamp_final="1724765674.521022" username="apac4" allocationname="unknown" flags="0" pid="917751" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09704e+02" utime="6.52360e+01" stime="8.89579e+00" mtime="4.35350e+01" gflop="0.00000e+00" gbyte="3.77693e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.35350e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006c146c14c4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09575e+02" utime="6.52022e+01" stime="8.88734e+00" mtime="4.35350e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.35350e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1893e+09" > 5.8625e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1962e+09" > 3.3236e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4390e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0149e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9087e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7322e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4234e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6099e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0643e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2263e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4670e+01 </func>
</region>
</regions>
<internal rank="515" log_i="1724765674.521022" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="516" mpi_size="696" stamp_init="1724765564.816303" stamp_final="1724765674.529968" username="apac4" allocationname="unknown" flags="0" pid="917752" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09714e+02" utime="8.09440e+01" stime="1.78677e+01" mtime="6.89698e+01" gflop="0.00000e+00" gbyte="3.77327e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.89698e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000811580154f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09584e+02" utime="8.09136e+01" stime="1.78559e+01" mtime="6.89698e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.89698e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1857e+09" > 7.6682e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1912e+09" > 3.4313e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1225e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.4534e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7396e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1909e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4240e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6083e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0655e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2207e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3548e+01 </func>
</region>
</regions>
<internal rank="516" log_i="1724765674.529968" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="517" mpi_size="696" stamp_init="1724765564.816162" stamp_final="1724765674.539261" username="apac4" allocationname="unknown" flags="0" pid="917753" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09723e+02" utime="6.46495e+01" stime="8.81901e+00" mtime="4.27836e+01" gflop="0.00000e+00" gbyte="3.76804e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.27836e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09594e+02" utime="6.46167e+01" stime="8.80987e+00" mtime="4.27836e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.27836e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2054e+09" > 6.0966e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2082e+09" > 3.2964e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2622e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.4402e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6226e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2321e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4235e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6095e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0661e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2262e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4790e+01 </func>
</region>
</regions>
<internal rank="517" log_i="1724765674.539261" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="518" mpi_size="696" stamp_init="1724765564.815155" stamp_final="1724765674.535038" username="apac4" allocationname="unknown" flags="0" pid="917754" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09720e+02" utime="7.90624e+01" stime="1.74876e+01" mtime="6.59688e+01" gflop="0.00000e+00" gbyte="3.76362e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.59688e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003e14a2563e143e1461" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09591e+02" utime="7.90331e+01" stime="1.74752e+01" mtime="6.59688e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.59688e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1846e+09" > 6.7188e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1950e+09" > 3.3335e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.5658e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0095e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3622e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.5092e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4233e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6149e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0666e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2691e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4323e+01 </func>
</region>
</regions>
<internal rank="518" log_i="1724765674.535038" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="519" mpi_size="696" stamp_init="1724765564.815200" stamp_final="1724765674.532911" username="apac4" allocationname="unknown" flags="0" pid="917755" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09718e+02" utime="6.50739e+01" stime="9.02755e+00" mtime="4.35452e+01" gflop="0.00000e+00" gbyte="3.77781e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.35452e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000e1155056e115e01507" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09588e+02" utime="6.50406e+01" stime="9.01854e+00" mtime="4.35452e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.35452e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2047e+09" > 6.0740e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2021e+09" > 3.2002e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1858e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0107e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.5411e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4220e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6283e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0638e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2071e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4957e+01 </func>
</region>
</regions>
<internal rank="519" log_i="1724765674.532911" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="520" mpi_size="696" stamp_init="1724765564.814555" stamp_final="1724765674.523409" username="apac4" allocationname="unknown" flags="0" pid="917756" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09709e+02" utime="7.55479e+01" stime="1.73784e+01" mtime="6.41389e+01" gflop="0.00000e+00" gbyte="3.78353e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.41389e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000095149514d7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09578e+02" utime="7.55169e+01" stime="1.73663e+01" mtime="6.41389e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.41389e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2145e+09" > 1.0309e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2038e+09" > 8.0802e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0882e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0003e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2176e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.4701e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4213e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6323e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0633e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2679e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3103e+01 </func>
</region>
</regions>
<internal rank="520" log_i="1724765674.523409" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="521" mpi_size="696" stamp_init="1724765564.814571" stamp_final="1724765674.527794" username="apac4" allocationname="unknown" flags="0" pid="917757" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09713e+02" utime="6.48679e+01" stime="9.11762e+00" mtime="4.31127e+01" gflop="0.00000e+00" gbyte="3.76698e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.31127e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09582e+02" utime="6.48368e+01" stime="9.10660e+00" mtime="4.31127e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.31127e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.2983e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2116e+09" > 5.8875e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2005e+09" > 3.3821e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4278e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0006e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.5089e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4223e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6257e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0632e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1962e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4401e+01 </func>
</region>
</regions>
<internal rank="521" log_i="1724765674.527794" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="522" mpi_size="696" stamp_init="1724765564.816478" stamp_final="1724765674.534382" username="apac4" allocationname="unknown" flags="0" pid="917758" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09718e+02" utime="7.14085e+01" stime="1.34142e+01" mtime="5.49605e+01" gflop="0.00000e+00" gbyte="3.74855e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.49605e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09589e+02" utime="7.13744e+01" stime="1.34065e+01" mtime="5.49605e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.49605e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1904e+09" > 6.4213e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1965e+09" > 5.1378e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7621e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.1301e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0917e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2823e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4223e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6262e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0660e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2677e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3638e+01 </func>
</region>
</regions>
<internal rank="522" log_i="1724765674.534382" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="523" mpi_size="696" stamp_init="1724765564.814564" stamp_final="1724765674.521058" username="apac4" allocationname="unknown" flags="0" pid="917759" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09706e+02" utime="6.62366e+01" stime="9.02889e+00" mtime="4.42678e+01" gflop="0.00000e+00" gbyte="3.77232e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.42678e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000461545152f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09575e+02" utime="6.62018e+01" stime="9.02125e+00" mtime="4.42678e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.42678e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1977e+09" > 6.2310e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1969e+09" > 3.1174e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4612e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.1297e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.5899e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4209e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6406e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0663e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1913e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4208e+01 </func>
</region>
</regions>
<internal rank="523" log_i="1724765674.521058" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="524" mpi_size="696" stamp_init="1724765564.815211" stamp_final="1724765674.529408" username="apac4" allocationname="unknown" flags="0" pid="917760" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09714e+02" utime="7.18416e+01" stime="1.16599e+01" mtime="5.32453e+01" gflop="0.00000e+00" gbyte="3.76961e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.32453e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004e154e1518" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09586e+02" utime="7.18118e+01" stime="1.16475e+01" mtime="5.32453e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.32453e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1896e+09" > 7.2796e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1890e+09" > 4.5518e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0826e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.7107e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.9351e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9295e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4203e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6432e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0650e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2684e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3698e+01 </func>
</region>
</regions>
<internal rank="524" log_i="1724765674.529408" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="525" mpi_size="696" stamp_init="1724765564.815152" stamp_final="1724765674.535939" username="apac4" allocationname="unknown" flags="0" pid="917761" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09721e+02" utime="7.10549e+01" stime="1.00265e+01" mtime="4.97872e+01" gflop="0.00000e+00" gbyte="3.76583e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.97872e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09592e+02" utime="7.10188e+01" stime="1.00207e+01" mtime="4.97872e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.97872e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2089e+09" > 6.0910e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2172e+09" > 3.5047e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2657e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.7104e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9325e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3014e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4207e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6384e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0617e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1849e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4126e+01 </func>
</region>
</regions>
<internal rank="525" log_i="1724765674.535939" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="526" mpi_size="696" stamp_init="1724765564.816645" stamp_final="1724765674.526002" username="apac4" allocationname="unknown" flags="0" pid="917762" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09709e+02" utime="8.43125e+01" stime="1.69306e+01" mtime="7.12024e+01" gflop="0.00000e+00" gbyte="3.75050e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.12024e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000da14c555da14da14ec" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09581e+02" utime="8.42807e+01" stime="1.69203e+01" mtime="7.12024e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.12024e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1833e+09" > 8.4673e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1928e+09" > 4.4764e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2987e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.4199e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5403e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3067e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4207e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6381e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0646e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2667e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2758e+01 </func>
</region>
</regions>
<internal rank="526" log_i="1724765674.526002" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="527" mpi_size="696" stamp_init="1724765564.814577" stamp_final="1724765674.529074" username="apac4" allocationname="unknown" flags="0" pid="917763" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u15a</host>
<perf wtime="1.09714e+02" utime="7.69904e+01" stime="1.12042e+01" mtime="5.69811e+01" gflop="0.00000e+00" gbyte="3.77441e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.69811e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008d1426558d148d1498" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09584e+02" utime="7.69561e+01" stime="1.11963e+01" mtime="5.69811e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.69811e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1871e+09" > 6.0148e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1907e+09" > 4.0455e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2379e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.4206e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5259e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3420e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4189e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6578e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0653e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1845e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4243e+01 </func>
</region>
</regions>
<internal rank="527" log_i="1724765674.529074" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="528" mpi_size="696" stamp_init="1724765564.933979" stamp_final="1724765674.518359" username="apac4" allocationname="unknown" flags="0" pid="3958243" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09584e+02" utime="7.64421e+01" stime="2.51656e+01" mtime="7.12424e+01" gflop="0.00000e+00" gbyte="3.85681e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.12424e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09456e+02" utime="7.64116e+01" stime="2.51544e+01" mtime="7.12424e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.12424e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2023e+09" > 6.7587e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1891e+09" > 3.6482e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.2832e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9452e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.6892e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.5629e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4206e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6442e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0500e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.0736e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4137e+01 </func>
</region>
</regions>
<internal rank="528" log_i="1724765674.518359" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="529" mpi_size="696" stamp_init="1724765564.932513" stamp_final="1724765674.540112" username="apac4" allocationname="unknown" flags="0" pid="3958244" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09608e+02" utime="8.92584e+01" stime="1.37449e+01" mtime="7.23115e+01" gflop="0.00000e+00" gbyte="3.77625e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23115e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09478e+02" utime="8.92313e+01" stime="1.37288e+01" mtime="7.23115e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23115e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2069e+09" > 6.1830e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2031e+09" > 2.5682e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3271e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9464e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8147e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.2094e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4257e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.5913e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0513e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.9180e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4231e+01 </func>
</region>
</regions>
<internal rank="529" log_i="1724765674.540112" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="530" mpi_size="696" stamp_init="1724765564.932378" stamp_final="1724765674.522458" username="apac4" allocationname="unknown" flags="0" pid="3958245" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09590e+02" utime="8.73865e+01" stime="1.42233e+01" mtime="7.19832e+01" gflop="0.00000e+00" gbyte="3.77617e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19832e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007a15b4567a15791546" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09462e+02" utime="8.73548e+01" stime="1.42127e+01" mtime="7.19832e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19832e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2140e+09" > 7.6504e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2170e+09" > 4.2143e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4971e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9460e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.3722e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9858e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4205e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6414e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0496e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1180e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3570e+01 </func>
</region>
</regions>
<internal rank="530" log_i="1724765674.522458" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="531" mpi_size="696" stamp_init="1724765564.932436" stamp_final="1724765674.531189" username="apac4" allocationname="unknown" flags="0" pid="3958246" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09599e+02" utime="8.92360e+01" stime="1.37609e+01" mtime="7.24086e+01" gflop="0.00000e+00" gbyte="3.78025e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24086e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf428142a142b1471552b142b14a3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09471e+02" utime="8.92011e+01" stime="1.37534e+01" mtime="7.24086e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24086e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2066e+09" > 6.0566e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1996e+09" > 2.7591e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2473e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9470e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.0020e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4236e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6132e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0547e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.9190e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4411e+01 </func>
</region>
</regions>
<internal rank="531" log_i="1724765674.531189" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="532" mpi_size="696" stamp_init="1724765564.934166" stamp_final="1724765674.528131" username="apac4" allocationname="unknown" flags="0" pid="3958247" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09594e+02" utime="8.62104e+01" stime="1.39984e+01" mtime="7.11017e+01" gflop="0.00000e+00" gbyte="3.76835e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.11017e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4171518151a155b561a15191548" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09467e+02" utime="8.61804e+01" stime="1.39868e+01" mtime="7.11017e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.11017e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.8835e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2201e+09" > 9.0202e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2241e+09" > 5.2676e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.4200e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9465e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.2268e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.1198e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4193e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6510e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0551e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.0550e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3608e+01 </func>
</region>
</regions>
<internal rank="532" log_i="1724765674.528131" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="533" mpi_size="696" stamp_init="1724765564.933183" stamp_final="1724765674.529845" username="apac4" allocationname="unknown" flags="0" pid="3958248" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09597e+02" utime="8.88721e+01" stime="1.41625e+01" mtime="7.23118e+01" gflop="0.00000e+00" gbyte="3.75999e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23118e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f015eb1542" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09469e+02" utime="8.88354e+01" stime="1.41570e+01" mtime="7.23118e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23118e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1995e+09" > 6.0689e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2022e+09" > 2.8694e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0917e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9464e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.7193e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9492e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4195e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6547e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0533e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.9188e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4475e+01 </func>
</region>
</regions>
<internal rank="533" log_i="1724765674.529845" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="534" mpi_size="696" stamp_init="1724765564.932234" stamp_final="1724765674.534377" username="apac4" allocationname="unknown" flags="0" pid="3958249" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09602e+02" utime="8.81210e+01" stime="1.38413e+01" mtime="7.19485e+01" gflop="0.00000e+00" gbyte="3.77293e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19485e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002214cd552214211493" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09476e+02" utime="8.80905e+01" stime="1.38286e+01" mtime="7.19485e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19485e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1922e+09" > 6.7900e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1923e+09" > 3.9308e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9914e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9446e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0028e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.2994e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4197e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6527e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0518e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.0443e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4138e+01 </func>
</region>
</regions>
<internal rank="534" log_i="1724765674.534377" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="535" mpi_size="696" stamp_init="1724765564.932569" stamp_final="1724765674.536393" username="apac4" allocationname="unknown" flags="0" pid="3958250" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09604e+02" utime="8.88654e+01" stime="1.41238e+01" mtime="7.25846e+01" gflop="0.00000e+00" gbyte="3.78220e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25846e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d214d414d514a155d514d514ff" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09478e+02" utime="8.88308e+01" stime="1.41161e+01" mtime="7.25846e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25846e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2135e+09" > 6.1308e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2080e+09" > 2.9329e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2301e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9453e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.2939e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4192e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6582e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0552e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.9179e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4606e+01 </func>
</region>
</regions>
<internal rank="535" log_i="1724765674.536393" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="536" mpi_size="696" stamp_init="1724765564.934116" stamp_final="1724765674.522320" username="apac4" allocationname="unknown" flags="0" pid="3958251" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09588e+02" utime="8.71271e+01" stime="1.40599e+01" mtime="7.15983e+01" gflop="0.00000e+00" gbyte="3.76190e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15983e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf474147614771440557714761456" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09461e+02" utime="8.70951e+01" stime="1.40498e+01" mtime="7.15983e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15983e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2012e+09" > 8.2073e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2041e+09" > 3.9956e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.6888e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9456e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1635e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.5889e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4188e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6583e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0525e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.0062e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3940e+01 </func>
</region>
</regions>
<internal rank="536" log_i="1724765674.522320" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="537" mpi_size="696" stamp_init="1724765564.934078" stamp_final="1724765674.538388" username="apac4" allocationname="unknown" flags="0" pid="3958252" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09604e+02" utime="8.97215e+01" stime="1.32645e+01" mtime="7.28974e+01" gflop="0.00000e+00" gbyte="3.77293e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28974e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf42d142e143014a75530142f1471" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09478e+02" utime="8.96880e+01" stime="1.32568e+01" mtime="7.28974e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28974e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1945e+09" > 5.8921e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1979e+09" > 2.6604e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1235e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9458e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.6580e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4174e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6731e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0562e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.9168e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.5054e+01 </func>
</region>
</regions>
<internal rank="537" log_i="1724765674.538388" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="538" mpi_size="696" stamp_init="1724765564.932715" stamp_final="1724765674.518178" username="apac4" allocationname="unknown" flags="0" pid="3958253" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09585e+02" utime="8.77550e+01" stime="1.38844e+01" mtime="7.20549e+01" gflop="0.00000e+00" gbyte="3.76495e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20549e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000060143156601460147a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09460e+02" utime="8.77220e+01" stime="1.38757e+01" mtime="7.20549e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20549e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2052e+09" > 7.1187e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2125e+09" > 3.1835e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2097e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9457e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.1314e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7558e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4182e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6640e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0535e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.9869e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3992e+01 </func>
</region>
</regions>
<internal rank="538" log_i="1724765674.518178" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="539" mpi_size="696" stamp_init="1724765564.934159" stamp_final="1724765674.523389" username="apac4" allocationname="unknown" flags="0" pid="3958254" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09589e+02" utime="8.93684e+01" stime="1.36325e+01" mtime="7.31156e+01" gflop="0.00000e+00" gbyte="3.76534e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.31156e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009b149c569b149614dc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09465e+02" utime="8.93356e+01" stime="1.36239e+01" mtime="7.31156e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.31156e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1934e+09" > 5.9228e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1861e+09" > 2.5938e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3194e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9463e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5599e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4171e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6790e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0559e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.9156e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.5070e+01 </func>
</region>
</regions>
<internal rank="539" log_i="1724765674.523389" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="540" mpi_size="696" stamp_init="1724765564.934166" stamp_final="1724765674.517122" username="apac4" allocationname="unknown" flags="0" pid="3958255" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09583e+02" utime="8.55956e+01" stime="1.43698e+01" mtime="7.14062e+01" gflop="0.00000e+00" gbyte="3.77773e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.14062e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf433143414351467563514351490" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09454e+02" utime="8.55702e+01" stime="1.43531e+01" mtime="7.14062e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.14062e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1870e+09" > 1.0221e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1892e+09" > 5.1987e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.3990e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9457e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3280e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.6551e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4167e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6794e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0481e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.9802e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3823e+01 </func>
</region>
</regions>
<internal rank="540" log_i="1724765674.517122" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="541" mpi_size="696" stamp_init="1724765564.932237" stamp_final="1724765674.529026" username="apac4" allocationname="unknown" flags="0" pid="3958256" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09597e+02" utime="8.91123e+01" stime="1.38481e+01" mtime="7.23064e+01" gflop="0.00000e+00" gbyte="3.76598e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23064e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09472e+02" utime="8.90792e+01" stime="1.38394e+01" mtime="7.23064e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23064e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127200" bytes="1.2110e+09" > 6.1479e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2147e+09" > 2.9860e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.5610e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9435e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0512e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4126e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7205e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0538e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.9162e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.5013e+01 </func>
</region>
</regions>
<internal rank="541" log_i="1724765674.529026" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="542" mpi_size="696" stamp_init="1724765564.932229" stamp_final="1724765674.516983" username="apac4" allocationname="unknown" flags="0" pid="3958257" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09585e+02" utime="8.76024e+01" stime="1.41238e+01" mtime="7.24010e+01" gflop="0.00000e+00" gbyte="3.75351e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24010e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09456e+02" utime="8.75682e+01" stime="1.41162e+01" mtime="7.24010e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24010e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2051e+09" > 7.2598e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2037e+09" > 3.2357e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3787e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9462e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.8692e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0781e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4172e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6776e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0547e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.9804e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4149e+01 </func>
</region>
</regions>
<internal rank="542" log_i="1724765674.516983" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="543" mpi_size="696" stamp_init="1724765564.933130" stamp_final="1724765674.535247" username="apac4" allocationname="unknown" flags="0" pid="3958258" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09602e+02" utime="8.89921e+01" stime="1.39918e+01" mtime="7.28143e+01" gflop="0.00000e+00" gbyte="3.76728e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28143e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09478e+02" utime="8.89613e+01" stime="1.39804e+01" mtime="7.28143e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28143e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2096e+09" > 6.0487e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2131e+09" > 2.8511e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4253e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9469e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9325e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4873e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4168e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6811e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0555e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.8390e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4639e+01 </func>
</region>
</regions>
<internal rank="543" log_i="1724765674.535247" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="544" mpi_size="696" stamp_init="1724765564.932253" stamp_final="1724765674.518137" username="apac4" allocationname="unknown" flags="0" pid="3958259" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09586e+02" utime="8.08789e+01" stime="1.99099e+01" mtime="7.16515e+01" gflop="0.00000e+00" gbyte="3.76614e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16515e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e515e715e8159f55e815e81545" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09459e+02" utime="8.08431e+01" stime="1.99038e+01" mtime="7.16515e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16515e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2112e+09" > 8.7375e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2116e+09" > 3.4137e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0528e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.0758e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6365e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1761e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4154e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6953e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0528e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.9658e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3604e+01 </func>
</region>
</regions>
<internal rank="544" log_i="1724765674.518137" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="545" mpi_size="696" stamp_init="1724765564.932241" stamp_final="1724765674.531619" username="apac4" allocationname="unknown" flags="0" pid="3958260" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09599e+02" utime="8.94047e+01" stime="1.35397e+01" mtime="7.22809e+01" gflop="0.00000e+00" gbyte="3.76595e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22809e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf41f152115221547552215221507" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09474e+02" utime="8.93698e+01" stime="1.35333e+01" mtime="7.22809e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22809e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2050e+09" > 6.1226e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2086e+09" > 2.8381e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9827e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9455e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.9908e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4154e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.6960e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0510e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.8880e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4550e+01 </func>
</region>
</regions>
<internal rank="545" log_i="1724765674.531619" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="546" mpi_size="696" stamp_init="1724765564.933542" stamp_final="1724765674.628668" username="apac4" allocationname="unknown" flags="0" pid="3958261" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09695e+02" utime="8.73281e+01" stime="1.40543e+01" mtime="7.19190e+01" gflop="0.00000e+00" gbyte="3.77518e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19190e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09567e+02" utime="8.73011e+01" stime="1.40394e+01" mtime="7.19190e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19190e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1921e+09" > 7.2575e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1978e+09" > 3.4217e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5222e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9453e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.9778e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6712e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4148e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7030e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0494e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.9089e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3535e+01 </func>
</region>
</regions>
<internal rank="546" log_i="1724765674.628668" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="547" mpi_size="696" stamp_init="1724765564.932251" stamp_final="1724765674.523793" username="apac4" allocationname="unknown" flags="0" pid="3958262" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09592e+02" utime="8.92083e+01" stime="1.38167e+01" mtime="7.20603e+01" gflop="0.00000e+00" gbyte="3.77724e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20603e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003a14391465" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09463e+02" utime="8.91783e+01" stime="1.38044e+01" mtime="7.20603e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20603e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1802e+09" > 6.2383e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1857e+09" > 2.5626e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8604e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9448e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6753e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4145e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7052e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0523e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.8383e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4456e+01 </func>
</region>
</regions>
<internal rank="547" log_i="1724765674.523793" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="548" mpi_size="696" stamp_init="1724765564.932246" stamp_final="1724765674.518244" username="apac4" allocationname="unknown" flags="0" pid="3958263" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09586e+02" utime="8.80263e+01" stime="1.36801e+01" mtime="7.16966e+01" gflop="0.00000e+00" gbyte="3.76919e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16966e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09459e+02" utime="8.79966e+01" stime="1.36679e+01" mtime="7.16966e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16966e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1856e+09" > 7.4570e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1934e+09" > 3.5008e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7921e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9460e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0008e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.4041e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4144e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7044e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0498e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.9539e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4009e+01 </func>
</region>
</regions>
<internal rank="548" log_i="1724765674.518244" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="549" mpi_size="696" stamp_init="1724765564.933646" stamp_final="1724765674.535263" username="apac4" allocationname="unknown" flags="0" pid="3958264" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09602e+02" utime="8.92885e+01" stime="1.36932e+01" mtime="7.28837e+01" gflop="0.00000e+00" gbyte="3.76942e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.28837e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09474e+02" utime="8.92528e+01" stime="1.36870e+01" mtime="7.28837e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.28837e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2137e+09" > 5.9981e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2095e+09" > 2.6871e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8138e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9455e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.7207e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4137e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7093e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0494e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.8758e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4344e+01 </func>
</region>
</regions>
<internal rank="549" log_i="1724765674.535263" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="550" mpi_size="696" stamp_init="1724765564.932254" stamp_final="1724765674.525439" username="apac4" allocationname="unknown" flags="0" pid="3958265" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09593e+02" utime="8.54957e+01" stime="1.45043e+01" mtime="7.12015e+01" gflop="0.00000e+00" gbyte="3.77190e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.12015e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b415de55b415b4152f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09465e+02" utime="8.54652e+01" stime="1.44928e+01" mtime="7.12015e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.12015e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1832e+09" > 1.0911e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1860e+09" > 5.6795e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0163e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9458e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7860e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.5672e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4140e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7113e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0512e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.9391e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2925e+01 </func>
</region>
</regions>
<internal rank="550" log_i="1724765674.525439" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="551" mpi_size="696" stamp_init="1724765564.932275" stamp_final="1724765674.529630" username="apac4" allocationname="unknown" flags="0" pid="3958266" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="1.09597e+02" utime="8.92524e+01" stime="1.37563e+01" mtime="7.24153e+01" gflop="0.00000e+00" gbyte="3.76476e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24153e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d1155855d115d11538" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09471e+02" utime="8.92172e+01" stime="1.37497e+01" mtime="7.24153e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24153e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1910e+09" > 6.1791e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1910e+09" > 2.6231e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4567e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9456e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.5960e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4127e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7236e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0509e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.8700e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4207e+01 </func>
</region>
</regions>
<internal rank="551" log_i="1724765674.529630" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="552" mpi_size="696" stamp_init="1724765564.748067" stamp_final="1724765674.536827" username="apac4" allocationname="unknown" flags="0" pid="504042" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09789e+02" utime="6.51799e+01" stime="2.06639e+01" mtime="5.63319e+01" gflop="0.00000e+00" gbyte="3.87054e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.63319e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d2149856d214d2148a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09665e+02" utime="6.51493e+01" stime="2.06522e+01" mtime="5.63319e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.63319e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2010e+09" > 7.7169e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2016e+09" > 3.6671e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4767e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.1028e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0504e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5528e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4125e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7199e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0569e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.8482e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3796e+01 </func>
</region>
</regions>
<internal rank="552" log_i="1724765674.536827" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="553" mpi_size="696" stamp_init="1724765564.746039" stamp_final="1724765674.525815" username="apac4" allocationname="unknown" flags="0" pid="504043" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09780e+02" utime="5.89148e+01" stime="7.81262e+00" mtime="3.61375e+01" gflop="0.00000e+00" gbyte="3.78059e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.61375e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b1141655b114b1145a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09648e+02" utime="5.88783e+01" stime="7.80577e+00" mtime="3.61375e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.61375e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2060e+09" > 6.5254e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2100e+09" > 3.2520e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5409e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.1030e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3491e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4125e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7089e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0551e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.7362e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4167e+01 </func>
</region>
</regions>
<internal rank="553" log_i="1724765674.525815" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="554" mpi_size="696" stamp_init="1724765564.746040" stamp_final="1724765674.535179" username="apac4" allocationname="unknown" flags="0" pid="504044" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09789e+02" utime="6.24733e+01" stime="9.60154e+00" mtime="4.27204e+01" gflop="0.00000e+00" gbyte="3.75099e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.27204e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4861487148814ad5588148814ff" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09662e+02" utime="6.24356e+01" stime="9.59745e+00" mtime="4.27204e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.27204e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2193e+09" > 7.8718e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2154e+09" > 4.4641e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7752e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0133e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0324e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1432e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4128e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7161e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0555e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.8467e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3332e+01 </func>
</region>
</regions>
<internal rank="554" log_i="1724765674.535179" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="555" mpi_size="696" stamp_init="1724765564.746042" stamp_final="1724765674.529561" username="apac4" allocationname="unknown" flags="0" pid="504045" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09784e+02" utime="8.49726e+01" stime="1.30892e+01" mtime="6.75028e+01" gflop="0.00000e+00" gbyte="3.74874e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.75028e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000096149614b5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09656e+02" utime="8.49413e+01" stime="1.30787e+01" mtime="6.75028e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.75028e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2040e+09" > 6.1643e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2102e+09" > 2.8399e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5434e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.4431e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0021e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4131e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7066e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0579e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.7355e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4241e+01 </func>
</region>
</regions>
<internal rank="555" log_i="1724765674.529561" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="556" mpi_size="696" stamp_init="1724765564.748239" stamp_final="1724765674.529799" username="apac4" allocationname="unknown" flags="0" pid="504046" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09782e+02" utime="6.43936e+01" stime="1.21425e+01" mtime="4.74561e+01" gflop="0.00000e+00" gbyte="3.76511e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.74561e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c614c514f9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09659e+02" utime="6.43616e+01" stime="1.21332e+01" mtime="4.74561e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.74561e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127200" bytes="1.2194e+09" > 7.6247e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2234e+09" > 4.0892e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7025e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 4.6239e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0137e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2840e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4121e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7230e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0560e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.8372e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3540e+01 </func>
</region>
</regions>
<internal rank="556" log_i="1724765674.529799" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="557" mpi_size="696" stamp_init="1724765564.746421" stamp_final="1724765674.530745" username="apac4" allocationname="unknown" flags="0" pid="504047" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09784e+02" utime="6.79631e+01" stime="9.66149e+00" mtime="4.71866e+01" gflop="0.00000e+00" gbyte="3.76926e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.71866e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008e148e146e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09656e+02" utime="6.79383e+01" stime="9.64413e+00" mtime="4.71866e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.71866e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2004e+09" > 6.3192e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1984e+09" > 2.6261e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7806e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.3925e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8133e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0531e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4122e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7132e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0580e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.7331e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4181e+01 </func>
</region>
</regions>
<internal rank="557" log_i="1724765674.530745" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="558" mpi_size="696" stamp_init="1724765564.747572" stamp_final="1724765674.538577" username="apac4" allocationname="unknown" flags="0" pid="504048" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09791e+02" utime="7.01963e+01" stime="1.38290e+01" mtime="5.39464e+01" gflop="0.00000e+00" gbyte="3.77777e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.39464e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001914fa55191419146c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09664e+02" utime="7.01652e+01" stime="1.38184e+01" mtime="5.39464e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.39464e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1939e+09" > 7.0133e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1895e+09" > 3.1887e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5925e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 4.6186e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6785e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2871e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4126e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7220e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0566e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.8371e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3567e+01 </func>
</region>
</regions>
<internal rank="558" log_i="1724765674.538577" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="559" mpi_size="696" stamp_init="1724765564.747808" stamp_final="1724765674.530476" username="apac4" allocationname="unknown" flags="0" pid="504049" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09783e+02" utime="6.03419e+01" stime="7.97467e+00" mtime="3.79432e+01" gflop="0.00000e+00" gbyte="3.77869e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.79432e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09658e+02" utime="6.03032e+01" stime="7.97120e+00" mtime="3.79432e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.79432e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2046e+09" > 6.2002e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2066e+09" > 2.4646e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6715e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 4.6305e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3214e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4114e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7244e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0603e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.7319e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4348e+01 </func>
</region>
</regions>
<internal rank="559" log_i="1724765674.530476" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="560" mpi_size="696" stamp_init="1724765564.747210" stamp_final="1724765674.524381" username="apac4" allocationname="unknown" flags="0" pid="504050" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09777e+02" utime="6.25869e+01" stime="1.02597e+01" mtime="4.30263e+01" gflop="0.00000e+00" gbyte="3.78475e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.30263e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d014d214d3143655d314d2146e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09652e+02" utime="6.25654e+01" stime="1.02398e+01" mtime="4.30263e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.30263e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2137e+09" > 6.8602e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2082e+09" > 3.3891e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1932e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.5379e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.9257e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.4911e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4108e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7405e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0583e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.8250e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4005e+01 </func>
</region>
</regions>
<internal rank="560" log_i="1724765674.524381" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="561" mpi_size="696" stamp_init="1724765564.746680" stamp_final="1724765674.533553" username="apac4" allocationname="unknown" flags="0" pid="504051" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09787e+02" utime="7.22120e+01" stime="1.02395e+01" mtime="5.18049e+01" gflop="0.00000e+00" gbyte="3.78357e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.18049e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44a154c154d1565554d154d1528" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09660e+02" utime="7.21807e+01" stime="1.02287e+01" mtime="5.18049e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.18049e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1916e+09" > 6.1020e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1975e+09" > 2.5080e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0902e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.8751e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0266e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.1597e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4100e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7337e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0587e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.7329e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4687e+01 </func>
</region>
</regions>
<internal rank="561" log_i="1724765674.533553" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="562" mpi_size="696" stamp_init="1724765564.746596" stamp_final="1724765674.524336" username="apac4" allocationname="unknown" flags="0" pid="504052" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09778e+02" utime="8.07822e+01" stime="1.99038e+01" mtime="7.24709e+01" gflop="0.00000e+00" gbyte="3.77831e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24709e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09656e+02" utime="8.07511e+01" stime="1.98927e+01" mtime="7.24709e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24709e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2125e+09" > 8.4057e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2116e+09" > 5.3909e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4659e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.5429e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6139e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0872e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4092e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7513e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0613e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.8243e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3785e+01 </func>
</region>
</regions>
<internal rank="562" log_i="1724765674.524336" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="563" mpi_size="696" stamp_init="1724765564.746033" stamp_final="1724765674.529720" username="apac4" allocationname="unknown" flags="0" pid="504053" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09784e+02" utime="5.95458e+01" stime="7.69922e+00" mtime="3.67719e+01" gflop="0.00000e+00" gbyte="3.77655e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.67719e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ec157b56ec15ec1540" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09655e+02" utime="5.95115e+01" stime="7.69145e+00" mtime="3.67719e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.67719e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1859e+09" > 6.3078e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1842e+09" > 2.5268e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2035e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.5493e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.4281e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4047e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7921e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0561e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.7337e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4720e+01 </func>
</region>
</regions>
<internal rank="563" log_i="1724765674.529720" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="564" mpi_size="696" stamp_init="1724765564.746726" stamp_final="1724765674.532677" username="apac4" allocationname="unknown" flags="0" pid="504054" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09786e+02" utime="6.12306e+01" stime="9.52458e+00" mtime="4.18408e+01" gflop="0.00000e+00" gbyte="3.77434e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.18408e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4221424142514cb56251424147d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09658e+02" utime="6.11994e+01" stime="9.51418e+00" mtime="4.18408e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.18408e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1878e+09" > 8.6052e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1915e+09" > 4.0162e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5753e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.1371e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1410e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8300e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4093e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7558e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0555e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.7168e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3579e+01 </func>
</region>
</regions>
<internal rank="564" log_i="1724765674.532677" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="565" mpi_size="696" stamp_init="1724765564.746723" stamp_final="1724765674.540157" username="apac4" allocationname="unknown" flags="0" pid="504055" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09793e+02" utime="6.50185e+01" stime="8.53487e+00" mtime="4.30771e+01" gflop="0.00000e+00" gbyte="3.77842e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.30771e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf419141a141b1491551b141b1462" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09668e+02" utime="6.49873e+01" stime="8.52408e+00" mtime="4.30771e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.30771e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2184e+09" > 6.1972e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2118e+09" > 2.9664e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3810e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.8220e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.1910e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4080e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7561e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0589e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.7339e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4577e+01 </func>
</region>
</regions>
<internal rank="565" log_i="1724765674.540157" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="566" mpi_size="696" stamp_init="1724765564.746040" stamp_final="1724765674.529824" username="apac4" allocationname="unknown" flags="0" pid="504056" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09784e+02" utime="7.36323e+01" stime="1.43558e+01" mtime="5.87200e+01" gflop="0.00000e+00" gbyte="3.77434e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.87200e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ef15f115f2153055f215f21522" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09659e+02" utime="7.36033e+01" stime="1.43425e+01" mtime="5.87200e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.87200e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2169e+09" > 8.3616e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2061e+09" > 4.5919e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8846e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.6936e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6802e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3610e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4079e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7621e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0588e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.7517e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3815e+01 </func>
</region>
</regions>
<internal rank="566" log_i="1724765674.529824" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="567" mpi_size="696" stamp_init="1724765564.747839" stamp_final="1724765674.533453" username="apac4" allocationname="unknown" flags="0" pid="504057" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09786e+02" utime="6.46558e+01" stime="8.77829e+00" mtime="4.30577e+01" gflop="0.00000e+00" gbyte="3.77827e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.30577e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09661e+02" utime="6.46302e+01" stime="8.76175e+00" mtime="4.30577e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.30577e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2168e+09" > 6.5089e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2107e+09" > 2.6911e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4522e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.7020e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.6018e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4057e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7834e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0607e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.7335e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4583e+01 </func>
</region>
</regions>
<internal rank="567" log_i="1724765674.533453" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="568" mpi_size="696" stamp_init="1724765564.748238" stamp_final="1724765674.529829" username="apac4" allocationname="unknown" flags="0" pid="504058" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09782e+02" utime="6.68636e+01" stime="1.30545e+01" mtime="5.02916e+01" gflop="0.00000e+00" gbyte="3.76541e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.02916e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000791482557914791464" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09656e+02" utime="6.68348e+01" stime="1.30416e+01" mtime="5.02916e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.02916e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2045e+09" > 8.2362e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2178e+09" > 3.2100e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7485e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.8700e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3839e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1036e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4066e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7816e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0573e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.7361e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3335e+01 </func>
</region>
</regions>
<internal rank="568" log_i="1724765674.529829" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="569" mpi_size="696" stamp_init="1724765564.746034" stamp_final="1724765674.537009" username="apac4" allocationname="unknown" flags="0" pid="504059" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09791e+02" utime="8.51291e+01" stime="1.32758e+01" mtime="6.78669e+01" gflop="0.00000e+00" gbyte="3.76869e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.78669e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005915591544" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09664e+02" utime="8.50975e+01" stime="1.32653e+01" mtime="6.78669e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.78669e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.5831e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2023e+09" > 6.5554e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2142e+09" > 2.7732e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4799e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.4689e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.4802e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4054e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7698e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0592e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.7329e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4373e+01 </func>
</region>
</regions>
<internal rank="569" log_i="1724765674.537009" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="570" mpi_size="696" stamp_init="1724765564.746879" stamp_final="1724765674.536254" username="apac4" allocationname="unknown" flags="0" pid="504060" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09789e+02" utime="8.11185e+01" stime="2.00823e+01" mtime="7.15434e+01" gflop="0.00000e+00" gbyte="3.76503e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15434e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4db14dd14de14d855de14de147f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09663e+02" utime="8.10869e+01" stime="2.00723e+01" mtime="7.15434e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15434e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2016e+09" > 8.2608e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1985e+09" > 4.3745e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.6195e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.8858e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5528e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1301e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4072e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7689e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0609e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.7357e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2998e+01 </func>
</region>
</regions>
<internal rank="570" log_i="1724765674.536254" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="571" mpi_size="696" stamp_init="1724765564.746502" stamp_final="1724765674.533719" username="apac4" allocationname="unknown" flags="0" pid="504061" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09787e+02" utime="5.96678e+01" stime="7.89883e+00" mtime="3.69836e+01" gflop="0.00000e+00" gbyte="3.77659e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.69836e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4151416141814ad5618141714c0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09660e+02" utime="5.96362e+01" stime="7.88862e+00" mtime="3.69836e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.69836e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1863e+09" > 6.5135e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1870e+09" > 2.9293e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1353e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.8741e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1349e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4073e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7531e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0602e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.7317e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4652e+01 </func>
</region>
</regions>
<internal rank="571" log_i="1724765674.533719" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="572" mpi_size="696" stamp_init="1724765564.747428" stamp_final="1724765674.533630" username="apac4" allocationname="unknown" flags="0" pid="504062" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09786e+02" utime="5.88097e+01" stime="8.83035e+00" mtime="3.84255e+01" gflop="0.00000e+00" gbyte="3.77834e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.84255e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000084148414f7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09661e+02" utime="5.87797e+01" stime="8.81820e+00" mtime="3.84255e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.84255e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2019e+09" > 7.6980e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1929e+09" > 3.4375e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8058e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 4.2193e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0326e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7915e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4060e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7869e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0576e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.7357e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3920e+01 </func>
</region>
</regions>
<internal rank="572" log_i="1724765674.533630" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="573" mpi_size="696" stamp_init="1724765564.748173" stamp_final="1724765674.529869" username="apac4" allocationname="unknown" flags="0" pid="504063" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09782e+02" utime="5.99368e+01" stime="7.90497e+00" mtime="3.75659e+01" gflop="0.00000e+00" gbyte="3.77029e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.75659e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09656e+02" utime="5.99039e+01" stime="7.89615e+00" mtime="3.75659e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.75659e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2196e+09" > 6.5333e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2073e+09" > 2.9221e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8336e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 4.2112e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6941e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0991e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4059e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7638e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0585e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.7291e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4198e+01 </func>
</region>
</regions>
<internal rank="573" log_i="1724765674.529869" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="574" mpi_size="696" stamp_init="1724765564.747091" stamp_final="1724765674.528748" username="apac4" allocationname="unknown" flags="0" pid="504064" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09782e+02" utime="5.92873e+01" stime="9.99143e+00" mtime="4.05815e+01" gflop="0.00000e+00" gbyte="3.77846e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.05815e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000601483556014601468" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09657e+02" utime="5.92486e+01" stime="9.98776e+00" mtime="4.05815e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.05815e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1968e+09" > 9.5966e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1986e+09" > 5.2466e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4313e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.3469e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1897e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1103e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4061e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7847e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0587e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.7360e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3148e+01 </func>
</region>
</regions>
<internal rank="574" log_i="1724765674.528748" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="575" mpi_size="696" stamp_init="1724765564.746092" stamp_final="1724765674.530454" username="apac4" allocationname="unknown" flags="0" pid="504065" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="1.09784e+02" utime="6.41981e+01" stime="8.71150e+00" mtime="4.26000e+01" gflop="0.00000e+00" gbyte="3.76720e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.26000e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf479147a147b14b1557b147b14df" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09657e+02" utime="6.41612e+01" stime="8.70638e+00" mtime="4.26000e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.26000e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1946e+09" > 6.3299e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1874e+09" > 3.1654e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8426e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.2074e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1683e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4058e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7680e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0595e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 4.6809e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4244e+01 </func>
</region>
</regions>
<internal rank="575" log_i="1724765674.530454" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="576" mpi_size="696" stamp_init="1724765564.931909" stamp_final="1724765674.540253" username="apac4" allocationname="unknown" flags="0" pid="617105" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09608e+02" utime="5.04331e+01" stime="1.41032e+01" mtime="3.39529e+01" gflop="0.00000e+00" gbyte="3.85971e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.39529e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002d152d1519" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09483e+02" utime="5.04037e+01" stime="1.40909e+01" mtime="3.39529e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.39529e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1942e+09" > 7.6562e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2043e+09" > 3.7403e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7174e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.5670e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.6255e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1120e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4036e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8046e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0534e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1493e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3492e+01 </func>
</region>
</regions>
<internal rank="576" log_i="1724765674.540253" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="577" mpi_size="696" stamp_init="1724765564.931989" stamp_final="1724765674.534203" username="apac4" allocationname="unknown" flags="0" pid="617106" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09602e+02" utime="5.95770e+01" stime="7.64017e+00" mtime="3.60262e+01" gflop="0.00000e+00" gbyte="3.76583e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.60262e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002f14a3552f142e146d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09472e+02" utime="5.95456e+01" stime="7.62824e+00" mtime="3.60262e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.60262e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2042e+09" > 5.8429e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2016e+09" > 3.6741e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8916e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.4535e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1322e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4051e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7890e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0533e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1495e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4349e+01 </func>
</region>
</regions>
<internal rank="577" log_i="1724765674.534203" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="578" mpi_size="696" stamp_init="1724765564.931918" stamp_final="1724765674.528332" username="apac4" allocationname="unknown" flags="0" pid="617107" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09596e+02" utime="5.91508e+01" stime="8.99753e+00" mtime="3.76625e+01" gflop="0.00000e+00" gbyte="3.77129e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.76625e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09472e+02" utime="5.91217e+01" stime="8.98470e+00" mtime="3.76625e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.76625e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2159e+09" > 7.4044e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2129e+09" > 4.0211e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.6489e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 6.0620e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8883e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3464e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4051e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7898e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0549e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1498e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3457e+01 </func>
</region>
</regions>
<internal rank="578" log_i="1724765674.528332" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="579" mpi_size="696" stamp_init="1724765564.931965" stamp_final="1724765674.531037" username="apac4" allocationname="unknown" flags="0" pid="617108" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09599e+02" utime="7.38456e+01" stime="1.04811e+01" mtime="5.36520e+01" gflop="0.00000e+00" gbyte="3.77388e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.36520e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c114c314c414f056c414c314ca" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09476e+02" utime="7.38118e+01" stime="1.04729e+01" mtime="5.36520e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.36520e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2097e+09" > 6.0286e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2170e+09" > 3.8064e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2446e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.0495e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3842e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.8283e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4051e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7943e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0506e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1497e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4560e+01 </func>
</region>
</regions>
<internal rank="579" log_i="1724765674.531037" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="580" mpi_size="696" stamp_init="1724765564.931932" stamp_final="1724765674.534042" username="apac4" allocationname="unknown" flags="0" pid="617109" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09602e+02" utime="5.63203e+01" stime="8.64169e+00" mtime="3.49076e+01" gflop="0.00000e+00" gbyte="3.77575e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.49076e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a015a215a3155b56a315a31515" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09475e+02" utime="5.62895e+01" stime="8.63077e+00" mtime="3.49076e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.49076e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2245e+09" > 7.7939e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2219e+09" > 5.1540e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.5443e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.0578e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6117e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2718e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4047e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7959e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0523e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1500e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3764e+01 </func>
</region>
</regions>
<internal rank="580" log_i="1724765674.534042" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="581" mpi_size="696" stamp_init="1724765564.931974" stamp_final="1724765674.531111" username="apac4" allocationname="unknown" flags="0" pid="617110" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09599e+02" utime="5.99339e+01" stime="7.63497e+00" mtime="3.65564e+01" gflop="0.00000e+00" gbyte="3.75198e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.65564e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003114d05531142c1476" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09475e+02" utime="5.99012e+01" stime="7.62577e+00" mtime="3.65564e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.65564e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1969e+09" > 5.7354e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1989e+09" > 3.4961e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2217e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.7450e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5272e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1547e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4044e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.7978e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0495e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1498e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4270e+01 </func>
</region>
</regions>
<internal rank="581" log_i="1724765674.531111" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="582" mpi_size="696" stamp_init="1724765564.931901" stamp_final="1724765674.533410" username="apac4" allocationname="unknown" flags="0" pid="617111" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09602e+02" utime="7.34161e+01" stime="1.45386e+01" mtime="5.77591e+01" gflop="0.00000e+00" gbyte="3.78273e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.77591e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005e145d14c7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09475e+02" utime="7.33803e+01" stime="1.45322e+01" mtime="5.77591e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.77591e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1895e+09" > 7.7249e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1908e+09" > 4.0125e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3058e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.1695e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6475e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.3575e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4033e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8113e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0516e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1494e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3278e+01 </func>
</region>
</regions>
<internal rank="582" log_i="1724765674.533410" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="583" mpi_size="696" stamp_init="1724765564.931949" stamp_final="1724765674.525028" username="apac4" allocationname="unknown" flags="0" pid="617112" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09593e+02" utime="6.43919e+01" stime="8.61710e+00" mtime="4.24029e+01" gflop="0.00000e+00" gbyte="3.77869e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.24029e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09469e+02" utime="6.43622e+01" stime="8.60476e+00" mtime="4.24029e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.24029e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1989e+09" > 5.7486e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2027e+09" > 3.9408e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5003e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.1671e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.4162e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4041e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8024e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0508e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1498e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4403e+01 </func>
</region>
</regions>
<internal rank="583" log_i="1724765674.525028" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="584" mpi_size="696" stamp_init="1724765564.931954" stamp_final="1724765674.539027" username="apac4" allocationname="unknown" flags="0" pid="617113" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09607e+02" utime="6.13637e+01" stime="1.19275e+01" mtime="4.38332e+01" gflop="0.00000e+00" gbyte="3.77228e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.38332e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b215b21553" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09483e+02" utime="6.13323e+01" stime="1.19164e+01" mtime="4.38332e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.38332e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2135e+09" > 1.0609e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2182e+09" > 5.5546e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.2654e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.5858e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.5770e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.6413e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4036e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8068e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0518e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1497e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3578e+01 </func>
</region>
</regions>
<internal rank="584" log_i="1724765674.539027" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="585" mpi_size="696" stamp_init="1724765564.931943" stamp_final="1724765674.524404" username="apac4" allocationname="unknown" flags="0" pid="617114" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09592e+02" utime="6.52169e+01" stime="8.50937e+00" mtime="4.32176e+01" gflop="0.00000e+00" gbyte="3.77407e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.32176e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000eb14ea55eb14ea14f8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09467e+02" utime="6.51845e+01" stime="8.50005e+00" mtime="4.32176e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.32176e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2020e+09" > 6.1434e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1918e+09" > 3.4576e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4398e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.9157e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.2244e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4035e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8102e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0461e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1496e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4502e+01 </func>
</region>
</regions>
<internal rank="585" log_i="1724765674.524404" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="586" mpi_size="696" stamp_init="1724765564.931944" stamp_final="1724765674.538822" username="apac4" allocationname="unknown" flags="0" pid="617115" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09607e+02" utime="7.61509e+01" stime="1.40202e+01" mtime="6.04970e+01" gflop="0.00000e+00" gbyte="3.77644e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.04970e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006915691552" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09481e+02" utime="7.61180e+01" stime="1.40112e+01" mtime="6.04970e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.04970e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2143e+09" > 7.2388e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2143e+09" > 4.0676e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2728e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.6038e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1724e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.3876e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4032e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8143e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0502e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1496e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3986e+01 </func>
</region>
</regions>
<internal rank="586" log_i="1724765674.538822" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="587" mpi_size="696" stamp_init="1724765564.931962" stamp_final="1724765674.531740" username="apac4" allocationname="unknown" flags="0" pid="617116" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09600e+02" utime="7.06276e+01" stime="9.26612e+00" mtime="4.90565e+01" gflop="0.00000e+00" gbyte="3.77724e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.90565e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09479e+02" utime="7.05894e+01" stime="9.26234e+00" mtime="4.90565e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.90565e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1778e+09" > 6.0376e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1880e+09" > 3.4636e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8011e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.6040e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.6904e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4030e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8149e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0511e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1498e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4866e+01 </func>
</region>
</regions>
<internal rank="587" log_i="1724765674.531740" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="588" mpi_size="696" stamp_init="1724765564.931936" stamp_final="1724765674.521058" username="apac4" allocationname="unknown" flags="0" pid="617117" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09589e+02" utime="6.22036e+01" stime="1.17305e+01" mtime="4.52325e+01" gflop="0.00000e+00" gbyte="3.76865e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.52325e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09461e+02" utime="6.21693e+01" stime="1.17232e+01" mtime="4.52325e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.52325e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1937e+09" > 8.7167e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1945e+09" > 6.2340e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9021e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.5725e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0593e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.6985e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4018e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8200e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0539e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1496e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3521e+01 </func>
</region>
</regions>
<internal rank="588" log_i="1724765674.521058" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="589" mpi_size="696" stamp_init="1724765564.931981" stamp_final="1724765674.524460" username="apac4" allocationname="unknown" flags="0" pid="617118" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09592e+02" utime="5.86984e+01" stime="7.58958e+00" mtime="3.57356e+01" gflop="0.00000e+00" gbyte="3.77224e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.57356e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004b154a151a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09470e+02" utime="5.86671e+01" stime="7.57911e+00" mtime="3.57356e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.57356e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2137e+09" > 6.0933e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2138e+09" > 3.2223e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3653e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.5758e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.2994e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4017e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8232e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0439e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1495e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4443e+01 </func>
</region>
</regions>
<internal rank="589" log_i="1724765674.524460" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="590" mpi_size="696" stamp_init="1724765564.931891" stamp_final="1724765674.538554" username="apac4" allocationname="unknown" flags="0" pid="617119" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09607e+02" utime="5.89209e+01" stime="8.47952e+00" mtime="3.73454e+01" gflop="0.00000e+00" gbyte="3.76751e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.73454e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49b15b315c5156f56c515c01553" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09481e+02" utime="5.88876e+01" stime="8.47113e+00" mtime="3.73454e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.73454e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2052e+09" > 6.9747e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2145e+09" > 3.5520e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8660e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 4.8961e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.3685e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3968e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4025e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8097e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0506e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1494e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4141e+01 </func>
</region>
</regions>
<internal rank="590" log_i="1724765674.538554" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="591" mpi_size="696" stamp_init="1724765564.931952" stamp_final="1724765674.524095" username="apac4" allocationname="unknown" flags="0" pid="617120" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09592e+02" utime="6.28157e+01" stime="8.16194e+00" mtime="4.00371e+01" gflop="0.00000e+00" gbyte="3.77300e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.00371e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09472e+02" utime="6.27843e+01" stime="8.15099e+00" mtime="4.00371e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.00371e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2112e+09" > 5.9731e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2213e+09" > 3.6304e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7287e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 7.2051e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.3709e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4028e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8166e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0509e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1496e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4759e+01 </func>
</region>
</regions>
<internal rank="591" log_i="1724765674.524095" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="592" mpi_size="696" stamp_init="1724765564.931930" stamp_final="1724765674.531702" username="apac4" allocationname="unknown" flags="0" pid="617121" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09600e+02" utime="6.49323e+01" stime="1.35999e+01" mtime="4.88384e+01" gflop="0.00000e+00" gbyte="3.76621e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.88384e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000fe159056fe15fe1528" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09474e+02" utime="6.48973e+01" stime="1.35933e+01" mtime="4.88384e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.88384e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2202e+09" > 8.5889e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2028e+09" > 4.9838e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7080e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.5614e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5334e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1418e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4014e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8276e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0521e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1498e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2616e+01 </func>
</region>
</regions>
<internal rank="592" log_i="1724765674.531702" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="593" mpi_size="696" stamp_init="1724765564.931942" stamp_final="1724765674.531766" username="apac4" allocationname="unknown" flags="0" pid="617122" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09600e+02" utime="7.26859e+01" stime="1.02837e+01" mtime="5.16926e+01" gflop="0.00000e+00" gbyte="3.76892e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.16926e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09474e+02" utime="7.26589e+01" stime="1.02687e+01" mtime="5.16926e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.16926e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2121e+09" > 5.8560e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2059e+09" > 3.7315e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7532e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.9149e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.4094e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.1863e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3985e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8577e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0494e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1497e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4456e+01 </func>
</region>
</regions>
<internal rank="593" log_i="1724765674.531766" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="594" mpi_size="696" stamp_init="1724765564.931904" stamp_final="1724765674.525125" username="apac4" allocationname="unknown" flags="0" pid="617123" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09593e+02" utime="5.90188e+01" stime="1.12476e+01" mtime="4.10702e+01" gflop="0.00000e+00" gbyte="3.76678e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.10702e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09468e+02" utime="5.89831e+01" stime="1.12417e+01" mtime="4.10702e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.10702e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1989e+09" > 1.3762e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1991e+09" > 7.9343e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.2196e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.5771e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.9136e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1501e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4007e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8338e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0528e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1495e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3194e+01 </func>
</region>
</regions>
<internal rank="594" log_i="1724765674.525125" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="595" mpi_size="696" stamp_init="1724765564.931942" stamp_final="1724765674.530478" username="apac4" allocationname="unknown" flags="0" pid="617124" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09599e+02" utime="5.78920e+01" stime="7.48104e+00" mtime="3.45227e+01" gflop="0.00000e+00" gbyte="3.77800e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.45227e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d5144b55d514d514cc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09473e+02" utime="5.78653e+01" stime="7.46572e+00" mtime="3.45227e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.45227e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1936e+09" > 6.1606e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1956e+09" > 3.6632e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1046e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.5668e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1773e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4002e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8422e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0498e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1497e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4487e+01 </func>
</region>
</regions>
<internal rank="595" log_i="1724765674.530478" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="596" mpi_size="696" stamp_init="1724765564.931893" stamp_final="1724765674.528607" username="apac4" allocationname="unknown" flags="0" pid="617125" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09597e+02" utime="6.49379e+01" stime="1.22785e+01" mtime="4.63423e+01" gflop="0.00000e+00" gbyte="3.77831e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.63423e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009d1498559d149d14e8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09471e+02" utime="6.49053e+01" stime="1.22687e+01" mtime="4.63423e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.63423e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1994e+09" > 7.2220e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2029e+09" > 3.7584e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7381e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.7839e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2809e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.8096e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4006e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8398e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0558e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1494e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3540e+01 </func>
</region>
</regions>
<internal rank="596" log_i="1724765674.528607" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="597" mpi_size="696" stamp_init="1724765564.931984" stamp_final="1724765674.535462" username="apac4" allocationname="unknown" flags="0" pid="617126" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09603e+02" utime="5.79367e+01" stime="7.67365e+00" mtime="3.43931e+01" gflop="0.00000e+00" gbyte="3.77743e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.43931e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf46c146e146f14fa566f146e14d7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09482e+02" utime="5.79103e+01" stime="7.65810e+00" mtime="3.43931e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.43931e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2074e+09" > 5.8712e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2098e+09" > 3.2120e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0333e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.7731e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.3198e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3996e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8493e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0554e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1499e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4256e+01 </func>
</region>
</regions>
<internal rank="597" log_i="1724765674.535462" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="598" mpi_size="696" stamp_init="1724765564.931952" stamp_final="1724765674.528138" username="apac4" allocationname="unknown" flags="0" pid="617127" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09596e+02" utime="5.59173e+01" stime="7.95903e+00" mtime="3.37097e+01" gflop="0.00000e+00" gbyte="3.76629e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.37097e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000941464569414941486" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09472e+02" utime="5.58798e+01" stime="7.95438e+00" mtime="3.37097e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.37097e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2136e+09" > 7.7126e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2055e+09" > 4.5347e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.5194e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.0562e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.9114e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0562e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4002e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8463e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 8.2016e-05 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.3273e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3599e+01 </func>
</region>
</regions>
<internal rank="598" log_i="1724765674.528138" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="599" mpi_size="696" stamp_init="1724765564.931976" stamp_final="1724765674.531352" username="apac4" allocationname="unknown" flags="0" pid="617128" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="1.09599e+02" utime="6.52510e+01" stime="8.65959e+00" mtime="4.27826e+01" gflop="0.00000e+00" gbyte="3.77365e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.27826e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b014b214b314a355b314b314d2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09478e+02" utime="6.52201e+01" stime="8.64836e+00" mtime="4.27826e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.27826e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1847e+09" > 6.1891e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1877e+09" > 3.4816e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9570e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0284e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5272e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2007e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3997e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8500e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0517e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1499e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4170e+01 </func>
</region>
</regions>
<internal rank="599" log_i="1724765674.531352" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="600" mpi_size="696" stamp_init="1724765564.724319" stamp_final="1724765674.535100" username="apac4" allocationname="unknown" flags="0" pid="3426223" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09811e+02" utime="7.44170e+01" stime="2.61627e+01" mtime="7.12904e+01" gflop="0.00000e+00" gbyte="3.85571e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.12904e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b0145155b014b014e4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09686e+02" utime="7.43817e+01" stime="2.61562e+01" mtime="7.12904e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.12904e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1990e+09" > 7.8475e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1954e+09" > 4.8547e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.5904e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9497e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3174e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1579e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3985e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8654e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0583e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1506e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3647e+01 </func>
</region>
</regions>
<internal rank="600" log_i="1724765674.535100" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="601" mpi_size="696" stamp_init="1724765564.723591" stamp_final="1724765674.523575" username="apac4" allocationname="unknown" flags="0" pid="3426224" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09800e+02" utime="8.98690e+01" stime="1.30260e+01" mtime="7.24157e+01" gflop="0.00000e+00" gbyte="3.76900e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24157e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4871489148a14b0568a148a147b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09672e+02" utime="8.98335e+01" stime="1.30188e+01" mtime="7.24157e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24157e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1933e+09" > 5.8656e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2024e+09" > 2.9017e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2867e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9504e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3855e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.2857e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4001e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8534e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0577e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1510e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4293e+01 </func>
</region>
</regions>
<internal rank="601" log_i="1724765674.523575" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="602" mpi_size="696" stamp_init="1724765564.723561" stamp_final="1724765674.530099" username="apac4" allocationname="unknown" flags="0" pid="3426225" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09807e+02" utime="8.49175e+01" stime="1.51633e+01" mtime="7.15320e+01" gflop="0.00000e+00" gbyte="3.78185e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15320e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005415195554154e150f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09681e+02" utime="8.48872e+01" stime="1.51512e+01" mtime="7.15320e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15320e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2064e+09" > 8.2730e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2120e+09" > 4.4777e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7870e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9485e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.3791e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1723e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3997e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8517e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0574e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1506e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3665e+01 </func>
</region>
</regions>
<internal rank="602" log_i="1724765674.530099" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="603" mpi_size="696" stamp_init="1724765564.723578" stamp_final="1724765674.530774" username="apac4" allocationname="unknown" flags="0" pid="3426226" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09807e+02" utime="8.95334e+01" stime="1.34618e+01" mtime="7.29282e+01" gflop="0.00000e+00" gbyte="3.77796e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.29282e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000af14c855af14af14af" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09683e+02" utime="8.94981e+01" stime="1.34544e+01" mtime="7.29282e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.29282e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2195e+09" > 5.7398e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2116e+09" > 3.0136e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3974e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9496e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7912e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4003e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8519e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0545e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1506e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4719e+01 </func>
</region>
</regions>
<internal rank="603" log_i="1724765674.530774" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="604" mpi_size="696" stamp_init="1724765564.724420" stamp_final="1724765674.535191" username="apac4" allocationname="unknown" flags="0" pid="3426227" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09811e+02" utime="8.74769e+01" stime="1.43659e+01" mtime="7.23386e+01" gflop="0.00000e+00" gbyte="3.74760e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23386e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004a1561554a154a151e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09687e+02" utime="8.74433e+01" stime="1.43577e+01" mtime="7.23386e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23386e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2202e+09" > 6.7837e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2201e+09" > 3.5390e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2285e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9489e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.2742e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6100e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3993e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8604e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0568e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1471e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4189e+01 </func>
</region>
</regions>
<internal rank="604" log_i="1724765674.535191" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="605" mpi_size="696" stamp_init="1724765564.723556" stamp_final="1724765674.535702" username="apac4" allocationname="unknown" flags="0" pid="3426228" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09812e+02" utime="8.96062e+01" stime="1.33892e+01" mtime="7.25755e+01" gflop="0.00000e+00" gbyte="3.77438e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25755e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b715a955b715b71533" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09689e+02" utime="8.95732e+01" stime="1.33794e+01" mtime="7.25755e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25755e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2006e+09" > 5.8574e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2003e+09" > 3.1467e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0118e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9486e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3232e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3997e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8581e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0593e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1505e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4747e+01 </func>
</region>
</regions>
<internal rank="605" log_i="1724765674.535702" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="606" mpi_size="696" stamp_init="1724765564.724171" stamp_final="1724765674.530644" username="apac4" allocationname="unknown" flags="0" pid="3426229" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09806e+02" utime="8.50537e+01" stime="1.49473e+01" mtime="7.14082e+01" gflop="0.00000e+00" gbyte="3.78227e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.14082e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09681e+02" utime="8.50243e+01" stime="1.49342e+01" mtime="7.14082e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.14082e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1859e+09" > 9.5725e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1890e+09" > 6.4726e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.4698e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9498e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.8452e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.4802e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3986e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8636e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0557e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1508e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3699e+01 </func>
</region>
</regions>
<internal rank="606" log_i="1724765674.530644" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="607" mpi_size="696" stamp_init="1724765564.723568" stamp_final="1724765674.524783" username="apac4" allocationname="unknown" flags="0" pid="3426230" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09801e+02" utime="8.94293e+01" stime="1.35114e+01" mtime="7.25067e+01" gflop="0.00000e+00" gbyte="3.76904e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25067e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ef15ef1515" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09678e+02" utime="8.93987e+01" stime="1.35000e+01" mtime="7.25067e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25067e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1951e+09" > 5.7434e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1974e+09" > 2.5949e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7784e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9510e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6005e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3978e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8735e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0565e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1504e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4907e+01 </func>
</region>
</regions>
<internal rank="607" log_i="1724765674.524783" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="608" mpi_size="696" stamp_init="1724765564.723545" stamp_final="1724765674.529423" username="apac4" allocationname="unknown" flags="0" pid="3426231" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09806e+02" utime="8.23867e+01" stime="1.90588e+01" mtime="7.20222e+01" gflop="0.00000e+00" gbyte="3.77995e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20222e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf419141b141c14c9551c141b147e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09680e+02" utime="8.23612e+01" stime="1.90422e+01" mtime="7.20222e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20222e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2221e+09" > 6.6965e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2087e+09" > 3.4615e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1315e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 6.3097e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3197e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0200e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3965e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8855e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0586e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1510e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3968e+01 </func>
</region>
</regions>
<internal rank="608" log_i="1724765674.529423" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="609" mpi_size="696" stamp_init="1724765564.724756" stamp_final="1724765674.530513" username="apac4" allocationname="unknown" flags="0" pid="3426232" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09806e+02" utime="8.93185e+01" stime="1.36636e+01" mtime="7.29481e+01" gflop="0.00000e+00" gbyte="3.76957e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.29481e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09682e+02" utime="8.92905e+01" stime="1.36502e+01" mtime="7.29481e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.29481e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1965e+09" > 5.7157e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1988e+09" > 2.6862e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4858e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9479e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2159e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2590e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3979e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8762e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0599e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1503e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4667e+01 </func>
</region>
</regions>
<internal rank="609" log_i="1724765674.530513" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="610" mpi_size="696" stamp_init="1724765564.723545" stamp_final="1724765674.523932" username="apac4" allocationname="unknown" flags="0" pid="3426233" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09800e+02" utime="8.78310e+01" stime="1.40099e+01" mtime="7.22621e+01" gflop="0.00000e+00" gbyte="3.78262e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22621e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f914f756f914f814f3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09675e+02" utime="8.77957e+01" stime="1.40028e+01" mtime="7.22621e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22621e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2098e+09" > 6.9299e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2105e+09" > 3.5356e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4041e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9492e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.2016e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1563e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3974e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8797e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0557e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1513e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3915e+01 </func>
</region>
</regions>
<internal rank="610" log_i="1724765674.523932" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="611" mpi_size="696" stamp_init="1724765564.723580" stamp_final="1724765674.523524" username="apac4" allocationname="unknown" flags="0" pid="3426234" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09800e+02" utime="8.91000e+01" stime="1.38519e+01" mtime="7.27348e+01" gflop="0.00000e+00" gbyte="3.76759e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27348e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09677e+02" utime="8.90649e+01" stime="1.38451e+01" mtime="7.27348e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27348e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1887e+09" > 5.5795e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1814e+09" > 3.2693e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0489e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9502e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.6012e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3970e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8862e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0572e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1506e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4877e+01 </func>
</region>
</regions>
<internal rank="611" log_i="1724765674.523524" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="612" mpi_size="696" stamp_init="1724765564.723558" stamp_final="1724765674.523578" username="apac4" allocationname="unknown" flags="0" pid="3426235" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09800e+02" utime="8.76763e+01" stime="1.41447e+01" mtime="7.21764e+01" gflop="0.00000e+00" gbyte="3.75298e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21764e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008f1555568f158e1536" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09673e+02" utime="8.76407e+01" stime="1.41384e+01" mtime="7.21764e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21764e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1974e+09" > 6.4824e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1945e+09" > 3.4897e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9385e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9502e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0967e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0204e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3970e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8850e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0542e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1510e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4340e+01 </func>
</region>
</regions>
<internal rank="612" log_i="1724765674.523578" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="613" mpi_size="696" stamp_init="1724765564.723558" stamp_final="1724765674.525561" username="apac4" allocationname="unknown" flags="0" pid="3426236" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09802e+02" utime="8.91483e+01" stime="1.37237e+01" mtime="7.30851e+01" gflop="0.00000e+00" gbyte="3.77457e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.30851e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49e15b615c8151855c815c3153a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09678e+02" utime="8.91145e+01" stime="1.37156e+01" mtime="7.30851e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.30851e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2097e+09" > 5.9190e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2211e+09" > 2.8069e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5794e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9503e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.1420e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3963e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8897e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0583e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1503e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4663e+01 </func>
</region>
</regions>
<internal rank="613" log_i="1724765674.525561" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="614" mpi_size="696" stamp_init="1724765564.723551" stamp_final="1724765674.530086" username="apac4" allocationname="unknown" flags="0" pid="3426237" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09807e+02" utime="8.70532e+01" stime="1.42735e+01" mtime="7.19873e+01" gflop="0.00000e+00" gbyte="3.78288e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19873e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43615371538151b56381538152a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09681e+02" utime="8.70263e+01" stime="1.42577e+01" mtime="7.19873e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19873e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2037e+09" > 7.0154e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2030e+09" > 3.7007e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7927e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9507e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0514e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.5380e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3958e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8925e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0599e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1511e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4236e+01 </func>
</region>
</regions>
<internal rank="614" log_i="1724765674.530086" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="615" mpi_size="696" stamp_init="1724765564.724491" stamp_final="1724765674.534143" username="apac4" allocationname="unknown" flags="0" pid="3426238" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09810e+02" utime="8.93505e+01" stime="1.36324e+01" mtime="7.29022e+01" gflop="0.00000e+00" gbyte="3.77487e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.29022e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b615b815b9156056b915b81523" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09685e+02" utime="8.93154e+01" stime="1.36258e+01" mtime="7.29022e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.29022e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2172e+09" > 5.6742e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2158e+09" > 2.6891e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0605e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9497e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6703e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0531e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3957e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.8953e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0554e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1502e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.5036e+01 </func>
</region>
</regions>
<internal rank="615" log_i="1724765674.534143" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="616" mpi_size="696" stamp_init="1724765564.724753" stamp_final="1724765674.525743" username="apac4" allocationname="unknown" flags="0" pid="3426239" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09801e+02" utime="8.37522e+01" stime="1.55091e+01" mtime="7.16896e+01" gflop="0.00000e+00" gbyte="3.76678e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16896e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09675e+02" utime="8.37243e+01" stime="1.54956e+01" mtime="7.16896e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16896e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2020e+09" > 1.2120e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2139e+09" > 9.2650e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0728e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9484e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.2671e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.0387e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3945e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9028e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0560e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1510e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3099e+01 </func>
</region>
</regions>
<internal rank="616" log_i="1724765674.525743" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="617" mpi_size="696" stamp_init="1724765564.723876" stamp_final="1724765674.530737" username="apac4" allocationname="unknown" flags="0" pid="3426240" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09807e+02" utime="8.97729e+01" stime="1.32831e+01" mtime="7.22076e+01" gflop="0.00000e+00" gbyte="3.75935e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22076e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09682e+02" utime="8.97438e+01" stime="1.32699e+01" mtime="7.22076e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22076e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2130e+09" > 6.1722e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2064e+09" > 2.8815e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.6848e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9494e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.7115e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3940e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9126e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0555e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1504e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4671e+01 </func>
</region>
</regions>
<internal rank="617" log_i="1724765674.530737" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="618" mpi_size="696" stamp_init="1724765564.723573" stamp_final="1724765674.535352" username="apac4" allocationname="unknown" flags="0" pid="3426241" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09812e+02" utime="8.67704e+01" stime="1.46548e+01" mtime="7.19230e+01" gflop="0.00000e+00" gbyte="3.75294e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19230e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000088154d56881588153f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09686e+02" utime="8.67390e+01" stime="1.46435e+01" mtime="7.19230e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19230e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1910e+09" > 7.0089e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1907e+09" > 3.4700e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7476e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9486e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5050e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.2983e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3948e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9084e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0574e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1509e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3238e+01 </func>
</region>
</regions>
<internal rank="618" log_i="1724765674.535352" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="619" mpi_size="696" stamp_init="1724765564.723591" stamp_final="1724765674.530718" username="apac4" allocationname="unknown" flags="0" pid="3426242" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09807e+02" utime="8.97162e+01" stime="1.32615e+01" mtime="7.18639e+01" gflop="0.00000e+00" gbyte="3.77884e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18639e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c914cb14cc14d955cc14cc149e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09681e+02" utime="8.96810e+01" stime="1.32548e+01" mtime="7.18639e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18639e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2026e+09" > 5.8880e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1945e+09" > 3.2110e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.3016e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9490e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.5201e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3953e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9035e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0602e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1506e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4739e+01 </func>
</region>
</regions>
<internal rank="619" log_i="1724765674.530718" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="620" mpi_size="696" stamp_init="1724765564.724634" stamp_final="1724765674.538886" username="apac4" allocationname="unknown" flags="0" pid="3426243" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09814e+02" utime="8.67280e+01" stime="1.45544e+01" mtime="7.16289e+01" gflop="0.00000e+00" gbyte="3.76709e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16289e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000981498147a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09689e+02" utime="8.66970e+01" stime="1.45435e+01" mtime="7.16289e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16289e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1995e+09" > 8.1321e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1976e+09" > 3.9454e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2705e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9484e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.4631e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.8310e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3941e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9116e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0568e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1508e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3300e+01 </func>
</region>
</regions>
<internal rank="620" log_i="1724765674.538886" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="621" mpi_size="696" stamp_init="1724765564.723575" stamp_final="1724765674.525942" username="apac4" allocationname="unknown" flags="0" pid="3426244" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09802e+02" utime="8.96311e+01" stime="1.34025e+01" mtime="7.23881e+01" gflop="0.00000e+00" gbyte="3.77361e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.23881e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f5151a55f515f5151f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09679e+02" utime="8.95945e+01" stime="1.33971e+01" mtime="7.23881e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.23881e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2008e+09" > 6.0091e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2018e+09" > 2.5805e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1301e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9502e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8668e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3930e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9276e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0554e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1505e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4416e+01 </func>
</region>
</regions>
<internal rank="621" log_i="1724765674.525942" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="622" mpi_size="696" stamp_init="1724765564.723592" stamp_final="1724765674.526447" username="apac4" allocationname="unknown" flags="0" pid="3426245" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09803e+02" utime="8.81754e+01" stime="1.37374e+01" mtime="7.13894e+01" gflop="0.00000e+00" gbyte="3.74840e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.13894e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f314f2149c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09676e+02" utime="8.81416e+01" stime="1.37295e+01" mtime="7.13894e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.13894e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127200" bytes="1.2106e+09" > 6.4324e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2133e+09" > 3.7171e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.4334e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9492e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1122e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.6914e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3930e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9216e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0545e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1508e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4071e+01 </func>
</region>
</regions>
<internal rank="622" log_i="1724765674.526447" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="623" mpi_size="696" stamp_init="1724765564.724791" stamp_final="1724765674.530983" username="apac4" allocationname="unknown" flags="0" pid="3426246" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u13b</host>
<perf wtime="1.09806e+02" utime="8.94843e+01" stime="1.35053e+01" mtime="7.22762e+01" gflop="0.00000e+00" gbyte="3.78063e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22762e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4621564156515b5566515641542" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09681e+02" utime="8.94548e+01" stime="1.34926e+01" mtime="7.22762e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22762e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1836e+09" > 5.8327e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1871e+09" > 2.8455e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7788e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9479e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0266e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8821e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3940e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9168e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0560e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1505e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4695e+01 </func>
</region>
</regions>
<internal rank="623" log_i="1724765674.530983" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="624" mpi_size="696" stamp_init="1724765564.748629" stamp_final="1724765674.524074" username="apac4" allocationname="unknown" flags="0" pid="158071" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09775e+02" utime="6.68026e+01" stime="2.20985e+01" mtime="5.87222e+01" gflop="0.00000e+00" gbyte="3.86520e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.87222e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09646e+02" utime="6.67695e+01" stime="2.20895e+01" mtime="5.87222e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.87222e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1996e+09" > 7.9611e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1976e+09" > 3.6814e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.1271e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 6.3174e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0645e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0579e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3925e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9240e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0555e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1524e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4089e+01 </func>
</region>
</regions>
<internal rank="624" log_i="1724765674.524074" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="625" mpi_size="696" stamp_init="1724765564.748691" stamp_final="1724765674.523838" username="apac4" allocationname="unknown" flags="0" pid="158072" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09775e+02" utime="8.58494e+01" stime="1.29475e+01" mtime="6.75863e+01" gflop="0.00000e+00" gbyte="3.77327e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.75863e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09642e+02" utime="8.58158e+01" stime="1.29381e+01" mtime="6.75863e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.75863e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2068e+09" > 6.1180e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1979e+09" > 3.2334e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.3009e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.5236e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2813e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3935e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9162e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0535e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1525e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4693e+01 </func>
</region>
</regions>
<internal rank="625" log_i="1724765674.523838" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="626" mpi_size="696" stamp_init="1724765564.748623" stamp_final="1724765674.527547" username="apac4" allocationname="unknown" flags="0" pid="158073" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09779e+02" utime="7.65112e+01" stime="1.55066e+01" mtime="6.25328e+01" gflop="0.00000e+00" gbyte="3.77605e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.25328e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000eb144255eb14eb14df" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09650e+02" utime="7.64772e+01" stime="1.54987e+01" mtime="6.25328e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.25328e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2063e+09" > 7.2119e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1975e+09" > 3.8160e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3129e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.2346e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7847e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9397e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3930e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9212e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0549e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1524e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3556e+01 </func>
</region>
</regions>
<internal rank="626" log_i="1724765674.527547" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="627" mpi_size="696" stamp_init="1724765564.748676" stamp_final="1724765674.525217" username="apac4" allocationname="unknown" flags="0" pid="158074" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09777e+02" utime="6.69448e+01" stime="8.96446e+00" mtime="4.54090e+01" gflop="0.00000e+00" gbyte="3.77903e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.54090e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4721574157515af557515751548" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09648e+02" utime="6.69109e+01" stime="8.95542e+00" mtime="4.54090e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.54090e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2143e+09" > 6.1110e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2167e+09" > 3.1387e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1137e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.2351e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0550e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3922e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9271e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0590e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1522e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4590e+01 </func>
</region>
</regions>
<internal rank="627" log_i="1724765674.525217" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="628" mpi_size="696" stamp_init="1724765564.748596" stamp_final="1724765674.529918" username="apac4" allocationname="unknown" flags="0" pid="158075" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09781e+02" utime="8.04648e+01" stime="1.90539e+01" mtime="7.02140e+01" gflop="0.00000e+00" gbyte="3.75500e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.02140e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008e14e4558e148e14db" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09655e+02" utime="8.04314e+01" stime="1.90453e+01" mtime="7.02140e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.02140e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2105e+09" > 7.5332e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2133e+09" > 4.0816e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0380e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 6.3095e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1349e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1153e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3925e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9200e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0586e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1523e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4012e+01 </func>
</region>
</regions>
<internal rank="628" log_i="1724765674.529918" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="629" mpi_size="696" stamp_init="1724765564.748701" stamp_final="1724765674.535908" username="apac4" allocationname="unknown" flags="0" pid="158076" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09787e+02" utime="6.30465e+01" stime="8.25646e+00" mtime="3.99753e+01" gflop="0.00000e+00" gbyte="3.78078e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.99753e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009614a05596149614c0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09657e+02" utime="6.30167e+01" stime="8.24379e+00" mtime="3.99753e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.99753e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2050e+09" > 6.0424e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1956e+09" > 2.7425e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.5712e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 7.7140e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.5979e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3920e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9337e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0592e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1521e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4346e+01 </func>
</region>
</regions>
<internal rank="629" log_i="1724765674.535908" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="630" mpi_size="696" stamp_init="1724765564.748644" stamp_final="1724765674.535941" username="apac4" allocationname="unknown" flags="0" pid="158077" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09787e+02" utime="5.91703e+01" stime="9.07212e+00" mtime="3.85176e+01" gflop="0.00000e+00" gbyte="3.78128e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.85176e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44e14501451141f55511451149a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09663e+02" utime="5.91392e+01" stime="9.06043e+00" mtime="3.85176e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.85176e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127200" bytes="1.1913e+09" > 7.2857e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1836e+09" > 4.2262e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9895e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 6.2966e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3995e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1399e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3923e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9266e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0534e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1524e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3749e+01 </func>
</region>
</regions>
<internal rank="630" log_i="1724765674.535941" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="631" mpi_size="696" stamp_init="1724765564.748705" stamp_final="1724765674.533480" username="apac4" allocationname="unknown" flags="0" pid="158078" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09785e+02" utime="7.84088e+01" stime="1.10580e+01" mtime="5.87780e+01" gflop="0.00000e+00" gbyte="3.77853e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.87780e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4121514151515d9551515141555" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09654e+02" utime="7.83737e+01" stime="1.10508e+01" mtime="5.87780e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.87780e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1962e+09" > 5.6976e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1920e+09" > 3.2428e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.6289e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.5874e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.5998e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3923e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9293e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0544e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1520e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4961e+01 </func>
</region>
</regions>
<internal rank="631" log_i="1724765674.533480" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="632" mpi_size="696" stamp_init="1724765564.748602" stamp_final="1724765674.536466" username="apac4" allocationname="unknown" flags="0" pid="158079" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09788e+02" utime="7.74768e+01" stime="1.70310e+01" mtime="6.51320e+01" gflop="0.00000e+00" gbyte="3.77827e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.51320e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002015b6552015201534" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09660e+02" utime="7.74515e+01" stime="1.70143e+01" mtime="6.51320e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.51320e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2038e+09" > 6.9477e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2169e+09" > 3.6104e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5059e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.3450e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3111e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.7403e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3915e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9394e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0544e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1523e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3721e+01 </func>
</region>
</regions>
<internal rank="632" log_i="1724765674.536466" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="633" mpi_size="696" stamp_init="1724765564.748678" stamp_final="1724765674.523961" username="apac4" allocationname="unknown" flags="0" pid="158080" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09775e+02" utime="6.48407e+01" stime="8.75061e+00" mtime="4.27332e+01" gflop="0.00000e+00" gbyte="3.77228e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.27332e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002914225629142914a8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09648e+02" utime="6.48064e+01" stime="8.74301e+00" mtime="4.27332e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.27332e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127200" bytes="1.2023e+09" > 5.6175e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1989e+09" > 2.9861e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.6794e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0191e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0841e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3919e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9343e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0593e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1519e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4559e+01 </func>
</region>
</regions>
<internal rank="633" log_i="1724765674.523961" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="634" mpi_size="696" stamp_init="1724765564.748616" stamp_final="1724765674.530665" username="apac4" allocationname="unknown" flags="0" pid="158081" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09782e+02" utime="8.25083e+01" stime="1.83154e+01" mtime="7.19871e+01" gflop="0.00000e+00" gbyte="3.76999e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19871e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09653e+02" utime="8.24694e+01" stime="1.83123e+01" mtime="7.19871e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19871e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127200" bytes="1.2110e+09" > 7.7175e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2113e+09" > 4.1877e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1244e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.5413e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4088e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0621e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3901e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9464e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0570e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1525e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3827e+01 </func>
</region>
</regions>
<internal rank="634" log_i="1724765674.530665" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="635" mpi_size="696" stamp_init="1724765564.748667" stamp_final="1724765674.536577" username="apac4" allocationname="unknown" flags="0" pid="158082" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09788e+02" utime="6.95261e+01" stime="9.37810e+00" mtime="4.87705e+01" gflop="0.00000e+00" gbyte="3.78029e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.87705e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ee15ef15f0159b55f015f01503" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09658e+02" utime="6.94928e+01" stime="9.36894e+00" mtime="4.87705e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.87705e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1880e+09" > 6.0159e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1883e+09" > 2.9114e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1597e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.5410e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0685e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3901e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9493e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0549e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1518e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4849e+01 </func>
</region>
</regions>
<internal rank="635" log_i="1724765674.536577" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="636" mpi_size="696" stamp_init="1724765564.748596" stamp_final="1724765674.524086" username="apac4" allocationname="unknown" flags="0" pid="158083" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09775e+02" utime="8.14630e+01" stime="1.95096e+01" mtime="7.20014e+01" gflop="0.00000e+00" gbyte="3.77815e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20014e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000be146c56be14be14fb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09651e+02" utime="8.14311e+01" stime="1.94988e+01" mtime="7.20014e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20014e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1989e+09" > 8.5267e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1958e+09" > 4.6096e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.4670e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.3547e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0313e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.7456e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3902e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9491e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0554e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1524e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4251e+01 </func>
</region>
</regions>
<internal rank="636" log_i="1724765674.524086" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="637" mpi_size="696" stamp_init="1724765564.748656" stamp_final="1724765674.528238" username="apac4" allocationname="unknown" flags="0" pid="158084" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09780e+02" utime="6.42122e+01" stime="8.69668e+00" mtime="4.28705e+01" gflop="0.00000e+00" gbyte="3.77037e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.28705e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09653e+02" utime="6.41828e+01" stime="8.68379e+00" mtime="4.28705e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.28705e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2253e+09" > 5.9188e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2150e+09" > 3.0107e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4136e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.3403e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.7723e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3902e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9484e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0576e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1518e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4777e+01 </func>
</region>
</regions>
<internal rank="637" log_i="1724765674.528238" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="638" mpi_size="696" stamp_init="1724765564.748635" stamp_final="1724765674.524513" username="apac4" allocationname="unknown" flags="0" pid="158085" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09776e+02" utime="8.82674e+01" stime="1.37826e+01" mtime="7.22259e+01" gflop="0.00000e+00" gbyte="3.76900e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22259e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49215931594156056941594152e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09651e+02" utime="8.82361e+01" stime="1.37713e+01" mtime="7.22259e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22259e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1962e+09" > 7.2419e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1975e+09" > 3.8293e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8141e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9697e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.7835e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0888e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3900e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9499e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0562e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1522e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4156e+01 </func>
</region>
</regions>
<internal rank="638" log_i="1724765674.524513" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="639" mpi_size="696" stamp_init="1724765564.748682" stamp_final="1724765674.531106" username="apac4" allocationname="unknown" flags="0" pid="158086" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09782e+02" utime="8.96286e+01" stime="1.35908e+01" mtime="7.26886e+01" gflop="0.00000e+00" gbyte="3.74565e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.26886e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ef15a856ef15ea1501" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09653e+02" utime="8.95943e+01" stime="1.35834e+01" mtime="7.26886e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.26886e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2168e+09" > 5.5751e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2138e+09" > 2.9607e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7923e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9710e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0781e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3890e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9610e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0532e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1518e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4880e+01 </func>
</region>
</regions>
<internal rank="639" log_i="1724765674.531106" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="640" mpi_size="696" stamp_init="1724765564.748636" stamp_final="1724765674.530865" username="apac4" allocationname="unknown" flags="0" pid="158087" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09782e+02" utime="7.13379e+01" stime="1.39633e+01" mtime="5.49881e+01" gflop="0.00000e+00" gbyte="3.77209e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.49881e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09653e+02" utime="7.13019e+01" stime="1.39572e+01" mtime="5.49881e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.49881e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2065e+09" > 7.3048e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2099e+09" > 4.3346e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9453e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 5.1835e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7810e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.1590e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3863e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9896e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0585e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1525e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3564e+01 </func>
</region>
</regions>
<internal rank="640" log_i="1724765674.530865" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="641" mpi_size="696" stamp_init="1724765564.748664" stamp_final="1724765674.531099" username="apac4" allocationname="unknown" flags="0" pid="158088" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09782e+02" utime="6.15199e+01" stime="8.41599e+00" mtime="3.94785e+01" gflop="0.00000e+00" gbyte="3.77426e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.94785e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09655e+02" utime="6.14863e+01" stime="8.40715e+00" mtime="3.94785e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.94785e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2044e+09" > 5.9152e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2125e+09" > 3.1385e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3884e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 6.5799e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.3498e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3859e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9906e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0571e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1518e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4176e+01 </func>
</region>
</regions>
<internal rank="641" log_i="1724765674.531099" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="642" mpi_size="696" stamp_init="1724765564.748603" stamp_final="1724765674.536165" username="apac4" allocationname="unknown" flags="0" pid="158089" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09788e+02" utime="6.08432e+01" stime="1.04158e+01" mtime="4.22965e+01" gflop="0.00000e+00" gbyte="3.76831e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.22965e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e414e3148c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09660e+02" utime="6.08095e+01" stime="1.04072e+01" mtime="4.22965e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.22965e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.2983e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1815e+09" > 9.5037e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1845e+09" > 5.8214e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1127e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.3937e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8569e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1040e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3842e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0057e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0536e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1525e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3203e+01 </func>
</region>
</regions>
<internal rank="642" log_i="1724765674.536165" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="643" mpi_size="696" stamp_init="1724765564.748694" stamp_final="1724765674.523783" username="apac4" allocationname="unknown" flags="0" pid="158090" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09775e+02" utime="6.33737e+01" stime="8.58710e+00" mtime="4.09778e+01" gflop="0.00000e+00" gbyte="3.76884e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.09778e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf46d156f157015c155701570151f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09647e+02" utime="6.33428e+01" stime="8.57618e+00" mtime="4.09778e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.09778e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1940e+09" > 5.6665e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1970e+09" > 3.2090e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.4830e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.3758e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1060e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3857e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9964e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0584e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1517e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4798e+01 </func>
</region>
</regions>
<internal rank="643" log_i="1724765674.523783" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="644" mpi_size="696" stamp_init="1724765564.749508" stamp_final="1724765674.532376" username="apac4" allocationname="unknown" flags="0" pid="158091" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09783e+02" utime="6.08345e+01" stime="1.06006e+01" mtime="4.26778e+01" gflop="0.00000e+00" gbyte="3.77113e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.26778e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4971499149a1491569a149a14bb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09655e+02" utime="6.07969e+01" stime="1.05964e+01" mtime="4.26778e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.26778e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1899e+09" > 1.0203e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1994e+09" > 5.7720e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.6078e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.2148e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6473e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1239e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3855e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 1.9970e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0549e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1526e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3414e+01 </func>
</region>
</regions>
<internal rank="644" log_i="1724765674.532376" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="645" mpi_size="696" stamp_init="1724765564.748656" stamp_final="1724765674.527562" username="apac4" allocationname="unknown" flags="0" pid="158092" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09779e+02" utime="6.48366e+01" stime="8.62176e+00" mtime="4.21258e+01" gflop="0.00000e+00" gbyte="3.75717e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.21258e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09649e+02" utime="6.48025e+01" stime="8.61339e+00" mtime="4.21258e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.21258e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1946e+09" > 5.7156e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2043e+09" > 2.9522e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7224e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.9121e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1683e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.9169e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3853e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0001e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0577e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1517e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4178e+01 </func>
</region>
</regions>
<internal rank="645" log_i="1724765674.527562" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="646" mpi_size="696" stamp_init="1724765564.749194" stamp_final="1724765674.527533" username="apac4" allocationname="unknown" flags="0" pid="158093" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09778e+02" utime="6.16542e+01" stime="9.55964e+00" mtime="4.13623e+01" gflop="0.00000e+00" gbyte="3.76556e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.13623e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4551457145814575658145814da" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09651e+02" utime="6.16217e+01" stime="9.55049e+00" mtime="4.13623e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.13623e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2189e+09" > 7.2469e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2194e+09" > 4.1141e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2122e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 9.2345e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.6253e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1417e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3848e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0059e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0584e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1524e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3433e+01 </func>
</region>
</regions>
<internal rank="646" log_i="1724765674.527533" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="647" mpi_size="696" stamp_init="1724765564.749482" stamp_final="1724765674.527311" username="apac4" allocationname="unknown" flags="0" pid="158094" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a02u11b</host>
<perf wtime="1.09778e+02" utime="6.48748e+01" stime="8.67615e+00" mtime="4.26372e+01" gflop="0.00000e+00" gbyte="3.76705e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.26372e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09649e+02" utime="6.48374e+01" stime="8.67142e+00" mtime="4.26372e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.26372e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1870e+09" > 5.8113e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1832e+09" > 3.2280e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8095e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0073e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1110e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3845e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0095e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0562e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1516e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4429e+01 </func>
</region>
</regions>
<internal rank="647" log_i="1724765674.527311" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="648" mpi_size="696" stamp_init="1724765564.881889" stamp_final="1724765674.533890" username="apac4" allocationname="unknown" flags="0" pid="894861" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09652e+02" utime="7.54107e+01" stime="2.55534e+01" mtime="7.16563e+01" gflop="0.00000e+00" gbyte="3.85693e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16563e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002c1576552c152b1505" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09523e+02" utime="7.53752e+01" stime="2.55471e+01" mtime="7.16563e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16563e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1967e+09" > 6.6288e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2044e+09" > 4.1166e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0457e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9431e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.3804e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.6928e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3845e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0074e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0625e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1526e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3762e+01 </func>
</region>
</regions>
<internal rank="648" log_i="1724765674.533890" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="649" mpi_size="696" stamp_init="1724765564.881785" stamp_final="1724765674.536513" username="apac4" allocationname="unknown" flags="0" pid="894862" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09655e+02" utime="8.98013e+01" stime="1.30698e+01" mtime="7.19265e+01" gflop="0.00000e+00" gbyte="3.75496e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19265e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e814e914eb14ae55eb14ea1475" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09523e+02" utime="8.97684e+01" stime="1.30595e+01" mtime="7.19265e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19265e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1986e+09" > 5.9786e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2060e+09" > 2.7965e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8485e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9417e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4168e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3853e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0020e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0575e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4765e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4351e+01 </func>
</region>
</regions>
<internal rank="649" log_i="1724765674.536513" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="650" mpi_size="696" stamp_init="1724765564.881906" stamp_final="1724765674.533113" username="apac4" allocationname="unknown" flags="0" pid="894863" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09651e+02" utime="8.67423e+01" stime="1.41402e+01" mtime="7.24315e+01" gflop="0.00000e+00" gbyte="3.74382e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24315e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001b147c551b141a14e9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09527e+02" utime="8.67050e+01" stime="1.41357e+01" mtime="7.24315e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24315e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1950e+09" > 7.9182e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1965e+09" > 4.5381e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0186e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9435e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.1750e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.7889e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3848e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0034e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0578e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1527e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3427e+01 </func>
</region>
</regions>
<internal rank="650" log_i="1724765674.533113" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="651" mpi_size="696" stamp_init="1724765564.881760" stamp_final="1724765674.539320" username="apac4" allocationname="unknown" flags="0" pid="894864" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09658e+02" utime="8.95936e+01" stime="1.33281e+01" mtime="7.24513e+01" gflop="0.00000e+00" gbyte="3.76343e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24513e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009414d2559414941480" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09530e+02" utime="8.95639e+01" stime="1.33161e+01" mtime="7.24513e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24513e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2238e+09" > 5.8882e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2146e+09" > 3.1018e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2283e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9431e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5751e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3845e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0075e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0614e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4762e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4488e+01 </func>
</region>
</regions>
<internal rank="651" log_i="1724765674.539320" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="652" mpi_size="696" stamp_init="1724765564.881879" stamp_final="1724765674.529286" username="apac4" allocationname="unknown" flags="0" pid="894865" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09647e+02" utime="8.73480e+01" stime="1.41243e+01" mtime="7.19694e+01" gflop="0.00000e+00" gbyte="3.76598e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.19694e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf479147a147b145d567b147b147e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09524e+02" utime="8.73121e+01" stime="1.41179e+01" mtime="7.19694e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.19694e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2057e+09" > 7.2175e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1990e+09" > 3.6989e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.3069e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9427e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5473e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.7834e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3845e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0110e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0596e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1527e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2764e+01 </func>
</region>
</regions>
<internal rank="652" log_i="1724765674.529286" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="653" mpi_size="696" stamp_init="1724765564.881796" stamp_final="1724765674.525906" username="apac4" allocationname="unknown" flags="0" pid="894866" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09644e+02" utime="8.94544e+01" stime="1.34636e+01" mtime="7.20777e+01" gflop="0.00000e+00" gbyte="3.77277e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20777e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4811482148414c45584148314db" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09515e+02" utime="8.94217e+01" stime="1.34542e+01" mtime="7.20777e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20777e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1938e+09" > 6.4192e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2114e+09" > 3.0369e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9196e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9412e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6451e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4433e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3845e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0109e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0623e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4745e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4388e+01 </func>
</region>
</regions>
<internal rank="653" log_i="1724765674.525906" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="654" mpi_size="696" stamp_init="1724765564.881858" stamp_final="1724765674.523835" username="apac4" allocationname="unknown" flags="0" pid="894867" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09642e+02" utime="8.60086e+01" stime="1.48266e+01" mtime="7.21193e+01" gflop="0.00000e+00" gbyte="3.75343e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21193e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000dd14d714c0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09516e+02" utime="8.59743e+01" stime="1.48192e+01" mtime="7.21193e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21193e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1839e+09" > 7.1158e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1906e+09" > 3.6002e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 7.0023e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9431e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8862e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9712e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3841e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0147e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0629e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1526e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3225e+01 </func>
</region>
</regions>
<internal rank="654" log_i="1724765674.523835" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="655" mpi_size="696" stamp_init="1724765564.881781" stamp_final="1724765674.535975" username="apac4" allocationname="unknown" flags="0" pid="894868" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09654e+02" utime="8.95548e+01" stime="1.33569e+01" mtime="7.18908e+01" gflop="0.00000e+00" gbyte="3.76534e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18908e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d314d414d5141055d514d514aa" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09527e+02" utime="8.95188e+01" stime="1.33509e+01" mtime="7.18908e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18908e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1963e+09" > 5.9993e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1994e+09" > 2.9551e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.6199e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9431e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4796e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.7072e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3834e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0231e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0590e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4368e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4526e+01 </func>
</region>
</regions>
<internal rank="655" log_i="1724765674.535975" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="656" mpi_size="696" stamp_init="1724765564.881899" stamp_final="1724765674.523192" username="apac4" allocationname="unknown" flags="0" pid="894869" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09641e+02" utime="8.62031e+01" stime="1.43231e+01" mtime="7.13078e+01" gflop="0.00000e+00" gbyte="3.77232e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.13078e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09513e+02" utime="8.61714e+01" stime="1.43130e+01" mtime="7.13078e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.13078e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.0252e-05 </func>
<func name="MPI_Isend" count="127200" bytes="1.2021e+09" > 1.0519e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1981e+09" > 4.1739e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8293e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9387e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.7731e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1012e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3819e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0341e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0589e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1513e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3241e+01 </func>
</region>
</regions>
<internal rank="656" log_i="1724765674.523192" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="657" mpi_size="696" stamp_init="1724765564.881809" stamp_final="1724765674.529649" username="apac4" allocationname="unknown" flags="0" pid="894870" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09648e+02" utime="8.94938e+01" stime="1.31714e+01" mtime="7.25144e+01" gflop="0.00000e+00" gbyte="3.77632e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.25144e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4771479147a14b6557a147914a8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09517e+02" utime="8.94581e+01" stime="1.31649e+01" mtime="7.25144e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.25144e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127200" bytes="1.1934e+09" > 6.0276e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2026e+09" > 2.2228e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4809e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9424e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.0242e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3808e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0485e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0591e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4750e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4298e+01 </func>
</region>
</regions>
<internal rank="657" log_i="1724765674.529649" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="658" mpi_size="696" stamp_init="1724765564.881977" stamp_final="1724765674.531632" username="apac4" allocationname="unknown" flags="0" pid="894871" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09650e+02" utime="8.68601e+01" stime="1.43988e+01" mtime="7.18720e+01" gflop="0.00000e+00" gbyte="3.76728e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18720e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f5155455f515f0151d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09526e+02" utime="8.68302e+01" stime="1.43872e+01" mtime="7.18720e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18720e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2127e+09" > 7.1255e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2160e+09" > 3.6585e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5530e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9427e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6311e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5367e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3811e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0410e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0596e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1481e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3428e+01 </func>
</region>
</regions>
<internal rank="658" log_i="1724765674.531632" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="659" mpi_size="696" stamp_init="1724765564.881794" stamp_final="1724765674.535047" username="apac4" allocationname="unknown" flags="0" pid="894872" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09653e+02" utime="8.89026e+01" stime="1.39550e+01" mtime="7.24040e+01" gflop="0.00000e+00" gbyte="3.76335e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24040e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09524e+02" utime="8.88778e+01" stime="1.39383e+01" mtime="7.24040e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24040e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1894e+09" > 6.1820e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1913e+09" > 2.8055e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2132e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9410e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5431e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3808e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0455e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0619e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4726e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4450e+01 </func>
</region>
</regions>
<internal rank="659" log_i="1724765674.535047" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="660" mpi_size="696" stamp_init="1724765564.881874" stamp_final="1724765674.533888" username="apac4" allocationname="unknown" flags="0" pid="894873" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09652e+02" utime="8.74593e+01" stime="1.37859e+01" mtime="7.16114e+01" gflop="0.00000e+00" gbyte="3.78052e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16114e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e715e915ea152a55ea15ea153b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09523e+02" utime="8.74291e+01" stime="1.37739e+01" mtime="7.16114e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16114e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1965e+09" > 6.9352e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1987e+09" > 3.8536e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.6716e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9423e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1492e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.7283e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3806e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0475e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0596e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1481e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4072e+01 </func>
</region>
</regions>
<internal rank="660" log_i="1724765674.533888" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="661" mpi_size="696" stamp_init="1724765564.881769" stamp_final="1724765674.530751" username="apac4" allocationname="unknown" flags="0" pid="894874" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09649e+02" utime="8.91790e+01" stime="1.36841e+01" mtime="7.24560e+01" gflop="0.00000e+00" gbyte="3.77781e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24560e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005e1495565e145d14ff" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09521e+02" utime="8.91472e+01" stime="1.36738e+01" mtime="7.24560e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24560e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.0967e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2170e+09" > 6.0360e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2146e+09" > 2.7100e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9309e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9427e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.4958e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3803e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0508e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0583e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4707e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4784e+01 </func>
</region>
</regions>
<internal rank="661" log_i="1724765674.530751" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="662" mpi_size="696" stamp_init="1724765564.881846" stamp_final="1724765674.526192" username="apac4" allocationname="unknown" flags="0" pid="894875" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09644e+02" utime="8.72997e+01" stime="1.39437e+01" mtime="7.17886e+01" gflop="0.00000e+00" gbyte="3.77445e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17886e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09516e+02" utime="8.72645e+01" stime="1.39371e+01" mtime="7.17886e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17886e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1940e+09" > 7.7264e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1887e+09" > 3.6197e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8022e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9432e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.2002e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.7760e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3796e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0594e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0584e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4370e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4060e+01 </func>
</region>
</regions>
<internal rank="662" log_i="1724765674.526192" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="663" mpi_size="696" stamp_init="1724765564.881760" stamp_final="1724765674.523414" username="apac4" allocationname="unknown" flags="0" pid="894876" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09642e+02" utime="8.90438e+01" stime="1.37832e+01" mtime="7.24226e+01" gflop="0.00000e+00" gbyte="3.77499e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.24226e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008e15f5558e158e152c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09513e+02" utime="8.90133e+01" stime="1.37714e+01" mtime="7.24226e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.24226e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2226e+09" > 6.3785e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2155e+09" > 2.6865e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0008e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9438e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2354e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3800e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0574e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0634e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4327e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4632e+01 </func>
</region>
</regions>
<internal rank="663" log_i="1724765674.523414" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="664" mpi_size="696" stamp_init="1724765564.881865" stamp_final="1724765674.526384" username="apac4" allocationname="unknown" flags="0" pid="894877" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09645e+02" utime="8.64978e+01" stime="1.40585e+01" mtime="7.17904e+01" gflop="0.00000e+00" gbyte="3.77003e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.17904e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ed14ec1476" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09515e+02" utime="8.64626e+01" stime="1.40516e+01" mtime="7.17904e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.17904e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2088e+09" > 9.5174e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2117e+09" > 4.9896e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0879e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9370e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.2502e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1048e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3788e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0607e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0632e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4745e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3604e+01 </func>
</region>
</regions>
<internal rank="664" log_i="1724765674.526384" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="665" mpi_size="696" stamp_init="1724765564.881819" stamp_final="1724765674.532477" username="apac4" allocationname="unknown" flags="0" pid="894878" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09651e+02" utime="8.88664e+01" stime="1.38013e+01" mtime="7.21787e+01" gflop="0.00000e+00" gbyte="3.77666e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.21787e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008d152d558d158d154f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09522e+02" utime="8.88321e+01" stime="1.37936e+01" mtime="7.21787e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.21787e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1967e+09" > 6.0380e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2030e+09" > 2.9120e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0924e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9439e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8029e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3791e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0630e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0578e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4709e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4332e+01 </func>
</region>
</regions>
<internal rank="665" log_i="1724765674.532477" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="666" mpi_size="696" stamp_init="1724765564.881842" stamp_final="1724765674.530488" username="apac4" allocationname="unknown" flags="0" pid="894879" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09649e+02" utime="8.74825e+01" stime="1.38742e+01" mtime="7.15970e+01" gflop="0.00000e+00" gbyte="3.76667e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.15970e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003014e25630143014dc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09525e+02" utime="8.74485e+01" stime="1.38660e+01" mtime="7.15970e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.15970e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1841e+09" > 7.5689e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1864e+09" > 4.4066e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3332e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9422e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.2677e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6722e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3784e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0690e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0621e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4742e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3354e+01 </func>
</region>
</regions>
<internal rank="666" log_i="1724765674.530488" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="667" mpi_size="696" stamp_init="1724765564.881760" stamp_final="1724765674.539524" username="apac4" allocationname="unknown" flags="0" pid="894880" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09658e+02" utime="8.91222e+01" stime="1.36765e+01" mtime="7.18340e+01" gflop="0.00000e+00" gbyte="3.77125e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.18340e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000dc14db14a3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09531e+02" utime="8.90882e+01" stime="1.36680e+01" mtime="7.18340e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.18340e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1912e+09" > 6.2373e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1901e+09" > 2.8303e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.6465e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9418e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.7740e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3788e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0696e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0589e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4691e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4428e+01 </func>
</region>
</regions>
<internal rank="667" log_i="1724765674.539524" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="668" mpi_size="696" stamp_init="1724765564.881856" stamp_final="1724765674.523914" username="apac4" allocationname="unknown" flags="0" pid="894881" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09642e+02" utime="8.63972e+01" stime="1.44556e+01" mtime="7.16560e+01" gflop="0.00000e+00" gbyte="3.77209e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.16560e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000021142114ed" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09513e+02" utime="8.63682e+01" stime="1.44424e+01" mtime="7.16560e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.16560e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.5831e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2026e+09" > 7.9587e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1905e+09" > 4.3144e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.8050e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9436e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.0662e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4876e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3789e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0666e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0580e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4739e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.2886e+01 </func>
</region>
</regions>
<internal rank="668" log_i="1724765674.523914" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="669" mpi_size="696" stamp_init="1724765564.881806" stamp_final="1724765674.531473" username="apac4" allocationname="unknown" flags="0" pid="894882" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09650e+02" utime="8.87127e+01" stime="1.40986e+01" mtime="7.27803e+01" gflop="0.00000e+00" gbyte="3.77850e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27803e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09522e+02" utime="8.86823e+01" stime="1.40863e+01" mtime="7.27803e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27803e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2014e+09" > 6.2290e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2026e+09" > 2.6328e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7897e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9422e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5330e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3780e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0735e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0630e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4682e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4234e+01 </func>
</region>
</regions>
<internal rank="669" log_i="1724765674.531473" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="670" mpi_size="696" stamp_init="1724765564.881878" stamp_final="1724765674.527046" username="apac4" allocationname="unknown" flags="0" pid="894883" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09645e+02" utime="8.57153e+01" stime="1.47442e+01" mtime="7.14205e+01" gflop="0.00000e+00" gbyte="3.78437e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.14205e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ba15bc15bd159355bd15bc1547" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09517e+02" utime="8.56856e+01" stime="1.47313e+01" mtime="7.14205e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.14205e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2291e+09" > 8.8008e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2200e+09" > 4.8654e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0831e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9423e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.2664e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4931e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3778e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0767e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0609e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4755e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3293e+01 </func>
</region>
</regions>
<internal rank="670" log_i="1724765674.527046" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="671" mpi_size="696" stamp_init="1724765564.881768" stamp_final="1724765674.538669" username="apac4" allocationname="unknown" flags="0" pid="894884" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="1.09657e+02" utime="8.94607e+01" stime="1.33735e+01" mtime="7.22363e+01" gflop="0.00000e+00" gbyte="3.76579e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22363e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d715a756d715d71542" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09529e+02" utime="8.94350e+01" stime="1.33569e+01" mtime="7.22363e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22363e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1865e+09" > 5.9322e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1893e+09" > 2.9151e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.3981e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9430e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4857e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3781e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0763e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0599e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4662e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4101e+01 </func>
</region>
</regions>
<internal rank="671" log_i="1724765674.538669" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="672" mpi_size="696" stamp_init="1724765564.768170" stamp_final="1724765674.528005" username="apac4" allocationname="unknown" flags="0" pid="1943552" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09760e+02" utime="5.58312e+01" stime="1.64895e+01" mtime="4.22488e+01" gflop="0.00000e+00" gbyte="3.85326e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.22488e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44b14631475146755751470145f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09631e+02" utime="5.57936e+01" stime="1.64853e+01" mtime="4.22488e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.22488e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2105e+09" > 6.8783e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2066e+09" > 4.3750e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0590e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 5.1864e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.5897e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.2496e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3769e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0890e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0579e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4480e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4001e+01 </func>
</region>
</regions>
<internal rank="672" log_i="1724765674.528005" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="673" mpi_size="696" stamp_init="1724765564.768052" stamp_final="1724765674.529257" username="apac4" allocationname="unknown" flags="0" pid="1943553" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09761e+02" utime="6.56444e+01" stime="8.49244e+00" mtime="4.27096e+01" gflop="0.00000e+00" gbyte="3.79967e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.27096e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d315d055d315d3154b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09625e+02" utime="6.56083e+01" stime="8.48477e+00" mtime="4.27096e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.27096e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2015e+09" > 6.1450e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1932e+09" > 3.3844e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7474e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.0198e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2902e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.6590e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.1911e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 3.9419e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0545e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2939e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4437e+01 </func>
</region>
</regions>
<internal rank="673" log_i="1724765674.529257" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="674" mpi_size="696" stamp_init="1724765564.768128" stamp_final="1724765674.523152" username="apac4" allocationname="unknown" flags="0" pid="1943554" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09755e+02" utime="8.00097e+01" stime="1.86944e+01" mtime="6.84791e+01" gflop="0.00000e+00" gbyte="3.76492e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.84791e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000060145f14f1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09627e+02" utime="7.99777e+01" stime="1.86841e+01" mtime="6.84791e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.84791e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1836e+09" > 6.9377e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1902e+09" > 3.5376e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4952e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 5.1969e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1117e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8242e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3783e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0739e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0534e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4455e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3260e+01 </func>
</region>
</regions>
<internal rank="674" log_i="1724765674.523152" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="675" mpi_size="696" stamp_init="1724765564.768008" stamp_final="1724765674.529081" username="apac4" allocationname="unknown" flags="0" pid="1943555" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09761e+02" utime="6.09269e+01" stime="7.91999e+00" mtime="3.77313e+01" gflop="0.00000e+00" gbyte="3.77392e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.77313e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006914b65569146814c5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09629e+02" utime="6.08940e+01" stime="7.91080e+00" mtime="3.77313e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.77313e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2164e+09" > 5.9703e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2202e+09" > 2.8028e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.7652e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 5.1865e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2425e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0913e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3775e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0779e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0607e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2912e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4475e+01 </func>
</region>
</regions>
<internal rank="675" log_i="1724765674.529081" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="676" mpi_size="696" stamp_init="1724765564.768151" stamp_final="1724765674.538100" username="apac4" allocationname="unknown" flags="0" pid="1943556" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09770e+02" utime="7.60610e+01" stime="1.50403e+01" mtime="6.22098e+01" gflop="0.00000e+00" gbyte="3.77483e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.22098e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09642e+02" utime="7.60272e+01" stime="1.50320e+01" mtime="6.22098e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.22098e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2016e+09" > 1.8232e+00 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1964e+09" > 9.1774e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 4.8990e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.3140e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.5720e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5939e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3762e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0887e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0598e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4399e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3000e+01 </func>
</region>
</regions>
<internal rank="676" log_i="1724765674.538100" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="677" mpi_size="696" stamp_init="1724765564.768250" stamp_final="1724765674.537977" username="apac4" allocationname="unknown" flags="0" pid="1943557" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09770e+02" utime="7.58890e+01" stime="1.09491e+01" mtime="5.60262e+01" gflop="0.00000e+00" gbyte="3.76648e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.60262e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000076147614ad" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09640e+02" utime="7.58592e+01" stime="1.09370e+01" mtime="5.60262e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.60262e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2115e+09" > 5.7298e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2020e+09" > 2.8771e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2144e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.3124e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2187e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5222e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3768e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0869e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0593e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2918e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4403e+01 </func>
</region>
</regions>
<internal rank="677" log_i="1724765674.537977" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="678" mpi_size="696" stamp_init="1724765564.768138" stamp_final="1724765674.533433" username="apac4" allocationname="unknown" flags="0" pid="1943558" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09765e+02" utime="7.79884e+01" stime="1.24760e+01" mtime="5.99535e+01" gflop="0.00000e+00" gbyte="3.76717e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.99535e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c614c814c914ed55c914c9145e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09639e+02" utime="7.79581e+01" stime="1.24646e+01" mtime="5.99535e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.99535e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1892e+09" > 7.4996e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1881e+09" > 3.9034e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1192e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.7929e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0490e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1132e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3763e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0910e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0534e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4357e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3435e+01 </func>
</region>
</regions>
<internal rank="678" log_i="1724765674.533433" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="679" mpi_size="696" stamp_init="1724765564.768759" stamp_final="1724765674.540888" username="apac4" allocationname="unknown" flags="0" pid="1943559" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09772e+02" utime="9.00066e+01" stime="1.34347e+01" mtime="7.22286e+01" gflop="0.00000e+00" gbyte="3.77438e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.22286e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004814625548144714be" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09641e+02" utime="8.99785e+01" stime="1.34210e+01" mtime="7.22286e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.22286e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2046e+09" > 5.7256e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2041e+09" > 2.8380e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8194e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9560e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4080e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0102e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3760e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0942e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0558e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2917e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4569e+01 </func>
</region>
</regions>
<internal rank="679" log_i="1724765674.540888" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="680" mpi_size="696" stamp_init="1724765564.768176" stamp_final="1724765674.527547" username="apac4" allocationname="unknown" flags="0" pid="1943560" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09759e+02" utime="6.06920e+01" stime="1.05280e+01" mtime="4.20339e+01" gflop="0.00000e+00" gbyte="3.77201e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.20339e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09630e+02" utime="6.06615e+01" stime="1.05164e+01" mtime="4.20339e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.20339e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1878e+09" > 9.2965e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1936e+09" > 5.0786e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8869e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 7.8930e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9916e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.0550e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3757e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0954e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0578e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4365e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3597e+01 </func>
</region>
</regions>
<internal rank="680" log_i="1724765674.527547" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="681" mpi_size="696" stamp_init="1724765564.768043" stamp_final="1724765674.534098" username="apac4" allocationname="unknown" flags="0" pid="1943561" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09766e+02" utime="6.29547e+01" stime="8.74294e+00" mtime="4.11863e+01" gflop="0.00000e+00" gbyte="3.78231e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.11863e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf463147c148e14dd558e1489145c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09636e+02" utime="6.29228e+01" stime="8.73255e+00" mtime="4.11863e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.11863e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1944e+09" > 6.0042e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1859e+09" > 3.0481e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5317e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 7.8770e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.0747e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3753e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.1048e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0556e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2909e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4462e+01 </func>
</region>
</regions>
<internal rank="681" log_i="1724765674.534098" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="682" mpi_size="696" stamp_init="1724765564.768196" stamp_final="1724765674.533276" username="apac4" allocationname="unknown" flags="0" pid="1943562" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09765e+02" utime="6.03977e+01" stime="9.70516e+00" mtime="4.11255e+01" gflop="0.00000e+00" gbyte="3.77945e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.11255e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000081148114b1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09634e+02" utime="6.03675e+01" stime="9.69313e+00" mtime="4.11255e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.11255e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2185e+09" > 7.9357e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2127e+09" > 4.3978e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5328e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.3347e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.1989e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0269e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3754e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0986e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0553e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.4246e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3730e+01 </func>
</region>
</regions>
<internal rank="682" log_i="1724765674.533276" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="683" mpi_size="696" stamp_init="1724765564.768010" stamp_final="1724765674.528573" username="apac4" allocationname="unknown" flags="0" pid="1943563" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09761e+02" utime="9.00203e+01" stime="1.36065e+01" mtime="7.27498e+01" gflop="0.00000e+00" gbyte="3.76564e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.27498e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf461156215631538556315631522" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09630e+02" utime="8.99858e+01" stime="1.35993e+01" mtime="7.27498e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.27498e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1931e+09" > 5.6100e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1884e+09" > 2.6180e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8636e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 3.9773e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.9101e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8077e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3754e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.0987e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0590e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2923e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4839e+01 </func>
</region>
</regions>
<internal rank="683" log_i="1724765674.528573" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="684" mpi_size="696" stamp_init="1724765564.768152" stamp_final="1724765674.553281" username="apac4" allocationname="unknown" flags="0" pid="1943564" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09785e+02" utime="7.91667e+01" stime="1.66764e+01" mtime="6.58608e+01" gflop="0.00000e+00" gbyte="3.77647e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.58608e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005a1515555a15591525" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09656e+02" utime="7.91346e+01" stime="1.66664e+01" mtime="6.58608e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.58608e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1912e+09" > 7.2061e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1908e+09" > 3.3046e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.4995e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.8307e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4287e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.4604e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3746e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.1061e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0546e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2912e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3810e+01 </func>
</region>
</regions>
<internal rank="684" log_i="1724765674.553281" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="685" mpi_size="696" stamp_init="1724765564.768031" stamp_final="1724765674.538948" username="apac4" allocationname="unknown" flags="0" pid="1943565" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09771e+02" utime="6.40278e+01" stime="8.67209e+00" mtime="4.18756e+01" gflop="0.00000e+00" gbyte="3.77480e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.18756e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09641e+02" utime="6.39957e+01" stime="8.66127e+00" mtime="4.18756e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.18756e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2037e+09" > 5.5567e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2050e+09" > 3.4393e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.8064e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 8.8119e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9230e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3747e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.1075e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0538e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2918e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4987e+01 </func>
</region>
</regions>
<internal rank="685" log_i="1724765674.538948" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="686" mpi_size="696" stamp_init="1724765564.768284" stamp_final="1724765674.538119" username="apac4" allocationname="unknown" flags="0" pid="1943566" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09770e+02" utime="8.53753e+01" stime="1.66315e+01" mtime="7.20290e+01" gflop="0.00000e+00" gbyte="3.77872e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="7.20290e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000bf15b455bf15bf153c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09642e+02" utime="8.53499e+01" stime="1.66141e+01" mtime="7.20290e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="7.20290e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1802e+09" > 6.6521e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1941e+09" > 3.3795e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2022e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.4838e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4620e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.6713e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3746e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.1113e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0538e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2938e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3989e+01 </func>
</region>
</regions>
<internal rank="686" log_i="1724765674.538119" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="687" mpi_size="696" stamp_init="1724765564.768016" stamp_final="1724765674.535340" username="apac4" allocationname="unknown" flags="0" pid="1943567" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09767e+02" utime="7.70790e+01" stime="1.12174e+01" mtime="5.75812e+01" gflop="0.00000e+00" gbyte="3.76747e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.75812e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09640e+02" utime="7.70468e+01" stime="1.12068e+01" mtime="5.75812e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.75812e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2153e+09" > 5.9518e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2203e+09" > 2.8291e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.1771e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 2.4814e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.9908e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3749e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.1086e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0546e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2902e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4286e+01 </func>
</region>
</regions>
<internal rank="687" log_i="1724765674.535340" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="688" mpi_size="696" stamp_init="1724765564.768185" stamp_final="1724765674.523823" username="apac4" allocationname="unknown" flags="0" pid="1943568" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09756e+02" utime="7.55451e+01" stime="1.62774e+01" mtime="6.18554e+01" gflop="0.00000e+00" gbyte="3.78143e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="6.18554e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ea14e956ea14e914dd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09626e+02" utime="7.55088e+01" stime="1.62713e+01" mtime="6.18554e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="6.18554e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2143e+09" > 6.9149e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2112e+09" > 3.4904e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.7734e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 5.6015e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3591e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0979e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3738e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.1198e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0544e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2930e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3483e+01 </func>
</region>
</regions>
<internal rank="688" log_i="1724765674.523823" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="689" mpi_size="696" stamp_init="1724765564.768001" stamp_final="1724765674.536532" username="apac4" allocationname="unknown" flags="0" pid="1943569" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09769e+02" utime="6.16494e+01" stime="7.85975e+00" mtime="3.79943e+01" gflop="0.00000e+00" gbyte="3.77686e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.79943e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c9141755c914c914f6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09639e+02" utime="6.16190e+01" stime="7.84803e+00" mtime="3.79943e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.79943e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2066e+09" > 5.7212e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1974e+09" > 2.6370e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.6274e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 5.5943e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8597e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1308e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3740e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.1179e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0553e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2898e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4494e+01 </func>
</region>
</regions>
<internal rank="689" log_i="1724765674.536532" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="690" mpi_size="696" stamp_init="1724765564.768188" stamp_final="1724765674.533242" username="apac4" allocationname="unknown" flags="0" pid="1943570" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09765e+02" utime="6.25619e+01" stime="1.01566e+01" mtime="4.29283e+01" gflop="0.00000e+00" gbyte="3.77262e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.29283e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f615f815f9152756f915f9154f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09635e+02" utime="6.25301e+01" stime="1.01462e+01" mtime="4.29283e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.29283e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1916e+09" > 7.1814e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1938e+09" > 3.9367e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.5075e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 6.4277e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8786e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1236e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3734e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.1171e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0520e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2926e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3667e+01 </func>
</region>
</regions>
<internal rank="690" log_i="1724765674.533242" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="691" mpi_size="696" stamp_init="1724765564.768045" stamp_final="1724765674.532858" username="apac4" allocationname="unknown" flags="0" pid="1943571" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09765e+02" utime="6.22762e+01" stime="8.12059e+00" mtime="3.90420e+01" gflop="0.00000e+00" gbyte="3.74744e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.90420e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000bc144256bc14bc14f0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09636e+02" utime="6.22425e+01" stime="8.11242e+00" mtime="3.90420e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.90420e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1861e+09" > 5.9982e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1888e+09" > 2.7346e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9278e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 6.4518e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.2425e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3738e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.1206e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0567e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2898e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4356e+01 </func>
</region>
</regions>
<internal rank="691" log_i="1724765674.532858" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="692" mpi_size="696" stamp_init="1724765564.768182" stamp_final="1724765674.533736" username="apac4" allocationname="unknown" flags="0" pid="1943572" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09766e+02" utime="6.63773e+01" stime="1.35884e+01" mtime="5.05298e+01" gflop="0.00000e+00" gbyte="3.78021e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="5.05298e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000851585150b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09638e+02" utime="6.63494e+01" stime="1.35733e+01" mtime="5.05298e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="5.05298e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1963e+09" > 8.0473e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1925e+09" > 4.8137e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2564e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 5.9316e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2435e+01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1539e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3732e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.1254e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0535e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2940e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3373e+01 </func>
</region>
</regions>
<internal rank="692" log_i="1724765674.533736" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="693" mpi_size="696" stamp_init="1724765564.768057" stamp_final="1724765674.527566" username="apac4" allocationname="unknown" flags="0" pid="1943573" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09760e+02" utime="6.66184e+01" stime="9.20931e+00" mtime="4.45196e+01" gflop="0.00000e+00" gbyte="3.77831e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="4.45196e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000019141814fb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09629e+02" utime="6.65901e+01" stime="9.19504e+00" mtime="4.45196e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="4.45196e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2016e+09" > 6.0544e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2038e+09" > 2.9779e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 5.9621e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 1.1935e+01 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5048e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1113e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3727e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.1279e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0600e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2889e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4307e+01 </func>
</region>
</regions>
<internal rank="693" log_i="1724765674.527566" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="694" mpi_size="696" stamp_init="1724765564.768167" stamp_final="1724765674.527825" username="apac4" allocationname="unknown" flags="0" pid="1943574" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09760e+02" utime="5.91616e+01" stime="9.13432e+00" mtime="3.82484e+01" gflop="0.00000e+00" gbyte="3.78117e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.82484e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000de14dd14d0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09629e+02" utime="5.91287e+01" stime="9.12462e+00" mtime="3.82484e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.82484e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.2202e+09" > 6.9530e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.2318e+09" > 4.1164e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.0978e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 5.9237e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.0347e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2062e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3719e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.1356e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0611e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2930e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.3810e+01 </func>
</region>
</regions>
<internal rank="694" log_i="1724765674.527825" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="695" mpi_size="696" stamp_init="1724765564.768010" stamp_final="1724765674.532516" username="apac4" allocationname="unknown" flags="0" pid="1943575" >
<job nhosts="29" ntasks="696" start="1724765564" final="1724765674" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a01u10b</host>
<perf wtime="1.09765e+02" utime="6.23714e+01" stime="8.53350e+00" mtime="3.99061e+01" gflop="0.00000e+00" gbyte="3.76770e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.99061e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000063154655631563152c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="1.09636e+02" utime="6.23392e+01" stime="8.52353e+00" mtime="3.99061e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.99061e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127200" bytes="1.1976e+09" > 6.1236e-01 </func>
<func name="MPI_Irecv" count="127200" bytes="1.1830e+09" > 2.9105e-02 </func>
<func name="MPI_Waitall" count="106800" bytes="0.0000e+00" > 6.2300e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.3715e+04" > 7.0222e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0041e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0889e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3717e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.5095e+05" > 2.1390e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0572e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 8.2883e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.4333e+01 </func>
</region>
</regions>
<internal rank="695" log_i="1724765674.532516" log_t="1.7248e+09" report_delta="-1.0000e+00" fname="./apac4.1724765564.465366.ipm.xml" logrank="0" ></internal>
</task>
</ipm_job_profile>
