<?xml version="1.0" encoding="iso-8859-1"?>
<ipm_job_profile>
<calltable nsections="1" >
<section module="MPI" nentries="69" >
<entry name="MPI_Init" />
<entry name="MPI_Init_thread" />
<entry name="MPI_Finalize" />
<entry name="MPI_Comm_rank" />
<entry name="MPI_Comm_size" />
<entry name="MPI_Send" />
<entry name="MPI_Ssend" />
<entry name="MPI_Rsend" />
<entry name="MPI_Bsend" />
<entry name="MPI_Isend" />
<entry name="MPI_Issend" />
<entry name="MPI_Irsend" />
<entry name="MPI_Ibsend" />
<entry name="MPI_Recv" />
<entry name="MPI_Irecv" />
<entry name="MPI_Sendrecv" />
<entry name="MPI_Sendrecv_replace" />
<entry name="MPI_Wait" />
<entry name="MPI_Waitany" />
<entry name="MPI_Waitall" />
<entry name="MPI_Waitsome" />
<entry name="MPI_Probe" />
<entry name="MPI_Iprobe" />
<entry name="MPI_Send_init" />
<entry name="MPI_Ssend_init" />
<entry name="MPI_Rsend_init" />
<entry name="MPI_Bsend_init" />
<entry name="MPI_Recv_init" />
<entry name="MPI_Buffer_attach" />
<entry name="MPI_Buffer_detach" />
<entry name="MPI_Test" />
<entry name="MPI_Testany" />
<entry name="MPI_Testall" />
<entry name="MPI_Testsome" />
<entry name="MPI_Start" />
<entry name="MPI_Startall" />
<entry name="MPI_Bcast" />
<entry name="MPI_Reduce" />
<entry name="MPI_Reduce_scatter" />
<entry name="MPI_Barrier" />
<entry name="MPI_Gather" />
<entry name="MPI_Gatherv" />
<entry name="MPI_Scatter" />
<entry name="MPI_Scatterv" />
<entry name="MPI_Scan" />
<entry name="MPI_Allgather" />
<entry name="MPI_Allgatherv" />
<entry name="MPI_Allreduce" />
<entry name="MPI_Alltoall" />
<entry name="MPI_Alltoallv" />
<entry name="MPI_Comm_group" />
<entry name="MPI_Comm_compare" />
<entry name="MPI_Comm_dup" />
<entry name="MPI_Comm_create" />
<entry name="MPI_Comm_split" />
<entry name="MPI_Comm_free" />
<entry name="MPI_Ibcast" />
<entry name="MPI_Ireduce" />
<entry name="MPI_Ireduce_scatter" />
<entry name="MPI_Igather" />
<entry name="MPI_Igatherv" />
<entry name="MPI_Iscatter" />
<entry name="MPI_Iscatterv" />
<entry name="MPI_Iscan" />
<entry name="MPI_Iallgather" />
<entry name="MPI_Iallgatherv" />
<entry name="MPI_Iallreduce" />
<entry name="MPI_Ialltoall" />
<entry name="MPI_Ialltoallv" />
</section>
</calltable>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="0" mpi_size="768" stamp_init="1723713791.080061" stamp_final="1723713849.514964" username="apac4" allocationname="unknown" flags="0" pid="1717063" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.84349e+01" utime="4.25193e+01" stime="1.24173e+01" mtime="2.94375e+01" gflop="0.00000e+00" gbyte="9.53526e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.94375e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4251426142714d55627142714d5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83919e+01" utime="4.24882e+01" stime="1.24113e+01" mtime="2.94375e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.94375e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="43" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4859e+08" > 7.3181e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4748e+08" > 5.9119e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9521e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4215e-03 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2479e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.0761e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 8.7309e-04 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2918e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7490e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6191e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8168e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383315" nkey="200" >
<hent key="02400100000000000000080000000001" call="MPI_Isend" bytes="2048" orank="1" region="0" commid="0" count="19" tid="0" op="" dtype="" >9.1076e-05 3.0994e-06 6.1989e-06</hent>
<hent key="02400100000000000000080000000007" call="MPI_Isend" bytes="2048" orank="7" region="0" commid="0" count="13" tid="0" op="" dtype="" >2.9802e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000008" call="MPI_Isend" bytes="2048" orank="8" region="0" commid="0" count="12" tid="0" op="" dtype="" >3.4332e-05 1.1921e-06 1.3113e-05</hent>
<hent key="02400100000000000000080000000038" call="MPI_Isend" bytes="2048" orank="56" region="0" commid="0" count="17" tid="0" op="" dtype="" >8.8930e-05 5.0068e-06 5.9605e-06</hent>
<hent key="02400100000000000000080000000040" call="MPI_Isend" bytes="2048" orank="64" region="0" commid="0" count="3352" tid="0" op="" dtype="" >2.3099e-02 9.5367e-07 1.0896e-04</hent>
<hent key="024001000000000000000800000002C0" call="MPI_Isend" bytes="2048" orank="704" region="0" commid="0" count="3355" tid="0" op="" dtype="" >1.4107e-02 9.5367e-07 5.9843e-05</hent>
<hent key="038001000000000000000E0000000008" call="MPI_Irecv" bytes="3584" orank="8" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000E00000002C0" call="MPI_Irecv" bytes="3584" orank="704" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.1921e-06 1.1921e-06 1.1921e-06</hent>
<hent key="024001000000000000000E0000000040" call="MPI_Isend" bytes="3584" orank="64" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.3828e-05 6.9141e-06 6.9141e-06</hent>
<hent key="03800100000000000000028000000001" call="MPI_Irecv" bytes="640" orank="1" region="0" commid="0" count="404" tid="0" op="" dtype="" >1.0371e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000007" call="MPI_Irecv" bytes="640" orank="7" region="0" commid="0" count="412" tid="0" op="" dtype="" >2.0885e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000028000000008" call="MPI_Irecv" bytes="640" orank="8" region="0" commid="0" count="414" tid="0" op="" dtype="" >1.1301e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000038" call="MPI_Irecv" bytes="640" orank="56" region="0" commid="0" count="420" tid="0" op="" dtype="" >1.7214e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000028000000040" call="MPI_Irecv" bytes="640" orank="64" region="0" commid="0" count="294" tid="0" op="" dtype="" >1.3947e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.5061e-05 4.5061e-05 4.5061e-05</hent>
<hent key="038001000000000000000280000002C0" call="MPI_Irecv" bytes="640" orank="704" region="0" commid="0" count="304" tid="0" op="" dtype="" >1.4305e-04 0.0000e+00 1.6928e-05</hent>
<hent key="02400100000000000000028000000001" call="MPI_Isend" bytes="640" orank="1" region="0" commid="0" count="403" tid="0" op="" dtype="" >1.5368e-03 2.1458e-06 1.8120e-05</hent>
<hent key="02400100000000000000028000000007" call="MPI_Isend" bytes="640" orank="7" region="0" commid="0" count="406" tid="0" op="" dtype="" >6.1846e-04 9.5367e-07 2.8610e-06</hent>
<hent key="02400100000000000000028000000008" call="MPI_Isend" bytes="640" orank="8" region="0" commid="0" count="404" tid="0" op="" dtype="" >5.0020e-04 9.5367e-07 3.8147e-06</hent>
<hent key="02400100000000000000028000000038" call="MPI_Isend" bytes="640" orank="56" region="0" commid="0" count="399" tid="0" op="" dtype="" >1.7877e-03 3.8147e-06 1.0967e-05</hent>
<hent key="02400100000000000000028000000040" call="MPI_Isend" bytes="640" orank="64" region="0" commid="0" count="284" tid="0" op="" dtype="" >1.7307e-03 3.0994e-06 3.3140e-05</hent>
<hent key="024001000000000000000280000002C0" call="MPI_Isend" bytes="640" orank="704" region="0" commid="0" count="278" tid="0" op="" dtype="" >1.5335e-03 2.8610e-06 2.5988e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="03800100000000000000014000000001" call="MPI_Irecv" bytes="320" orank="1" region="0" commid="0" count="325" tid="0" op="" dtype="" >7.6294e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000007" call="MPI_Irecv" bytes="320" orank="7" region="0" commid="0" count="405" tid="0" op="" dtype="" >2.1696e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000014000000008" call="MPI_Irecv" bytes="320" orank="8" region="0" commid="0" count="373" tid="0" op="" dtype="" >9.0837e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000038" call="MPI_Irecv" bytes="320" orank="56" region="0" commid="0" count="343" tid="0" op="" dtype="" >1.4877e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000014000000040" call="MPI_Irecv" bytes="320" orank="64" region="0" commid="0" count="168" tid="0" op="" dtype="" >6.4135e-05 0.0000e+00 6.9141e-06</hent>
<hent key="038001000000000000000140000002C0" call="MPI_Irecv" bytes="320" orank="704" region="0" commid="0" count="159" tid="0" op="" dtype="" >5.0068e-05 0.0000e+00 5.9605e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >8.3413e-03 1.9531e-03 2.4481e-03</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.9521e+00 0.0000e+00 1.3102e-01</hent>
<hent key="03800100000000000000400000000040" call="MPI_Irecv" bytes="16384" orank="64" region="0" commid="0" count="12280" tid="0" op="" dtype="" >6.6464e-03 0.0000e+00 4.5061e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >6.6757e-06 1.9073e-06 2.8610e-06</hent>
<hent key="038001000000000000004000000002C0" call="MPI_Irecv" bytes="16384" orank="704" region="0" commid="0" count="12126" tid="0" op="" dtype="" >7.3676e-03 0.0000e+00 6.3896e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.3794e-04 6.1035e-05 1.1396e-04</hent>
<hent key="02400100000000000000014000000001" call="MPI_Isend" bytes="320" orank="1" region="0" commid="0" count="342" tid="0" op="" dtype="" >1.1680e-03 1.9073e-06 6.9141e-06</hent>
<hent key="02400100000000000000014000000007" call="MPI_Isend" bytes="320" orank="7" region="0" commid="0" count="399" tid="0" op="" dtype="" >4.7112e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000008" call="MPI_Isend" bytes="320" orank="8" region="0" commid="0" count="349" tid="0" op="" dtype="" >4.0102e-04 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000014000000038" call="MPI_Isend" bytes="320" orank="56" region="0" commid="0" count="371" tid="0" op="" dtype="" >1.5726e-03 2.8610e-06 1.1206e-05</hent>
<hent key="02400100000000000000014000000040" call="MPI_Isend" bytes="320" orank="64" region="0" commid="0" count="181" tid="0" op="" dtype="" >1.2047e-03 2.8610e-06 4.1962e-05</hent>
<hent key="03800100000000000000200000000001" call="MPI_Irecv" bytes="8192" orank="1" region="0" commid="0" count="3774" tid="0" op="" dtype="" >8.3518e-04 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000200000000007" call="MPI_Irecv" bytes="8192" orank="7" region="0" commid="0" count="3470" tid="0" op="" dtype="" >1.1342e-03 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000200000000008" call="MPI_Irecv" bytes="8192" orank="8" region="0" commid="0" count="12550" tid="0" op="" dtype="" >1.9503e-02 0.0000e+00 2.6393e-04</hent>
<hent key="03800100000000000000200000000038" call="MPI_Irecv" bytes="8192" orank="56" region="0" commid="0" count="12635" tid="0" op="" dtype="" >2.3408e-03 0.0000e+00 1.9073e-05</hent>
<hent key="024001000000000000000140000002C0" call="MPI_Isend" bytes="320" orank="704" region="0" commid="0" count="186" tid="0" op="" dtype="" >9.7561e-04 2.8610e-06 2.0981e-05</hent>
<hent key="03800100000000000000000000000001" call="MPI_Irecv" bytes="0" orank="1" region="0" commid="0" count="263" tid="0" op="" dtype="" >5.7459e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000000000000007" call="MPI_Irecv" bytes="0" orank="7" region="0" commid="0" count="281" tid="0" op="" dtype="" >1.1778e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000008" call="MPI_Irecv" bytes="0" orank="8" region="0" commid="0" count="254" tid="0" op="" dtype="" >6.1274e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000038" call="MPI_Irecv" bytes="0" orank="56" region="0" commid="0" count="241" tid="0" op="" dtype="" >7.8440e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000000000000040" call="MPI_Irecv" bytes="0" orank="64" region="0" commid="0" count="162" tid="0" op="" dtype="" >6.1989e-05 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000400000000040" call="MPI_Isend" bytes="16384" orank="64" region="0" commid="0" count="12233" tid="0" op="" dtype="" >2.2264e-01 4.0531e-06 3.7694e-04</hent>
<hent key="038001000000000000000000000002C0" call="MPI_Irecv" bytes="0" orank="704" region="0" commid="0" count="151" tid="0" op="" dtype="" >4.3392e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002C0" call="MPI_Isend" bytes="16384" orank="704" region="0" commid="0" count="12309" tid="0" op="" dtype="" >1.3484e-01 2.8610e-06 1.1396e-04</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >2.8491e-04 9.5367e-07 2.3103e-04</hent>
<hent key="02400100000000000000200000000001" call="MPI_Isend" bytes="8192" orank="1" region="0" commid="0" count="3680" tid="0" op="" dtype="" >7.7946e-03 0.0000e+00 7.7009e-05</hent>
<hent key="02400100000000000000200000000007" call="MPI_Isend" bytes="8192" orank="7" region="0" commid="0" count="4139" tid="0" op="" dtype="" >4.6430e-03 0.0000e+00 1.7881e-05</hent>
<hent key="02400100000000000000200000000008" call="MPI_Isend" bytes="8192" orank="8" region="0" commid="0" count="12411" tid="0" op="" dtype="" >1.0099e-02 0.0000e+00 4.8161e-05</hent>
<hent key="02400100000000000000200000000038" call="MPI_Isend" bytes="8192" orank="56" region="0" commid="0" count="12677" tid="0" op="" dtype="" >1.1220e-01 3.8147e-06 1.7500e-04</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.1062e-06 8.1062e-06 8.1062e-06</hent>
<hent key="03800100000000000000030000000007" call="MPI_Irecv" bytes="768" orank="7" region="0" commid="0" count="6" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="02400100000000000000000000000001" call="MPI_Isend" bytes="0" orank="1" region="0" commid="0" count="289" tid="0" op="" dtype="" >6.7210e-04 9.5367e-07 7.3910e-05</hent>
<hent key="02400100000000000000000000000007" call="MPI_Isend" bytes="0" orank="7" region="0" commid="0" count="248" tid="0" op="" dtype="" >1.9526e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000000000008" call="MPI_Isend" bytes="0" orank="8" region="0" commid="0" count="257" tid="0" op="" dtype="" >1.9765e-04 0.0000e+00 4.7684e-06</hent>
<hent key="02400100000000000000000000000038" call="MPI_Isend" bytes="0" orank="56" region="0" commid="0" count="256" tid="0" op="" dtype="" >9.5797e-04 2.8610e-06 1.0967e-05</hent>
<hent key="02400100000000000000000000000040" call="MPI_Isend" bytes="0" orank="64" region="0" commid="0" count="150" tid="0" op="" dtype="" >7.8082e-04 9.5367e-07 2.0981e-05</hent>
<hent key="024001000000000000000000000002C0" call="MPI_Isend" bytes="0" orank="704" region="0" commid="0" count="146" tid="0" op="" dtype="" >6.8045e-04 2.1458e-06 2.0027e-05</hent>
<hent key="03800100000000000000060000000001" call="MPI_Irecv" bytes="1536" orank="1" region="0" commid="0" count="114" tid="0" op="" dtype="" >2.5511e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000007" call="MPI_Irecv" bytes="1536" orank="7" region="0" commid="0" count="88" tid="0" op="" dtype="" >3.7432e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000008" call="MPI_Irecv" bytes="1536" orank="8" region="0" commid="0" count="101" tid="0" op="" dtype="" >2.0981e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000038" call="MPI_Irecv" bytes="1536" orank="56" region="0" commid="0" count="99" tid="0" op="" dtype="" >4.0531e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000040" call="MPI_Irecv" bytes="1536" orank="64" region="0" commid="0" count="240" tid="0" op="" dtype="" >1.3971e-04 0.0000e+00 1.3113e-05</hent>
<hent key="038001000000000000000600000002C0" call="MPI_Irecv" bytes="1536" orank="704" region="0" commid="0" count="210" tid="0" op="" dtype="" >8.1062e-05 0.0000e+00 5.0068e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="43" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000001" call="MPI_Isend" bytes="1536" orank="1" region="0" commid="0" count="101" tid="0" op="" dtype="" >4.4990e-04 2.8610e-06 9.0599e-06</hent>
<hent key="02400100000000000000060000000007" call="MPI_Isend" bytes="1536" orank="7" region="0" commid="0" count="95" tid="0" op="" dtype="" >2.0695e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000008" call="MPI_Isend" bytes="1536" orank="8" region="0" commid="0" count="105" tid="0" op="" dtype="" >1.7905e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000060000000038" call="MPI_Isend" bytes="1536" orank="56" region="0" commid="0" count="95" tid="0" op="" dtype="" >4.6945e-04 3.8147e-06 9.0599e-06</hent>
<hent key="02400100000000000000060000000040" call="MPI_Isend" bytes="1536" orank="64" region="0" commid="0" count="221" tid="0" op="" dtype="" >1.4901e-03 3.8147e-06 2.3842e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C0" call="MPI_Isend" bytes="1536" orank="704" region="0" commid="0" count="226" tid="0" op="" dtype="" >1.3337e-03 2.8610e-06 2.0027e-05</hent>
<hent key="038001000000000000000C0000000007" call="MPI_Irecv" bytes="3072" orank="7" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C0000000008" call="MPI_Irecv" bytes="3072" orank="8" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C0000000040" call="MPI_Irecv" bytes="3072" orank="64" region="0" commid="0" count="5" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C0" call="MPI_Irecv" bytes="3072" orank="704" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000040" call="MPI_Isend" bytes="3072" orank="64" region="0" commid="0" count="11" tid="0" op="" dtype="" >7.0572e-05 5.0068e-06 1.0967e-05</hent>
<hent key="024001000000000000000C00000002C0" call="MPI_Isend" bytes="3072" orank="704" region="0" commid="0" count="5" tid="0" op="" dtype="" >3.5286e-05 5.9605e-06 1.0967e-05</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0898e-02 1.0898e-02 1.0898e-02</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.3320e-02 7.7369e-03 7.7989e-03</hent>
<hent key="03800100000000000000038000000001" call="MPI_Irecv" bytes="896" orank="1" region="0" commid="0" count="2723" tid="0" op="" dtype="" >4.7517e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000007" call="MPI_Irecv" bytes="896" orank="7" region="0" commid="0" count="2747" tid="0" op="" dtype="" >5.7578e-04 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000038000000008" call="MPI_Irecv" bytes="896" orank="8" region="0" commid="0" count="311" tid="0" op="" dtype="" >7.2718e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000038" call="MPI_Irecv" bytes="896" orank="56" region="0" commid="0" count="329" tid="0" op="" dtype="" >1.3256e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000038000000040" call="MPI_Irecv" bytes="896" orank="64" region="0" commid="0" count="279" tid="0" op="" dtype="" >1.4758e-04 0.0000e+00 6.9141e-06</hent>
<hent key="038001000000000000000380000002C0" call="MPI_Irecv" bytes="896" orank="704" region="0" commid="0" count="331" tid="0" op="" dtype="" >1.3542e-04 0.0000e+00 2.0027e-05</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.1062e-06 8.1062e-06 8.1062e-06</hent>
<hent key="03800100000000000000380000000040" call="MPI_Irecv" bytes="14336" orank="64" region="0" commid="0" count="419" tid="0" op="" dtype="" >2.7108e-04 0.0000e+00 2.0981e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="038001000000000000003800000002C0" call="MPI_Irecv" bytes="14336" orank="704" region="0" commid="0" count="573" tid="0" op="" dtype="" >3.9983e-04 0.0000e+00 4.6968e-05</hent>
<hent key="03800100000000000000180000000007" call="MPI_Irecv" bytes="6144" orank="7" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000038000000001" call="MPI_Isend" bytes="896" orank="1" region="0" commid="0" count="2723" tid="0" op="" dtype="" >3.0684e-03 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000038000000007" call="MPI_Isend" bytes="896" orank="7" region="0" commid="0" count="2584" tid="0" op="" dtype="" >1.8873e-03 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000038000000008" call="MPI_Isend" bytes="896" orank="8" region="0" commid="0" count="346" tid="0" op="" dtype="" >5.2905e-04 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000038000000038" call="MPI_Isend" bytes="896" orank="56" region="0" commid="0" count="295" tid="0" op="" dtype="" >1.3785e-03 3.8147e-06 1.1921e-05</hent>
<hent key="02400100000000000000038000000040" call="MPI_Isend" bytes="896" orank="64" region="0" commid="0" count="309" tid="0" op="" dtype="" >1.9221e-03 2.8610e-06 2.0027e-05</hent>
<hent key="024001000000000000000380000002C0" call="MPI_Isend" bytes="896" orank="704" region="0" commid="0" count="342" tid="0" op="" dtype="" >1.8339e-03 2.8610e-06 1.7166e-05</hent>
<hent key="02400100000000000000380000000040" call="MPI_Isend" bytes="14336" orank="64" region="0" commid="0" count="466" tid="0" op="" dtype="" >7.5269e-03 5.0068e-06 1.0991e-04</hent>
<hent key="024001000000000000003800000002C0" call="MPI_Isend" bytes="14336" orank="704" region="0" commid="0" count="390" tid="0" op="" dtype="" >4.2999e-03 3.8147e-06 6.8903e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.2856e+00 8.1062e-06 1.3910e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.6720e-04 5.6720e-04 5.6720e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >2.7490e-04 2.7490e-04 2.7490e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >5.2769e-03 5.2769e-03 5.2769e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.1576e-01 3.5310e-04 1.1186e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >1.0395e-04 1.0395e-04 1.0395e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.5487e-03 1.9648e-03 3.5839e-03</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >8.7309e-04 1.5974e-05 9.5844e-05</hent>
<hent key="03800100000000000000040000000001" call="MPI_Irecv" bytes="1024" orank="1" region="0" commid="0" count="1012" tid="0" op="" dtype="" >1.6117e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000040000000007" call="MPI_Irecv" bytes="1024" orank="7" region="0" commid="0" count="934" tid="0" op="" dtype="" >1.3995e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000008" call="MPI_Irecv" bytes="1024" orank="8" region="0" commid="0" count="3358" tid="0" op="" dtype="" >1.5802e-03 0.0000e+00 9.6083e-05</hent>
<hent key="03800100000000000000040000000038" call="MPI_Irecv" bytes="1024" orank="56" region="0" commid="0" count="3382" tid="0" op="" dtype="" >5.8937e-04 0.0000e+00 3.0041e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >3.2479e+00 9.5367e-07 3.2257e+00</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="03800100000000000000070000000001" call="MPI_Irecv" bytes="1792" orank="1" region="0" commid="0" count="35" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000007" call="MPI_Irecv" bytes="1792" orank="7" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.8358e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000008" call="MPI_Irecv" bytes="1792" orank="8" region="0" commid="0" count="36" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000038" call="MPI_Irecv" bytes="1792" orank="56" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.5974e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000040" call="MPI_Irecv" bytes="1792" orank="64" region="0" commid="0" count="232" tid="0" op="" dtype="" >9.7990e-05 0.0000e+00 1.0967e-05</hent>
<hent key="038001000000000000000700000002C0" call="MPI_Irecv" bytes="1792" orank="704" region="0" commid="0" count="288" tid="0" op="" dtype="" >7.8440e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >2.5272e-05 1.9073e-06 5.0068e-06</hent>
<hent key="02400100000000000000040000000001" call="MPI_Isend" bytes="1024" orank="1" region="0" commid="0" count="984" tid="0" op="" dtype="" >7.7724e-04 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000040000000007" call="MPI_Isend" bytes="1024" orank="7" region="0" commid="0" count="1114" tid="0" op="" dtype="" >7.3624e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000040000000008" call="MPI_Isend" bytes="1024" orank="8" region="0" commid="0" count="3314" tid="0" op="" dtype="" >2.1787e-03 0.0000e+00 3.3855e-05</hent>
<hent key="02400100000000000000040000000038" call="MPI_Isend" bytes="1024" orank="56" region="0" commid="0" count="3392" tid="0" op="" dtype="" >1.2509e-02 9.5367e-07 9.7036e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >8.8215e-06 2.8610e-06 5.9605e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.0656e-03 1.9073e-06 1.7200e-03</hent>
<hent key="038001000000000000000A0000000001" call="MPI_Irecv" bytes="2560" orank="1" region="0" commid="0" count="7" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000007" call="MPI_Irecv" bytes="2560" orank="7" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000008" call="MPI_Irecv" bytes="2560" orank="8" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000038" call="MPI_Irecv" bytes="2560" orank="56" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000040" call="MPI_Irecv" bytes="2560" orank="64" region="0" commid="0" count="54" tid="0" op="" dtype="" >2.3842e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002C0" call="MPI_Irecv" bytes="2560" orank="704" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.9789e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000001" call="MPI_Isend" bytes="1792" orank="1" region="0" commid="0" count="45" tid="0" op="" dtype="" >2.3246e-04 3.8147e-06 3.0041e-05</hent>
<hent key="02400100000000000000070000000007" call="MPI_Isend" bytes="1792" orank="7" region="0" commid="0" count="37" tid="0" op="" dtype="" >8.2254e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000008" call="MPI_Isend" bytes="1792" orank="8" region="0" commid="0" count="37" tid="0" op="" dtype="" >6.7711e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000038" call="MPI_Isend" bytes="1792" orank="56" region="0" commid="0" count="49" tid="0" op="" dtype="" >2.4080e-04 3.8147e-06 5.9605e-06</hent>
<hent key="02400100000000000000070000000040" call="MPI_Isend" bytes="1792" orank="64" region="0" commid="0" count="252" tid="0" op="" dtype="" >1.9188e-03 9.5367e-07 1.0490e-04</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6191e-02 3.6191e-02 3.6191e-02</hent>
<hent key="024001000000000000000700000002C0" call="MPI_Isend" bytes="1792" orank="704" region="0" commid="0" count="218" tid="0" op="" dtype="" >1.0629e-03 9.5367e-07 4.2915e-05</hent>
<hent key="024001000000000000000A0000000001" call="MPI_Isend" bytes="2560" orank="1" region="0" commid="0" count="9" tid="0" op="" dtype="" >4.5300e-05 4.0531e-06 6.1989e-06</hent>
<hent key="024001000000000000000A0000000007" call="MPI_Isend" bytes="2560" orank="7" region="0" commid="0" count="3" tid="0" op="" dtype="" >6.9141e-06 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000008" call="MPI_Isend" bytes="2560" orank="8" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-06 2.1458e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000038" call="MPI_Isend" bytes="2560" orank="56" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.7752e-06 4.7684e-06 5.0068e-06</hent>
<hent key="024001000000000000000A0000000040" call="MPI_Isend" bytes="2560" orank="64" region="0" commid="0" count="43" tid="0" op="" dtype="" >3.6168e-04 5.0068e-06 2.6226e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.4931e-02 4.9160e-03 5.0519e-03</hent>
<hent key="024001000000000000000A00000002C0" call="MPI_Isend" bytes="2560" orank="704" region="0" commid="0" count="39" tid="0" op="" dtype="" >2.3317e-04 3.8147e-06 1.4067e-05</hent>
<hent key="03800100000000000000100000000040" call="MPI_Irecv" bytes="4096" orank="64" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.5140e-02 2.7562e-02 2.7578e-02</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.7931e-03 8.7931e-03 8.7931e-03</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 1.9073e-06 4.0531e-06</hent>
<hent key="03800100000000000000000400000001" call="MPI_Irecv" bytes="4" orank="1" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.0855e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000007" call="MPI_Irecv" bytes="4" orank="7" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2305e-03 0.0000e+00 1.1921e-05</hent>
<hent key="03800100000000000000000400000008" call="MPI_Irecv" bytes="4" orank="8" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1058e-03 0.0000e+00 3.1948e-05</hent>
<hent key="03800100000000000000000400000038" call="MPI_Irecv" bytes="4" orank="56" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.8675e-04 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000000400000040" call="MPI_Irecv" bytes="4" orank="64" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2197e-03 0.0000e+00 3.0041e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.1100e-01 2.0547e-02 4.7770e-02</hent>
<hent key="038001000000000000001C0000000001" call="MPI_Irecv" bytes="7168" orank="1" region="0" commid="0" count="8925" tid="0" op="" dtype="" >2.1465e-03 0.0000e+00 3.6955e-05</hent>
<hent key="038001000000000000001C0000000007" call="MPI_Irecv" bytes="7168" orank="7" region="0" commid="0" count="9207" tid="0" op="" dtype="" >2.9888e-03 0.0000e+00 1.2875e-05</hent>
<hent key="038001000000000000000004000002C0" call="MPI_Irecv" bytes="4" orank="704" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2348e-03 0.0000e+00 5.7936e-05</hent>
<hent key="02400100000000000000000400000001" call="MPI_Isend" bytes="4" orank="1" region="0" commid="0" count="3398" tid="0" op="" dtype="" >3.1588e-03 0.0000e+00 2.1935e-05</hent>
<hent key="02400100000000000000000400000007" call="MPI_Isend" bytes="4" orank="7" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6701e-03 0.0000e+00 1.9073e-05</hent>
<hent key="02400100000000000000000400000008" call="MPI_Isend" bytes="4" orank="8" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4396e-03 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000000400000038" call="MPI_Isend" bytes="4" orank="56" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3486e-02 3.8147e-06 8.2016e-05</hent>
<hent key="02400100000000000000000400000040" call="MPI_Isend" bytes="4" orank="64" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.7919e-02 3.8147e-06 1.1897e-04</hent>
<hent key="024001000000000000001C0000000001" call="MPI_Isend" bytes="7168" orank="1" region="0" commid="0" count="9019" tid="0" op="" dtype="" >2.9511e-02 9.5367e-07 2.5034e-05</hent>
<hent key="024001000000000000001C0000000007" call="MPI_Isend" bytes="7168" orank="7" region="0" commid="0" count="8560" tid="0" op="" dtype="" >1.9547e-02 9.5367e-07 2.3127e-05</hent>
<hent key="024001000000000000000004000002C0" call="MPI_Isend" bytes="4" orank="704" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.4548e-02 3.8147e-06 7.5102e-05</hent>
<hent key="03800100000000000000050000000001" call="MPI_Irecv" bytes="1280" orank="1" region="0" commid="0" count="201" tid="0" op="" dtype="" >4.9591e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000007" call="MPI_Irecv" bytes="1280" orank="7" region="0" commid="0" count="177" tid="0" op="" dtype="" >8.1301e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000050000000008" call="MPI_Irecv" bytes="1280" orank="8" region="0" commid="0" count="230" tid="0" op="" dtype="" >5.9843e-05 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000050000000038" call="MPI_Irecv" bytes="1280" orank="56" region="0" commid="0" count="226" tid="0" op="" dtype="" >8.5115e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000040" call="MPI_Irecv" bytes="1280" orank="64" region="0" commid="0" count="311" tid="0" op="" dtype="" >1.5783e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0760e+01 9.0599e-06 1.2805e-01</hent>
<hent key="03800100000000000000280000000008" call="MPI_Irecv" bytes="10240" orank="8" region="0" commid="0" count="149" tid="0" op="" dtype="" >1.7953e-04 0.0000e+00 3.1948e-05</hent>
<hent key="03800100000000000000280000000038" call="MPI_Irecv" bytes="10240" orank="56" region="0" commid="0" count="64" tid="0" op="" dtype="" >1.3590e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002C0" call="MPI_Irecv" bytes="1280" orank="704" region="0" commid="0" count="292" tid="0" op="" dtype="" >1.2469e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000080000000001" call="MPI_Irecv" bytes="2048" orank="1" region="0" commid="0" count="13" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000007" call="MPI_Irecv" bytes="2048" orank="7" region="0" commid="0" count="10" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000008" call="MPI_Irecv" bytes="2048" orank="8" region="0" commid="0" count="10" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000038" call="MPI_Irecv" bytes="2048" orank="56" region="0" commid="0" count="15" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000040" call="MPI_Irecv" bytes="2048" orank="64" region="0" commid="0" count="3351" tid="0" op="" dtype="" >9.0718e-04 0.0000e+00 1.9789e-05</hent>
<hent key="038001000000000000000800000002C0" call="MPI_Irecv" bytes="2048" orank="704" region="0" commid="0" count="3315" tid="0" op="" dtype="" >1.3373e-03 0.0000e+00 7.4863e-05</hent>
<hent key="02400100000000000000050000000001" call="MPI_Isend" bytes="1280" orank="1" region="0" commid="0" count="182" tid="0" op="" dtype="" >7.8559e-04 2.8610e-06 9.0599e-06</hent>
<hent key="02400100000000000000050000000007" call="MPI_Isend" bytes="1280" orank="7" region="0" commid="0" count="198" tid="0" op="" dtype="" >3.9077e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000008" call="MPI_Isend" bytes="1280" orank="8" region="0" commid="0" count="269" tid="0" op="" dtype="" >3.7718e-04 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000050000000038" call="MPI_Isend" bytes="1280" orank="56" region="0" commid="0" count="221" tid="0" op="" dtype="" >1.0736e-03 2.1458e-06 2.1935e-05</hent>
<hent key="02400100000000000000050000000040" call="MPI_Isend" bytes="1280" orank="64" region="0" commid="0" count="292" tid="0" op="" dtype="" >1.9901e-03 3.8147e-06 3.9101e-05</hent>
<hent key="02400100000000000000280000000008" call="MPI_Isend" bytes="10240" orank="8" region="0" commid="0" count="288" tid="0" op="" dtype="" >1.8167e-04 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000280000000038" call="MPI_Isend" bytes="10240" orank="56" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.9932e-04 5.9605e-06 3.1948e-05</hent>
<hent key="024001000000000000000500000002C0" call="MPI_Isend" bytes="1280" orank="704" region="0" commid="0" count="302" tid="0" op="" dtype="" >1.7183e-03 2.8610e-06 1.9073e-05</hent>
</hash>
<internal rank="0" log_i="1723713849.514964" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="1" mpi_size="768" stamp_init="1723713791.080091" stamp_final="1723713849.509740" username="apac4" allocationname="unknown" flags="0" pid="1717064" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.84296e+01" utime="5.02918e+01" stime="6.70752e+00" mtime="3.21803e+01" gflop="0.00000e+00" gbyte="3.76331e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21803e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000070156b151d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83821e+01" utime="5.02602e+01" stime="6.70031e+00" mtime="3.21803e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21803e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4952e+08" > 4.4528e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4806e+08" > 3.2186e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2683e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4316e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.8610e-06 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6159e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.3848e-04 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6798e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6184e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9350e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="196" >
<hent key="02400100000000000000080000000000" call="MPI_Isend" bytes="2048" orank="0" region="0" commid="0" count="13" tid="0" op="" dtype="" >2.9087e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000002" call="MPI_Isend" bytes="2048" orank="2" region="0" commid="0" count="14" tid="0" op="" dtype="" >6.2227e-05 3.8147e-06 5.0068e-06</hent>
<hent key="02400100000000000000080000000009" call="MPI_Isend" bytes="2048" orank="9" region="0" commid="0" count="13" tid="0" op="" dtype="" >4.0293e-05 9.5367e-07 1.0014e-05</hent>
<hent key="02400100000000000000080000000039" call="MPI_Isend" bytes="2048" orank="57" region="0" commid="0" count="11" tid="0" op="" dtype="" >6.1274e-05 5.0068e-06 6.1989e-06</hent>
<hent key="02400100000000000000080000000041" call="MPI_Isend" bytes="2048" orank="65" region="0" commid="0" count="3473" tid="0" op="" dtype="" >1.1135e-02 9.5367e-07 2.3127e-05</hent>
<hent key="024001000000000000000800000002C1" call="MPI_Isend" bytes="2048" orank="705" region="0" commid="0" count="3445" tid="0" op="" dtype="" >1.0342e-02 9.5367e-07 1.4067e-05</hent>
<hent key="038001000000000000000E00000002C1" call="MPI_Irecv" bytes="3584" orank="705" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000E0000000041" call="MPI_Isend" bytes="3584" orank="65" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000000" call="MPI_Irecv" bytes="640" orank="0" region="0" commid="0" count="403" tid="0" op="" dtype="" >1.7810e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000028000000002" call="MPI_Irecv" bytes="640" orank="2" region="0" commid="0" count="421" tid="0" op="" dtype="" >1.2445e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000028000000009" call="MPI_Irecv" bytes="640" orank="9" region="0" commid="0" count="392" tid="0" op="" dtype="" >1.0800e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000039" call="MPI_Irecv" bytes="640" orank="57" region="0" commid="0" count="416" tid="0" op="" dtype="" >1.2779e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000041" call="MPI_Irecv" bytes="640" orank="65" region="0" commid="0" count="267" tid="0" op="" dtype="" >1.0610e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002C1" call="MPI_Irecv" bytes="640" orank="705" region="0" commid="0" count="282" tid="0" op="" dtype="" >1.1015e-04 0.0000e+00 4.7684e-06</hent>
<hent key="02400100000000000000028000000000" call="MPI_Isend" bytes="640" orank="0" region="0" commid="0" count="404" tid="0" op="" dtype="" >5.8579e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000028000000002" call="MPI_Isend" bytes="640" orank="2" region="0" commid="0" count="422" tid="0" op="" dtype="" >1.5178e-03 1.9073e-06 8.8215e-06</hent>
<hent key="02400100000000000000028000000009" call="MPI_Isend" bytes="640" orank="9" region="0" commid="0" count="390" tid="0" op="" dtype="" >5.0497e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000028000000039" call="MPI_Isend" bytes="640" orank="57" region="0" commid="0" count="413" tid="0" op="" dtype="" >1.9221e-03 3.8147e-06 9.0599e-06</hent>
<hent key="02400100000000000000028000000041" call="MPI_Isend" bytes="640" orank="65" region="0" commid="0" count="281" tid="0" op="" dtype="" >1.2267e-03 2.8610e-06 1.0967e-05</hent>
<hent key="024001000000000000000280000002C1" call="MPI_Isend" bytes="640" orank="705" region="0" commid="0" count="274" tid="0" op="" dtype="" >1.1115e-03 2.8610e-06 1.0967e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0014e-05 1.0014e-05 1.0014e-05</hent>
<hent key="03800100000000000000014000000000" call="MPI_Irecv" bytes="320" orank="0" region="0" commid="0" count="342" tid="0" op="" dtype="" >1.2541e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000002" call="MPI_Irecv" bytes="320" orank="2" region="0" commid="0" count="369" tid="0" op="" dtype="" >1.0443e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000014000000009" call="MPI_Irecv" bytes="320" orank="9" region="0" commid="0" count="370" tid="0" op="" dtype="" >7.1049e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000039" call="MPI_Irecv" bytes="320" orank="57" region="0" commid="0" count="351" tid="0" op="" dtype="" >9.4891e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000014000000041" call="MPI_Irecv" bytes="320" orank="65" region="0" commid="0" count="172" tid="0" op="" dtype="" >6.3896e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002C1" call="MPI_Irecv" bytes="320" orank="705" region="0" commid="0" count="167" tid="0" op="" dtype="" >5.1737e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.1206e-05 9.5367e-07 7.8678e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.2683e+00 0.0000e+00 1.3103e-01</hent>
<hent key="03800100000000000000400000000041" call="MPI_Irecv" bytes="16384" orank="65" region="0" commid="0" count="12691" tid="0" op="" dtype="" >3.3956e-03 0.0000e+00 7.1526e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.8835e-05 0.0000e+00 1.7881e-05</hent>
<hent key="038001000000000000004000000002C1" call="MPI_Irecv" bytes="16384" orank="705" region="0" commid="0" count="12699" tid="0" op="" dtype="" >2.2378e-03 0.0000e+00 3.3855e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000000" call="MPI_Isend" bytes="320" orank="0" region="0" commid="0" count="325" tid="0" op="" dtype="" >3.7146e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000002" call="MPI_Isend" bytes="320" orank="2" region="0" commid="0" count="338" tid="0" op="" dtype="" >1.1189e-03 1.9073e-06 9.0599e-06</hent>
<hent key="02400100000000000000014000000009" call="MPI_Isend" bytes="320" orank="9" region="0" commid="0" count="355" tid="0" op="" dtype="" >3.7432e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000014000000039" call="MPI_Isend" bytes="320" orank="57" region="0" commid="0" count="352" tid="0" op="" dtype="" >1.5607e-03 3.8147e-06 5.9605e-06</hent>
<hent key="02400100000000000000014000000041" call="MPI_Isend" bytes="320" orank="65" region="0" commid="0" count="164" tid="0" op="" dtype="" >6.8569e-04 2.8610e-06 5.0068e-06</hent>
<hent key="03800100000000000000200000000000" call="MPI_Irecv" bytes="8192" orank="0" region="0" commid="0" count="3680" tid="0" op="" dtype="" >8.4662e-04 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000200000000002" call="MPI_Irecv" bytes="8192" orank="2" region="0" commid="0" count="2473" tid="0" op="" dtype="" >3.8171e-04 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000200000000009" call="MPI_Irecv" bytes="8192" orank="9" region="0" commid="0" count="12684" tid="0" op="" dtype="" >1.0067e-02 0.0000e+00 3.1948e-05</hent>
<hent key="03800100000000000000200000000039" call="MPI_Irecv" bytes="8192" orank="57" region="0" commid="0" count="12691" tid="0" op="" dtype="" >1.3092e-03 0.0000e+00 4.0531e-06</hent>
<hent key="024001000000000000000140000002C1" call="MPI_Isend" bytes="320" orank="705" region="0" commid="0" count="181" tid="0" op="" dtype="" >7.1859e-04 2.8610e-06 8.8215e-06</hent>
<hent key="03800100000000000000000000000000" call="MPI_Irecv" bytes="0" orank="0" region="0" commid="0" count="289" tid="0" op="" dtype="" >8.6546e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000002" call="MPI_Irecv" bytes="0" orank="2" region="0" commid="0" count="248" tid="0" op="" dtype="" >7.2241e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000009" call="MPI_Irecv" bytes="0" orank="9" region="0" commid="0" count="289" tid="0" op="" dtype="" >6.5088e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000039" call="MPI_Irecv" bytes="0" orank="57" region="0" commid="0" count="258" tid="0" op="" dtype="" >7.6056e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000000000041" call="MPI_Irecv" bytes="0" orank="65" region="0" commid="0" count="140" tid="0" op="" dtype="" >2.8610e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000041" call="MPI_Isend" bytes="16384" orank="65" region="0" commid="0" count="12685" tid="0" op="" dtype="" >8.4810e-02 3.8147e-06 3.5048e-05</hent>
<hent key="038001000000000000000000000002C1" call="MPI_Irecv" bytes="0" orank="705" region="0" commid="0" count="140" tid="0" op="" dtype="" >3.0518e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002C1" call="MPI_Isend" bytes="16384" orank="705" region="0" commid="0" count="12645" tid="0" op="" dtype="" >8.2825e-02 3.8147e-06 2.9087e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >8.0156e-04 0.0000e+00 6.3705e-04</hent>
<hent key="02400100000000000000200000000000" call="MPI_Isend" bytes="8192" orank="0" region="0" commid="0" count="3774" tid="0" op="" dtype="" >5.0566e-03 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000200000000002" call="MPI_Isend" bytes="8192" orank="2" region="0" commid="0" count="3615" tid="0" op="" dtype="" >7.7038e-03 0.0000e+00 5.4121e-05</hent>
<hent key="02400100000000000000200000000009" call="MPI_Isend" bytes="8192" orank="9" region="0" commid="0" count="12632" tid="0" op="" dtype="" >1.0163e-02 0.0000e+00 6.3896e-05</hent>
<hent key="02400100000000000000200000000039" call="MPI_Isend" bytes="8192" orank="57" region="0" commid="0" count="12613" tid="0" op="" dtype="" >7.5276e-02 3.8147e-06 2.2888e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.6928e-05 1.6928e-05 1.6928e-05</hent>
<hent key="02400100000000000000000000000000" call="MPI_Isend" bytes="0" orank="0" region="0" commid="0" count="263" tid="0" op="" dtype="" >2.0456e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000002" call="MPI_Isend" bytes="0" orank="2" region="0" commid="0" count="264" tid="0" op="" dtype="" >5.9056e-04 9.5367e-07 5.1022e-05</hent>
<hent key="02400100000000000000000000000009" call="MPI_Isend" bytes="0" orank="9" region="0" commid="0" count="265" tid="0" op="" dtype="" >1.8930e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000039" call="MPI_Isend" bytes="0" orank="57" region="0" commid="0" count="255" tid="0" op="" dtype="" >9.6941e-04 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000000000000041" call="MPI_Isend" bytes="0" orank="65" region="0" commid="0" count="151" tid="0" op="" dtype="" >5.6648e-04 9.5367e-07 1.0014e-05</hent>
<hent key="024001000000000000000000000002C1" call="MPI_Isend" bytes="0" orank="705" region="0" commid="0" count="145" tid="0" op="" dtype="" >5.3215e-04 2.1458e-06 8.1062e-06</hent>
<hent key="03800100000000000000060000000000" call="MPI_Irecv" bytes="1536" orank="0" region="0" commid="0" count="101" tid="0" op="" dtype="" >4.1008e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000002" call="MPI_Irecv" bytes="1536" orank="2" region="0" commid="0" count="90" tid="0" op="" dtype="" >2.0266e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000009" call="MPI_Irecv" bytes="1536" orank="9" region="0" commid="0" count="113" tid="0" op="" dtype="" >2.4557e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000039" call="MPI_Irecv" bytes="1536" orank="57" region="0" commid="0" count="104" tid="0" op="" dtype="" >4.6015e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000060000000041" call="MPI_Irecv" bytes="1536" orank="65" region="0" commid="0" count="241" tid="0" op="" dtype="" >1.2589e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002C1" call="MPI_Irecv" bytes="1536" orank="705" region="0" commid="0" count="217" tid="0" op="" dtype="" >8.6308e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000000" call="MPI_Isend" bytes="1536" orank="0" region="0" commid="0" count="114" tid="0" op="" dtype="" >2.3007e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000002" call="MPI_Isend" bytes="1536" orank="2" region="0" commid="0" count="105" tid="0" op="" dtype="" >4.6229e-04 3.8147e-06 8.1062e-06</hent>
<hent key="02400100000000000000060000000009" call="MPI_Isend" bytes="1536" orank="9" region="0" commid="0" count="104" tid="0" op="" dtype="" >1.7977e-04 9.5367e-07 1.0014e-05</hent>
<hent key="02400100000000000000060000000039" call="MPI_Isend" bytes="1536" orank="57" region="0" commid="0" count="90" tid="0" op="" dtype="" >4.6253e-04 4.0531e-06 6.1989e-06</hent>
<hent key="02400100000000000000060000000041" call="MPI_Isend" bytes="1536" orank="65" region="0" commid="0" count="200" tid="0" op="" dtype="" >9.4509e-04 3.8147e-06 1.0014e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C1" call="MPI_Isend" bytes="1536" orank="705" region="0" commid="0" count="204" tid="0" op="" dtype="" >8.9717e-04 2.8610e-06 5.9605e-06</hent>
<hent key="038001000000000000000C0000000039" call="MPI_Irecv" bytes="3072" orank="57" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C0000000041" call="MPI_Irecv" bytes="3072" orank="65" region="0" commid="0" count="9" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C1" call="MPI_Irecv" bytes="3072" orank="705" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C0000000002" call="MPI_Isend" bytes="3072" orank="2" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="024001000000000000000C0000000041" call="MPI_Isend" bytes="3072" orank="65" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.5763e-05 5.9605e-06 5.9605e-06</hent>
<hent key="024001000000000000000C00000002C1" call="MPI_Isend" bytes="3072" orank="705" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.5974e-05 5.0068e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.5075e-05 5.5075e-05 5.5075e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.7275e-05 3.1233e-05 3.4094e-05</hent>
<hent key="03800100000000000000038000000000" call="MPI_Irecv" bytes="896" orank="0" region="0" commid="0" count="2723" tid="0" op="" dtype="" >3.7789e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000002" call="MPI_Irecv" bytes="896" orank="2" region="0" commid="0" count="3072" tid="0" op="" dtype="" >5.8866e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000009" call="MPI_Irecv" bytes="896" orank="9" region="0" commid="0" count="311" tid="0" op="" dtype="" >8.3447e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000039" call="MPI_Irecv" bytes="896" orank="57" region="0" commid="0" count="324" tid="0" op="" dtype="" >1.0991e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000041" call="MPI_Irecv" bytes="896" orank="65" region="0" commid="0" count="320" tid="0" op="" dtype="" >1.4758e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002C1" call="MPI_Irecv" bytes="896" orank="705" region="0" commid="0" count="326" tid="0" op="" dtype="" >1.0991e-04 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.7166e-05 1.7166e-05 1.7166e-05</hent>
<hent key="03800100000000000000380000000041" call="MPI_Irecv" bytes="14336" orank="65" region="0" commid="0" count="8" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="02400100000000000000038000000000" call="MPI_Isend" bytes="896" orank="0" region="0" commid="0" count="2723" tid="0" op="" dtype="" >2.0850e-03 0.0000e+00 1.2159e-05</hent>
<hent key="02400100000000000000038000000002" call="MPI_Isend" bytes="896" orank="2" region="0" commid="0" count="2772" tid="0" op="" dtype="" >2.9356e-03 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000038000000009" call="MPI_Isend" bytes="896" orank="9" region="0" commid="0" count="353" tid="0" op="" dtype="" >5.1618e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000038000000039" call="MPI_Isend" bytes="896" orank="57" region="0" commid="0" count="319" tid="0" op="" dtype="" >1.5442e-03 3.8147e-06 9.0599e-06</hent>
<hent key="02400100000000000000038000000041" call="MPI_Isend" bytes="896" orank="65" region="0" commid="0" count="314" tid="0" op="" dtype="" >1.4033e-03 3.8147e-06 1.0967e-05</hent>
<hent key="024001000000000000000380000002C1" call="MPI_Isend" bytes="896" orank="705" region="0" commid="0" count="374" tid="0" op="" dtype="" >1.5764e-03 2.8610e-06 1.0014e-05</hent>
<hent key="02400100000000000000380000000041" call="MPI_Isend" bytes="14336" orank="65" region="0" commid="0" count="14" tid="0" op="" dtype="" >8.7976e-05 5.0068e-06 8.1062e-06</hent>
<hent key="024001000000000000003800000002C1" call="MPI_Isend" bytes="14336" orank="705" region="0" commid="0" count="54" tid="0" op="" dtype="" >3.6454e-04 4.0531e-06 1.3113e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.2753e+00 9.7752e-06 1.3922e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.4908e-04 5.4908e-04 5.4908e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >2.6798e-04 2.6798e-04 2.6798e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >9.1815e-04 9.1815e-04 9.1815e-04</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >7.2450e-02 3.0303e-04 6.7599e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.1403e-04 5.1403e-04 5.1403e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 9.5367e-07 1.9073e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6159e+00 4.4203e-04 2.5696e-01</hent>
<hent key="03800100000000000000040000000000" call="MPI_Irecv" bytes="1024" orank="0" region="0" commid="0" count="984" tid="0" op="" dtype="" >8.1539e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000002" call="MPI_Irecv" bytes="1024" orank="2" region="0" commid="0" count="668" tid="0" op="" dtype="" >1.1921e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000009" call="MPI_Irecv" bytes="1024" orank="9" region="0" commid="0" count="3394" tid="0" op="" dtype="" >1.0066e-03 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000040000000039" call="MPI_Irecv" bytes="1024" orank="57" region="0" commid="0" count="3396" tid="0" op="" dtype="" >2.6774e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.3127e-05 0.0000e+00 1.0014e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="03800100000000000000070000000000" call="MPI_Irecv" bytes="1792" orank="0" region="0" commid="0" count="45" tid="0" op="" dtype="" >1.6451e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000002" call="MPI_Irecv" bytes="1792" orank="2" region="0" commid="0" count="43" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000009" call="MPI_Irecv" bytes="1792" orank="9" region="0" commid="0" count="49" tid="0" op="" dtype="" >9.5367e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000039" call="MPI_Irecv" bytes="1792" orank="57" region="0" commid="0" count="37" tid="0" op="" dtype="" >8.8215e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000041" call="MPI_Irecv" bytes="1792" orank="65" region="0" commid="0" count="128" tid="0" op="" dtype="" >6.5088e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002C1" call="MPI_Irecv" bytes="1792" orank="705" region="0" commid="0" count="133" tid="0" op="" dtype="" >4.1962e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >8.3923e-05 9.5367e-07 5.1975e-05</hent>
<hent key="02400100000000000000040000000000" call="MPI_Isend" bytes="1024" orank="0" region="0" commid="0" count="1012" tid="0" op="" dtype="" >6.2251e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000040000000002" call="MPI_Isend" bytes="1024" orank="2" region="0" commid="0" count="954" tid="0" op="" dtype="" >6.9261e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000040000000009" call="MPI_Isend" bytes="1024" orank="9" region="0" commid="0" count="3378" tid="0" op="" dtype="" >1.9197e-03 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000040000000039" call="MPI_Isend" bytes="1024" orank="57" region="0" commid="0" count="3374" tid="0" op="" dtype="" >8.4739e-03 9.5367e-07 8.1062e-06</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >4.2915e-05 4.0531e-06 3.8862e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4306e+00 0.0000e+00 3.2492e+00</hent>
<hent key="038001000000000000000A0000000000" call="MPI_Irecv" bytes="2560" orank="0" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000002" call="MPI_Irecv" bytes="2560" orank="2" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000000A0000000009" call="MPI_Irecv" bytes="2560" orank="9" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000039" call="MPI_Irecv" bytes="2560" orank="57" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000041" call="MPI_Irecv" bytes="2560" orank="65" region="0" commid="0" count="39" tid="0" op="" dtype="" >2.4557e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002C1" call="MPI_Irecv" bytes="2560" orank="705" region="0" commid="0" count="51" tid="0" op="" dtype="" >2.0504e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000000" call="MPI_Isend" bytes="1792" orank="0" region="0" commid="0" count="35" tid="0" op="" dtype="" >7.4863e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000002" call="MPI_Isend" bytes="1792" orank="2" region="0" commid="0" count="43" tid="0" op="" dtype="" >1.9407e-04 2.8610e-06 9.0599e-06</hent>
<hent key="02400100000000000000070000000009" call="MPI_Isend" bytes="1792" orank="9" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.2970e-04 9.5367e-07 1.2159e-05</hent>
<hent key="02400100000000000000070000000039" call="MPI_Isend" bytes="1792" orank="57" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.6379e-04 4.0531e-06 5.9605e-06</hent>
<hent key="02400100000000000000070000000041" call="MPI_Isend" bytes="1792" orank="65" region="0" commid="0" count="148" tid="0" op="" dtype="" >7.0095e-04 9.5367e-07 6.1989e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6184e-02 3.6184e-02 3.6184e-02</hent>
<hent key="024001000000000000000700000002C1" call="MPI_Isend" bytes="1792" orank="705" region="0" commid="0" count="124" tid="0" op="" dtype="" >5.4073e-04 9.5367e-07 8.8215e-06</hent>
<hent key="024001000000000000000A0000000000" call="MPI_Isend" bytes="2560" orank="0" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.7166e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000002" call="MPI_Isend" bytes="2560" orank="2" region="0" commid="0" count="6" tid="0" op="" dtype="" >5.6028e-05 4.0531e-06 3.2902e-05</hent>
<hent key="024001000000000000000A0000000009" call="MPI_Isend" bytes="2560" orank="9" region="0" commid="0" count="5" tid="0" op="" dtype="" >3.0041e-05 1.9073e-06 1.3113e-05</hent>
<hent key="024001000000000000000A0000000039" call="MPI_Isend" bytes="2560" orank="57" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.5988e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000041" call="MPI_Isend" bytes="2560" orank="65" region="0" commid="0" count="45" tid="0" op="" dtype="" >2.3341e-04 3.8147e-06 9.0599e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >8.2970e-05 1.7881e-05 3.5048e-05</hent>
<hent key="024001000000000000000A00000002C1" call="MPI_Isend" bytes="2560" orank="705" region="0" commid="0" count="49" tid="0" op="" dtype="" >2.4486e-04 3.8147e-06 6.9141e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.2994e-04 6.3896e-05 6.6042e-05</hent>
<hent key="024001000000000000001000000002C1" call="MPI_Isend" bytes="4096" orank="705" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >4.7684e-06 1.9073e-06 2.8610e-06</hent>
<hent key="03800100000000000000000400000000" call="MPI_Irecv" bytes="4" orank="0" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.1788e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000000400000002" call="MPI_Irecv" bytes="4" orank="2" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.3559e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000009" call="MPI_Irecv" bytes="4" orank="9" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.4993e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000039" call="MPI_Irecv" bytes="4" orank="57" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.4918e-04 0.0000e+00 3.8147e-06</hent>
<hent key="03800100000000000000000400000041" call="MPI_Irecv" bytes="4" orank="65" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.2452e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.5916e-04 4.8161e-05 7.2002e-05</hent>
<hent key="038001000000000000001C0000000000" call="MPI_Irecv" bytes="7168" orank="0" region="0" commid="0" count="9019" tid="0" op="" dtype="" >2.2991e-03 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000001C0000000002" call="MPI_Irecv" bytes="7168" orank="2" region="0" commid="0" count="10226" tid="0" op="" dtype="" >1.6749e-03 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000004000002C1" call="MPI_Irecv" bytes="4" orank="705" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.1737e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000400000000" call="MPI_Isend" bytes="4" orank="0" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3266e-03 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000000400000002" call="MPI_Isend" bytes="4" orank="2" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3909e-03 0.0000e+00 6.1035e-05</hent>
<hent key="02400100000000000000000400000009" call="MPI_Isend" bytes="4" orank="9" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6131e-03 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000000400000039" call="MPI_Isend" bytes="4" orank="57" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.9174e-02 2.8610e-06 1.0395e-04</hent>
<hent key="02400100000000000000000400000041" call="MPI_Isend" bytes="4" orank="65" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7145e-02 3.8147e-06 1.6928e-05</hent>
<hent key="024001000000000000001C0000000000" call="MPI_Isend" bytes="7168" orank="0" region="0" commid="0" count="8925" tid="0" op="" dtype="" >2.0786e-02 9.5367e-07 1.2875e-05</hent>
<hent key="024001000000000000001C0000000002" call="MPI_Isend" bytes="7168" orank="2" region="0" commid="0" count="9084" tid="0" op="" dtype="" >2.9596e-02 9.5367e-07 2.3842e-05</hent>
<hent key="024001000000000000000004000002C1" call="MPI_Isend" bytes="4" orank="705" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7205e-02 2.8610e-06 6.2943e-05</hent>
<hent key="03800100000000000000050000000000" call="MPI_Irecv" bytes="1280" orank="0" region="0" commid="0" count="182" tid="0" op="" dtype="" >6.4611e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000002" call="MPI_Irecv" bytes="1280" orank="2" region="0" commid="0" count="168" tid="0" op="" dtype="" >4.6492e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000009" call="MPI_Irecv" bytes="1280" orank="9" region="0" commid="0" count="164" tid="0" op="" dtype="" >2.9325e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000039" call="MPI_Irecv" bytes="1280" orank="57" region="0" commid="0" count="190" tid="0" op="" dtype="" >5.9366e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000041" call="MPI_Irecv" bytes="1280" orank="65" region="0" commid="0" count="305" tid="0" op="" dtype="" >1.4186e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.1000e+01 8.1062e-06 1.2808e-01</hent>
<hent key="03800100000000000000280000000009" call="MPI_Irecv" bytes="10240" orank="9" region="0" commid="0" count="15" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000280000000039" call="MPI_Irecv" bytes="10240" orank="57" region="0" commid="0" count="8" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000500000002C1" call="MPI_Irecv" bytes="1280" orank="705" region="0" commid="0" count="311" tid="0" op="" dtype="" >1.1563e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000000" call="MPI_Irecv" bytes="2048" orank="0" region="0" commid="0" count="19" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000002" call="MPI_Irecv" bytes="2048" orank="2" region="0" commid="0" count="11" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000009" call="MPI_Irecv" bytes="2048" orank="9" region="0" commid="0" count="12" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000039" call="MPI_Irecv" bytes="2048" orank="57" region="0" commid="0" count="16" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000041" call="MPI_Irecv" bytes="2048" orank="65" region="0" commid="0" count="3476" tid="0" op="" dtype="" >6.1536e-04 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000800000002C1" call="MPI_Irecv" bytes="2048" orank="705" region="0" commid="0" count="3464" tid="0" op="" dtype="" >5.1212e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000050000000000" call="MPI_Isend" bytes="1280" orank="0" region="0" commid="0" count="201" tid="0" op="" dtype="" >3.9244e-04 9.5367e-07 6.1989e-06</hent>
<hent key="02400100000000000000050000000002" call="MPI_Isend" bytes="1280" orank="2" region="0" commid="0" count="178" tid="0" op="" dtype="" >7.3576e-04 2.8610e-06 8.1062e-06</hent>
<hent key="02400100000000000000050000000009" call="MPI_Isend" bytes="1280" orank="9" region="0" commid="0" count="194" tid="0" op="" dtype="" >3.0708e-04 0.0000e+00 9.7752e-06</hent>
<hent key="02400100000000000000050000000039" call="MPI_Isend" bytes="1280" orank="57" region="0" commid="0" count="246" tid="0" op="" dtype="" >1.1857e-03 9.5367e-07 1.6928e-05</hent>
<hent key="02400100000000000000050000000041" call="MPI_Isend" bytes="1280" orank="65" region="0" commid="0" count="314" tid="0" op="" dtype="" >1.4398e-03 2.8610e-06 7.8678e-06</hent>
<hent key="02400100000000000000280000000009" call="MPI_Isend" bytes="10240" orank="9" region="0" commid="0" count="67" tid="0" op="" dtype="" >4.2677e-05 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000280000000039" call="MPI_Isend" bytes="10240" orank="57" region="0" commid="0" count="86" tid="0" op="" dtype="" >5.4240e-04 3.8147e-06 1.0014e-05</hent>
<hent key="024001000000000000000500000002C1" call="MPI_Isend" bytes="1280" orank="705" region="0" commid="0" count="297" tid="0" op="" dtype="" >1.2703e-03 2.8610e-06 5.0068e-06</hent>
</hash>
<internal rank="1" log_i="1723713849.509740" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="2" mpi_size="768" stamp_init="1723713791.082026" stamp_final="1723713849.503247" username="apac4" allocationname="unknown" flags="0" pid="1717065" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.84212e+01" utime="4.87002e+01" stime="7.26015e+00" mtime="3.19751e+01" gflop="0.00000e+00" gbyte="3.78258e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19751e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf41a141c141d14b1551d141c145f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83754e+01" utime="4.86701e+01" stime="7.25275e+00" mtime="3.19751e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19751e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4859e+08" > 5.5626e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4958e+08" > 3.7874e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9829e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4485e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0777e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.8610e-06 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6154e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.9622e-04 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1400e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6180e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9297e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="190" >
<hent key="02400100000000000000080000000001" call="MPI_Isend" bytes="2048" orank="1" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.6703e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000003" call="MPI_Isend" bytes="2048" orank="3" region="0" commid="0" count="13" tid="0" op="" dtype="" >5.2929e-05 3.0994e-06 5.0068e-06</hent>
<hent key="0240010000000000000008000000000A" call="MPI_Isend" bytes="2048" orank="10" region="0" commid="0" count="15" tid="0" op="" dtype="" >4.8399e-05 9.5367e-07 1.5020e-05</hent>
<hent key="0240010000000000000008000000003A" call="MPI_Isend" bytes="2048" orank="58" region="0" commid="0" count="10" tid="0" op="" dtype="" >5.3883e-05 5.0068e-06 5.9605e-06</hent>
<hent key="02400100000000000000080000000042" call="MPI_Isend" bytes="2048" orank="66" region="0" commid="0" count="3466" tid="0" op="" dtype="" >1.4596e-02 9.5367e-07 8.3208e-05</hent>
<hent key="024001000000000000000800000002C2" call="MPI_Isend" bytes="2048" orank="706" region="0" commid="0" count="3475" tid="0" op="" dtype="" >1.1968e-02 9.5367e-07 2.3127e-05</hent>
<hent key="038001000000000000000E00000002C2" call="MPI_Irecv" bytes="3584" orank="706" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000E0000000042" call="MPI_Isend" bytes="3584" orank="66" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.2875e-05 5.9605e-06 6.9141e-06</hent>
<hent key="03800100000000000000028000000001" call="MPI_Irecv" bytes="640" orank="1" region="0" commid="0" count="422" tid="0" op="" dtype="" >2.0885e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000003" call="MPI_Irecv" bytes="640" orank="3" region="0" commid="0" count="429" tid="0" op="" dtype="" >1.0395e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000002800000000A" call="MPI_Irecv" bytes="640" orank="10" region="0" commid="0" count="412" tid="0" op="" dtype="" >1.1373e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000002800000003A" call="MPI_Irecv" bytes="640" orank="58" region="0" commid="0" count="404" tid="0" op="" dtype="" >9.2506e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000028000000042" call="MPI_Irecv" bytes="640" orank="66" region="0" commid="0" count="281" tid="0" op="" dtype="" >9.1076e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002C2" call="MPI_Irecv" bytes="640" orank="706" region="0" commid="0" count="275" tid="0" op="" dtype="" >8.7738e-05 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000028000000001" call="MPI_Isend" bytes="640" orank="1" region="0" commid="0" count="421" tid="0" op="" dtype="" >5.5885e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000028000000003" call="MPI_Isend" bytes="640" orank="3" region="0" commid="0" count="428" tid="0" op="" dtype="" >1.3540e-03 1.9073e-06 7.1526e-06</hent>
<hent key="0240010000000000000002800000000A" call="MPI_Isend" bytes="640" orank="10" region="0" commid="0" count="407" tid="0" op="" dtype="" >5.9772e-04 9.5367e-07 2.0981e-05</hent>
<hent key="0240010000000000000002800000003A" call="MPI_Isend" bytes="640" orank="58" region="0" commid="0" count="420" tid="0" op="" dtype="" >2.1615e-03 3.8147e-06 1.9073e-05</hent>
<hent key="02400100000000000000028000000042" call="MPI_Isend" bytes="640" orank="66" region="0" commid="0" count="273" tid="0" op="" dtype="" >1.3745e-03 2.8610e-06 1.1921e-05</hent>
<hent key="024001000000000000000280000002C2" call="MPI_Isend" bytes="640" orank="706" region="0" commid="0" count="270" tid="0" op="" dtype="" >1.1024e-03 2.8610e-06 1.1921e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="03800100000000000000014000000001" call="MPI_Irecv" bytes="320" orank="1" region="0" commid="0" count="338" tid="0" op="" dtype="" >1.5378e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000003" call="MPI_Irecv" bytes="320" orank="3" region="0" commid="0" count="382" tid="0" op="" dtype="" >9.0599e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000A" call="MPI_Irecv" bytes="320" orank="10" region="0" commid="0" count="357" tid="0" op="" dtype="" >1.0395e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0380010000000000000001400000003A" call="MPI_Irecv" bytes="320" orank="58" region="0" commid="0" count="398" tid="0" op="" dtype="" >8.4639e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000042" call="MPI_Irecv" bytes="320" orank="66" region="0" commid="0" count="198" tid="0" op="" dtype="" >7.7248e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002C2" call="MPI_Irecv" bytes="320" orank="706" region="0" commid="0" count="166" tid="0" op="" dtype="" >4.7207e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >5.2452e-06 9.5367e-07 1.9073e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.9829e+00 0.0000e+00 1.1631e-01</hent>
<hent key="03800100000000000000400000000042" call="MPI_Irecv" bytes="16384" orank="66" region="0" commid="0" count="12699" tid="0" op="" dtype="" >5.7499e-03 0.0000e+00 2.1935e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6928e-05 0.0000e+00 1.5974e-05</hent>
<hent key="038001000000000000004000000002C2" call="MPI_Irecv" bytes="16384" orank="706" region="0" commid="0" count="12699" tid="0" op="" dtype="" >3.1321e-03 0.0000e+00 1.7881e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000001" call="MPI_Isend" bytes="320" orank="1" region="0" commid="0" count="369" tid="0" op="" dtype="" >3.7479e-04 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000014000000003" call="MPI_Isend" bytes="320" orank="3" region="0" commid="0" count="364" tid="0" op="" dtype="" >1.0726e-03 1.1921e-06 3.4809e-05</hent>
<hent key="0240010000000000000001400000000A" call="MPI_Isend" bytes="320" orank="10" region="0" commid="0" count="358" tid="0" op="" dtype="" >4.5419e-04 0.0000e+00 1.2159e-05</hent>
<hent key="0240010000000000000001400000003A" call="MPI_Isend" bytes="320" orank="58" region="0" commid="0" count="375" tid="0" op="" dtype="" >1.7915e-03 2.8610e-06 1.5020e-05</hent>
<hent key="02400100000000000000014000000042" call="MPI_Isend" bytes="320" orank="66" region="0" commid="0" count="166" tid="0" op="" dtype="" >8.3709e-04 3.0994e-06 1.0967e-05</hent>
<hent key="03800100000000000000200000000001" call="MPI_Irecv" bytes="8192" orank="1" region="0" commid="0" count="3615" tid="0" op="" dtype="" >1.2231e-03 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000200000000003" call="MPI_Irecv" bytes="8192" orank="3" region="0" commid="0" count="3956" tid="0" op="" dtype="" >1.0014e-03 0.0000e+00 3.0041e-05</hent>
<hent key="0380010000000000000020000000000A" call="MPI_Irecv" bytes="8192" orank="10" region="0" commid="0" count="12699" tid="0" op="" dtype="" >7.1425e-03 0.0000e+00 3.5048e-05</hent>
<hent key="0380010000000000000020000000003A" call="MPI_Irecv" bytes="8192" orank="58" region="0" commid="0" count="12644" tid="0" op="" dtype="" >2.1379e-03 0.0000e+00 3.4809e-05</hent>
<hent key="024001000000000000000140000002C2" call="MPI_Isend" bytes="320" orank="706" region="0" commid="0" count="185" tid="0" op="" dtype="" >7.3671e-04 2.8610e-06 9.0599e-06</hent>
<hent key="03800100000000000000000000000001" call="MPI_Irecv" bytes="0" orank="1" region="0" commid="0" count="264" tid="0" op="" dtype="" >1.0252e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000003" call="MPI_Irecv" bytes="0" orank="3" region="0" commid="0" count="260" tid="0" op="" dtype="" >6.6757e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000A" call="MPI_Irecv" bytes="0" orank="10" region="0" commid="0" count="260" tid="0" op="" dtype="" >9.2268e-05 0.0000e+00 8.1062e-06</hent>
<hent key="0380010000000000000000000000003A" call="MPI_Irecv" bytes="0" orank="58" region="0" commid="0" count="269" tid="0" op="" dtype="" >7.7963e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000000000042" call="MPI_Irecv" bytes="0" orank="66" region="0" commid="0" count="143" tid="0" op="" dtype="" >3.7670e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000042" call="MPI_Isend" bytes="16384" orank="66" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.3254e-01 3.8147e-06 9.7036e-05</hent>
<hent key="038001000000000000000000000002C2" call="MPI_Irecv" bytes="0" orank="706" region="0" commid="0" count="149" tid="0" op="" dtype="" >3.7909e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002C2" call="MPI_Isend" bytes="16384" orank="706" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.0420e-01 2.8610e-06 7.7963e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >8.0490e-04 0.0000e+00 6.3705e-04</hent>
<hent key="02400100000000000000200000000001" call="MPI_Isend" bytes="8192" orank="1" region="0" commid="0" count="2473" tid="0" op="" dtype="" >3.6967e-03 0.0000e+00 1.7166e-05</hent>
<hent key="02400100000000000000200000000003" call="MPI_Isend" bytes="8192" orank="3" region="0" commid="0" count="4145" tid="0" op="" dtype="" >7.1554e-03 0.0000e+00 3.6001e-05</hent>
<hent key="0240010000000000000020000000000A" call="MPI_Isend" bytes="8192" orank="10" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.4399e-02 0.0000e+00 5.1975e-05</hent>
<hent key="0240010000000000000020000000003A" call="MPI_Isend" bytes="8192" orank="58" region="0" commid="0" count="12642" tid="0" op="" dtype="" >9.8038e-02 3.8147e-06 1.0514e-04</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.3127e-05 2.3127e-05 2.3127e-05</hent>
<hent key="02400100000000000000000000000001" call="MPI_Isend" bytes="0" orank="1" region="0" commid="0" count="248" tid="0" op="" dtype="" >1.5831e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000003" call="MPI_Isend" bytes="0" orank="3" region="0" commid="0" count="262" tid="0" op="" dtype="" >5.0163e-04 0.0000e+00 2.9087e-05</hent>
<hent key="0240010000000000000000000000000A" call="MPI_Isend" bytes="0" orank="10" region="0" commid="0" count="252" tid="0" op="" dtype="" >2.3365e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000000000000003A" call="MPI_Isend" bytes="0" orank="58" region="0" commid="0" count="252" tid="0" op="" dtype="" >1.0347e-03 1.9073e-06 1.5974e-05</hent>
<hent key="02400100000000000000000000000042" call="MPI_Isend" bytes="0" orank="66" region="0" commid="0" count="156" tid="0" op="" dtype="" >6.6996e-04 1.9073e-06 1.5020e-05</hent>
<hent key="024001000000000000000000000002C2" call="MPI_Isend" bytes="0" orank="706" region="0" commid="0" count="148" tid="0" op="" dtype="" >6.0606e-04 1.9073e-06 4.8161e-05</hent>
<hent key="03800100000000000000060000000001" call="MPI_Irecv" bytes="1536" orank="1" region="0" commid="0" count="105" tid="0" op="" dtype="" >5.1260e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000003" call="MPI_Irecv" bytes="1536" orank="3" region="0" commid="0" count="101" tid="0" op="" dtype="" >2.4319e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000A" call="MPI_Irecv" bytes="1536" orank="10" region="0" commid="0" count="130" tid="0" op="" dtype="" >3.7670e-05 0.0000e+00 8.1062e-06</hent>
<hent key="0380010000000000000006000000003A" call="MPI_Irecv" bytes="1536" orank="58" region="0" commid="0" count="75" tid="0" op="" dtype="" >1.9789e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000042" call="MPI_Irecv" bytes="1536" orank="66" region="0" commid="0" count="211" tid="0" op="" dtype="" >1.0109e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002C2" call="MPI_Irecv" bytes="1536" orank="706" region="0" commid="0" count="212" tid="0" op="" dtype="" >5.8413e-05 0.0000e+00 6.9141e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000001" call="MPI_Isend" bytes="1536" orank="1" region="0" commid="0" count="90" tid="0" op="" dtype="" >1.7571e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000003" call="MPI_Isend" bytes="1536" orank="3" region="0" commid="0" count="87" tid="0" op="" dtype="" >3.4523e-04 2.8610e-06 1.2875e-05</hent>
<hent key="0240010000000000000006000000000A" call="MPI_Isend" bytes="1536" orank="10" region="0" commid="0" count="99" tid="0" op="" dtype="" >2.3913e-04 9.5367e-07 1.9073e-05</hent>
<hent key="0240010000000000000006000000003A" call="MPI_Isend" bytes="1536" orank="58" region="0" commid="0" count="71" tid="0" op="" dtype="" >4.1485e-04 3.8147e-06 1.2159e-05</hent>
<hent key="02400100000000000000060000000042" call="MPI_Isend" bytes="1536" orank="66" region="0" commid="0" count="213" tid="0" op="" dtype="" >1.1439e-03 3.0994e-06 1.0967e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C2" call="MPI_Isend" bytes="1536" orank="706" region="0" commid="0" count="225" tid="0" op="" dtype="" >1.0090e-03 3.0994e-06 1.0014e-05</hent>
<hent key="038001000000000000000C0000000001" call="MPI_Irecv" bytes="3072" orank="1" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.1921e-06 1.1921e-06 1.1921e-06</hent>
<hent key="038001000000000000000C0000000042" call="MPI_Irecv" bytes="3072" orank="66" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C2" call="MPI_Irecv" bytes="3072" orank="706" region="0" commid="0" count="7" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C000000003A" call="MPI_Isend" bytes="3072" orank="58" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="024001000000000000000C0000000042" call="MPI_Isend" bytes="3072" orank="66" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.0014e-05 5.0068e-06 5.0068e-06</hent>
<hent key="024001000000000000000C00000002C2" call="MPI_Isend" bytes="3072" orank="706" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.4796e-05 3.8147e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.8903e-05 6.8903e-05 6.8903e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.4400e-04 4.5061e-05 5.1022e-05</hent>
<hent key="03800100000000000000038000000001" call="MPI_Irecv" bytes="896" orank="1" region="0" commid="0" count="2772" tid="0" op="" dtype="" >3.8171e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000038000000003" call="MPI_Irecv" bytes="896" orank="3" region="0" commid="0" count="2641" tid="0" op="" dtype="" >6.9976e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0380010000000000000003800000000A" call="MPI_Irecv" bytes="896" orank="10" region="0" commid="0" count="300" tid="0" op="" dtype="" >7.9870e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000003A" call="MPI_Irecv" bytes="896" orank="58" region="0" commid="0" count="313" tid="0" op="" dtype="" >8.3447e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000042" call="MPI_Irecv" bytes="896" orank="66" region="0" commid="0" count="318" tid="0" op="" dtype="" >1.3757e-04 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000000380000002C2" call="MPI_Irecv" bytes="896" orank="706" region="0" commid="0" count="337" tid="0" op="" dtype="" >1.0037e-04 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.1035e-05 6.1035e-05 6.1035e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="02400100000000000000038000000001" call="MPI_Isend" bytes="896" orank="1" region="0" commid="0" count="3072" tid="0" op="" dtype="" >2.2175e-03 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000038000000003" call="MPI_Isend" bytes="896" orank="3" region="0" commid="0" count="2588" tid="0" op="" dtype="" >2.7254e-03 0.0000e+00 1.3113e-05</hent>
<hent key="0240010000000000000003800000000A" call="MPI_Isend" bytes="896" orank="10" region="0" commid="0" count="321" tid="0" op="" dtype="" >5.3501e-04 9.5367e-07 1.0967e-05</hent>
<hent key="0240010000000000000003800000003A" call="MPI_Isend" bytes="896" orank="58" region="0" commid="0" count="317" tid="0" op="" dtype="" >1.6832e-03 3.8147e-06 1.5974e-05</hent>
<hent key="02400100000000000000038000000042" call="MPI_Isend" bytes="896" orank="66" region="0" commid="0" count="336" tid="0" op="" dtype="" >1.7273e-03 2.8610e-06 1.0014e-05</hent>
<hent key="024001000000000000000380000002C2" call="MPI_Isend" bytes="896" orank="706" region="0" commid="0" count="304" tid="0" op="" dtype="" >1.2982e-03 2.8610e-06 2.5034e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.1760e+00 8.1062e-06 1.3920e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.0998e-04 5.0998e-04 5.0998e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >3.1400e-04 3.1400e-04 3.1400e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >1.8399e-03 1.8399e-03 1.8399e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >7.4311e-02 2.9111e-04 6.8309e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.5313e-04 5.5313e-04 5.5313e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 9.5367e-07 1.1921e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6154e+00 4.5705e-04 2.5690e-01</hent>
<hent key="03800100000000000000040000000001" call="MPI_Irecv" bytes="1024" orank="1" region="0" commid="0" count="954" tid="0" op="" dtype="" >7.1764e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000003" call="MPI_Irecv" bytes="1024" orank="3" region="0" commid="0" count="1036" tid="0" op="" dtype="" >2.9683e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0380010000000000000004000000000A" call="MPI_Irecv" bytes="1024" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3053e-03 0.0000e+00 3.3140e-05</hent>
<hent key="0380010000000000000004000000003A" call="MPI_Irecv" bytes="1024" orank="58" region="0" commid="0" count="3384" tid="0" op="" dtype="" >3.3355e-04 0.0000e+00 1.5020e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.0777e-04 1.9073e-06 7.7009e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000070000000001" call="MPI_Irecv" bytes="1792" orank="1" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.2650e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000003" call="MPI_Irecv" bytes="1792" orank="3" region="0" commid="0" count="44" tid="0" op="" dtype="" >8.8215e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000A" call="MPI_Irecv" bytes="1792" orank="10" region="0" commid="0" count="47" tid="0" op="" dtype="" >9.2983e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000003A" call="MPI_Irecv" bytes="1792" orank="58" region="0" commid="0" count="41" tid="0" op="" dtype="" >7.6294e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000042" call="MPI_Irecv" bytes="1792" orank="66" region="0" commid="0" count="131" tid="0" op="" dtype="" >5.6744e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002C2" call="MPI_Irecv" bytes="1792" orank="706" region="0" commid="0" count="124" tid="0" op="" dtype="" >3.8147e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >7.1764e-05 0.0000e+00 4.6015e-05</hent>
<hent key="02400100000000000000040000000001" call="MPI_Isend" bytes="1024" orank="1" region="0" commid="0" count="668" tid="0" op="" dtype="" >3.9887e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000040000000003" call="MPI_Isend" bytes="1024" orank="3" region="0" commid="0" count="1122" tid="0" op="" dtype="" >8.4496e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000004000000000A" call="MPI_Isend" bytes="1024" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.4359e-03 0.0000e+00 2.3842e-05</hent>
<hent key="0240010000000000000004000000003A" call="MPI_Isend" bytes="1024" orank="58" region="0" commid="0" count="3376" tid="0" op="" dtype="" >1.2076e-02 9.5367e-07 7.3195e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >5.0068e-06 2.1458e-06 2.8610e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4475e+00 0.0000e+00 3.2485e+00</hent>
<hent key="038001000000000000000A0000000001" call="MPI_Irecv" bytes="2560" orank="1" region="0" commid="0" count="6" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000003" call="MPI_Irecv" bytes="2560" orank="3" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000000A" call="MPI_Irecv" bytes="2560" orank="10" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000003A" call="MPI_Irecv" bytes="2560" orank="58" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000042" call="MPI_Irecv" bytes="2560" orank="66" region="0" commid="0" count="52" tid="0" op="" dtype="" >2.1458e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002C2" call="MPI_Irecv" bytes="2560" orank="706" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.6928e-05 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000070000000001" call="MPI_Isend" bytes="1792" orank="1" region="0" commid="0" count="43" tid="0" op="" dtype="" >9.7990e-05 1.9073e-06 5.9605e-06</hent>
<hent key="02400100000000000000070000000003" call="MPI_Isend" bytes="1792" orank="3" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.6379e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000007000000000A" call="MPI_Isend" bytes="1792" orank="10" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.1277e-04 9.5367e-07 2.0027e-05</hent>
<hent key="0240010000000000000007000000003A" call="MPI_Isend" bytes="1792" orank="58" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.8167e-04 3.8147e-06 1.9073e-05</hent>
<hent key="02400100000000000000070000000042" call="MPI_Isend" bytes="1792" orank="66" region="0" commid="0" count="137" tid="0" op="" dtype="" >7.5269e-04 3.8147e-06 1.0014e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6180e-02 3.6180e-02 3.6180e-02</hent>
<hent key="024001000000000000000700000002C2" call="MPI_Isend" bytes="1792" orank="706" region="0" commid="0" count="155" tid="0" op="" dtype="" >6.9189e-04 3.0994e-06 1.0014e-05</hent>
<hent key="024001000000000000000A0000000001" call="MPI_Isend" bytes="2560" orank="1" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.6451e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000003" call="MPI_Isend" bytes="2560" orank="3" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.2159e-05 4.0531e-06 4.0531e-06</hent>
<hent key="024001000000000000000A000000000A" call="MPI_Isend" bytes="2560" orank="10" region="0" commid="0" count="3" tid="0" op="" dtype="" >6.1989e-06 1.9073e-06 2.1458e-06</hent>
<hent key="024001000000000000000A000000003A" call="MPI_Isend" bytes="2560" orank="58" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.6928e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000042" call="MPI_Isend" bytes="2560" orank="66" region="0" commid="0" count="47" tid="0" op="" dtype="" >2.6822e-04 4.7684e-06 1.1921e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.9897e-05 1.9073e-05 4.4823e-05</hent>
<hent key="024001000000000000000A00000002C2" call="MPI_Isend" bytes="2560" orank="706" region="0" commid="0" count="35" tid="0" op="" dtype="" >1.7428e-04 3.8147e-06 1.0014e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.8287e-04 9.1076e-05 9.1791e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >6.9141e-06 1.9073e-06 5.0068e-06</hent>
<hent key="03800100000000000000000400000001" call="MPI_Irecv" bytes="4" orank="1" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.4782e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000400000003" call="MPI_Irecv" bytes="4" orank="3" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.7530e-04 0.0000e+00 1.3828e-05</hent>
<hent key="0380010000000000000000040000000A" call="MPI_Irecv" bytes="4" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0898e-03 0.0000e+00 2.0027e-05</hent>
<hent key="0380010000000000000000040000003A" call="MPI_Irecv" bytes="4" orank="58" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.1594e-04 0.0000e+00 1.3113e-05</hent>
<hent key="03800100000000000000000400000042" call="MPI_Irecv" bytes="4" orank="66" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.3494e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.9315e-04 6.8188e-05 1.1301e-04</hent>
<hent key="038001000000000000001C0000000001" call="MPI_Irecv" bytes="7168" orank="1" region="0" commid="0" count="9084" tid="0" op="" dtype="" >3.1328e-03 0.0000e+00 1.5020e-05</hent>
<hent key="038001000000000000001C0000000003" call="MPI_Irecv" bytes="7168" orank="3" region="0" commid="0" count="8743" tid="0" op="" dtype="" >2.1608e-03 0.0000e+00 1.1921e-05</hent>
<hent key="038001000000000000000004000002C2" call="MPI_Irecv" bytes="4" orank="706" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.8726e-04 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000000400000001" call="MPI_Isend" bytes="4" orank="1" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4968e-03 0.0000e+00 4.0054e-05</hent>
<hent key="02400100000000000000000400000003" call="MPI_Isend" bytes="4" orank="3" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.9960e-03 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000000040000000A" call="MPI_Isend" bytes="4" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4772e-03 0.0000e+00 8.8215e-06</hent>
<hent key="0240010000000000000000040000003A" call="MPI_Isend" bytes="4" orank="58" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3044e-02 2.8610e-06 1.2112e-04</hent>
<hent key="02400100000000000000000400000042" call="MPI_Isend" bytes="4" orank="66" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.1820e-02 3.8147e-06 7.0095e-05</hent>
<hent key="024001000000000000001C0000000001" call="MPI_Isend" bytes="7168" orank="1" region="0" commid="0" count="10226" tid="0" op="" dtype="" >2.2899e-02 9.5367e-07 5.4121e-05</hent>
<hent key="024001000000000000001C0000000003" call="MPI_Isend" bytes="7168" orank="3" region="0" commid="0" count="8554" tid="0" op="" dtype="" >2.3623e-02 9.5367e-07 1.9073e-05</hent>
<hent key="024001000000000000000004000002C2" call="MPI_Isend" bytes="4" orank="706" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.8675e-02 2.8610e-06 5.5075e-05</hent>
<hent key="03800100000000000000050000000001" call="MPI_Irecv" bytes="1280" orank="1" region="0" commid="0" count="178" tid="0" op="" dtype="" >7.7486e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000003" call="MPI_Irecv" bytes="1280" orank="3" region="0" commid="0" count="187" tid="0" op="" dtype="" >4.7445e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000A" call="MPI_Irecv" bytes="1280" orank="10" region="0" commid="0" count="178" tid="0" op="" dtype="" >4.6253e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000003A" call="MPI_Irecv" bytes="1280" orank="58" region="0" commid="0" count="198" tid="0" op="" dtype="" >6.0558e-05 0.0000e+00 8.8215e-06</hent>
<hent key="03800100000000000000050000000042" call="MPI_Irecv" bytes="1280" orank="66" region="0" commid="0" count="278" tid="0" op="" dtype="" >1.3757e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.1043e+01 6.9141e-06 1.3894e-01</hent>
<hent key="0380010000000000000028000000003A" call="MPI_Irecv" bytes="10240" orank="58" region="0" commid="0" count="55" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002C2" call="MPI_Irecv" bytes="1280" orank="706" region="0" commid="0" count="319" tid="0" op="" dtype="" >1.0252e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000080000000001" call="MPI_Irecv" bytes="2048" orank="1" region="0" commid="0" count="14" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000003" call="MPI_Irecv" bytes="2048" orank="3" region="0" commid="0" count="14" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000A" call="MPI_Irecv" bytes="2048" orank="10" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000003A" call="MPI_Irecv" bytes="2048" orank="58" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000042" call="MPI_Irecv" bytes="2048" orank="66" region="0" commid="0" count="3478" tid="0" op="" dtype="" >7.4053e-04 0.0000e+00 2.0981e-05</hent>
<hent key="038001000000000000000800000002C2" call="MPI_Irecv" bytes="2048" orank="706" region="0" commid="0" count="3467" tid="0" op="" dtype="" >6.5494e-04 0.0000e+00 1.2159e-05</hent>
<hent key="02400100000000000000050000000001" call="MPI_Isend" bytes="1280" orank="1" region="0" commid="0" count="168" tid="0" op="" dtype="" >3.0112e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000003" call="MPI_Isend" bytes="1280" orank="3" region="0" commid="0" count="189" tid="0" op="" dtype="" >7.1001e-04 2.8610e-06 7.1526e-06</hent>
<hent key="0240010000000000000005000000000A" call="MPI_Isend" bytes="1280" orank="10" region="0" commid="0" count="207" tid="0" op="" dtype="" >3.7479e-04 9.5367e-07 1.2875e-05</hent>
<hent key="0240010000000000000005000000003A" call="MPI_Isend" bytes="1280" orank="58" region="0" commid="0" count="242" tid="0" op="" dtype="" >1.2581e-03 1.9073e-06 1.4067e-05</hent>
<hent key="02400100000000000000050000000042" call="MPI_Isend" bytes="1280" orank="66" region="0" commid="0" count="299" tid="0" op="" dtype="" >1.5781e-03 3.8147e-06 2.0027e-05</hent>
<hent key="0240010000000000000028000000003A" call="MPI_Isend" bytes="10240" orank="58" region="0" commid="0" count="57" tid="0" op="" dtype="" >5.2786e-04 5.0068e-06 2.9802e-05</hent>
<hent key="024001000000000000000500000002C2" call="MPI_Isend" bytes="1280" orank="706" region="0" commid="0" count="295" tid="0" op="" dtype="" >1.2679e-03 2.8610e-06 1.0014e-05</hent>
</hash>
<internal rank="2" log_i="1723713849.503247" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="3" mpi_size="768" stamp_init="1723713791.085080" stamp_final="1723713849.509642" username="apac4" allocationname="unknown" flags="0" pid="1717066" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.84246e+01" utime="5.02613e+01" stime="6.79330e+00" mtime="3.24175e+01" gflop="0.00000e+00" gbyte="3.78368e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24175e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003514b456351435149b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83743e+01" utime="5.02293e+01" stime="6.78693e+00" mtime="3.24175e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24175e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4936e+08" > 4.3063e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4967e+08" > 2.8050e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2305e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4442e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0266e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4809e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6153e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2500e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7394e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6194e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9631e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="192" >
<hent key="02400100000000000000080000000002" call="MPI_Isend" bytes="2048" orank="2" region="0" commid="0" count="14" tid="0" op="" dtype="" >3.0756e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000004" call="MPI_Isend" bytes="2048" orank="4" region="0" commid="0" count="5" tid="0" op="" dtype="" >4.2915e-05 3.8147e-06 2.6941e-05</hent>
<hent key="0240010000000000000008000000000B" call="MPI_Isend" bytes="2048" orank="11" region="0" commid="0" count="17" tid="0" op="" dtype="" >5.2691e-05 9.5367e-07 1.2875e-05</hent>
<hent key="0240010000000000000008000000003B" call="MPI_Isend" bytes="2048" orank="59" region="0" commid="0" count="16" tid="0" op="" dtype="" >8.0824e-05 3.8147e-06 5.9605e-06</hent>
<hent key="02400100000000000000080000000043" call="MPI_Isend" bytes="2048" orank="67" region="0" commid="0" count="3465" tid="0" op="" dtype="" >1.1008e-02 9.5367e-07 2.0027e-05</hent>
<hent key="024001000000000000000800000002C3" call="MPI_Isend" bytes="2048" orank="707" region="0" commid="0" count="3466" tid="0" op="" dtype="" >1.0300e-02 9.5367e-07 3.3855e-05</hent>
<hent key="024001000000000000000E0000000043" call="MPI_Isend" bytes="3584" orank="67" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000002" call="MPI_Irecv" bytes="640" orank="2" region="0" commid="0" count="428" tid="0" op="" dtype="" >1.4877e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000028000000004" call="MPI_Irecv" bytes="640" orank="4" region="0" commid="0" count="391" tid="0" op="" dtype="" >9.3222e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000B" call="MPI_Irecv" bytes="640" orank="11" region="0" commid="0" count="402" tid="0" op="" dtype="" >7.7009e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000003B" call="MPI_Irecv" bytes="640" orank="59" region="0" commid="0" count="393" tid="0" op="" dtype="" >1.4091e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000043" call="MPI_Irecv" bytes="640" orank="67" region="0" commid="0" count="292" tid="0" op="" dtype="" >9.9182e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000000280000002C3" call="MPI_Irecv" bytes="640" orank="707" region="0" commid="0" count="251" tid="0" op="" dtype="" >7.4148e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000002" call="MPI_Isend" bytes="640" orank="2" region="0" commid="0" count="429" tid="0" op="" dtype="" >5.6338e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000028000000004" call="MPI_Isend" bytes="640" orank="4" region="0" commid="0" count="415" tid="0" op="" dtype="" >1.2639e-03 1.9073e-06 7.1526e-06</hent>
<hent key="0240010000000000000002800000000B" call="MPI_Isend" bytes="640" orank="11" region="0" commid="0" count="396" tid="0" op="" dtype="" >4.7302e-04 9.5367e-07 8.8215e-06</hent>
<hent key="0240010000000000000002800000003B" call="MPI_Isend" bytes="640" orank="59" region="0" commid="0" count="415" tid="0" op="" dtype="" >1.8644e-03 3.8147e-06 9.0599e-06</hent>
<hent key="02400100000000000000028000000043" call="MPI_Isend" bytes="640" orank="67" region="0" commid="0" count="277" tid="0" op="" dtype="" >1.1396e-03 2.8610e-06 8.8215e-06</hent>
<hent key="024001000000000000000280000002C3" call="MPI_Isend" bytes="640" orank="707" region="0" commid="0" count="292" tid="0" op="" dtype="" >1.1415e-03 2.8610e-06 1.0014e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0967e-05 1.0967e-05 1.0967e-05</hent>
<hent key="03800100000000000000014000000002" call="MPI_Irecv" bytes="320" orank="2" region="0" commid="0" count="364" tid="0" op="" dtype="" >1.2970e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000004" call="MPI_Irecv" bytes="320" orank="4" region="0" commid="0" count="388" tid="0" op="" dtype="" >9.3460e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000B" call="MPI_Irecv" bytes="320" orank="11" region="0" commid="0" count="363" tid="0" op="" dtype="" >7.7963e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000003B" call="MPI_Irecv" bytes="320" orank="59" region="0" commid="0" count="369" tid="0" op="" dtype="" >1.1063e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000043" call="MPI_Irecv" bytes="320" orank="67" region="0" commid="0" count="178" tid="0" op="" dtype="" >5.9128e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002C3" call="MPI_Irecv" bytes="320" orank="707" region="0" commid="0" count="198" tid="0" op="" dtype="" >5.7936e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.2305e+00 0.0000e+00 1.1631e-01</hent>
<hent key="03800100000000000000400000000043" call="MPI_Irecv" bytes="16384" orank="67" region="0" commid="0" count="12699" tid="0" op="" dtype="" >3.3276e-03 0.0000e+00 8.8215e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.0742e-05 9.5367e-07 1.8835e-05</hent>
<hent key="038001000000000000004000000002C3" call="MPI_Irecv" bytes="16384" orank="707" region="0" commid="0" count="12678" tid="0" op="" dtype="" >1.8709e-03 0.0000e+00 8.1062e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000014000000002" call="MPI_Isend" bytes="320" orank="2" region="0" commid="0" count="382" tid="0" op="" dtype="" >3.8528e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000004" call="MPI_Isend" bytes="320" orank="4" region="0" commid="0" count="361" tid="0" op="" dtype="" >1.0016e-03 9.5367e-07 6.9141e-06</hent>
<hent key="0240010000000000000001400000000B" call="MPI_Isend" bytes="320" orank="11" region="0" commid="0" count="348" tid="0" op="" dtype="" >3.3426e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000001400000003B" call="MPI_Isend" bytes="320" orank="59" region="0" commid="0" count="369" tid="0" op="" dtype="" >1.5557e-03 2.8610e-06 8.1062e-06</hent>
<hent key="02400100000000000000014000000043" call="MPI_Isend" bytes="320" orank="67" region="0" commid="0" count="166" tid="0" op="" dtype="" >6.6638e-04 2.8610e-06 8.8215e-06</hent>
<hent key="03800100000000000000200000000002" call="MPI_Irecv" bytes="8192" orank="2" region="0" commid="0" count="4145" tid="0" op="" dtype="" >1.1449e-03 0.0000e+00 1.2159e-05</hent>
<hent key="03800100000000000000200000000004" call="MPI_Irecv" bytes="8192" orank="4" region="0" commid="0" count="3669" tid="0" op="" dtype="" >5.5504e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000020000000000B" call="MPI_Irecv" bytes="8192" orank="11" region="0" commid="0" count="12699" tid="0" op="" dtype="" >6.5989e-03 0.0000e+00 6.1989e-06</hent>
<hent key="0380010000000000000020000000003B" call="MPI_Irecv" bytes="8192" orank="59" region="0" commid="0" count="12676" tid="0" op="" dtype="" >1.3866e-03 0.0000e+00 5.0068e-06</hent>
<hent key="024001000000000000000140000002C3" call="MPI_Isend" bytes="320" orank="707" region="0" commid="0" count="178" tid="0" op="" dtype="" >6.8069e-04 2.8610e-06 5.0068e-06</hent>
<hent key="03800100000000000000000000000002" call="MPI_Irecv" bytes="0" orank="2" region="0" commid="0" count="262" tid="0" op="" dtype="" >9.2745e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000004" call="MPI_Irecv" bytes="0" orank="4" region="0" commid="0" count="276" tid="0" op="" dtype="" >7.1526e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000B" call="MPI_Irecv" bytes="0" orank="11" region="0" commid="0" count="269" tid="0" op="" dtype="" >6.1512e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000003B" call="MPI_Irecv" bytes="0" orank="59" region="0" commid="0" count="263" tid="0" op="" dtype="" >7.7009e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000043" call="MPI_Irecv" bytes="0" orank="67" region="0" commid="0" count="135" tid="0" op="" dtype="" >3.8385e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000043" call="MPI_Isend" bytes="16384" orank="67" region="0" commid="0" count="12693" tid="0" op="" dtype="" >8.5484e-02 3.8147e-06 2.0027e-05</hent>
<hent key="038001000000000000000000000002C3" call="MPI_Irecv" bytes="0" orank="707" region="0" commid="0" count="155" tid="0" op="" dtype="" >4.3631e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002C3" call="MPI_Isend" bytes="16384" orank="707" region="0" commid="0" count="12692" tid="0" op="" dtype="" >8.2577e-02 2.8610e-06 2.7180e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.4553e-04 0.0000e+00 5.7197e-04</hent>
<hent key="02400100000000000000200000000002" call="MPI_Isend" bytes="8192" orank="2" region="0" commid="0" count="3956" tid="0" op="" dtype="" >5.2195e-03 0.0000e+00 5.2214e-05</hent>
<hent key="02400100000000000000200000000004" call="MPI_Isend" bytes="8192" orank="4" region="0" commid="0" count="3535" tid="0" op="" dtype="" >5.9237e-03 0.0000e+00 6.6996e-05</hent>
<hent key="0240010000000000000020000000000B" call="MPI_Isend" bytes="8192" orank="11" region="0" commid="0" count="12683" tid="0" op="" dtype="" >6.5000e-03 0.0000e+00 5.2929e-05</hent>
<hent key="0240010000000000000020000000003B" call="MPI_Isend" bytes="8192" orank="59" region="0" commid="0" count="12691" tid="0" op="" dtype="" >7.4956e-02 3.0994e-06 2.0027e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1921e-05 1.1921e-05 1.1921e-05</hent>
<hent key="02400100000000000000000000000002" call="MPI_Isend" bytes="0" orank="2" region="0" commid="0" count="260" tid="0" op="" dtype="" >1.6570e-04 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000000000000004" call="MPI_Isend" bytes="0" orank="4" region="0" commid="0" count="291" tid="0" op="" dtype="" >5.9009e-04 9.5367e-07 5.0068e-05</hent>
<hent key="0240010000000000000000000000000B" call="MPI_Isend" bytes="0" orank="11" region="0" commid="0" count="265" tid="0" op="" dtype="" >1.8859e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0240010000000000000000000000003B" call="MPI_Isend" bytes="0" orank="59" region="0" commid="0" count="256" tid="0" op="" dtype="" >9.7585e-04 1.9073e-06 5.0068e-06</hent>
<hent key="02400100000000000000000000000043" call="MPI_Isend" bytes="0" orank="67" region="0" commid="0" count="147" tid="0" op="" dtype="" >5.2810e-04 9.5367e-07 5.0068e-06</hent>
<hent key="024001000000000000000000000002C3" call="MPI_Isend" bytes="0" orank="707" region="0" commid="0" count="139" tid="0" op="" dtype="" >4.9305e-04 9.5367e-07 7.1526e-06</hent>
<hent key="03800100000000000000060000000002" call="MPI_Irecv" bytes="1536" orank="2" region="0" commid="0" count="87" tid="0" op="" dtype="" >3.0756e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000004" call="MPI_Irecv" bytes="1536" orank="4" region="0" commid="0" count="80" tid="0" op="" dtype="" >1.9312e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000B" call="MPI_Irecv" bytes="1536" orank="11" region="0" commid="0" count="90" tid="0" op="" dtype="" >1.6928e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000003B" call="MPI_Irecv" bytes="1536" orank="59" region="0" commid="0" count="94" tid="0" op="" dtype="" >3.8624e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000043" call="MPI_Irecv" bytes="1536" orank="67" region="0" commid="0" count="210" tid="0" op="" dtype="" >8.0585e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002C3" call="MPI_Irecv" bytes="1536" orank="707" region="0" commid="0" count="231" tid="0" op="" dtype="" >7.3195e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000002" call="MPI_Isend" bytes="1536" orank="2" region="0" commid="0" count="101" tid="0" op="" dtype="" >1.9312e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000004" call="MPI_Isend" bytes="1536" orank="4" region="0" commid="0" count="83" tid="0" op="" dtype="" >3.1924e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000006000000000B" call="MPI_Isend" bytes="1536" orank="11" region="0" commid="0" count="91" tid="0" op="" dtype="" >1.5950e-04 9.5367e-07 1.1921e-05</hent>
<hent key="0240010000000000000006000000003B" call="MPI_Isend" bytes="1536" orank="59" region="0" commid="0" count="81" tid="0" op="" dtype="" >3.9983e-04 3.8147e-06 5.9605e-06</hent>
<hent key="02400100000000000000060000000043" call="MPI_Isend" bytes="1536" orank="67" region="0" commid="0" count="205" tid="0" op="" dtype="" >9.7442e-04 3.8147e-06 6.2943e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C3" call="MPI_Isend" bytes="1536" orank="707" region="0" commid="0" count="213" tid="0" op="" dtype="" >8.9264e-04 2.8610e-06 8.8215e-06</hent>
<hent key="038001000000000000000C0000000043" call="MPI_Irecv" bytes="3072" orank="67" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C3" call="MPI_Irecv" bytes="3072" orank="707" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C000000000B" call="MPI_Isend" bytes="3072" orank="11" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.0967e-05 1.0967e-05 1.0967e-05</hent>
<hent key="024001000000000000000C0000000043" call="MPI_Isend" bytes="3072" orank="67" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.0981e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000C00000002C3" call="MPI_Isend" bytes="3072" orank="707" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.4080e-05 4.0531e-06 5.0068e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.2983e-05 9.2983e-05 9.2983e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9169e-04 6.2943e-05 6.4850e-05</hent>
<hent key="03800100000000000000038000000002" call="MPI_Irecv" bytes="896" orank="2" region="0" commid="0" count="2588" tid="0" op="" dtype="" >3.8338e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000004" call="MPI_Irecv" bytes="896" orank="4" region="0" commid="0" count="2735" tid="0" op="" dtype="" >5.1284e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0380010000000000000003800000000B" call="MPI_Irecv" bytes="896" orank="11" region="0" commid="0" count="323" tid="0" op="" dtype="" >6.1035e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000003B" call="MPI_Irecv" bytes="896" orank="59" region="0" commid="0" count="339" tid="0" op="" dtype="" >1.2708e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000043" call="MPI_Irecv" bytes="896" orank="67" region="0" commid="0" count="331" tid="0" op="" dtype="" >1.2183e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002C3" call="MPI_Irecv" bytes="896" orank="707" region="0" commid="0" count="328" tid="0" op="" dtype="" >7.9632e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >7.1049e-05 7.1049e-05 7.1049e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="038001000000000000003800000002C3" call="MPI_Irecv" bytes="14336" orank="707" region="0" commid="0" count="21" tid="0" op="" dtype="" >2.3842e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000038000000002" call="MPI_Isend" bytes="896" orank="2" region="0" commid="0" count="2641" tid="0" op="" dtype="" >1.8013e-03 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000038000000004" call="MPI_Isend" bytes="896" orank="4" region="0" commid="0" count="2741" tid="0" op="" dtype="" >2.8126e-03 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000003800000000B" call="MPI_Isend" bytes="896" orank="11" region="0" commid="0" count="341" tid="0" op="" dtype="" >4.9186e-04 9.5367e-07 1.2159e-05</hent>
<hent key="0240010000000000000003800000003B" call="MPI_Isend" bytes="896" orank="59" region="0" commid="0" count="323" tid="0" op="" dtype="" >1.5140e-03 3.8147e-06 6.1989e-06</hent>
<hent key="02400100000000000000038000000043" call="MPI_Isend" bytes="896" orank="67" region="0" commid="0" count="342" tid="0" op="" dtype="" >1.4942e-03 2.8610e-06 1.1206e-05</hent>
<hent key="024001000000000000000380000002C3" call="MPI_Isend" bytes="896" orank="707" region="0" commid="0" count="335" tid="0" op="" dtype="" >1.3192e-03 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000380000000043" call="MPI_Isend" bytes="14336" orank="67" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.7909e-05 5.0068e-06 8.1062e-06</hent>
<hent key="024001000000000000003800000002C3" call="MPI_Isend" bytes="14336" orank="707" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.3154e-05 4.0531e-06 8.1062e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.3289e+00 3.9101e-05 1.3924e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.3000e-04 5.3000e-04 5.3000e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >2.7394e-04 2.7394e-04 2.7394e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >1.7121e-03 1.7121e-03 1.7121e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >6.7843e-02 3.4618e-04 6.2178e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.4312e-04 5.4312e-04 5.4312e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6153e+00 4.3106e-04 2.5688e-01</hent>
<hent key="03800100000000000000040000000002" call="MPI_Irecv" bytes="1024" orank="2" region="0" commid="0" count="1122" tid="0" op="" dtype="" >1.2326e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000004" call="MPI_Irecv" bytes="1024" orank="4" region="0" commid="0" count="992" tid="0" op="" dtype="" >2.0599e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000000B" call="MPI_Irecv" bytes="1024" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >9.3961e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000004000000003B" call="MPI_Irecv" bytes="1024" orank="59" region="0" commid="0" count="3392" tid="0" op="" dtype="" >2.3389e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.0266e-05 0.0000e+00 9.0599e-06</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.9141e-06 6.9141e-06 6.9141e-06</hent>
<hent key="03800100000000000000070000000002" call="MPI_Irecv" bytes="1792" orank="2" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000004" call="MPI_Irecv" bytes="1792" orank="4" region="0" commid="0" count="38" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000B" call="MPI_Irecv" bytes="1792" orank="11" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.3590e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000003B" call="MPI_Irecv" bytes="1792" orank="59" region="0" commid="0" count="35" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000043" call="MPI_Irecv" bytes="1792" orank="67" region="0" commid="0" count="124" tid="0" op="" dtype="" >4.7207e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002C3" call="MPI_Irecv" bytes="1792" orank="707" region="0" commid="0" count="151" tid="0" op="" dtype="" >3.8624e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >1.0943e-04 0.0000e+00 5.1975e-05</hent>
<hent key="02400100000000000000040000000002" call="MPI_Isend" bytes="1024" orank="2" region="0" commid="0" count="1036" tid="0" op="" dtype="" >5.8365e-04 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000040000000004" call="MPI_Isend" bytes="1024" orank="4" region="0" commid="0" count="968" tid="0" op="" dtype="" >7.0858e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000004000000000B" call="MPI_Isend" bytes="1024" orank="11" region="0" commid="0" count="3394" tid="0" op="" dtype="" >1.9867e-03 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000004000000003B" call="MPI_Isend" bytes="1024" orank="59" region="0" commid="0" count="3396" tid="0" op="" dtype="" >8.4291e-03 9.5367e-07 8.8215e-06</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >4.1962e-05 2.8610e-06 3.9101e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4432e+00 0.0000e+00 3.2484e+00</hent>
<hent key="038001000000000000000A0000000002" call="MPI_Irecv" bytes="2560" orank="2" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000004" call="MPI_Irecv" bytes="2560" orank="4" region="0" commid="0" count="5" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000000B" call="MPI_Irecv" bytes="2560" orank="11" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A000000003B" call="MPI_Irecv" bytes="2560" orank="59" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000043" call="MPI_Irecv" bytes="2560" orank="67" region="0" commid="0" count="41" tid="0" op="" dtype="" >2.0266e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002C3" call="MPI_Irecv" bytes="2560" orank="707" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.2398e-05 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000070000000002" call="MPI_Isend" bytes="1792" orank="2" region="0" commid="0" count="44" tid="0" op="" dtype="" >9.4175e-05 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000070000000004" call="MPI_Isend" bytes="1792" orank="4" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.3137e-04 2.8610e-06 8.8215e-06</hent>
<hent key="0240010000000000000007000000000B" call="MPI_Isend" bytes="1792" orank="11" region="0" commid="0" count="42" tid="0" op="" dtype="" >9.3937e-05 9.5367e-07 1.2875e-05</hent>
<hent key="0240010000000000000007000000003B" call="MPI_Isend" bytes="1792" orank="59" region="0" commid="0" count="36" tid="0" op="" dtype="" >1.8477e-04 3.8147e-06 9.0599e-06</hent>
<hent key="02400100000000000000070000000043" call="MPI_Isend" bytes="1792" orank="67" region="0" commid="0" count="150" tid="0" op="" dtype="" >6.7830e-04 2.1458e-06 8.1062e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6194e-02 3.6194e-02 3.6194e-02</hent>
<hent key="024001000000000000000700000002C3" call="MPI_Isend" bytes="1792" orank="707" region="0" commid="0" count="137" tid="0" op="" dtype="" >5.9867e-04 1.9073e-06 2.0027e-05</hent>
<hent key="024001000000000000000A0000000002" call="MPI_Isend" bytes="2560" orank="2" region="0" commid="0" count="3" tid="0" op="" dtype="" >7.1526e-06 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000004" call="MPI_Isend" bytes="2560" orank="4" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.9073e-05 3.0994e-06 4.0531e-06</hent>
<hent key="024001000000000000000A000000000B" call="MPI_Isend" bytes="2560" orank="11" region="0" commid="0" count="5" tid="0" op="" dtype="" >8.8215e-06 9.5367e-07 2.1458e-06</hent>
<hent key="024001000000000000000A000000003B" call="MPI_Isend" bytes="2560" orank="59" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.5020e-05 5.0068e-06 5.0068e-06</hent>
<hent key="024001000000000000000A0000000043" call="MPI_Isend" bytes="2560" orank="67" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.1172e-04 3.8147e-06 6.1989e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.2302e-04 2.0027e-05 5.6982e-05</hent>
<hent key="024001000000000000000A00000002C3" call="MPI_Isend" bytes="2560" orank="707" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.8311e-04 3.8147e-06 5.0068e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >2.6608e-04 1.2207e-04 1.4400e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-05 3.0994e-05 3.0994e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 9.5367e-07 1.9073e-06</hent>
<hent key="03800100000000000000000400000002" call="MPI_Irecv" bytes="4" orank="2" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.0453e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000004" call="MPI_Irecv" bytes="4" orank="4" region="0" commid="0" count="3398" tid="0" op="" dtype="" >3.7694e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000000B" call="MPI_Irecv" bytes="4" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.8222e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000003B" call="MPI_Irecv" bytes="4" orank="59" region="0" commid="0" count="3398" tid="0" op="" dtype="" >3.9554e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000043" call="MPI_Irecv" bytes="4" orank="67" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.5337e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >5.7411e-04 8.6069e-05 1.8811e-04</hent>
<hent key="038001000000000000001C0000000002" call="MPI_Irecv" bytes="7168" orank="2" region="0" commid="0" count="8554" tid="0" op="" dtype="" >2.3320e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000001C0000000004" call="MPI_Irecv" bytes="7168" orank="4" region="0" commid="0" count="9030" tid="0" op="" dtype="" >1.3456e-03 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000004000002C3" call="MPI_Irecv" bytes="4" orank="707" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.4870e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000400000002" call="MPI_Isend" bytes="4" orank="2" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2589e-03 0.0000e+00 3.0994e-06</hent>
<hent key="02400100000000000000000400000004" call="MPI_Isend" bytes="4" orank="4" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.0914e-03 0.0000e+00 7.2002e-05</hent>
<hent key="0240010000000000000000040000000B" call="MPI_Isend" bytes="4" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0338e-03 0.0000e+00 4.7684e-06</hent>
<hent key="0240010000000000000000040000003B" call="MPI_Isend" bytes="4" orank="59" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.8534e-02 2.8610e-06 1.3995e-04</hent>
<hent key="02400100000000000000000400000043" call="MPI_Isend" bytes="4" orank="67" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7803e-02 2.8610e-06 1.0967e-05</hent>
<hent key="024001000000000000001C0000000002" call="MPI_Isend" bytes="7168" orank="2" region="0" commid="0" count="8743" tid="0" op="" dtype="" >1.9449e-02 9.5367e-07 2.0981e-05</hent>
<hent key="024001000000000000001C0000000004" call="MPI_Isend" bytes="7168" orank="4" region="0" commid="0" count="9164" tid="0" op="" dtype="" >2.6121e-02 9.5367e-07 1.5974e-05</hent>
<hent key="024001000000000000000004000002C3" call="MPI_Isend" bytes="4" orank="707" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6830e-02 2.8610e-06 6.8903e-05</hent>
<hent key="03800100000000000000050000000002" call="MPI_Irecv" bytes="1280" orank="2" region="0" commid="0" count="189" tid="0" op="" dtype="" >7.3195e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000004" call="MPI_Irecv" bytes="1280" orank="4" region="0" commid="0" count="185" tid="0" op="" dtype="" >4.5300e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000B" call="MPI_Irecv" bytes="1280" orank="11" region="0" commid="0" count="190" tid="0" op="" dtype="" >3.7432e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000003B" call="MPI_Irecv" bytes="1280" orank="59" region="0" commid="0" count="197" tid="0" op="" dtype="" >7.3671e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000043" call="MPI_Irecv" bytes="1280" orank="67" region="0" commid="0" count="298" tid="0" op="" dtype="" >1.1802e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.1232e+01 8.8215e-06 1.3907e-01</hent>
<hent key="0380010000000000000028000000003B" call="MPI_Irecv" bytes="10240" orank="59" region="0" commid="0" count="23" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002C3" call="MPI_Irecv" bytes="1280" orank="707" region="0" commid="0" count="265" tid="0" op="" dtype="" >7.5340e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000080000000002" call="MPI_Irecv" bytes="2048" orank="2" region="0" commid="0" count="13" tid="0" op="" dtype="" >6.6757e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000004" call="MPI_Irecv" bytes="2048" orank="4" region="0" commid="0" count="7" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000008000000000B" call="MPI_Irecv" bytes="2048" orank="11" region="0" commid="0" count="10" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000003B" call="MPI_Irecv" bytes="2048" orank="59" region="0" commid="0" count="12" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000043" call="MPI_Irecv" bytes="2048" orank="67" region="0" commid="0" count="3480" tid="0" op="" dtype="" >7.2670e-04 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000800000002C3" call="MPI_Irecv" bytes="2048" orank="707" region="0" commid="0" count="3472" tid="0" op="" dtype="" >4.9067e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000050000000002" call="MPI_Isend" bytes="1280" orank="2" region="0" commid="0" count="187" tid="0" op="" dtype="" >3.3140e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000004" call="MPI_Isend" bytes="1280" orank="4" region="0" commid="0" count="196" tid="0" op="" dtype="" >7.2217e-04 2.8610e-06 9.0599e-06</hent>
<hent key="0240010000000000000005000000000B" call="MPI_Isend" bytes="1280" orank="11" region="0" commid="0" count="197" tid="0" op="" dtype="" >3.1090e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000005000000003B" call="MPI_Isend" bytes="1280" orank="59" region="0" commid="0" count="202" tid="0" op="" dtype="" >9.7752e-04 1.9073e-06 6.1989e-06</hent>
<hent key="02400100000000000000050000000043" call="MPI_Isend" bytes="1280" orank="67" region="0" commid="0" count="297" tid="0" op="" dtype="" >1.2941e-03 2.8610e-06 5.9605e-06</hent>
<hent key="0240010000000000000028000000000B" call="MPI_Isend" bytes="10240" orank="11" region="0" commid="0" count="16" tid="0" op="" dtype="" >1.4305e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0240010000000000000028000000003B" call="MPI_Isend" bytes="10240" orank="59" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.9114e-05 5.0068e-06 6.9141e-06</hent>
<hent key="024001000000000000000500000002C3" call="MPI_Isend" bytes="1280" orank="707" region="0" commid="0" count="293" tid="0" op="" dtype="" >1.1716e-03 2.8610e-06 5.0068e-06</hent>
</hash>
<internal rank="3" log_i="1723713849.509642" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="4" mpi_size="768" stamp_init="1723713791.088504" stamp_final="1723713849.506833" username="apac4" allocationname="unknown" flags="0" pid="1717067" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.84183e+01" utime="4.71664e+01" stime="7.80848e+00" mtime="3.17693e+01" gflop="0.00000e+00" gbyte="3.76720e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17693e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83487e+01" utime="4.71343e+01" stime="7.80233e+00" mtime="3.17693e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17693e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.5763e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4906e+08" > 7.2601e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4894e+08" > 7.1437e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1266e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4425e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3919e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.8610e-06 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6150e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3852e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4690e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6156e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8748e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="192" >
<hent key="02400100000000000000080000000003" call="MPI_Isend" bytes="2048" orank="3" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.5974e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000005" call="MPI_Isend" bytes="2048" orank="5" region="0" commid="0" count="14" tid="0" op="" dtype="" >5.9366e-05 2.8610e-06 9.0599e-06</hent>
<hent key="0240010000000000000008000000000C" call="MPI_Isend" bytes="2048" orank="12" region="0" commid="0" count="9" tid="0" op="" dtype="" >2.1696e-05 1.9073e-06 5.9605e-06</hent>
<hent key="0240010000000000000008000000003C" call="MPI_Isend" bytes="2048" orank="60" region="0" commid="0" count="13" tid="0" op="" dtype="" >7.7009e-05 5.0068e-06 1.1921e-05</hent>
<hent key="02400100000000000000080000000044" call="MPI_Isend" bytes="2048" orank="68" region="0" commid="0" count="3469" tid="0" op="" dtype="" >2.4518e-02 9.5367e-07 9.6083e-05</hent>
<hent key="024001000000000000000800000002C4" call="MPI_Isend" bytes="2048" orank="708" region="0" commid="0" count="3464" tid="0" op="" dtype="" >1.5919e-02 9.5367e-07 6.3896e-05</hent>
<hent key="038001000000000000000E00000002C4" call="MPI_Irecv" bytes="3584" orank="708" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000E0000000044" call="MPI_Isend" bytes="3584" orank="68" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000003" call="MPI_Irecv" bytes="640" orank="3" region="0" commid="0" count="415" tid="0" op="" dtype="" >1.9288e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000028000000005" call="MPI_Irecv" bytes="640" orank="5" region="0" commid="0" count="409" tid="0" op="" dtype="" >1.1611e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000002800000000C" call="MPI_Irecv" bytes="640" orank="12" region="0" commid="0" count="409" tid="0" op="" dtype="" >2.4891e-04 0.0000e+00 9.0599e-06</hent>
<hent key="024001000000000000000E00000002C4" call="MPI_Isend" bytes="3584" orank="708" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.0967e-05 5.0068e-06 5.9605e-06</hent>
<hent key="0380010000000000000002800000003C" call="MPI_Irecv" bytes="640" orank="60" region="0" commid="0" count="395" tid="0" op="" dtype="" >1.2636e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000028000000044" call="MPI_Irecv" bytes="640" orank="68" region="0" commid="0" count="273" tid="0" op="" dtype="" >2.5415e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000280000002C4" call="MPI_Irecv" bytes="640" orank="708" region="0" commid="0" count="281" tid="0" op="" dtype="" >3.1090e-04 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000028000000003" call="MPI_Isend" bytes="640" orank="3" region="0" commid="0" count="391" tid="0" op="" dtype="" >5.6791e-04 9.5367e-07 1.3113e-05</hent>
<hent key="02400100000000000000028000000005" call="MPI_Isend" bytes="640" orank="5" region="0" commid="0" count="425" tid="0" op="" dtype="" >1.3213e-03 1.9073e-06 7.1526e-06</hent>
<hent key="0240010000000000000002800000000C" call="MPI_Isend" bytes="640" orank="12" region="0" commid="0" count="410" tid="0" op="" dtype="" >5.6171e-04 9.5367e-07 9.0599e-06</hent>
<hent key="0240010000000000000002800000003C" call="MPI_Isend" bytes="640" orank="60" region="0" commid="0" count="395" tid="0" op="" dtype="" >1.9903e-03 3.8147e-06 1.5020e-05</hent>
<hent key="02400100000000000000028000000044" call="MPI_Isend" bytes="640" orank="68" region="0" commid="0" count="268" tid="0" op="" dtype="" >1.5361e-03 2.8610e-06 2.6941e-05</hent>
<hent key="024001000000000000000280000002C4" call="MPI_Isend" bytes="640" orank="708" region="0" commid="0" count="307" tid="0" op="" dtype="" >1.5371e-03 2.8610e-06 2.1935e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.0599e-06 9.0599e-06 9.0599e-06</hent>
<hent key="03800100000000000000014000000003" call="MPI_Irecv" bytes="320" orank="3" region="0" commid="0" count="361" tid="0" op="" dtype="" >1.7214e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000014000000005" call="MPI_Irecv" bytes="320" orank="5" region="0" commid="0" count="341" tid="0" op="" dtype="" >8.9645e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000C" call="MPI_Irecv" bytes="320" orank="12" region="0" commid="0" count="343" tid="0" op="" dtype="" >2.2197e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0380010000000000000001400000003C" call="MPI_Irecv" bytes="320" orank="60" region="0" commid="0" count="371" tid="0" op="" dtype="" >1.2469e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000014000000044" call="MPI_Irecv" bytes="320" orank="68" region="0" commid="0" count="171" tid="0" op="" dtype="" >1.5783e-04 0.0000e+00 1.1206e-05</hent>
<hent key="038001000000000000000140000002C4" call="MPI_Irecv" bytes="320" orank="708" region="0" commid="0" count="192" tid="0" op="" dtype="" >1.5664e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >5.7220e-06 9.5367e-07 1.9073e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.1266e+00 0.0000e+00 1.3112e-01</hent>
<hent key="03800100000000000000400000000044" call="MPI_Irecv" bytes="16384" orank="68" region="0" commid="0" count="12683" tid="0" op="" dtype="" >8.7175e-03 0.0000e+00 5.3883e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.7881e-05 0.0000e+00 1.6928e-05</hent>
<hent key="038001000000000000004000000002C4" call="MPI_Irecv" bytes="16384" orank="708" region="0" commid="0" count="12699" tid="0" op="" dtype="" >9.5091e-03 0.0000e+00 6.4135e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000003" call="MPI_Isend" bytes="320" orank="3" region="0" commid="0" count="388" tid="0" op="" dtype="" >4.4608e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000014000000005" call="MPI_Isend" bytes="320" orank="5" region="0" commid="0" count="350" tid="0" op="" dtype="" >9.5677e-04 9.5367e-07 7.1526e-06</hent>
<hent key="0240010000000000000001400000000C" call="MPI_Isend" bytes="320" orank="12" region="0" commid="0" count="361" tid="0" op="" dtype="" >4.3154e-04 0.0000e+00 1.4067e-05</hent>
<hent key="0240010000000000000001400000003C" call="MPI_Isend" bytes="320" orank="60" region="0" commid="0" count="351" tid="0" op="" dtype="" >1.6367e-03 3.0994e-06 1.4067e-05</hent>
<hent key="02400100000000000000014000000044" call="MPI_Isend" bytes="320" orank="68" region="0" commid="0" count="202" tid="0" op="" dtype="" >1.0695e-03 2.8610e-06 1.6212e-05</hent>
<hent key="03800100000000000000200000000003" call="MPI_Irecv" bytes="8192" orank="3" region="0" commid="0" count="3535" tid="0" op="" dtype="" >1.8191e-03 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000200000000005" call="MPI_Irecv" bytes="8192" orank="5" region="0" commid="0" count="3139" tid="0" op="" dtype="" >7.4887e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0380010000000000000020000000000C" call="MPI_Irecv" bytes="8192" orank="12" region="0" commid="0" count="12639" tid="0" op="" dtype="" >1.6390e-02 0.0000e+00 9.4891e-05</hent>
<hent key="0380010000000000000020000000003C" call="MPI_Irecv" bytes="8192" orank="60" region="0" commid="0" count="12523" tid="0" op="" dtype="" >1.7231e-03 0.0000e+00 1.8120e-05</hent>
<hent key="024001000000000000000140000002C4" call="MPI_Isend" bytes="320" orank="708" region="0" commid="0" count="184" tid="0" op="" dtype="" >8.7953e-04 2.8610e-06 2.4080e-05</hent>
<hent key="03800100000000000000000000000003" call="MPI_Irecv" bytes="0" orank="3" region="0" commid="0" count="291" tid="0" op="" dtype="" >1.2207e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000005" call="MPI_Irecv" bytes="0" orank="5" region="0" commid="0" count="265" tid="0" op="" dtype="" >8.4639e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000C" call="MPI_Irecv" bytes="0" orank="12" region="0" commid="0" count="276" tid="0" op="" dtype="" >1.4257e-04 0.0000e+00 8.8215e-06</hent>
<hent key="0380010000000000000000000000003C" call="MPI_Irecv" bytes="0" orank="60" region="0" commid="0" count="255" tid="0" op="" dtype="" >6.9618e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000044" call="MPI_Irecv" bytes="0" orank="68" region="0" commid="0" count="148" tid="0" op="" dtype="" >1.4758e-04 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000400000000044" call="MPI_Isend" bytes="16384" orank="68" region="0" commid="0" count="12699" tid="0" op="" dtype="" >2.0081e-01 3.8147e-06 1.1897e-04</hent>
<hent key="038001000000000000000000000002C4" call="MPI_Irecv" bytes="0" orank="708" region="0" commid="0" count="153" tid="0" op="" dtype="" >1.6332e-04 0.0000e+00 1.4067e-05</hent>
<hent key="024001000000000000004000000002C4" call="MPI_Isend" bytes="16384" orank="708" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.4138e-01 2.8610e-06 8.8930e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.8702e-04 0.0000e+00 6.1893e-04</hent>
<hent key="02400100000000000000200000000003" call="MPI_Isend" bytes="8192" orank="3" region="0" commid="0" count="3669" tid="0" op="" dtype="" >4.6532e-03 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000200000000005" call="MPI_Isend" bytes="8192" orank="5" region="0" commid="0" count="3199" tid="0" op="" dtype="" >6.3152e-03 0.0000e+00 1.7881e-05</hent>
<hent key="0240010000000000000020000000000C" call="MPI_Isend" bytes="8192" orank="12" region="0" commid="0" count="12659" tid="0" op="" dtype="" >1.0184e-02 0.0000e+00 6.9857e-05</hent>
<hent key="0240010000000000000020000000003C" call="MPI_Isend" bytes="8192" orank="60" region="0" commid="0" count="12557" tid="0" op="" dtype="" >1.3134e-01 3.8147e-06 1.3494e-04</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.3127e-05 2.3127e-05 2.3127e-05</hent>
<hent key="02400100000000000000000000000003" call="MPI_Isend" bytes="0" orank="3" region="0" commid="0" count="276" tid="0" op="" dtype="" >1.9550e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000005" call="MPI_Isend" bytes="0" orank="5" region="0" commid="0" count="234" tid="0" op="" dtype="" >4.6253e-04 9.5367e-07 3.4094e-05</hent>
<hent key="0240010000000000000000000000000C" call="MPI_Isend" bytes="0" orank="12" region="0" commid="0" count="266" tid="0" op="" dtype="" >2.1601e-04 0.0000e+00 1.1921e-05</hent>
<hent key="0240010000000000000000000000003C" call="MPI_Isend" bytes="0" orank="60" region="0" commid="0" count="274" tid="0" op="" dtype="" >1.1070e-03 2.8610e-06 1.0967e-05</hent>
<hent key="02400100000000000000000000000044" call="MPI_Isend" bytes="0" orank="68" region="0" commid="0" count="155" tid="0" op="" dtype="" >8.1873e-04 2.1458e-06 1.9073e-05</hent>
<hent key="024001000000000000000000000002C4" call="MPI_Isend" bytes="0" orank="708" region="0" commid="0" count="144" tid="0" op="" dtype="" >6.0105e-04 1.9073e-06 1.4067e-05</hent>
<hent key="03800100000000000000060000000003" call="MPI_Irecv" bytes="1536" orank="3" region="0" commid="0" count="83" tid="0" op="" dtype="" >3.6716e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000005" call="MPI_Irecv" bytes="1536" orank="5" region="0" commid="0" count="87" tid="0" op="" dtype="" >2.9564e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000C" call="MPI_Irecv" bytes="1536" orank="12" region="0" commid="0" count="105" tid="0" op="" dtype="" >5.7459e-05 0.0000e+00 3.0994e-06</hent>
<hent key="0380010000000000000006000000003C" call="MPI_Irecv" bytes="1536" orank="60" region="0" commid="0" count="104" tid="0" op="" dtype="" >3.0756e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000044" call="MPI_Irecv" bytes="1536" orank="68" region="0" commid="0" count="238" tid="0" op="" dtype="" >2.7895e-04 0.0000e+00 1.1921e-05</hent>
<hent key="038001000000000000000600000002C4" call="MPI_Irecv" bytes="1536" orank="708" region="0" commid="0" count="194" tid="0" op="" dtype="" >2.1458e-04 0.0000e+00 1.0967e-05</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.5763e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000003" call="MPI_Isend" bytes="1536" orank="3" region="0" commid="0" count="80" tid="0" op="" dtype="" >1.6570e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000005" call="MPI_Isend" bytes="1536" orank="5" region="0" commid="0" count="90" tid="0" op="" dtype="" >3.3879e-04 2.8610e-06 7.8678e-06</hent>
<hent key="0240010000000000000006000000000C" call="MPI_Isend" bytes="1536" orank="12" region="0" commid="0" count="103" tid="0" op="" dtype="" >2.6679e-04 9.5367e-07 1.3113e-05</hent>
<hent key="0240010000000000000006000000003C" call="MPI_Isend" bytes="1536" orank="60" region="0" commid="0" count="96" tid="0" op="" dtype="" >5.4979e-04 4.0531e-06 1.4067e-05</hent>
<hent key="02400100000000000000060000000044" call="MPI_Isend" bytes="1536" orank="68" region="0" commid="0" count="205" tid="0" op="" dtype="" >1.1744e-03 3.8147e-06 1.9073e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C4" call="MPI_Isend" bytes="1536" orank="708" region="0" commid="0" count="221" tid="0" op="" dtype="" >1.1683e-03 2.8610e-06 2.3842e-05</hent>
<hent key="038001000000000000000C0000000044" call="MPI_Irecv" bytes="3072" orank="68" region="0" commid="0" count="10" tid="0" op="" dtype="" >1.4067e-05 0.0000e+00 3.8147e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C4" call="MPI_Irecv" bytes="3072" orank="708" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000044" call="MPI_Isend" bytes="3072" orank="68" region="0" commid="0" count="4" tid="0" op="" dtype="" >3.8147e-05 5.9605e-06 1.8120e-05</hent>
<hent key="024001000000000000000C00000002C4" call="MPI_Isend" bytes="3072" orank="708" region="0" commid="0" count="4" tid="0" op="" dtype="" >3.0756e-05 5.0068e-06 1.0967e-05</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0395e-04 1.0395e-04 1.0395e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.0480e-04 6.6996e-05 6.8903e-05</hent>
<hent key="03800100000000000000038000000003" call="MPI_Irecv" bytes="896" orank="3" region="0" commid="0" count="2741" tid="0" op="" dtype="" >4.3106e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000005" call="MPI_Irecv" bytes="896" orank="5" region="0" commid="0" count="2896" tid="0" op="" dtype="" >7.3647e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0380010000000000000003800000000C" call="MPI_Irecv" bytes="896" orank="12" region="0" commid="0" count="307" tid="0" op="" dtype="" >1.4806e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000003800000003C" call="MPI_Irecv" bytes="896" orank="60" region="0" commid="0" count="321" tid="0" op="" dtype="" >1.1396e-04 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000038000000044" call="MPI_Irecv" bytes="896" orank="68" region="0" commid="0" count="315" tid="0" op="" dtype="" >2.7466e-04 0.0000e+00 1.0967e-05</hent>
<hent key="038001000000000000000380000002C4" call="MPI_Irecv" bytes="896" orank="708" region="0" commid="0" count="374" tid="0" op="" dtype="" >3.6883e-04 0.0000e+00 1.3113e-05</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.1989e-05 6.1989e-05 6.1989e-05</hent>
<hent key="03800100000000000000380000000044" call="MPI_Irecv" bytes="14336" orank="68" region="0" commid="0" count="16" tid="0" op="" dtype="" >2.3127e-05 0.0000e+00 1.3113e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="02400100000000000000038000000003" call="MPI_Isend" bytes="896" orank="3" region="0" commid="0" count="2735" tid="0" op="" dtype="" >1.9574e-03 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000038000000005" call="MPI_Isend" bytes="896" orank="5" region="0" commid="0" count="2885" tid="0" op="" dtype="" >3.0634e-03 0.0000e+00 3.0994e-05</hent>
<hent key="0240010000000000000003800000000C" call="MPI_Isend" bytes="896" orank="12" region="0" commid="0" count="320" tid="0" op="" dtype="" >4.7541e-04 9.5367e-07 8.1062e-06</hent>
<hent key="0240010000000000000003800000003C" call="MPI_Isend" bytes="896" orank="60" region="0" commid="0" count="315" tid="0" op="" dtype="" >1.6561e-03 3.8147e-06 1.3113e-05</hent>
<hent key="02400100000000000000038000000044" call="MPI_Isend" bytes="896" orank="68" region="0" commid="0" count="322" tid="0" op="" dtype="" >1.7483e-03 3.0994e-06 4.2200e-05</hent>
<hent key="024001000000000000000380000002C4" call="MPI_Isend" bytes="896" orank="708" region="0" commid="0" count="330" tid="0" op="" dtype="" >1.6720e-03 2.8610e-06 2.9087e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.7333e+00 7.8678e-06 1.3916e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.3501e-04 5.3501e-04 5.3501e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >3.4690e-04 3.4690e-04 3.4690e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >1.3139e-03 1.3139e-03 1.3139e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >7.5759e-02 3.1614e-04 7.0116e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.1808e-04 5.1808e-04 5.1808e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 1.1921e-06 1.9073e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6150e+00 4.8304e-04 2.5683e-01</hent>
<hent key="03800100000000000000040000000003" call="MPI_Irecv" bytes="1024" orank="3" region="0" commid="0" count="968" tid="0" op="" dtype="" >1.1873e-04 0.0000e+00 3.8147e-06</hent>
<hent key="03800100000000000000040000000005" call="MPI_Irecv" bytes="1024" orank="5" region="0" commid="0" count="832" tid="0" op="" dtype="" >2.2030e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000004000000000C" call="MPI_Irecv" bytes="1024" orank="12" region="0" commid="0" count="3382" tid="0" op="" dtype="" >1.7443e-03 0.0000e+00 5.1022e-05</hent>
<hent key="0380010000000000000004000000003C" call="MPI_Irecv" bytes="1024" orank="60" region="0" commid="0" count="3350" tid="0" op="" dtype="" >3.5715e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.3919e-03 1.9073e-06 1.0960e-03</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="03800100000000000000070000000003" call="MPI_Irecv" bytes="1792" orank="3" region="0" commid="0" count="32" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000005" call="MPI_Irecv" bytes="1792" orank="5" region="0" commid="0" count="33" tid="0" op="" dtype="" >1.0490e-05 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000007000000000C" call="MPI_Irecv" bytes="1792" orank="12" region="0" commid="0" count="43" tid="0" op="" dtype="" >3.2425e-05 0.0000e+00 3.0994e-06</hent>
<hent key="0380010000000000000007000000003C" call="MPI_Irecv" bytes="1792" orank="60" region="0" commid="0" count="46" tid="0" op="" dtype="" >1.3113e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000044" call="MPI_Irecv" bytes="1792" orank="68" region="0" commid="0" count="138" tid="0" op="" dtype="" >1.5378e-04 0.0000e+00 1.2159e-05</hent>
<hent key="038001000000000000000700000002C4" call="MPI_Irecv" bytes="1792" orank="708" region="0" commid="0" count="107" tid="0" op="" dtype="" >1.5807e-04 0.0000e+00 1.0967e-05</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >7.3195e-05 0.0000e+00 5.3167e-05</hent>
<hent key="02400100000000000000040000000003" call="MPI_Isend" bytes="1024" orank="3" region="0" commid="0" count="992" tid="0" op="" dtype="" >5.8222e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000040000000005" call="MPI_Isend" bytes="1024" orank="5" region="0" commid="0" count="844" tid="0" op="" dtype="" >6.6018e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000004000000000C" call="MPI_Isend" bytes="1024" orank="12" region="0" commid="0" count="3388" tid="0" op="" dtype="" >2.5139e-03 0.0000e+00 3.7909e-05</hent>
<hent key="0240010000000000000004000000003C" call="MPI_Isend" bytes="1024" orank="60" region="0" commid="0" count="3358" tid="0" op="" dtype="" >1.5378e-02 9.5367e-07 1.6284e-04</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >3.6955e-05 5.0068e-06 3.1948e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4415e+00 0.0000e+00 3.2495e+00</hent>
<hent key="038001000000000000000A0000000003" call="MPI_Irecv" bytes="2560" orank="3" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000005" call="MPI_Irecv" bytes="2560" orank="5" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000000C" call="MPI_Irecv" bytes="2560" orank="12" region="0" commid="0" count="9" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 3.0994e-06</hent>
<hent key="038001000000000000000A000000003C" call="MPI_Irecv" bytes="2560" orank="60" region="0" commid="0" count="7" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000A0000000044" call="MPI_Irecv" bytes="2560" orank="68" region="0" commid="0" count="45" tid="0" op="" dtype="" >3.8147e-05 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000A00000002C4" call="MPI_Irecv" bytes="2560" orank="708" region="0" commid="0" count="40" tid="0" op="" dtype="" >4.9591e-05 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000070000000003" call="MPI_Isend" bytes="1792" orank="3" region="0" commid="0" count="38" tid="0" op="" dtype="" >8.4877e-05 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000070000000005" call="MPI_Isend" bytes="1792" orank="5" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.4830e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000007000000000C" call="MPI_Isend" bytes="1792" orank="12" region="0" commid="0" count="46" tid="0" op="" dtype="" >1.3256e-04 9.5367e-07 1.4067e-05</hent>
<hent key="0240010000000000000007000000003C" call="MPI_Isend" bytes="1792" orank="60" region="0" commid="0" count="44" tid="0" op="" dtype="" >2.5916e-04 5.0068e-06 1.5020e-05</hent>
<hent key="02400100000000000000070000000044" call="MPI_Isend" bytes="1792" orank="68" region="0" commid="0" count="152" tid="0" op="" dtype="" >9.5654e-04 3.8147e-06 2.0981e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6156e-02 3.6156e-02 3.6156e-02</hent>
<hent key="024001000000000000000700000002C4" call="MPI_Isend" bytes="1792" orank="708" region="0" commid="0" count="123" tid="0" op="" dtype="" >7.0524e-04 2.8610e-06 2.0027e-05</hent>
<hent key="024001000000000000000A0000000003" call="MPI_Isend" bytes="2560" orank="3" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.3113e-05 1.9073e-06 4.0531e-06</hent>
<hent key="024001000000000000000A0000000005" call="MPI_Isend" bytes="2560" orank="5" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.6212e-05 4.0531e-06 4.0531e-06</hent>
<hent key="024001000000000000000A000000000C" call="MPI_Isend" bytes="2560" orank="12" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.4305e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A000000003C" call="MPI_Isend" bytes="2560" orank="60" region="0" commid="0" count="6" tid="0" op="" dtype="" >4.3869e-05 5.0068e-06 1.7881e-05</hent>
<hent key="024001000000000000000A0000000044" call="MPI_Isend" bytes="2560" orank="68" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.6155e-04 3.8147e-06 1.9073e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.2898e-04 1.5974e-05 6.2943e-05</hent>
<hent key="024001000000000000000A00000002C4" call="MPI_Isend" bytes="2560" orank="708" region="0" commid="0" count="38" tid="0" op="" dtype="" >2.3365e-04 3.8147e-06 2.0027e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >3.0494e-04 1.3089e-04 1.7405e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.7881e-05 4.0531e-06 1.3828e-05</hent>
<hent key="03800100000000000000000400000003" call="MPI_Irecv" bytes="4" orank="3" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1501e-03 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000000400000005" call="MPI_Irecv" bytes="4" orank="5" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.8341e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0380010000000000000000040000000C" call="MPI_Irecv" bytes="4" orank="12" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.7020e-03 0.0000e+00 3.7193e-05</hent>
<hent key="0380010000000000000000040000003C" call="MPI_Irecv" bytes="4" orank="60" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.5957e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000000400000044" call="MPI_Irecv" bytes="4" orank="68" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.0281e-03 0.0000e+00 6.1989e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >6.3372e-04 8.8930e-05 1.9097e-04</hent>
<hent key="038001000000000000001C0000000003" call="MPI_Irecv" bytes="7168" orank="3" region="0" commid="0" count="9164" tid="0" op="" dtype="" >4.4944e-03 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000001C0000000005" call="MPI_Irecv" bytes="7168" orank="5" region="0" commid="0" count="9560" tid="0" op="" dtype="" >2.2585e-03 0.0000e+00 1.3113e-05</hent>
<hent key="038001000000000000000004000002C4" call="MPI_Irecv" bytes="4" orank="708" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.0720e-03 0.0000e+00 8.6069e-05</hent>
<hent key="02400100000000000000000400000003" call="MPI_Isend" bytes="4" orank="3" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4489e-03 0.0000e+00 3.9101e-05</hent>
<hent key="02400100000000000000000400000005" call="MPI_Isend" bytes="4" orank="5" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.4748e-03 0.0000e+00 1.2875e-05</hent>
<hent key="0240010000000000000000040000000C" call="MPI_Isend" bytes="4" orank="12" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.5388e-03 0.0000e+00 1.1921e-05</hent>
<hent key="0240010000000000000000040000003C" call="MPI_Isend" bytes="4" orank="60" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.7052e-02 2.8610e-06 1.4210e-04</hent>
<hent key="02400100000000000000000400000044" call="MPI_Isend" bytes="4" orank="68" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.6310e-02 3.0994e-06 7.1049e-05</hent>
<hent key="024001000000000000001C0000000003" call="MPI_Isend" bytes="7168" orank="3" region="0" commid="0" count="9030" tid="0" op="" dtype="" >1.9658e-02 9.5367e-07 1.6212e-05</hent>
<hent key="024001000000000000001C0000000005" call="MPI_Isend" bytes="7168" orank="5" region="0" commid="0" count="9500" tid="0" op="" dtype="" >2.7315e-02 9.5367e-07 3.4094e-05</hent>
<hent key="024001000000000000000004000002C4" call="MPI_Isend" bytes="4" orank="708" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.4622e-02 2.8610e-06 1.1015e-04</hent>
<hent key="03800100000000000000050000000003" call="MPI_Irecv" bytes="1280" orank="3" region="0" commid="0" count="196" tid="0" op="" dtype="" >9.4414e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000005" call="MPI_Irecv" bytes="1280" orank="5" region="0" commid="0" count="209" tid="0" op="" dtype="" >6.5088e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000C" call="MPI_Irecv" bytes="1280" orank="12" region="0" commid="0" count="211" tid="0" op="" dtype="" >1.4210e-04 0.0000e+00 3.3140e-05</hent>
<hent key="0380010000000000000005000000003C" call="MPI_Irecv" bytes="1280" orank="60" region="0" commid="0" count="233" tid="0" op="" dtype="" >7.6294e-05 0.0000e+00 1.0967e-05</hent>
<hent key="03800100000000000000050000000044" call="MPI_Irecv" bytes="1280" orank="68" region="0" commid="0" count="306" tid="0" op="" dtype="" >3.3355e-04 0.0000e+00 1.8835e-05</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0937e+01 7.8678e-06 1.2807e-01</hent>
<hent key="0380010000000000000028000000000C" call="MPI_Irecv" bytes="10240" orank="12" region="0" commid="0" count="60" tid="0" op="" dtype="" >2.1458e-05 0.0000e+00 6.1989e-06</hent>
<hent key="0380010000000000000028000000003C" call="MPI_Irecv" bytes="10240" orank="60" region="0" commid="0" count="176" tid="0" op="" dtype="" >1.8597e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002C4" call="MPI_Irecv" bytes="1280" orank="708" region="0" commid="0" count="297" tid="0" op="" dtype="" >2.8467e-04 0.0000e+00 1.2875e-05</hent>
<hent key="03800100000000000000080000000003" call="MPI_Irecv" bytes="2048" orank="3" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000005" call="MPI_Irecv" bytes="2048" orank="5" region="0" commid="0" count="16" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000C" call="MPI_Irecv" bytes="2048" orank="12" region="0" commid="0" count="12" tid="0" op="" dtype="" >2.0504e-05 0.0000e+00 1.0967e-05</hent>
<hent key="0380010000000000000008000000003C" call="MPI_Irecv" bytes="2048" orank="60" region="0" commid="0" count="15" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000044" call="MPI_Irecv" bytes="2048" orank="68" region="0" commid="0" count="3453" tid="0" op="" dtype="" >1.2155e-03 0.0000e+00 6.2943e-05</hent>
<hent key="038001000000000000000800000002C4" call="MPI_Irecv" bytes="2048" orank="708" region="0" commid="0" count="3455" tid="0" op="" dtype="" >1.8134e-03 0.0000e+00 6.6996e-05</hent>
<hent key="02400100000000000000050000000003" call="MPI_Isend" bytes="1280" orank="3" region="0" commid="0" count="185" tid="0" op="" dtype="" >3.4285e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000005" call="MPI_Isend" bytes="1280" orank="5" region="0" commid="0" count="212" tid="0" op="" dtype="" >7.8154e-04 2.8610e-06 2.2173e-05</hent>
<hent key="0240010000000000000005000000000C" call="MPI_Isend" bytes="1280" orank="12" region="0" commid="0" count="188" tid="0" op="" dtype="" >3.1805e-04 0.0000e+00 1.2159e-05</hent>
<hent key="0240010000000000000005000000003C" call="MPI_Isend" bytes="1280" orank="60" region="0" commid="0" count="245" tid="0" op="" dtype="" >1.3509e-03 9.5367e-07 3.1948e-05</hent>
<hent key="02400100000000000000050000000044" call="MPI_Isend" bytes="1280" orank="68" region="0" commid="0" count="279" tid="0" op="" dtype="" >1.6365e-03 3.8147e-06 2.7895e-05</hent>
<hent key="0240010000000000000028000000000C" call="MPI_Isend" bytes="10240" orank="12" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.7418e-05 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000028000000003C" call="MPI_Isend" bytes="10240" orank="60" region="0" commid="0" count="142" tid="0" op="" dtype="" >1.7116e-03 5.0068e-06 7.9155e-05</hent>
<hent key="024001000000000000000500000002C4" call="MPI_Isend" bytes="1280" orank="708" region="0" commid="0" count="280" tid="0" op="" dtype="" >1.4608e-03 2.8610e-06 1.9073e-05</hent>
</hash>
<internal rank="4" log_i="1723713849.506833" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="5" mpi_size="768" stamp_init="1723713791.090567" stamp_final="1723713849.513072" username="apac4" allocationname="unknown" flags="0" pid="1717068" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.84225e+01" utime="5.04811e+01" stime="6.59742e+00" mtime="3.21245e+01" gflop="0.00000e+00" gbyte="3.76949e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21245e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c8142056c814c81476" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83646e+01" utime="5.04502e+01" stime="6.58899e+00" mtime="3.21245e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21245e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4847e+08" > 4.4778e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4926e+08" > 3.0082e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8850e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4470e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3392e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6145e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7586e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2091e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6202e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9662e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="188" >
<hent key="02400100000000000000080000000004" call="MPI_Isend" bytes="2048" orank="4" region="0" commid="0" count="16" tid="0" op="" dtype="" >3.7193e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000006" call="MPI_Isend" bytes="2048" orank="6" region="0" commid="0" count="10" tid="0" op="" dtype="" >3.9816e-05 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000008000000000D" call="MPI_Isend" bytes="2048" orank="13" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.8120e-05 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000008000000003D" call="MPI_Isend" bytes="2048" orank="61" region="0" commid="0" count="18" tid="0" op="" dtype="" >1.0014e-04 5.0068e-06 9.0599e-06</hent>
<hent key="02400100000000000000080000000045" call="MPI_Isend" bytes="2048" orank="69" region="0" commid="0" count="3471" tid="0" op="" dtype="" >1.1258e-02 9.5367e-07 2.5988e-05</hent>
<hent key="024001000000000000000800000002C5" call="MPI_Isend" bytes="2048" orank="709" region="0" commid="0" count="3472" tid="0" op="" dtype="" >1.0665e-02 9.5367e-07 1.6928e-05</hent>
<hent key="03800100000000000000028000000004" call="MPI_Irecv" bytes="640" orank="4" region="0" commid="0" count="425" tid="0" op="" dtype="" >1.6284e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000006" call="MPI_Irecv" bytes="640" orank="6" region="0" commid="0" count="406" tid="0" op="" dtype="" >1.3566e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000002800000000D" call="MPI_Irecv" bytes="640" orank="13" region="0" commid="0" count="426" tid="0" op="" dtype="" >1.0133e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000003D" call="MPI_Irecv" bytes="640" orank="61" region="0" commid="0" count="419" tid="0" op="" dtype="" >1.2612e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000045" call="MPI_Irecv" bytes="640" orank="69" region="0" commid="0" count="273" tid="0" op="" dtype="" >8.0109e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="038001000000000000000280000002C5" call="MPI_Irecv" bytes="640" orank="709" region="0" commid="0" count="297" tid="0" op="" dtype="" >9.7752e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000004" call="MPI_Isend" bytes="640" orank="4" region="0" commid="0" count="409" tid="0" op="" dtype="" >5.6577e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000028000000006" call="MPI_Isend" bytes="640" orank="6" region="0" commid="0" count="409" tid="0" op="" dtype="" >1.2259e-03 1.9073e-06 6.9141e-06</hent>
<hent key="0240010000000000000002800000000D" call="MPI_Isend" bytes="640" orank="13" region="0" commid="0" count="447" tid="0" op="" dtype="" >5.5122e-04 9.5367e-07 8.8215e-06</hent>
<hent key="0240010000000000000002800000003D" call="MPI_Isend" bytes="640" orank="61" region="0" commid="0" count="387" tid="0" op="" dtype="" >1.8311e-03 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000028000000045" call="MPI_Isend" bytes="640" orank="69" region="0" commid="0" count="269" tid="0" op="" dtype="" >1.1389e-03 2.8610e-06 1.0014e-05</hent>
<hent key="024001000000000000000280000002C5" call="MPI_Isend" bytes="640" orank="709" region="0" commid="0" count="284" tid="0" op="" dtype="" >1.1716e-03 2.8610e-06 8.8215e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0014e-05 1.0014e-05 1.0014e-05</hent>
<hent key="03800100000000000000014000000004" call="MPI_Irecv" bytes="320" orank="4" region="0" commid="0" count="350" tid="0" op="" dtype="" >1.1826e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000006" call="MPI_Irecv" bytes="320" orank="6" region="0" commid="0" count="350" tid="0" op="" dtype="" >9.7752e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000001400000000D" call="MPI_Irecv" bytes="320" orank="13" region="0" commid="0" count="324" tid="0" op="" dtype="" >6.5327e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000003D" call="MPI_Irecv" bytes="320" orank="61" region="0" commid="0" count="327" tid="0" op="" dtype="" >9.8467e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000045" call="MPI_Irecv" bytes="320" orank="69" region="0" commid="0" count="187" tid="0" op="" dtype="" >5.7936e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002C5" call="MPI_Irecv" bytes="320" orank="709" region="0" commid="0" count="182" tid="0" op="" dtype="" >4.8637e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.8850e+00 0.0000e+00 1.3112e-01</hent>
<hent key="03800100000000000000400000000045" call="MPI_Irecv" bytes="16384" orank="69" region="0" commid="0" count="12699" tid="0" op="" dtype="" >2.8877e-03 0.0000e+00 5.0068e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.1219e-05 0.0000e+00 2.0027e-05</hent>
<hent key="038001000000000000004000000002C5" call="MPI_Irecv" bytes="16384" orank="709" region="0" commid="0" count="12699" tid="0" op="" dtype="" >2.0592e-03 0.0000e+00 7.1526e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 0.0000e+00 3.0994e-06</hent>
<hent key="02400100000000000000014000000004" call="MPI_Isend" bytes="320" orank="4" region="0" commid="0" count="341" tid="0" op="" dtype="" >3.6550e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000006" call="MPI_Isend" bytes="320" orank="6" region="0" commid="0" count="357" tid="0" op="" dtype="" >9.5797e-04 9.5367e-07 8.8215e-06</hent>
<hent key="0240010000000000000001400000000D" call="MPI_Isend" bytes="320" orank="13" region="0" commid="0" count="377" tid="0" op="" dtype="" >3.9816e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000001400000003D" call="MPI_Isend" bytes="320" orank="61" region="0" commid="0" count="366" tid="0" op="" dtype="" >1.6377e-03 2.8610e-06 1.3113e-05</hent>
<hent key="02400100000000000000014000000045" call="MPI_Isend" bytes="320" orank="69" region="0" commid="0" count="182" tid="0" op="" dtype="" >7.4720e-04 2.8610e-06 5.0068e-06</hent>
<hent key="03800100000000000000200000000004" call="MPI_Irecv" bytes="8192" orank="4" region="0" commid="0" count="3199" tid="0" op="" dtype="" >9.6607e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000200000000006" call="MPI_Irecv" bytes="8192" orank="6" region="0" commid="0" count="3515" tid="0" op="" dtype="" >4.9496e-04 0.0000e+00 3.8147e-06</hent>
<hent key="0380010000000000000020000000000D" call="MPI_Irecv" bytes="8192" orank="13" region="0" commid="0" count="12597" tid="0" op="" dtype="" >8.3144e-03 0.0000e+00 6.1989e-06</hent>
<hent key="0380010000000000000020000000003D" call="MPI_Irecv" bytes="8192" orank="61" region="0" commid="0" count="12480" tid="0" op="" dtype="" >1.2159e-03 0.0000e+00 5.0068e-06</hent>
<hent key="024001000000000000000140000002C5" call="MPI_Isend" bytes="320" orank="709" region="0" commid="0" count="186" tid="0" op="" dtype="" >7.5436e-04 2.8610e-06 5.0068e-06</hent>
<hent key="03800100000000000000000000000004" call="MPI_Irecv" bytes="0" orank="4" region="0" commid="0" count="234" tid="0" op="" dtype="" >6.8665e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000000000000006" call="MPI_Irecv" bytes="0" orank="6" region="0" commid="0" count="265" tid="0" op="" dtype="" >7.2956e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000000000000000D" call="MPI_Irecv" bytes="0" orank="13" region="0" commid="0" count="279" tid="0" op="" dtype="" >6.3896e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000003D" call="MPI_Irecv" bytes="0" orank="61" region="0" commid="0" count="259" tid="0" op="" dtype="" >6.7472e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000045" call="MPI_Irecv" bytes="0" orank="69" region="0" commid="0" count="158" tid="0" op="" dtype="" >4.7207e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000045" call="MPI_Isend" bytes="16384" orank="69" region="0" commid="0" count="12699" tid="0" op="" dtype="" >8.7225e-02 4.0531e-06 5.1975e-05</hent>
<hent key="038001000000000000000000000002C5" call="MPI_Irecv" bytes="0" orank="709" region="0" commid="0" count="156" tid="0" op="" dtype="" >3.0756e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002C5" call="MPI_Isend" bytes="16384" orank="709" region="0" commid="0" count="12699" tid="0" op="" dtype="" >8.5635e-02 2.8610e-06 3.0994e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.7319e-04 0.0000e+00 5.9509e-04</hent>
<hent key="02400100000000000000200000000004" call="MPI_Isend" bytes="8192" orank="4" region="0" commid="0" count="3139" tid="0" op="" dtype="" >4.2894e-03 0.0000e+00 6.9857e-05</hent>
<hent key="02400100000000000000200000000006" call="MPI_Isend" bytes="8192" orank="6" region="0" commid="0" count="3027" tid="0" op="" dtype="" >6.4027e-03 0.0000e+00 6.6042e-05</hent>
<hent key="0240010000000000000020000000000D" call="MPI_Isend" bytes="8192" orank="13" region="0" commid="0" count="12672" tid="0" op="" dtype="" >9.1090e-03 0.0000e+00 5.9843e-05</hent>
<hent key="0240010000000000000020000000003D" call="MPI_Isend" bytes="8192" orank="61" region="0" commid="0" count="12510" tid="0" op="" dtype="" >7.5546e-02 3.8147e-06 2.4080e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.0981e-05 2.0981e-05 2.0981e-05</hent>
<hent key="02400100000000000000000000000004" call="MPI_Isend" bytes="0" orank="4" region="0" commid="0" count="265" tid="0" op="" dtype="" >1.7238e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000000000006" call="MPI_Isend" bytes="0" orank="6" region="0" commid="0" count="245" tid="0" op="" dtype="" >4.9257e-04 9.5367e-07 6.0081e-05</hent>
<hent key="0240010000000000000000000000000D" call="MPI_Isend" bytes="0" orank="13" region="0" commid="0" count="245" tid="0" op="" dtype="" >1.9288e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0240010000000000000000000000003D" call="MPI_Isend" bytes="0" orank="61" region="0" commid="0" count="246" tid="0" op="" dtype="" >9.4748e-04 1.1921e-06 5.0068e-06</hent>
<hent key="02400100000000000000000000000045" call="MPI_Isend" bytes="0" orank="69" region="0" commid="0" count="163" tid="0" op="" dtype="" >5.9509e-04 9.5367e-07 6.1989e-06</hent>
<hent key="024001000000000000000000000002C5" call="MPI_Isend" bytes="0" orank="709" region="0" commid="0" count="149" tid="0" op="" dtype="" >5.4574e-04 9.5367e-07 5.0068e-06</hent>
<hent key="03800100000000000000060000000004" call="MPI_Irecv" bytes="1536" orank="4" region="0" commid="0" count="90" tid="0" op="" dtype="" >3.6001e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000006" call="MPI_Irecv" bytes="1536" orank="6" region="0" commid="0" count="109" tid="0" op="" dtype="" >3.6240e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000006000000000D" call="MPI_Irecv" bytes="1536" orank="13" region="0" commid="0" count="76" tid="0" op="" dtype="" >2.0266e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000003D" call="MPI_Irecv" bytes="1536" orank="61" region="0" commid="0" count="97" tid="0" op="" dtype="" >2.1935e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000045" call="MPI_Irecv" bytes="1536" orank="69" region="0" commid="0" count="209" tid="0" op="" dtype="" >8.3923e-05 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000000600000002C5" call="MPI_Irecv" bytes="1536" orank="709" region="0" commid="0" count="219" tid="0" op="" dtype="" >6.6042e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000004" call="MPI_Isend" bytes="1536" orank="4" region="0" commid="0" count="87" tid="0" op="" dtype="" >1.7595e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000006" call="MPI_Isend" bytes="1536" orank="6" region="0" commid="0" count="108" tid="0" op="" dtype="" >4.1151e-04 2.8610e-06 2.1935e-05</hent>
<hent key="0240010000000000000006000000000D" call="MPI_Isend" bytes="1536" orank="13" region="0" commid="0" count="87" tid="0" op="" dtype="" >1.9097e-04 9.5367e-07 1.1921e-05</hent>
<hent key="0240010000000000000006000000003D" call="MPI_Isend" bytes="1536" orank="61" region="0" commid="0" count="96" tid="0" op="" dtype="" >4.8614e-04 3.8147e-06 8.8215e-06</hent>
<hent key="02400100000000000000060000000045" call="MPI_Isend" bytes="1536" orank="69" region="0" commid="0" count="211" tid="0" op="" dtype="" >9.8157e-04 3.8147e-06 6.1989e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C5" call="MPI_Isend" bytes="1536" orank="709" region="0" commid="0" count="214" tid="0" op="" dtype="" >9.2387e-04 3.8147e-06 5.9605e-06</hent>
<hent key="038001000000000000000C0000000045" call="MPI_Irecv" bytes="3072" orank="69" region="0" commid="0" count="11" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C5" call="MPI_Irecv" bytes="3072" orank="709" region="0" commid="0" count="6" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C0000000045" call="MPI_Isend" bytes="3072" orank="69" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.5020e-05 5.0068e-06 5.0068e-06</hent>
<hent key="024001000000000000000C00000002C5" call="MPI_Isend" bytes="3072" orank="709" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.0054e-05 4.0531e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.2493e-04 1.2493e-04 1.2493e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.5511e-04 8.1062e-05 8.7976e-05</hent>
<hent key="03800100000000000000038000000004" call="MPI_Irecv" bytes="896" orank="4" region="0" commid="0" count="2885" tid="0" op="" dtype="" >4.1795e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000006" call="MPI_Irecv" bytes="896" orank="6" region="0" commid="0" count="2761" tid="0" op="" dtype="" >4.7946e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000003800000000D" call="MPI_Irecv" bytes="896" orank="13" region="0" commid="0" count="317" tid="0" op="" dtype="" >6.6042e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000003D" call="MPI_Irecv" bytes="896" orank="61" region="0" commid="0" count="328" tid="0" op="" dtype="" >9.4891e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000045" call="MPI_Irecv" bytes="896" orank="69" region="0" commid="0" count="350" tid="0" op="" dtype="" >1.2684e-04 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000380000002C5" call="MPI_Irecv" bytes="896" orank="709" region="0" commid="0" count="313" tid="0" op="" dtype="" >8.3685e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.1008e-05 4.1008e-05 4.1008e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="02400100000000000000038000000004" call="MPI_Isend" bytes="896" orank="4" region="0" commid="0" count="2896" tid="0" op="" dtype="" >2.0642e-03 0.0000e+00 8.8215e-06</hent>
<hent key="02400100000000000000038000000006" call="MPI_Isend" bytes="896" orank="6" region="0" commid="0" count="2876" tid="0" op="" dtype="" >2.7702e-03 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000003800000000D" call="MPI_Isend" bytes="896" orank="13" region="0" commid="0" count="297" tid="0" op="" dtype="" >4.2343e-04 9.5367e-07 6.9141e-06</hent>
<hent key="0240010000000000000003800000003D" call="MPI_Isend" bytes="896" orank="61" region="0" commid="0" count="312" tid="0" op="" dtype="" >1.5156e-03 3.8147e-06 8.8215e-06</hent>
<hent key="02400100000000000000038000000045" call="MPI_Isend" bytes="896" orank="69" region="0" commid="0" count="328" tid="0" op="" dtype="" >1.4479e-03 3.8147e-06 5.0068e-06</hent>
<hent key="024001000000000000000380000002C5" call="MPI_Isend" bytes="896" orank="709" region="0" commid="0" count="341" tid="0" op="" dtype="" >1.4234e-03 2.8610e-06 8.8215e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.2651e+00 5.0068e-05 1.3921e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.2905e-04 5.2905e-04 5.2905e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >3.2091e-04 3.2091e-04 3.2091e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >1.5211e-03 1.5211e-03 1.5211e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >6.7514e-02 3.3116e-04 6.2599e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.7817e-04 5.7817e-04 5.7817e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6145e+00 4.6086e-04 2.5642e-01</hent>
<hent key="03800100000000000000040000000004" call="MPI_Irecv" bytes="1024" orank="4" region="0" commid="0" count="844" tid="0" op="" dtype="" >1.0109e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000006" call="MPI_Irecv" bytes="1024" orank="6" region="0" commid="0" count="950" tid="0" op="" dtype="" >1.3161e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000004000000000D" call="MPI_Irecv" bytes="1024" orank="13" region="0" commid="0" count="3372" tid="0" op="" dtype="" >7.4577e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000004000000003D" call="MPI_Irecv" bytes="1024" orank="61" region="0" commid="0" count="3338" tid="0" op="" dtype="" >3.2878e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.1219e-05 0.0000e+00 9.0599e-06</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1921e-05 1.1921e-05 1.1921e-05</hent>
<hent key="03800100000000000000070000000004" call="MPI_Irecv" bytes="1792" orank="4" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.4782e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000006" call="MPI_Irecv" bytes="1792" orank="6" region="0" commid="0" count="34" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000007000000000D" call="MPI_Irecv" bytes="1792" orank="13" region="0" commid="0" count="32" tid="0" op="" dtype="" >8.5831e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000007000000003D" call="MPI_Irecv" bytes="1792" orank="61" region="0" commid="0" count="42" tid="0" op="" dtype="" >1.3113e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000045" call="MPI_Irecv" bytes="1792" orank="69" region="0" commid="0" count="127" tid="0" op="" dtype="" >3.8624e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002C5" call="MPI_Irecv" bytes="1792" orank="709" region="0" commid="0" count="137" tid="0" op="" dtype="" >3.9101e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >1.2565e-04 9.5367e-07 5.5075e-05</hent>
<hent key="02400100000000000000040000000004" call="MPI_Isend" bytes="1024" orank="4" region="0" commid="0" count="832" tid="0" op="" dtype="" >5.1069e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000040000000006" call="MPI_Isend" bytes="1024" orank="6" region="0" commid="0" count="826" tid="0" op="" dtype="" >5.8174e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0240010000000000000004000000000D" call="MPI_Isend" bytes="1024" orank="13" region="0" commid="0" count="3392" tid="0" op="" dtype="" >1.9398e-03 0.0000e+00 1.2159e-05</hent>
<hent key="0240010000000000000004000000003D" call="MPI_Isend" bytes="1024" orank="61" region="0" commid="0" count="3338" tid="0" op="" dtype="" >8.2827e-03 9.5367e-07 1.6928e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4460e+00 0.0000e+00 3.2507e+00</hent>
<hent key="038001000000000000000A0000000004" call="MPI_Irecv" bytes="2560" orank="4" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000006" call="MPI_Irecv" bytes="2560" orank="6" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000000D" call="MPI_Irecv" bytes="2560" orank="13" region="0" commid="0" count="7" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000003D" call="MPI_Irecv" bytes="2560" orank="61" region="0" commid="0" count="9" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000045" call="MPI_Irecv" bytes="2560" orank="69" region="0" commid="0" count="46" tid="0" op="" dtype="" >1.6212e-05 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A00000002C5" call="MPI_Irecv" bytes="2560" orank="709" region="0" commid="0" count="34" tid="0" op="" dtype="" >1.4067e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000004" call="MPI_Isend" bytes="1792" orank="4" region="0" commid="0" count="33" tid="0" op="" dtype="" >7.3671e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000006" call="MPI_Isend" bytes="1792" orank="6" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.5306e-04 2.8610e-06 7.1526e-06</hent>
<hent key="0240010000000000000007000000000D" call="MPI_Isend" bytes="1792" orank="13" region="0" commid="0" count="29" tid="0" op="" dtype="" >7.4148e-05 9.5367e-07 1.1206e-05</hent>
<hent key="0240010000000000000007000000003D" call="MPI_Isend" bytes="1792" orank="61" region="0" commid="0" count="55" tid="0" op="" dtype="" >2.8992e-04 4.0531e-06 7.1526e-06</hent>
<hent key="02400100000000000000070000000045" call="MPI_Isend" bytes="1792" orank="69" region="0" commid="0" count="129" tid="0" op="" dtype="" >6.0916e-04 3.8147e-06 1.0014e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6202e-02 3.6202e-02 3.6202e-02</hent>
<hent key="024001000000000000000700000002C5" call="MPI_Isend" bytes="1792" orank="709" region="0" commid="0" count="125" tid="0" op="" dtype="" >5.7888e-04 3.8147e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000004" call="MPI_Isend" bytes="2560" orank="4" region="0" commid="0" count="9" tid="0" op="" dtype="" >2.0981e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000006" call="MPI_Isend" bytes="2560" orank="6" region="0" commid="0" count="8" tid="0" op="" dtype="" >3.3855e-05 3.0994e-06 5.0068e-06</hent>
<hent key="024001000000000000000A000000000D" call="MPI_Isend" bytes="2560" orank="13" region="0" commid="0" count="8" tid="0" op="" dtype="" >2.3365e-05 9.5367e-07 1.0014e-05</hent>
<hent key="024001000000000000000A000000003D" call="MPI_Isend" bytes="2560" orank="61" region="0" commid="0" count="13" tid="0" op="" dtype="" >7.3433e-05 5.0068e-06 6.1989e-06</hent>
<hent key="024001000000000000000A0000000045" call="MPI_Isend" bytes="2560" orank="69" region="0" commid="0" count="31" tid="0" op="" dtype="" >1.5926e-04 3.8147e-06 7.8678e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.4472e-04 1.8835e-05 6.8903e-05</hent>
<hent key="024001000000000000000A00000002C5" call="MPI_Isend" bytes="2560" orank="709" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.3709e-04 3.8147e-06 5.0068e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >3.8791e-04 1.6093e-04 2.2697e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.7193e-05 3.7193e-05 3.7193e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 1.9073e-06 3.0994e-06</hent>
<hent key="03800100000000000000000400000004" call="MPI_Irecv" bytes="4" orank="4" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.3423e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000400000006" call="MPI_Irecv" bytes="4" orank="6" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.2939e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000000D" call="MPI_Irecv" bytes="4" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.3160e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000040000003D" call="MPI_Irecv" bytes="4" orank="61" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.3607e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000045" call="MPI_Irecv" bytes="4" orank="69" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.5269e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >8.4472e-04 1.0800e-04 2.9588e-04</hent>
<hent key="038001000000000000001C0000000004" call="MPI_Irecv" bytes="7168" orank="4" region="0" commid="0" count="9500" tid="0" op="" dtype="" >2.7897e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000001C0000000006" call="MPI_Irecv" bytes="7168" orank="6" region="0" commid="0" count="9184" tid="0" op="" dtype="" >1.3099e-03 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000004000002C5" call="MPI_Irecv" bytes="4" orank="709" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.9591e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000400000004" call="MPI_Isend" bytes="4" orank="4" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3528e-03 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000400000006" call="MPI_Isend" bytes="4" orank="6" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.4219e-03 0.0000e+00 6.3896e-05</hent>
<hent key="0240010000000000000000040000000D" call="MPI_Isend" bytes="4" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2777e-03 0.0000e+00 2.8610e-06</hent>
<hent key="0240010000000000000000040000003D" call="MPI_Isend" bytes="4" orank="61" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.9192e-02 2.8610e-06 1.3399e-04</hent>
<hent key="02400100000000000000000400000045" call="MPI_Isend" bytes="4" orank="69" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7663e-02 3.8147e-06 1.2875e-05</hent>
<hent key="024001000000000000001C0000000004" call="MPI_Isend" bytes="7168" orank="4" region="0" commid="0" count="9560" tid="0" op="" dtype="" >2.0633e-02 9.5367e-07 6.0081e-05</hent>
<hent key="024001000000000000001C0000000006" call="MPI_Isend" bytes="7168" orank="6" region="0" commid="0" count="9672" tid="0" op="" dtype="" >3.0679e-02 9.5367e-07 1.5020e-05</hent>
<hent key="024001000000000000000004000002C5" call="MPI_Isend" bytes="4" orank="709" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7062e-02 2.8610e-06 6.8903e-05</hent>
<hent key="03800100000000000000050000000004" call="MPI_Irecv" bytes="1280" orank="4" region="0" commid="0" count="212" tid="0" op="" dtype="" >8.4400e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000006" call="MPI_Irecv" bytes="1280" orank="6" region="0" commid="0" count="207" tid="0" op="" dtype="" >6.2704e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000005000000000D" call="MPI_Irecv" bytes="1280" orank="13" region="0" commid="0" count="250" tid="0" op="" dtype="" >6.1989e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000003D" call="MPI_Irecv" bytes="1280" orank="61" region="0" commid="0" count="257" tid="0" op="" dtype="" >5.7697e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000045" call="MPI_Irecv" bytes="1280" orank="69" region="0" commid="0" count="281" tid="0" op="" dtype="" >9.9182e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.1327e+01 2.2173e-05 1.2808e-01</hent>
<hent key="0380010000000000000028000000000D" call="MPI_Irecv" bytes="10240" orank="13" region="0" commid="0" count="102" tid="0" op="" dtype="" >3.2902e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000028000000003D" call="MPI_Irecv" bytes="10240" orank="61" region="0" commid="0" count="219" tid="0" op="" dtype="" >2.9564e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000500000002C5" call="MPI_Irecv" bytes="1280" orank="709" region="0" commid="0" count="292" tid="0" op="" dtype="" >8.7976e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000004" call="MPI_Irecv" bytes="2048" orank="4" region="0" commid="0" count="14" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000006" call="MPI_Irecv" bytes="2048" orank="6" region="0" commid="0" count="11" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000D" call="MPI_Irecv" bytes="2048" orank="13" region="0" commid="0" count="14" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000003D" call="MPI_Irecv" bytes="2048" orank="61" region="0" commid="0" count="21" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000045" call="MPI_Irecv" bytes="2048" orank="69" region="0" commid="0" count="3455" tid="0" op="" dtype="" >6.2060e-04 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000000800000002C5" call="MPI_Irecv" bytes="2048" orank="709" region="0" commid="0" count="3461" tid="0" op="" dtype="" >5.6410e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000050000000004" call="MPI_Isend" bytes="1280" orank="4" region="0" commid="0" count="209" tid="0" op="" dtype="" >3.8600e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000006" call="MPI_Isend" bytes="1280" orank="6" region="0" commid="0" count="219" tid="0" op="" dtype="" >7.6675e-04 2.8610e-06 7.1526e-06</hent>
<hent key="0240010000000000000005000000000D" call="MPI_Isend" bytes="1280" orank="13" region="0" commid="0" count="206" tid="0" op="" dtype="" >3.2091e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000005000000003D" call="MPI_Isend" bytes="1280" orank="61" region="0" commid="0" count="266" tid="0" op="" dtype="" >1.1814e-03 9.5367e-07 1.7881e-05</hent>
<hent key="02400100000000000000050000000045" call="MPI_Isend" bytes="1280" orank="69" region="0" commid="0" count="310" tid="0" op="" dtype="" >1.4026e-03 2.8610e-06 7.8678e-06</hent>
<hent key="0240010000000000000028000000000D" call="MPI_Isend" bytes="10240" orank="13" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.4544e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000028000000003D" call="MPI_Isend" bytes="10240" orank="61" region="0" commid="0" count="189" tid="0" op="" dtype="" >1.1783e-03 4.7684e-06 1.2159e-05</hent>
<hent key="024001000000000000000500000002C5" call="MPI_Isend" bytes="1280" orank="709" region="0" commid="0" count="289" tid="0" op="" dtype="" >1.2224e-03 2.8610e-06 8.1062e-06</hent>
</hash>
<internal rank="5" log_i="1723713849.513072" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="6" mpi_size="768" stamp_init="1723713791.093812" stamp_final="1723713849.516019" username="apac4" allocationname="unknown" flags="0" pid="1717069" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.84222e+01" utime="4.75621e+01" stime="7.85439e+00" mtime="3.20690e+01" gflop="0.00000e+00" gbyte="3.77445e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20690e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83533e+01" utime="4.75286e+01" stime="7.84799e+00" mtime="3.20690e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20690e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4946e+08" > 6.5400e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4833e+08" > 4.8843e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7502e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4077e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.3869e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5048e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6143e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0149e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9802e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6150e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8555e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="194" >
<hent key="02400100000000000000080000000005" call="MPI_Isend" bytes="2048" orank="5" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.6226e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000007" call="MPI_Isend" bytes="2048" orank="7" region="0" commid="0" count="12" tid="0" op="" dtype="" >4.5061e-05 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000008000000000E" call="MPI_Isend" bytes="2048" orank="14" region="0" commid="0" count="9" tid="0" op="" dtype="" >5.6982e-05 9.5367e-07 1.6212e-05</hent>
<hent key="0240010000000000000008000000003E" call="MPI_Isend" bytes="2048" orank="62" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.4162e-04 4.7684e-06 1.5020e-05</hent>
<hent key="02400100000000000000080000000046" call="MPI_Isend" bytes="2048" orank="70" region="0" commid="0" count="3469" tid="0" op="" dtype="" >1.7526e-02 9.5367e-07 7.4148e-05</hent>
<hent key="024001000000000000000800000002C6" call="MPI_Isend" bytes="2048" orank="710" region="0" commid="0" count="3447" tid="0" op="" dtype="" >1.4686e-02 9.5367e-07 5.0068e-05</hent>
<hent key="038001000000000000000E00000002C6" call="MPI_Irecv" bytes="3584" orank="710" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000E0000000046" call="MPI_Isend" bytes="3584" orank="70" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000005" call="MPI_Irecv" bytes="640" orank="5" region="0" commid="0" count="409" tid="0" op="" dtype="" >1.7142e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000028000000007" call="MPI_Irecv" bytes="640" orank="7" region="0" commid="0" count="415" tid="0" op="" dtype="" >1.0347e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000002800000000E" call="MPI_Irecv" bytes="640" orank="14" region="0" commid="0" count="406" tid="0" op="" dtype="" >1.3256e-04 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000E00000002C6" call="MPI_Isend" bytes="3584" orank="710" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.0599e-06 9.0599e-06 9.0599e-06</hent>
<hent key="0380010000000000000002800000003E" call="MPI_Irecv" bytes="640" orank="62" region="0" commid="0" count="385" tid="0" op="" dtype="" >1.1706e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000028000000046" call="MPI_Irecv" bytes="640" orank="70" region="0" commid="0" count="285" tid="0" op="" dtype="" >1.4663e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000000280000002C6" call="MPI_Irecv" bytes="640" orank="710" region="0" commid="0" count="279" tid="0" op="" dtype="" >1.2684e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000028000000005" call="MPI_Isend" bytes="640" orank="5" region="0" commid="0" count="406" tid="0" op="" dtype="" >5.7578e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000028000000007" call="MPI_Isend" bytes="640" orank="7" region="0" commid="0" count="400" tid="0" op="" dtype="" >1.1299e-03 1.9073e-06 6.9141e-06</hent>
<hent key="0240010000000000000002800000000E" call="MPI_Isend" bytes="640" orank="14" region="0" commid="0" count="412" tid="0" op="" dtype="" >6.0630e-04 9.5367e-07 1.4782e-05</hent>
<hent key="0240010000000000000002800000003E" call="MPI_Isend" bytes="640" orank="62" region="0" commid="0" count="394" tid="0" op="" dtype="" >2.0494e-03 3.8147e-06 2.5988e-05</hent>
<hent key="02400100000000000000028000000046" call="MPI_Isend" bytes="640" orank="70" region="0" commid="0" count="292" tid="0" op="" dtype="" >1.5426e-03 2.8610e-06 1.5974e-05</hent>
<hent key="024001000000000000000280000002C6" call="MPI_Isend" bytes="640" orank="710" region="0" commid="0" count="267" tid="0" op="" dtype="" >1.2138e-03 2.8610e-06 1.3113e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.1062e-06 8.1062e-06 8.1062e-06</hent>
<hent key="03800100000000000000014000000005" call="MPI_Irecv" bytes="320" orank="5" region="0" commid="0" count="357" tid="0" op="" dtype="" >1.4234e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000007" call="MPI_Irecv" bytes="320" orank="7" region="0" commid="0" count="329" tid="0" op="" dtype="" >8.7500e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000001400000000E" call="MPI_Irecv" bytes="320" orank="14" region="0" commid="0" count="375" tid="0" op="" dtype="" >1.4305e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0380010000000000000001400000003E" call="MPI_Irecv" bytes="320" orank="62" region="0" commid="0" count="373" tid="0" op="" dtype="" >1.4997e-04 0.0000e+00 1.0967e-05</hent>
<hent key="03800100000000000000014000000046" call="MPI_Irecv" bytes="320" orank="70" region="0" commid="0" count="184" tid="0" op="" dtype="" >9.1553e-05 0.0000e+00 7.1526e-06</hent>
<hent key="038001000000000000000140000002C6" call="MPI_Irecv" bytes="320" orank="710" region="0" commid="0" count="178" tid="0" op="" dtype="" >8.2254e-05 0.0000e+00 7.8678e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.7502e+00 0.0000e+00 1.3106e-01</hent>
<hent key="03800100000000000000400000000046" call="MPI_Irecv" bytes="16384" orank="70" region="0" commid="0" count="12699" tid="0" op="" dtype="" >4.2484e-03 0.0000e+00 4.5061e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.5974e-05 0.0000e+00 1.5974e-05</hent>
<hent key="038001000000000000004000000002C6" call="MPI_Irecv" bytes="16384" orank="710" region="0" commid="0" count="12639" tid="0" op="" dtype="" >5.4066e-03 0.0000e+00 7.2002e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000005" call="MPI_Isend" bytes="320" orank="5" region="0" commid="0" count="350" tid="0" op="" dtype="" >3.9816e-04 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000014000000007" call="MPI_Isend" bytes="320" orank="7" region="0" commid="0" count="339" tid="0" op="" dtype="" >8.7547e-04 1.9073e-06 7.1526e-06</hent>
<hent key="0240010000000000000001400000000E" call="MPI_Isend" bytes="320" orank="14" region="0" commid="0" count="364" tid="0" op="" dtype="" >4.3106e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000001400000003E" call="MPI_Isend" bytes="320" orank="62" region="0" commid="0" count="351" tid="0" op="" dtype="" >1.7016e-03 2.8610e-06 1.5020e-05</hent>
<hent key="02400100000000000000014000000046" call="MPI_Isend" bytes="320" orank="70" region="0" commid="0" count="198" tid="0" op="" dtype="" >9.4128e-04 2.8610e-06 2.5034e-05</hent>
<hent key="03800100000000000000200000000005" call="MPI_Irecv" bytes="8192" orank="5" region="0" commid="0" count="3027" tid="0" op="" dtype="" >1.0688e-03 0.0000e+00 1.0967e-05</hent>
<hent key="03800100000000000000200000000007" call="MPI_Irecv" bytes="8192" orank="7" region="0" commid="0" count="3061" tid="0" op="" dtype="" >5.7054e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000020000000000E" call="MPI_Irecv" bytes="8192" orank="14" region="0" commid="0" count="12501" tid="0" op="" dtype="" >1.4477e-02 0.0000e+00 4.7922e-05</hent>
<hent key="0380010000000000000020000000003E" call="MPI_Irecv" bytes="8192" orank="62" region="0" commid="0" count="12670" tid="0" op="" dtype="" >3.0978e-03 0.0000e+00 3.2902e-05</hent>
<hent key="024001000000000000000140000002C6" call="MPI_Isend" bytes="320" orank="710" region="0" commid="0" count="178" tid="0" op="" dtype="" >8.4639e-04 2.8610e-06 2.0981e-05</hent>
<hent key="03800100000000000000000000000005" call="MPI_Irecv" bytes="0" orank="5" region="0" commid="0" count="245" tid="0" op="" dtype="" >8.9407e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000007" call="MPI_Irecv" bytes="0" orank="7" region="0" commid="0" count="259" tid="0" op="" dtype="" >5.2214e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000E" call="MPI_Irecv" bytes="0" orank="14" region="0" commid="0" count="267" tid="0" op="" dtype="" >9.1314e-05 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000000000000003E" call="MPI_Irecv" bytes="0" orank="62" region="0" commid="0" count="271" tid="0" op="" dtype="" >6.9141e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000046" call="MPI_Irecv" bytes="0" orank="70" region="0" commid="0" count="142" tid="0" op="" dtype="" >7.4148e-05 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000400000000046" call="MPI_Isend" bytes="16384" orank="70" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.6945e-01 3.8147e-06 1.0514e-04</hent>
<hent key="038001000000000000000000000002C6" call="MPI_Irecv" bytes="0" orank="710" region="0" commid="0" count="151" tid="0" op="" dtype="" >6.6280e-05 0.0000e+00 1.3113e-05</hent>
<hent key="024001000000000000004000000002C6" call="MPI_Isend" bytes="16384" orank="710" region="0" commid="0" count="12576" tid="0" op="" dtype="" >1.3294e-01 3.0994e-06 8.0109e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.8321e-04 0.0000e+00 6.1798e-04</hent>
<hent key="02400100000000000000200000000005" call="MPI_Isend" bytes="8192" orank="5" region="0" commid="0" count="3515" tid="0" op="" dtype="" >4.4873e-03 0.0000e+00 1.5974e-05</hent>
<hent key="02400100000000000000200000000007" call="MPI_Isend" bytes="8192" orank="7" region="0" commid="0" count="4202" tid="0" op="" dtype="" >7.8921e-03 0.0000e+00 5.1975e-05</hent>
<hent key="0240010000000000000020000000000E" call="MPI_Isend" bytes="8192" orank="14" region="0" commid="0" count="12691" tid="0" op="" dtype="" >1.3510e-02 0.0000e+00 8.7976e-05</hent>
<hent key="0240010000000000000020000000003E" call="MPI_Isend" bytes="8192" orank="62" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.1769e-01 3.8147e-06 1.1992e-04</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8133e-05 2.8133e-05 2.8133e-05</hent>
<hent key="02400100000000000000000000000005" call="MPI_Isend" bytes="0" orank="5" region="0" commid="0" count="265" tid="0" op="" dtype="" >2.0337e-04 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000000000000007" call="MPI_Isend" bytes="0" orank="7" region="0" commid="0" count="263" tid="0" op="" dtype="" >4.7851e-04 9.5367e-07 3.1948e-05</hent>
<hent key="0240010000000000000000000000000E" call="MPI_Isend" bytes="0" orank="14" region="0" commid="0" count="254" tid="0" op="" dtype="" >2.2459e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000000000000003E" call="MPI_Isend" bytes="0" orank="62" region="0" commid="0" count="275" tid="0" op="" dtype="" >1.1737e-03 9.5367e-07 1.5020e-05</hent>
<hent key="02400100000000000000000000000046" call="MPI_Isend" bytes="0" orank="70" region="0" commid="0" count="148" tid="0" op="" dtype="" >6.2394e-04 1.9073e-06 9.0599e-06</hent>
<hent key="024001000000000000000000000002C6" call="MPI_Isend" bytes="0" orank="710" region="0" commid="0" count="152" tid="0" op="" dtype="" >6.3634e-04 2.8610e-06 1.2159e-05</hent>
<hent key="03800100000000000000060000000005" call="MPI_Irecv" bytes="1536" orank="5" region="0" commid="0" count="108" tid="0" op="" dtype="" >4.2200e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000007" call="MPI_Irecv" bytes="1536" orank="7" region="0" commid="0" count="87" tid="0" op="" dtype="" >2.0742e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000E" call="MPI_Irecv" bytes="1536" orank="14" region="0" commid="0" count="97" tid="0" op="" dtype="" >4.2200e-05 0.0000e+00 7.1526e-06</hent>
<hent key="0380010000000000000006000000003E" call="MPI_Irecv" bytes="1536" orank="62" region="0" commid="0" count="89" tid="0" op="" dtype="" >2.9325e-05 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000060000000046" call="MPI_Irecv" bytes="1536" orank="70" region="0" commid="0" count="213" tid="0" op="" dtype="" >1.2851e-04 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000600000002C6" call="MPI_Irecv" bytes="1536" orank="710" region="0" commid="0" count="207" tid="0" op="" dtype="" >8.7023e-05 0.0000e+00 5.9605e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000005" call="MPI_Isend" bytes="1536" orank="5" region="0" commid="0" count="109" tid="0" op="" dtype="" >2.1744e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000007" call="MPI_Isend" bytes="1536" orank="7" region="0" commid="0" count="89" tid="0" op="" dtype="" >3.2091e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000006000000000E" call="MPI_Isend" bytes="1536" orank="14" region="0" commid="0" count="93" tid="0" op="" dtype="" >2.5511e-04 9.5367e-07 2.0027e-05</hent>
<hent key="0240010000000000000006000000003E" call="MPI_Isend" bytes="1536" orank="62" region="0" commid="0" count="89" tid="0" op="" dtype="" >4.8852e-04 3.8147e-06 1.2159e-05</hent>
<hent key="02400100000000000000060000000046" call="MPI_Isend" bytes="1536" orank="70" region="0" commid="0" count="222" tid="0" op="" dtype="" >1.2529e-03 3.8147e-06 1.6928e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C6" call="MPI_Isend" bytes="1536" orank="710" region="0" commid="0" count="214" tid="0" op="" dtype="" >1.0979e-03 2.8610e-06 1.7166e-05</hent>
<hent key="038001000000000000000C0000000007" call="MPI_Irecv" bytes="3072" orank="7" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C0000000046" call="MPI_Irecv" bytes="3072" orank="70" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C6" call="MPI_Irecv" bytes="3072" orank="710" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C0000000046" call="MPI_Isend" bytes="3072" orank="70" region="0" commid="0" count="6" tid="0" op="" dtype="" >4.1962e-05 5.0068e-06 1.0014e-05</hent>
<hent key="024001000000000000000C00000002C6" call="MPI_Isend" bytes="3072" orank="710" region="0" commid="0" count="9" tid="0" op="" dtype="" >4.5061e-05 5.0068e-06 5.0068e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.3590e-04 1.3590e-04 1.3590e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.0184e-04 9.3937e-05 1.0395e-04</hent>
<hent key="03800100000000000000038000000005" call="MPI_Irecv" bytes="896" orank="5" region="0" commid="0" count="2876" tid="0" op="" dtype="" >4.5180e-04 0.0000e+00 1.5020e-05</hent>
<hent key="03800100000000000000038000000007" call="MPI_Irecv" bytes="896" orank="7" region="0" commid="0" count="2916" tid="0" op="" dtype="" >5.0926e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0380010000000000000003800000000E" call="MPI_Irecv" bytes="896" orank="14" region="0" commid="0" count="281" tid="0" op="" dtype="" >1.1444e-04 0.0000e+00 1.1921e-05</hent>
<hent key="0380010000000000000003800000003E" call="MPI_Irecv" bytes="896" orank="62" region="0" commid="0" count="318" tid="0" op="" dtype="" >9.8228e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000046" call="MPI_Irecv" bytes="896" orank="70" region="0" commid="0" count="326" tid="0" op="" dtype="" >1.7905e-04 0.0000e+00 7.8678e-06</hent>
<hent key="038001000000000000000380000002C6" call="MPI_Irecv" bytes="896" orank="710" region="0" commid="0" count="339" tid="0" op="" dtype="" >1.5283e-04 0.0000e+00 1.5974e-05</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.4850e-05 6.4850e-05 6.4850e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="038001000000000000003800000002C6" call="MPI_Irecv" bytes="14336" orank="710" region="0" commid="0" count="60" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000038000000005" call="MPI_Isend" bytes="896" orank="5" region="0" commid="0" count="2761" tid="0" op="" dtype="" >2.1284e-03 0.0000e+00 1.5974e-05</hent>
<hent key="02400100000000000000038000000007" call="MPI_Isend" bytes="896" orank="7" region="0" commid="0" count="2598" tid="0" op="" dtype="" >2.6948e-03 0.0000e+00 2.3842e-05</hent>
<hent key="0240010000000000000003800000000E" call="MPI_Isend" bytes="896" orank="14" region="0" commid="0" count="336" tid="0" op="" dtype="" >6.3038e-04 9.5367e-07 1.3113e-05</hent>
<hent key="0240010000000000000003800000003E" call="MPI_Isend" bytes="896" orank="62" region="0" commid="0" count="328" tid="0" op="" dtype="" >1.8055e-03 3.8147e-06 2.1219e-05</hent>
<hent key="02400100000000000000038000000046" call="MPI_Isend" bytes="896" orank="70" region="0" commid="0" count="302" tid="0" op="" dtype="" >1.5864e-03 2.8610e-06 1.9073e-05</hent>
<hent key="024001000000000000000380000002C6" call="MPI_Isend" bytes="896" orank="710" region="0" commid="0" count="319" tid="0" op="" dtype="" >1.5259e-03 2.8610e-06 1.1921e-05</hent>
<hent key="024001000000000000003800000002C6" call="MPI_Isend" bytes="14336" orank="710" region="0" commid="0" count="123" tid="0" op="" dtype="" >1.1101e-03 3.8147e-06 3.9101e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.4932e+00 7.1526e-06 1.3916e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.3215e-04 5.3215e-04 5.3215e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >2.9802e-04 2.9802e-04 2.9802e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.0001e-03 2.0001e-03 2.0001e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >7.2460e-02 3.1209e-04 6.7460e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.7602e-04 5.7602e-04 5.7602e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6143e+00 4.8709e-04 2.5674e-01</hent>
<hent key="03800100000000000000040000000005" call="MPI_Irecv" bytes="1024" orank="5" region="0" commid="0" count="826" tid="0" op="" dtype="" >7.8678e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000007" call="MPI_Irecv" bytes="1024" orank="7" region="0" commid="0" count="806" tid="0" op="" dtype="" >1.6236e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0380010000000000000004000000000E" call="MPI_Irecv" bytes="1024" orank="14" region="0" commid="0" count="3344" tid="0" op="" dtype="" >1.1370e-03 0.0000e+00 1.5020e-05</hent>
<hent key="0380010000000000000004000000003E" call="MPI_Irecv" bytes="1024" orank="62" region="0" commid="0" count="3392" tid="0" op="" dtype="" >4.0674e-04 0.0000e+00 1.3113e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >4.3869e-05 9.5367e-07 1.7881e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="03800100000000000000070000000005" call="MPI_Irecv" bytes="1792" orank="5" region="0" commid="0" count="39" tid="0" op="" dtype="" >2.0266e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000007" call="MPI_Irecv" bytes="1792" orank="7" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.5259e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000007000000000E" call="MPI_Irecv" bytes="1792" orank="14" region="0" commid="0" count="50" tid="0" op="" dtype="" >1.5974e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000003E" call="MPI_Irecv" bytes="1792" orank="62" region="0" commid="0" count="58" tid="0" op="" dtype="" >2.3842e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000070000000046" call="MPI_Irecv" bytes="1792" orank="70" region="0" commid="0" count="132" tid="0" op="" dtype="" >8.2016e-05 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000700000002C6" call="MPI_Irecv" bytes="1792" orank="710" region="0" commid="0" count="133" tid="0" op="" dtype="" >5.0068e-05 0.0000e+00 5.9605e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >1.5950e-04 0.0000e+00 9.2983e-05</hent>
<hent key="02400100000000000000040000000005" call="MPI_Isend" bytes="1024" orank="5" region="0" commid="0" count="950" tid="0" op="" dtype="" >6.7258e-04 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000040000000007" call="MPI_Isend" bytes="1024" orank="7" region="0" commid="0" count="1134" tid="0" op="" dtype="" >9.2936e-04 0.0000e+00 1.3113e-05</hent>
<hent key="0240010000000000000004000000000E" call="MPI_Isend" bytes="1024" orank="14" region="0" commid="0" count="3396" tid="0" op="" dtype="" >2.5599e-03 0.0000e+00 2.0981e-05</hent>
<hent key="0240010000000000000004000000003E" call="MPI_Isend" bytes="1024" orank="62" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1765e-02 9.5367e-07 3.6955e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >2.1696e-05 2.8610e-06 1.8835e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4066e+00 0.0000e+00 3.2495e+00</hent>
<hent key="038001000000000000000A0000000005" call="MPI_Irecv" bytes="2560" orank="5" region="0" commid="0" count="8" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000007" call="MPI_Irecv" bytes="2560" orank="7" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000000E" call="MPI_Irecv" bytes="2560" orank="14" region="0" commid="0" count="3" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000003E" call="MPI_Irecv" bytes="2560" orank="62" region="0" commid="0" count="6" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000046" call="MPI_Irecv" bytes="2560" orank="70" region="0" commid="0" count="45" tid="0" op="" dtype="" >4.0770e-05 0.0000e+00 9.0599e-06</hent>
<hent key="038001000000000000000A00000002C6" call="MPI_Irecv" bytes="2560" orank="710" region="0" commid="0" count="48" tid="0" op="" dtype="" >1.5736e-05 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000070000000005" call="MPI_Isend" bytes="1792" orank="5" region="0" commid="0" count="34" tid="0" op="" dtype="" >7.5340e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000007" call="MPI_Isend" bytes="1792" orank="7" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.8048e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000007000000000E" call="MPI_Isend" bytes="1792" orank="14" region="0" commid="0" count="42" tid="0" op="" dtype="" >1.2994e-04 9.5367e-07 2.5034e-05</hent>
<hent key="0240010000000000000007000000003E" call="MPI_Isend" bytes="1792" orank="62" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.6035e-04 4.0531e-06 2.0027e-05</hent>
<hent key="02400100000000000000070000000046" call="MPI_Isend" bytes="1792" orank="70" region="0" commid="0" count="135" tid="0" op="" dtype="" >7.7820e-04 3.8147e-06 1.9073e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6150e-02 3.6150e-02 3.6150e-02</hent>
<hent key="024001000000000000000700000002C6" call="MPI_Isend" bytes="1792" orank="710" region="0" commid="0" count="158" tid="0" op="" dtype="" >7.5674e-04 9.5367e-07 1.4067e-05</hent>
<hent key="024001000000000000000A0000000005" call="MPI_Isend" bytes="2560" orank="5" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.7752e-06 1.9073e-06 2.8610e-06</hent>
<hent key="024001000000000000000A0000000007" call="MPI_Isend" bytes="2560" orank="7" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.5048e-05 2.8610e-06 5.0068e-06</hent>
<hent key="024001000000000000000A000000000E" call="MPI_Isend" bytes="2560" orank="14" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.1008e-05 9.5367e-07 1.6928e-05</hent>
<hent key="024001000000000000000A000000003E" call="MPI_Isend" bytes="2560" orank="62" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.1008e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000046" call="MPI_Isend" bytes="2560" orank="70" region="0" commid="0" count="44" tid="0" op="" dtype="" >2.7180e-04 4.0531e-06 1.5974e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.5402e-04 1.6928e-05 7.2956e-05</hent>
<hent key="024001000000000000000A00000002C6" call="MPI_Isend" bytes="2560" orank="710" region="0" commid="0" count="52" tid="0" op="" dtype="" >2.6894e-04 3.8147e-06 1.3113e-05</hent>
<hent key="02400100000000000000100000000046" call="MPI_Isend" bytes="4096" orank="70" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >4.4107e-04 1.8692e-04 2.5415e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.2187e-05 3.2187e-05 3.2187e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.4782e-05 1.9073e-06 1.2875e-05</hent>
<hent key="03800100000000000000000400000005" call="MPI_Irecv" bytes="4" orank="5" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3771e-03 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000000400000007" call="MPI_Irecv" bytes="4" orank="7" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.9973e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000000040000000E" call="MPI_Irecv" bytes="4" orank="14" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4031e-03 0.0000e+00 2.0981e-05</hent>
<hent key="0380010000000000000000040000003E" call="MPI_Irecv" bytes="4" orank="62" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.7963e-04 0.0000e+00 2.0981e-05</hent>
<hent key="03800100000000000000000400000046" call="MPI_Irecv" bytes="4" orank="70" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0936e-03 0.0000e+00 1.6928e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.8014e-04 1.2112e-04 3.5095e-04</hent>
<hent key="038001000000000000001C0000000005" call="MPI_Irecv" bytes="7168" orank="5" region="0" commid="0" count="9672" tid="0" op="" dtype="" >3.4101e-03 0.0000e+00 1.3828e-05</hent>
<hent key="038001000000000000001C0000000007" call="MPI_Irecv" bytes="7168" orank="7" region="0" commid="0" count="9638" tid="0" op="" dtype="" >1.8129e-03 0.0000e+00 1.0967e-05</hent>
<hent key="038001000000000000000004000002C6" call="MPI_Irecv" bytes="4" orank="710" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0142e-03 0.0000e+00 3.0041e-05</hent>
<hent key="02400100000000000000000400000005" call="MPI_Isend" bytes="4" orank="5" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.5519e-03 0.0000e+00 4.5061e-05</hent>
<hent key="02400100000000000000000400000007" call="MPI_Isend" bytes="4" orank="7" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.8977e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000000040000000E" call="MPI_Isend" bytes="4" orank="14" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6017e-03 0.0000e+00 3.4094e-05</hent>
<hent key="0240010000000000000000040000003E" call="MPI_Isend" bytes="4" orank="62" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3212e-02 2.8610e-06 1.1182e-04</hent>
<hent key="02400100000000000000000400000046" call="MPI_Isend" bytes="4" orank="70" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3295e-02 2.8610e-06 5.1022e-05</hent>
<hent key="024001000000000000001C0000000005" call="MPI_Isend" bytes="7168" orank="5" region="0" commid="0" count="9184" tid="0" op="" dtype="" >2.0888e-02 9.5367e-07 2.9087e-05</hent>
<hent key="024001000000000000001C0000000007" call="MPI_Isend" bytes="7168" orank="7" region="0" commid="0" count="8497" tid="0" op="" dtype="" >2.5357e-02 9.5367e-07 2.2888e-05</hent>
<hent key="024001000000000000000004000002C6" call="MPI_Isend" bytes="4" orank="710" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.1560e-02 2.8610e-06 8.9169e-05</hent>
<hent key="03800100000000000000050000000005" call="MPI_Irecv" bytes="1280" orank="5" region="0" commid="0" count="219" tid="0" op="" dtype="" >8.0109e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000007" call="MPI_Irecv" bytes="1280" orank="7" region="0" commid="0" count="221" tid="0" op="" dtype="" >5.9366e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000005000000000E" call="MPI_Irecv" bytes="1280" orank="14" region="0" commid="0" count="262" tid="0" op="" dtype="" >9.7752e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000005000000003E" call="MPI_Irecv" bytes="1280" orank="62" region="0" commid="0" count="183" tid="0" op="" dtype="" >7.1287e-05 0.0000e+00 1.1921e-05</hent>
<hent key="03800100000000000000050000000046" call="MPI_Irecv" bytes="1280" orank="70" region="0" commid="0" count="300" tid="0" op="" dtype="" >1.5974e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0987e+01 6.9141e-06 1.2806e-01</hent>
<hent key="0380010000000000000028000000000E" call="MPI_Irecv" bytes="10240" orank="14" region="0" commid="0" count="198" tid="0" op="" dtype="" >1.4639e-04 0.0000e+00 2.1935e-05</hent>
<hent key="0380010000000000000028000000003E" call="MPI_Irecv" bytes="10240" orank="62" region="0" commid="0" count="29" tid="0" op="" dtype="" >9.5367e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000500000002C6" call="MPI_Irecv" bytes="1280" orank="710" region="0" commid="0" count="298" tid="0" op="" dtype="" >1.3137e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000080000000005" call="MPI_Irecv" bytes="2048" orank="5" region="0" commid="0" count="10" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000007" call="MPI_Irecv" bytes="2048" orank="7" region="0" commid="0" count="11" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000E" call="MPI_Irecv" bytes="2048" orank="14" region="0" commid="0" count="12" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000003E" call="MPI_Irecv" bytes="2048" orank="62" region="0" commid="0" count="22" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000046" call="MPI_Irecv" bytes="2048" orank="70" region="0" commid="0" count="3464" tid="0" op="" dtype="" >1.0266e-03 0.0000e+00 3.2902e-05</hent>
<hent key="038001000000000000000800000002C6" call="MPI_Irecv" bytes="2048" orank="710" region="0" commid="0" count="3451" tid="0" op="" dtype="" >9.1600e-04 0.0000e+00 7.4863e-05</hent>
<hent key="02400100000000000000050000000005" call="MPI_Isend" bytes="1280" orank="5" region="0" commid="0" count="207" tid="0" op="" dtype="" >3.9577e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000007" call="MPI_Isend" bytes="1280" orank="7" region="0" commid="0" count="206" tid="0" op="" dtype="" >6.9690e-04 1.9073e-06 8.1062e-06</hent>
<hent key="0240010000000000000005000000000E" call="MPI_Isend" bytes="1280" orank="14" region="0" commid="0" count="184" tid="0" op="" dtype="" >3.9387e-04 9.5367e-07 1.3113e-05</hent>
<hent key="0240010000000000000005000000003E" call="MPI_Isend" bytes="1280" orank="62" region="0" commid="0" count="189" tid="0" op="" dtype="" >1.0629e-03 3.8147e-06 3.0994e-05</hent>
<hent key="02400100000000000000050000000046" call="MPI_Isend" bytes="1280" orank="70" region="0" commid="0" count="279" tid="0" op="" dtype="" >1.5054e-03 2.8610e-06 2.0981e-05</hent>
<hent key="0240010000000000000028000000000E" call="MPI_Isend" bytes="10240" orank="14" region="0" commid="0" count="8" tid="0" op="" dtype="" >5.4836e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000500000002C6" call="MPI_Isend" bytes="1280" orank="710" region="0" commid="0" count="300" tid="0" op="" dtype="" >1.4765e-03 2.8610e-06 2.0027e-05</hent>
</hash>
<internal rank="6" log_i="1723713849.516019" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="7" mpi_size="768" stamp_init="1723713791.096037" stamp_final="1723713849.510571" username="apac4" allocationname="unknown" flags="0" pid="1717070" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.84145e+01" utime="5.01886e+01" stime="6.82752e+00" mtime="3.27775e+01" gflop="0.00000e+00" gbyte="3.75671e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27775e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4501451145214ea5552145214ba" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83475e+01" utime="5.01603e+01" stime="6.81561e+00" mtime="3.27775e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27775e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4709e+08" > 4.4056e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4964e+08" > 2.8751e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7952e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4242e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9577e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6138e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.3291e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9492e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6195e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9436e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="200" >
<hent key="02400100000000000000080000000000" call="MPI_Isend" bytes="2048" orank="0" region="0" commid="0" count="10" tid="0" op="" dtype="" >5.0068e-05 3.8147e-06 6.1989e-06</hent>
<hent key="02400100000000000000080000000006" call="MPI_Isend" bytes="2048" orank="6" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.2173e-05 1.1921e-06 2.8610e-06</hent>
<hent key="0240010000000000000008000000000F" call="MPI_Isend" bytes="2048" orank="15" region="0" commid="0" count="15" tid="0" op="" dtype="" >6.3419e-05 9.5367e-07 1.1921e-05</hent>
<hent key="0240010000000000000008000000003F" call="MPI_Isend" bytes="2048" orank="63" region="0" commid="0" count="14" tid="0" op="" dtype="" >7.3910e-05 5.0068e-06 5.9605e-06</hent>
<hent key="02400100000000000000080000000047" call="MPI_Isend" bytes="2048" orank="71" region="0" commid="0" count="3422" tid="0" op="" dtype="" >1.0735e-02 9.5367e-07 2.2888e-05</hent>
<hent key="024001000000000000000800000002C7" call="MPI_Isend" bytes="2048" orank="711" region="0" commid="0" count="3249" tid="0" op="" dtype="" >9.6247e-03 9.5367e-07 1.4782e-05</hent>
<hent key="038001000000000000000E0000000047" call="MPI_Irecv" bytes="3584" orank="71" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000E0000000047" call="MPI_Isend" bytes="3584" orank="71" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000000" call="MPI_Irecv" bytes="640" orank="0" region="0" commid="0" count="406" tid="0" op="" dtype="" >7.1287e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000028000000006" call="MPI_Irecv" bytes="640" orank="6" region="0" commid="0" count="400" tid="0" op="" dtype="" >2.7776e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000002800000000F" call="MPI_Irecv" bytes="640" orank="15" region="0" commid="0" count="396" tid="0" op="" dtype="" >9.2983e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000003F" call="MPI_Irecv" bytes="640" orank="63" region="0" commid="0" count="387" tid="0" op="" dtype="" >9.0599e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000047" call="MPI_Irecv" bytes="640" orank="71" region="0" commid="0" count="289" tid="0" op="" dtype="" >1.1849e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="038001000000000000000280000002C7" call="MPI_Irecv" bytes="640" orank="711" region="0" commid="0" count="283" tid="0" op="" dtype="" >7.3671e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000000" call="MPI_Isend" bytes="640" orank="0" region="0" commid="0" count="412" tid="0" op="" dtype="" >1.6825e-03 1.9073e-06 8.8215e-06</hent>
<hent key="02400100000000000000028000000006" call="MPI_Isend" bytes="640" orank="6" region="0" commid="0" count="415" tid="0" op="" dtype="" >5.4336e-04 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000002800000000F" call="MPI_Isend" bytes="640" orank="15" region="0" commid="0" count="396" tid="0" op="" dtype="" >4.6682e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000002800000003F" call="MPI_Isend" bytes="640" orank="63" region="0" commid="0" count="396" tid="0" op="" dtype="" >1.7929e-03 3.8147e-06 9.0599e-06</hent>
<hent key="02400100000000000000028000000047" call="MPI_Isend" bytes="640" orank="71" region="0" commid="0" count="278" tid="0" op="" dtype="" >1.1270e-03 2.8610e-06 9.0599e-06</hent>
<hent key="024001000000000000000280000002C7" call="MPI_Isend" bytes="640" orank="711" region="0" commid="0" count="274" tid="0" op="" dtype="" >1.0762e-03 2.8610e-06 7.8678e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0014e-05 1.0014e-05 1.0014e-05</hent>
<hent key="03800100000000000000014000000000" call="MPI_Irecv" bytes="320" orank="0" region="0" commid="0" count="399" tid="0" op="" dtype="" >8.3447e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000006" call="MPI_Irecv" bytes="320" orank="6" region="0" commid="0" count="339" tid="0" op="" dtype="" >2.5320e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000001400000000F" call="MPI_Irecv" bytes="320" orank="15" region="0" commid="0" count="357" tid="0" op="" dtype="" >8.9169e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000003F" call="MPI_Irecv" bytes="320" orank="63" region="0" commid="0" count="404" tid="0" op="" dtype="" >1.0896e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000047" call="MPI_Irecv" bytes="320" orank="71" region="0" commid="0" count="161" tid="0" op="" dtype="" >5.3167e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002C7" call="MPI_Irecv" bytes="320" orank="711" region="0" commid="0" count="191" tid="0" op="" dtype="" >4.7684e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.7952e+00 0.0000e+00 1.3919e-01</hent>
<hent key="03800100000000000000400000000047" call="MPI_Irecv" bytes="16384" orank="71" region="0" commid="0" count="12589" tid="0" op="" dtype="" >2.9280e-03 0.0000e+00 9.0599e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.0027e-05 0.0000e+00 1.9073e-05</hent>
<hent key="038001000000000000004000000002C7" call="MPI_Irecv" bytes="16384" orank="711" region="0" commid="0" count="11973" tid="0" op="" dtype="" >1.7796e-03 0.0000e+00 7.8678e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000014000000000" call="MPI_Isend" bytes="320" orank="0" region="0" commid="0" count="405" tid="0" op="" dtype="" >1.5030e-03 1.9073e-06 9.0599e-06</hent>
<hent key="02400100000000000000014000000006" call="MPI_Isend" bytes="320" orank="6" region="0" commid="0" count="329" tid="0" op="" dtype="" >3.3116e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000001400000000F" call="MPI_Isend" bytes="320" orank="15" region="0" commid="0" count="360" tid="0" op="" dtype="" >3.6407e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000001400000003F" call="MPI_Isend" bytes="320" orank="63" region="0" commid="0" count="340" tid="0" op="" dtype="" >1.6060e-03 2.8610e-06 1.4901e-04</hent>
<hent key="02400100000000000000014000000047" call="MPI_Isend" bytes="320" orank="71" region="0" commid="0" count="172" tid="0" op="" dtype="" >6.7687e-04 2.8610e-06 5.0068e-06</hent>
<hent key="03800100000000000000200000000000" call="MPI_Irecv" bytes="8192" orank="0" region="0" commid="0" count="4139" tid="0" op="" dtype="" >6.2108e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000200000000006" call="MPI_Irecv" bytes="8192" orank="6" region="0" commid="0" count="4202" tid="0" op="" dtype="" >1.2305e-03 0.0000e+00 1.8835e-05</hent>
<hent key="0380010000000000000020000000000F" call="MPI_Irecv" bytes="8192" orank="15" region="0" commid="0" count="12321" tid="0" op="" dtype="" >6.3455e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0380010000000000000020000000003F" call="MPI_Irecv" bytes="8192" orank="63" region="0" commid="0" count="12547" tid="0" op="" dtype="" >1.4606e-03 0.0000e+00 4.0531e-06</hent>
<hent key="024001000000000000000140000002C7" call="MPI_Isend" bytes="320" orank="711" region="0" commid="0" count="179" tid="0" op="" dtype="" >6.7759e-04 2.8610e-06 5.0068e-06</hent>
<hent key="03800100000000000000000000000000" call="MPI_Irecv" bytes="0" orank="0" region="0" commid="0" count="248" tid="0" op="" dtype="" >4.9114e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000006" call="MPI_Irecv" bytes="0" orank="6" region="0" commid="0" count="263" tid="0" op="" dtype="" >1.4710e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000000000000000F" call="MPI_Irecv" bytes="0" orank="15" region="0" commid="0" count="268" tid="0" op="" dtype="" >6.1035e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000003F" call="MPI_Irecv" bytes="0" orank="63" region="0" commid="0" count="278" tid="0" op="" dtype="" >7.1049e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000047" call="MPI_Irecv" bytes="0" orank="71" region="0" commid="0" count="146" tid="0" op="" dtype="" >3.4094e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000047" call="MPI_Isend" bytes="16384" orank="71" region="0" commid="0" count="12523" tid="0" op="" dtype="" >8.6118e-02 3.8147e-06 3.2902e-05</hent>
<hent key="038001000000000000000000000002C7" call="MPI_Irecv" bytes="0" orank="711" region="0" commid="0" count="150" tid="0" op="" dtype="" >4.3869e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002C7" call="MPI_Isend" bytes="16384" orank="711" region="0" commid="0" count="11898" tid="0" op="" dtype="" >7.9220e-02 2.8610e-06 1.4067e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.5006e-04 0.0000e+00 5.7602e-04</hent>
<hent key="02400100000000000000200000000000" call="MPI_Isend" bytes="8192" orank="0" region="0" commid="0" count="3470" tid="0" op="" dtype="" >5.3377e-03 0.0000e+00 1.0800e-04</hent>
<hent key="02400100000000000000200000000006" call="MPI_Isend" bytes="8192" orank="6" region="0" commid="0" count="3061" tid="0" op="" dtype="" >3.3138e-03 0.0000e+00 1.2875e-05</hent>
<hent key="0240010000000000000020000000000F" call="MPI_Isend" bytes="8192" orank="15" region="0" commid="0" count="12373" tid="0" op="" dtype="" >7.0648e-03 0.0000e+00 5.0068e-05</hent>
<hent key="0240010000000000000020000000003F" call="MPI_Isend" bytes="8192" orank="63" region="0" commid="0" count="12644" tid="0" op="" dtype="" >7.5374e-02 3.8147e-06 1.2898e-04</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6001e-05 3.6001e-05 3.6001e-05</hent>
<hent key="02400100000000000000000000000000" call="MPI_Isend" bytes="0" orank="0" region="0" commid="0" count="281" tid="0" op="" dtype="" >6.7139e-04 9.5367e-07 7.1049e-05</hent>
<hent key="02400100000000000000000000000006" call="MPI_Isend" bytes="0" orank="6" region="0" commid="0" count="259" tid="0" op="" dtype="" >1.5712e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000000000000000F" call="MPI_Isend" bytes="0" orank="15" region="0" commid="0" count="262" tid="0" op="" dtype="" >1.6904e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000000000003F" call="MPI_Isend" bytes="0" orank="63" region="0" commid="0" count="279" tid="0" op="" dtype="" >1.0550e-03 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000000000000047" call="MPI_Isend" bytes="0" orank="71" region="0" commid="0" count="143" tid="0" op="" dtype="" >5.0139e-04 9.5367e-07 8.1062e-06</hent>
<hent key="024001000000000000000000000002C7" call="MPI_Isend" bytes="0" orank="711" region="0" commid="0" count="148" tid="0" op="" dtype="" >5.0974e-04 9.5367e-07 6.9141e-06</hent>
<hent key="03800100000000000000060000000000" call="MPI_Irecv" bytes="1536" orank="0" region="0" commid="0" count="95" tid="0" op="" dtype="" >1.7405e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000006" call="MPI_Irecv" bytes="1536" orank="6" region="0" commid="0" count="89" tid="0" op="" dtype="" >6.7234e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000F" call="MPI_Irecv" bytes="1536" orank="15" region="0" commid="0" count="99" tid="0" op="" dtype="" >2.0504e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000003F" call="MPI_Irecv" bytes="1536" orank="63" region="0" commid="0" count="103" tid="0" op="" dtype="" >2.2173e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000047" call="MPI_Irecv" bytes="1536" orank="71" region="0" commid="0" count="226" tid="0" op="" dtype="" >9.3699e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002C7" call="MPI_Irecv" bytes="1536" orank="711" region="0" commid="0" count="228" tid="0" op="" dtype="" >4.6968e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000030000000000" call="MPI_Isend" bytes="768" orank="0" region="0" commid="0" count="6" tid="0" op="" dtype="" >6.1989e-06 9.5367e-07 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000000" call="MPI_Isend" bytes="1536" orank="0" region="0" commid="0" count="88" tid="0" op="" dtype="" >4.5252e-04 3.0994e-06 3.0994e-05</hent>
<hent key="02400100000000000000060000000006" call="MPI_Isend" bytes="1536" orank="6" region="0" commid="0" count="87" tid="0" op="" dtype="" >1.7214e-04 9.5367e-07 5.0068e-06</hent>
<hent key="0240010000000000000006000000000F" call="MPI_Isend" bytes="1536" orank="15" region="0" commid="0" count="107" tid="0" op="" dtype="" >1.8144e-04 9.5367e-07 1.0967e-05</hent>
<hent key="0240010000000000000006000000003F" call="MPI_Isend" bytes="1536" orank="63" region="0" commid="0" count="104" tid="0" op="" dtype="" >5.1212e-04 3.8147e-06 6.1989e-06</hent>
<hent key="02400100000000000000060000000047" call="MPI_Isend" bytes="1536" orank="71" region="0" commid="0" count="232" tid="0" op="" dtype="" >1.0176e-03 2.8610e-06 5.9605e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C7" call="MPI_Isend" bytes="1536" orank="711" region="0" commid="0" count="227" tid="0" op="" dtype="" >9.4843e-04 2.8610e-06 7.8678e-06</hent>
<hent key="038001000000000000000C000000003F" call="MPI_Irecv" bytes="3072" orank="63" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C0000000047" call="MPI_Irecv" bytes="3072" orank="71" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C7" call="MPI_Irecv" bytes="3072" orank="711" region="0" commid="0" count="4" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000C0000000000" call="MPI_Isend" bytes="3072" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="024001000000000000000C0000000006" call="MPI_Isend" bytes="3072" orank="6" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000C0000000047" call="MPI_Isend" bytes="3072" orank="71" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.1935e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000C00000002C7" call="MPI_Isend" bytes="3072" orank="711" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.9789e-05 4.7684e-06 5.0068e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.5903e-04 1.5903e-04 1.5903e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.3712e-04 1.1206e-04 1.1301e-04</hent>
<hent key="03800100000000000000038000000000" call="MPI_Irecv" bytes="896" orank="0" region="0" commid="0" count="2584" tid="0" op="" dtype="" >4.2439e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000006" call="MPI_Irecv" bytes="896" orank="6" region="0" commid="0" count="2598" tid="0" op="" dtype="" >4.3464e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000003800000000F" call="MPI_Irecv" bytes="896" orank="15" region="0" commid="0" count="314" tid="0" op="" dtype="" >7.4625e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000003F" call="MPI_Irecv" bytes="896" orank="63" region="0" commid="0" count="288" tid="0" op="" dtype="" >8.4639e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000047" call="MPI_Irecv" bytes="896" orank="71" region="0" commid="0" count="334" tid="0" op="" dtype="" >1.0824e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002C7" call="MPI_Irecv" bytes="896" orank="711" region="0" commid="0" count="322" tid="0" op="" dtype="" >7.3433e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8133e-05 2.8133e-05 2.8133e-05</hent>
<hent key="03800100000000000000380000000047" call="MPI_Irecv" bytes="14336" orank="71" region="0" commid="0" count="110" tid="0" op="" dtype="" >2.1935e-05 0.0000e+00 1.9073e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000003800000002C7" call="MPI_Irecv" bytes="14336" orank="711" region="0" commid="0" count="726" tid="0" op="" dtype="" >9.9421e-05 0.0000e+00 3.8147e-06</hent>
<hent key="02400100000000000000038000000000" call="MPI_Isend" bytes="896" orank="0" region="0" commid="0" count="2747" tid="0" op="" dtype="" >2.9294e-03 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000038000000006" call="MPI_Isend" bytes="896" orank="6" region="0" commid="0" count="2916" tid="0" op="" dtype="" >1.9269e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000003800000000F" call="MPI_Isend" bytes="896" orank="15" region="0" commid="0" count="334" tid="0" op="" dtype="" >4.7207e-04 9.5367e-07 8.1062e-06</hent>
<hent key="0240010000000000000003800000003F" call="MPI_Isend" bytes="896" orank="63" region="0" commid="0" count="310" tid="0" op="" dtype="" >1.4462e-03 3.8147e-06 5.9605e-06</hent>
<hent key="02400100000000000000038000000047" call="MPI_Isend" bytes="896" orank="71" region="0" commid="0" count="345" tid="0" op="" dtype="" >1.4396e-03 2.8610e-06 9.0599e-06</hent>
<hent key="024001000000000000000380000002C7" call="MPI_Isend" bytes="896" orank="711" region="0" commid="0" count="344" tid="0" op="" dtype="" >1.3735e-03 2.8610e-06 9.0599e-06</hent>
<hent key="02400100000000000000380000000047" call="MPI_Isend" bytes="14336" orank="71" region="0" commid="0" count="176" tid="0" op="" dtype="" >1.1790e-03 5.0068e-06 1.2159e-05</hent>
<hent key="024001000000000000003800000002C7" call="MPI_Isend" bytes="14336" orank="711" region="0" commid="0" count="801" tid="0" op="" dtype="" >9.3343e-03 2.8610e-06 3.8688e-03</hent>
<hent key="02400100000000000000180000000000" call="MPI_Isend" bytes="6144" orank="0" region="0" commid="0" count="22" tid="0" op="" dtype="" >5.7459e-05 1.9073e-06 5.0068e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.2665e+00 1.0014e-05 1.3414e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.0998e-04 5.0998e-04 5.0998e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >2.9492e-04 2.9492e-04 2.9492e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >1.5011e-03 1.5011e-03 1.5011e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >6.5004e-02 3.3808e-04 5.9998e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.6195e-04 5.6195e-04 5.6195e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6138e+00 4.4703e-04 2.5672e-01</hent>
<hent key="03800100000000000000040000000000" call="MPI_Irecv" bytes="1024" orank="0" region="0" commid="0" count="1114" tid="0" op="" dtype="" >1.6451e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000006" call="MPI_Irecv" bytes="1024" orank="6" region="0" commid="0" count="1134" tid="0" op="" dtype="" >1.1063e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000000F" call="MPI_Irecv" bytes="1024" orank="15" region="0" commid="0" count="3292" tid="0" op="" dtype="" >8.0657e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000003F" call="MPI_Irecv" bytes="1024" orank="63" region="0" commid="0" count="3352" tid="0" op="" dtype="" >2.8157e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.3127e-05 0.0000e+00 1.2159e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.0599e-06 9.0599e-06 9.0599e-06</hent>
<hent key="03800100000000000000070000000000" call="MPI_Irecv" bytes="1792" orank="0" region="0" commid="0" count="37" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000006" call="MPI_Irecv" bytes="1792" orank="6" region="0" commid="0" count="47" tid="0" op="" dtype="" >2.9564e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000007000000000F" call="MPI_Irecv" bytes="1792" orank="15" region="0" commid="0" count="41" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000003F" call="MPI_Irecv" bytes="1792" orank="63" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.0252e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000047" call="MPI_Irecv" bytes="1792" orank="71" region="0" commid="0" count="165" tid="0" op="" dtype="" >5.7459e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002C7" call="MPI_Irecv" bytes="1792" orank="711" region="0" commid="0" count="314" tid="0" op="" dtype="" >4.4584e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >8.6308e-05 0.0000e+00 5.1022e-05</hent>
<hent key="02400100000000000000040000000000" call="MPI_Isend" bytes="1024" orank="0" region="0" commid="0" count="934" tid="0" op="" dtype="" >6.9976e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000040000000006" call="MPI_Isend" bytes="1024" orank="6" region="0" commid="0" count="806" tid="0" op="" dtype="" >4.9257e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000004000000000F" call="MPI_Isend" bytes="1024" orank="15" region="0" commid="0" count="3296" tid="0" op="" dtype="" >1.9650e-03 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000004000000003F" call="MPI_Isend" bytes="1024" orank="63" region="0" commid="0" count="3384" tid="0" op="" dtype="" >8.4341e-03 9.5367e-07 1.1921e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4232e+00 0.0000e+00 3.2498e+00</hent>
<hent key="038001000000000000000A0000000000" call="MPI_Irecv" bytes="2560" orank="0" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000006" call="MPI_Irecv" bytes="2560" orank="6" region="0" commid="0" count="9" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000000F" call="MPI_Irecv" bytes="2560" orank="15" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000003F" call="MPI_Irecv" bytes="2560" orank="63" region="0" commid="0" count="11" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000047" call="MPI_Irecv" bytes="2560" orank="71" region="0" commid="0" count="48" tid="0" op="" dtype="" >2.2888e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002C7" call="MPI_Irecv" bytes="2560" orank="711" region="0" commid="0" count="43" tid="0" op="" dtype="" >1.0729e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000000" call="MPI_Isend" bytes="1792" orank="0" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.5712e-04 3.8147e-06 6.1989e-06</hent>
<hent key="02400100000000000000070000000006" call="MPI_Isend" bytes="1792" orank="6" region="0" commid="0" count="49" tid="0" op="" dtype="" >9.5844e-05 9.5367e-07 2.8610e-06</hent>
<hent key="0240010000000000000007000000000F" call="MPI_Isend" bytes="1792" orank="15" region="0" commid="0" count="32" tid="0" op="" dtype="" >9.0122e-05 9.5367e-07 1.2875e-05</hent>
<hent key="0240010000000000000007000000003F" call="MPI_Isend" bytes="1792" orank="63" region="0" commid="0" count="56" tid="0" op="" dtype="" >2.8062e-04 4.0531e-06 5.9605e-06</hent>
<hent key="02400100000000000000070000000047" call="MPI_Isend" bytes="1792" orank="71" region="0" commid="0" count="153" tid="0" op="" dtype="" >6.1226e-04 9.5367e-07 5.9605e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6195e-02 3.6195e-02 3.6195e-02</hent>
<hent key="024001000000000000000700000002C7" call="MPI_Isend" bytes="1792" orank="711" region="0" commid="0" count="335" tid="0" op="" dtype="" >1.0908e-03 9.5367e-07 6.1989e-06</hent>
<hent key="024001000000000000000A0000000000" call="MPI_Isend" bytes="2560" orank="0" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.3127e-05 5.0068e-06 6.1989e-06</hent>
<hent key="024001000000000000000A0000000006" call="MPI_Isend" bytes="2560" orank="6" region="0" commid="0" count="3" tid="0" op="" dtype="" >6.9141e-06 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A000000000F" call="MPI_Isend" bytes="2560" orank="15" region="0" commid="0" count="4" tid="0" op="" dtype="" >8.8215e-06 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A000000003F" call="MPI_Isend" bytes="2560" orank="63" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.7895e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000047" call="MPI_Isend" bytes="2560" orank="71" region="0" commid="0" count="52" tid="0" op="" dtype="" >2.6488e-04 3.8147e-06 6.1989e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.8406e-04 2.0981e-05 8.9169e-05</hent>
<hent key="024001000000000000000A00000002C7" call="MPI_Isend" bytes="2560" orank="711" region="0" commid="0" count="50" tid="0" op="" dtype="" >2.3723e-04 3.8147e-06 5.9605e-06</hent>
<hent key="03800100000000000000100000000047" call="MPI_Irecv" bytes="4096" orank="71" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.2309e-04 1.9908e-04 3.2401e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.3855e-05 3.3855e-05 3.3855e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.2452e-06 2.1458e-06 3.0994e-06</hent>
<hent key="03800100000000000000000400000000" call="MPI_Irecv" bytes="4" orank="0" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.6849e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000000400000006" call="MPI_Irecv" bytes="4" orank="6" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.5293e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000000040000000F" call="MPI_Irecv" bytes="4" orank="15" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.6652e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000000040000003F" call="MPI_Irecv" bytes="4" orank="63" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.2261e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000400000047" call="MPI_Irecv" bytes="4" orank="71" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.3017e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.1258e-03 1.4591e-04 3.9697e-04</hent>
<hent key="038001000000000000001C0000000000" call="MPI_Irecv" bytes="7168" orank="0" region="0" commid="0" count="8560" tid="0" op="" dtype="" >1.2865e-03 0.0000e+00 3.0994e-06</hent>
<hent key="038001000000000000001C0000000006" call="MPI_Irecv" bytes="7168" orank="6" region="0" commid="0" count="8497" tid="0" op="" dtype="" >2.6708e-03 0.0000e+00 1.4067e-05</hent>
<hent key="038001000000000000000004000002C7" call="MPI_Irecv" bytes="4" orank="711" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.7684e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000400000000" call="MPI_Isend" bytes="4" orank="0" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.4524e-03 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000000400000006" call="MPI_Isend" bytes="4" orank="6" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4262e-03 0.0000e+00 2.8610e-06</hent>
<hent key="0240010000000000000000040000000F" call="MPI_Isend" bytes="4" orank="15" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4637e-03 0.0000e+00 3.0994e-06</hent>
<hent key="0240010000000000000000040000003F" call="MPI_Isend" bytes="4" orank="63" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.8961e-02 2.8610e-06 1.1396e-04</hent>
<hent key="02400100000000000000000400000047" call="MPI_Isend" bytes="4" orank="71" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7519e-02 2.8610e-06 1.0967e-05</hent>
<hent key="024001000000000000001C0000000000" call="MPI_Isend" bytes="7168" orank="0" region="0" commid="0" count="9207" tid="0" op="" dtype="" >2.6232e-02 9.5367e-07 1.3113e-05</hent>
<hent key="024001000000000000001C0000000006" call="MPI_Isend" bytes="7168" orank="6" region="0" commid="0" count="9638" tid="0" op="" dtype="" >2.0280e-02 9.5367e-07 1.0014e-05</hent>
<hent key="024001000000000000000004000002C7" call="MPI_Isend" bytes="4" orank="711" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6921e-02 2.8610e-06 6.8903e-05</hent>
<hent key="03800100000000000000050000000000" call="MPI_Irecv" bytes="1280" orank="0" region="0" commid="0" count="198" tid="0" op="" dtype="" >3.7432e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000006" call="MPI_Irecv" bytes="1280" orank="6" region="0" commid="0" count="206" tid="0" op="" dtype="" >1.5473e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000005000000000F" call="MPI_Irecv" bytes="1280" orank="15" region="0" commid="0" count="309" tid="0" op="" dtype="" >7.7963e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000003F" call="MPI_Irecv" bytes="1280" orank="63" region="0" commid="0" count="211" tid="0" op="" dtype="" >5.0783e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000047" call="MPI_Irecv" bytes="1280" orank="71" region="0" commid="0" count="294" tid="0" op="" dtype="" >1.2064e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.1102e+01 6.9141e-06 1.2807e-01</hent>
<hent key="0380010000000000000028000000000F" call="MPI_Irecv" bytes="10240" orank="15" region="0" commid="0" count="378" tid="0" op="" dtype="" >8.9169e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000028000000003F" call="MPI_Irecv" bytes="10240" orank="63" region="0" commid="0" count="152" tid="0" op="" dtype="" >3.7193e-05 0.0000e+00 3.8147e-06</hent>
<hent key="038001000000000000000500000002C7" call="MPI_Irecv" bytes="1280" orank="711" region="0" commid="0" count="296" tid="0" op="" dtype="" >6.8665e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000000" call="MPI_Irecv" bytes="2048" orank="0" region="0" commid="0" count="13" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000006" call="MPI_Irecv" bytes="2048" orank="6" region="0" commid="0" count="12" tid="0" op="" dtype="" >8.5831e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000F" call="MPI_Irecv" bytes="2048" orank="15" region="0" commid="0" count="16" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000008000000003F" call="MPI_Irecv" bytes="2048" orank="63" region="0" commid="0" count="25" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000047" call="MPI_Irecv" bytes="2048" orank="71" region="0" commid="0" count="3426" tid="0" op="" dtype="" >6.7663e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002C7" call="MPI_Irecv" bytes="2048" orank="711" region="0" commid="0" count="3266" tid="0" op="" dtype="" >3.5763e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000050000000000" call="MPI_Isend" bytes="1280" orank="0" region="0" commid="0" count="177" tid="0" op="" dtype="" >8.3327e-04 3.0994e-06 1.0967e-05</hent>
<hent key="02400100000000000000050000000006" call="MPI_Isend" bytes="1280" orank="6" region="0" commid="0" count="221" tid="0" op="" dtype="" >4.0102e-04 9.5367e-07 5.0068e-06</hent>
<hent key="0240010000000000000005000000000F" call="MPI_Isend" bytes="1280" orank="15" region="0" commid="0" count="291" tid="0" op="" dtype="" >3.3569e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000005000000003F" call="MPI_Isend" bytes="1280" orank="63" region="0" commid="0" count="209" tid="0" op="" dtype="" >9.7156e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000050000000047" call="MPI_Isend" bytes="1280" orank="71" region="0" commid="0" count="295" tid="0" op="" dtype="" >1.2581e-03 3.0994e-06 5.0068e-06</hent>
<hent key="0240010000000000000028000000000F" call="MPI_Isend" bytes="10240" orank="15" region="0" commid="0" count="326" tid="0" op="" dtype="" >1.2493e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0240010000000000000028000000003F" call="MPI_Isend" bytes="10240" orank="63" region="0" commid="0" count="55" tid="0" op="" dtype="" >3.3569e-04 4.0531e-06 8.8215e-06</hent>
<hent key="024001000000000000000500000002C7" call="MPI_Isend" bytes="1280" orank="711" region="0" commid="0" count="287" tid="0" op="" dtype="" >1.1487e-03 2.8610e-06 9.0599e-06</hent>
</hash>
<internal rank="7" log_i="1723713849.510571" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="8" mpi_size="768" stamp_init="1723713791.099532" stamp_final="1723713849.510183" username="apac4" allocationname="unknown" flags="0" pid="1717071" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.84107e+01" utime="4.72913e+01" stime="7.93298e+00" mtime="3.20213e+01" gflop="0.00000e+00" gbyte="3.75340e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20213e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007714761484" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83400e+01" utime="4.72569e+01" stime="7.92731e+00" mtime="3.20213e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20213e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4882e+08" > 5.8828e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4921e+08" > 4.0817e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9633e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4461e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.8249e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2803e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6135e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.8734e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7490e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6146e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8329e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="197" >
<hent key="02400100000000000000080000000000" call="MPI_Isend" bytes="2048" orank="0" region="0" commid="0" count="10" tid="0" op="" dtype="" >3.6716e-05 1.9073e-06 1.3113e-05</hent>
<hent key="02400100000000000000080000000009" call="MPI_Isend" bytes="2048" orank="9" region="0" commid="0" count="12" tid="0" op="" dtype="" >4.8399e-05 3.0994e-06 5.0068e-06</hent>
<hent key="0240010000000000000008000000000F" call="MPI_Isend" bytes="2048" orank="15" region="0" commid="0" count="13" tid="0" op="" dtype="" >2.7418e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000010" call="MPI_Isend" bytes="2048" orank="16" region="0" commid="0" count="15" tid="0" op="" dtype="" >3.9816e-05 9.5367e-07 1.3113e-05</hent>
<hent key="02400100000000000000080000000048" call="MPI_Isend" bytes="2048" orank="72" region="0" commid="0" count="3437" tid="0" op="" dtype="" >2.1875e-02 9.5367e-07 1.0014e-04</hent>
<hent key="024001000000000000000800000002C8" call="MPI_Isend" bytes="2048" orank="712" region="0" commid="0" count="3408" tid="0" op="" dtype="" >1.4107e-02 9.5367e-07 4.4823e-05</hent>
<hent key="038001000000000000000E0000000048" call="MPI_Irecv" bytes="3584" orank="72" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="024001000000000000000E0000000000" call="MPI_Isend" bytes="3584" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="03800100000000000000028000000000" call="MPI_Irecv" bytes="640" orank="0" region="0" commid="0" count="404" tid="0" op="" dtype="" >8.6308e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000009" call="MPI_Irecv" bytes="640" orank="9" region="0" commid="0" count="406" tid="0" op="" dtype="" >8.2731e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000F" call="MPI_Irecv" bytes="640" orank="15" region="0" commid="0" count="411" tid="0" op="" dtype="" >1.2803e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000010" call="MPI_Irecv" bytes="640" orank="16" region="0" commid="0" count="422" tid="0" op="" dtype="" >7.0810e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000E00000002C8" call="MPI_Isend" bytes="3584" orank="712" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000048" call="MPI_Irecv" bytes="640" orank="72" region="0" commid="0" count="280" tid="0" op="" dtype="" >1.2445e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002C8" call="MPI_Irecv" bytes="640" orank="712" region="0" commid="0" count="277" tid="0" op="" dtype="" >8.4162e-05 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000028000000000" call="MPI_Isend" bytes="640" orank="0" region="0" commid="0" count="414" tid="0" op="" dtype="" >4.7398e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000028000000009" call="MPI_Isend" bytes="640" orank="9" region="0" commid="0" count="395" tid="0" op="" dtype="" >1.2162e-03 1.9073e-06 8.1062e-06</hent>
<hent key="0240010000000000000002800000000F" call="MPI_Isend" bytes="640" orank="15" region="0" commid="0" count="425" tid="0" op="" dtype="" >5.8103e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000028000000010" call="MPI_Isend" bytes="640" orank="16" region="0" commid="0" count="382" tid="0" op="" dtype="" >4.2772e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000028000000048" call="MPI_Isend" bytes="640" orank="72" region="0" commid="0" count="261" tid="0" op="" dtype="" >1.7095e-03 3.8147e-06 2.0027e-05</hent>
<hent key="024001000000000000000280000002C8" call="MPI_Isend" bytes="640" orank="712" region="0" commid="0" count="254" tid="0" op="" dtype="" >1.2808e-03 2.8610e-06 2.5034e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="03800100000000000000014000000000" call="MPI_Irecv" bytes="320" orank="0" region="0" commid="0" count="349" tid="0" op="" dtype="" >6.2943e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000009" call="MPI_Irecv" bytes="320" orank="9" region="0" commid="0" count="365" tid="0" op="" dtype="" >6.8188e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000F" call="MPI_Irecv" bytes="320" orank="15" region="0" commid="0" count="372" tid="0" op="" dtype="" >1.2326e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000010" call="MPI_Irecv" bytes="320" orank="16" region="0" commid="0" count="352" tid="0" op="" dtype="" >5.3167e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000048" call="MPI_Irecv" bytes="320" orank="72" region="0" commid="0" count="165" tid="0" op="" dtype="" >6.0320e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002C8" call="MPI_Irecv" bytes="320" orank="712" region="0" commid="0" count="180" tid="0" op="" dtype="" >7.8440e-05 0.0000e+00 5.9605e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >6.1989e-06 9.5367e-07 2.8610e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.9633e+00 0.0000e+00 1.3103e-01</hent>
<hent key="03800100000000000000400000000048" call="MPI_Irecv" bytes="16384" orank="72" region="0" commid="0" count="12474" tid="0" op="" dtype="" >7.9951e-03 0.0000e+00 3.6955e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.5020e-05 0.0000e+00 1.5020e-05</hent>
<hent key="038001000000000000004000000002C8" call="MPI_Irecv" bytes="16384" orank="712" region="0" commid="0" count="12672" tid="0" op="" dtype="" >8.2335e-03 0.0000e+00 6.3896e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.2612e-04 0.0000e+00 1.2612e-04</hent>
<hent key="02400100000000000000014000000000" call="MPI_Isend" bytes="320" orank="0" region="0" commid="0" count="373" tid="0" op="" dtype="" >3.7050e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000014000000009" call="MPI_Isend" bytes="320" orank="9" region="0" commid="0" count="382" tid="0" op="" dtype="" >1.0657e-03 1.9073e-06 6.9141e-06</hent>
<hent key="0240010000000000000001400000000F" call="MPI_Isend" bytes="320" orank="15" region="0" commid="0" count="362" tid="0" op="" dtype="" >3.8528e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000010" call="MPI_Isend" bytes="320" orank="16" region="0" commid="0" count="339" tid="0" op="" dtype="" >3.0994e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000014000000048" call="MPI_Isend" bytes="320" orank="72" region="0" commid="0" count="178" tid="0" op="" dtype="" >1.1160e-03 3.8147e-06 3.8147e-05</hent>
<hent key="03800100000000000000200000000000" call="MPI_Irecv" bytes="8192" orank="0" region="0" commid="0" count="12411" tid="0" op="" dtype="" >2.4359e-03 0.0000e+00 1.2875e-05</hent>
<hent key="03800100000000000000200000000009" call="MPI_Irecv" bytes="8192" orank="9" region="0" commid="0" count="3848" tid="0" op="" dtype="" >6.6781e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000020000000000F" call="MPI_Irecv" bytes="8192" orank="15" region="0" commid="0" count="3126" tid="0" op="" dtype="" >1.3044e-03 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000200000000010" call="MPI_Irecv" bytes="8192" orank="16" region="0" commid="0" count="12630" tid="0" op="" dtype="" >2.2595e-03 0.0000e+00 8.0109e-05</hent>
<hent key="024001000000000000000140000002C8" call="MPI_Isend" bytes="320" orank="712" region="0" commid="0" count="187" tid="0" op="" dtype="" >8.6570e-04 2.8610e-06 1.5974e-05</hent>
<hent key="03800100000000000000000000000000" call="MPI_Irecv" bytes="0" orank="0" region="0" commid="0" count="257" tid="0" op="" dtype="" >4.9353e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000009" call="MPI_Irecv" bytes="0" orank="9" region="0" commid="0" count="258" tid="0" op="" dtype="" >6.7472e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000F" call="MPI_Irecv" bytes="0" orank="15" region="0" commid="0" count="253" tid="0" op="" dtype="" >7.0333e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000010" call="MPI_Irecv" bytes="0" orank="16" region="0" commid="0" count="228" tid="0" op="" dtype="" >4.1962e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000048" call="MPI_Irecv" bytes="0" orank="72" region="0" commid="0" count="147" tid="0" op="" dtype="" >6.2704e-05 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000400000000048" call="MPI_Isend" bytes="16384" orank="72" region="0" commid="0" count="12586" tid="0" op="" dtype="" >2.5397e-01 4.0531e-06 1.5616e-04</hent>
<hent key="038001000000000000000000000002C8" call="MPI_Irecv" bytes="0" orank="712" region="0" commid="0" count="145" tid="0" op="" dtype="" >8.2493e-05 0.0000e+00 1.0967e-05</hent>
<hent key="024001000000000000004000000002C8" call="MPI_Isend" bytes="16384" orank="712" region="0" commid="0" count="12462" tid="0" op="" dtype="" >1.1225e-01 2.8610e-06 8.1062e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.6628e-04 0.0000e+00 5.8985e-04</hent>
<hent key="02400100000000000000200000000000" call="MPI_Isend" bytes="8192" orank="0" region="0" commid="0" count="12550" tid="0" op="" dtype="" >7.0677e-03 0.0000e+00 3.7909e-05</hent>
<hent key="02400100000000000000200000000009" call="MPI_Isend" bytes="8192" orank="9" region="0" commid="0" count="3353" tid="0" op="" dtype="" >7.2925e-03 0.0000e+00 8.7976e-05</hent>
<hent key="0240010000000000000020000000000F" call="MPI_Isend" bytes="8192" orank="15" region="0" commid="0" count="3714" tid="0" op="" dtype="" >4.1323e-03 0.0000e+00 5.1975e-05</hent>
<hent key="02400100000000000000200000000010" call="MPI_Isend" bytes="8192" orank="16" region="0" commid="0" count="12624" tid="0" op="" dtype="" >7.7970e-03 0.0000e+00 4.2915e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.3127e-05 2.3127e-05 2.3127e-05</hent>
<hent key="02400100000000000000000000000000" call="MPI_Isend" bytes="0" orank="0" region="0" commid="0" count="254" tid="0" op="" dtype="" >1.9455e-04 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000000000000009" call="MPI_Isend" bytes="0" orank="9" region="0" commid="0" count="247" tid="0" op="" dtype="" >5.2810e-04 9.5367e-07 3.0994e-05</hent>
<hent key="0240010000000000000000000000000F" call="MPI_Isend" bytes="0" orank="15" region="0" commid="0" count="250" tid="0" op="" dtype="" >1.6403e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000000000010" call="MPI_Isend" bytes="0" orank="16" region="0" commid="0" count="234" tid="0" op="" dtype="" >1.7953e-04 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000000000000048" call="MPI_Isend" bytes="0" orank="72" region="0" commid="0" count="153" tid="0" op="" dtype="" >8.7500e-04 1.9073e-06 1.7166e-05</hent>
<hent key="024001000000000000000000000002C8" call="MPI_Isend" bytes="0" orank="712" region="0" commid="0" count="145" tid="0" op="" dtype="" >6.0296e-04 1.1921e-06 1.2159e-05</hent>
<hent key="03800100000000000000060000000000" call="MPI_Irecv" bytes="1536" orank="0" region="0" commid="0" count="105" tid="0" op="" dtype="" >1.5020e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000009" call="MPI_Irecv" bytes="1536" orank="9" region="0" commid="0" count="94" tid="0" op="" dtype="" >2.1935e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000F" call="MPI_Irecv" bytes="1536" orank="15" region="0" commid="0" count="82" tid="0" op="" dtype="" >2.2888e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000010" call="MPI_Irecv" bytes="1536" orank="16" region="0" commid="0" count="111" tid="0" op="" dtype="" >1.7643e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000048" call="MPI_Irecv" bytes="1536" orank="72" region="0" commid="0" count="216" tid="0" op="" dtype="" >1.0586e-04 0.0000e+00 1.0014e-05</hent>
<hent key="038001000000000000000600000002C8" call="MPI_Irecv" bytes="1536" orank="712" region="0" commid="0" count="206" tid="0" op="" dtype="" >1.2636e-04 0.0000e+00 1.9789e-05</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000000" call="MPI_Isend" bytes="1536" orank="0" region="0" commid="0" count="101" tid="0" op="" dtype="" >2.0671e-04 9.5367e-07 1.2875e-05</hent>
<hent key="02400100000000000000060000000009" call="MPI_Isend" bytes="1536" orank="9" region="0" commid="0" count="92" tid="0" op="" dtype="" >3.7456e-04 2.8610e-06 1.8120e-05</hent>
<hent key="0240010000000000000006000000000F" call="MPI_Isend" bytes="1536" orank="15" region="0" commid="0" count="94" tid="0" op="" dtype="" >1.9097e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000010" call="MPI_Isend" bytes="1536" orank="16" region="0" commid="0" count="110" tid="0" op="" dtype="" >2.0027e-04 9.5367e-07 1.5020e-05</hent>
<hent key="02400100000000000000060000000048" call="MPI_Isend" bytes="1536" orank="72" region="0" commid="0" count="222" tid="0" op="" dtype="" >1.5674e-03 3.8147e-06 2.0981e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C8" call="MPI_Isend" bytes="1536" orank="712" region="0" commid="0" count="246" tid="0" op="" dtype="" >1.3058e-03 3.0994e-06 1.6212e-05</hent>
<hent key="038001000000000000000C0000000009" call="MPI_Irecv" bytes="3072" orank="9" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C0000000048" call="MPI_Irecv" bytes="3072" orank="72" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C8" call="MPI_Irecv" bytes="3072" orank="712" region="0" commid="0" count="16" tid="0" op="" dtype="" >6.6757e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C0000000000" call="MPI_Isend" bytes="3072" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.7881e-05 1.7881e-05 1.7881e-05</hent>
<hent key="024001000000000000000C0000000048" call="MPI_Isend" bytes="3072" orank="72" region="0" commid="0" count="4" tid="0" op="" dtype="" >4.8876e-05 5.9605e-06 2.3842e-05</hent>
<hent key="024001000000000000000C00000002C8" call="MPI_Isend" bytes="3072" orank="712" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.1008e-05 5.0068e-06 9.0599e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9884e-04 1.9884e-04 1.9884e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.3416e-04 1.3995e-04 1.4806e-04</hent>
<hent key="03800100000000000000038000000000" call="MPI_Irecv" bytes="896" orank="0" region="0" commid="0" count="346" tid="0" op="" dtype="" >6.8188e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000009" call="MPI_Irecv" bytes="896" orank="9" region="0" commid="0" count="2662" tid="0" op="" dtype="" >4.4727e-04 0.0000e+00 6.1989e-06</hent>
<hent key="0380010000000000000003800000000F" call="MPI_Irecv" bytes="896" orank="15" region="0" commid="0" count="2896" tid="0" op="" dtype="" >4.3130e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000038000000010" call="MPI_Irecv" bytes="896" orank="16" region="0" commid="0" count="311" tid="0" op="" dtype="" >5.9366e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000048" call="MPI_Irecv" bytes="896" orank="72" region="0" commid="0" count="342" tid="0" op="" dtype="" >1.3733e-04 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000380000002C8" call="MPI_Irecv" bytes="896" orank="712" region="0" commid="0" count="316" tid="0" op="" dtype="" >1.2469e-04 0.0000e+00 1.0014e-05</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.4080e-05 2.4080e-05 2.4080e-05</hent>
<hent key="03800100000000000000380000000048" call="MPI_Irecv" bytes="14336" orank="72" region="0" commid="0" count="225" tid="0" op="" dtype="" >1.5759e-04 0.0000e+00 1.9073e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="038001000000000000003800000002C8" call="MPI_Irecv" bytes="14336" orank="712" region="0" commid="0" count="27" tid="0" op="" dtype="" >3.4094e-05 0.0000e+00 2.1219e-05</hent>
<hent key="02400100000000000000038000000000" call="MPI_Isend" bytes="896" orank="0" region="0" commid="0" count="311" tid="0" op="" dtype="" >4.5228e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000038000000009" call="MPI_Isend" bytes="896" orank="9" region="0" commid="0" count="2833" tid="0" op="" dtype="" >3.1261e-03 0.0000e+00 1.5020e-05</hent>
<hent key="0240010000000000000003800000000F" call="MPI_Isend" bytes="896" orank="15" region="0" commid="0" count="2695" tid="0" op="" dtype="" >1.8690e-03 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000038000000010" call="MPI_Isend" bytes="896" orank="16" region="0" commid="0" count="365" tid="0" op="" dtype="" >5.2547e-04 9.5367e-07 1.2875e-05</hent>
<hent key="02400100000000000000038000000048" call="MPI_Isend" bytes="896" orank="72" region="0" commid="0" count="358" tid="0" op="" dtype="" >2.3599e-03 3.8147e-06 2.8849e-05</hent>
<hent key="024001000000000000000380000002C8" call="MPI_Isend" bytes="896" orank="712" region="0" commid="0" count="302" tid="0" op="" dtype="" >1.5512e-03 2.8610e-06 1.8120e-05</hent>
<hent key="02400100000000000000380000000048" call="MPI_Isend" bytes="14336" orank="72" region="0" commid="0" count="113" tid="0" op="" dtype="" >1.9960e-03 5.0068e-06 5.8889e-05</hent>
<hent key="024001000000000000003800000002C8" call="MPI_Isend" bytes="14336" orank="712" region="0" commid="0" count="237" tid="0" op="" dtype="" >2.3828e-03 3.8147e-06 7.4863e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.4059e+00 6.9141e-06 1.3911e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.5385e-04 5.5385e-04 5.5385e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >2.7490e-04 2.7490e-04 2.7490e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >1.3421e-03 1.3421e-03 1.3421e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >7.7256e-02 2.9516e-04 7.2718e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.6100e-04 5.6100e-04 5.6100e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 9.5367e-07 1.1921e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6135e+00 4.4513e-04 2.5662e-01</hent>
<hent key="03800100000000000000040000000000" call="MPI_Irecv" bytes="1024" orank="0" region="0" commid="0" count="3314" tid="0" op="" dtype="" >4.0698e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000040000000009" call="MPI_Irecv" bytes="1024" orank="9" region="0" commid="0" count="1026" tid="0" op="" dtype="" >2.0456e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000000F" call="MPI_Irecv" bytes="1024" orank="15" region="0" commid="0" count="846" tid="0" op="" dtype="" >1.0777e-04 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000040000000010" call="MPI_Irecv" bytes="1024" orank="16" region="0" commid="0" count="3380" tid="0" op="" dtype="" >6.8641e-04 0.0000e+00 3.5048e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >7.8249e-04 1.9073e-06 4.4584e-04</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000070000000000" call="MPI_Irecv" bytes="1792" orank="0" region="0" commid="0" count="37" tid="0" op="" dtype="" >6.6757e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000009" call="MPI_Irecv" bytes="1792" orank="9" region="0" commid="0" count="43" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000F" call="MPI_Irecv" bytes="1792" orank="15" region="0" commid="0" count="40" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000010" call="MPI_Irecv" bytes="1792" orank="16" region="0" commid="0" count="27" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000048" call="MPI_Irecv" bytes="1792" orank="72" region="0" commid="0" count="193" tid="0" op="" dtype="" >8.5831e-05 0.0000e+00 1.6928e-05</hent>
<hent key="038001000000000000000700000002C8" call="MPI_Irecv" bytes="1792" orank="712" region="0" commid="0" count="153" tid="0" op="" dtype="" >6.8188e-05 0.0000e+00 5.0068e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >7.5102e-05 0.0000e+00 4.9114e-05</hent>
<hent key="02400100000000000000040000000000" call="MPI_Isend" bytes="1024" orank="0" region="0" commid="0" count="3358" tid="0" op="" dtype="" >2.2483e-03 0.0000e+00 2.5988e-05</hent>
<hent key="02400100000000000000040000000009" call="MPI_Isend" bytes="1024" orank="9" region="0" commid="0" count="886" tid="0" op="" dtype="" >7.9107e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000004000000000F" call="MPI_Isend" bytes="1024" orank="15" region="0" commid="0" count="1008" tid="0" op="" dtype="" >5.8889e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000040000000010" call="MPI_Isend" bytes="1024" orank="16" region="0" commid="0" count="3378" tid="0" op="" dtype="" >1.9310e-03 0.0000e+00 1.8835e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >3.6955e-05 4.0531e-06 3.2902e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4451e+00 0.0000e+00 3.2496e+00</hent>
<hent key="038001000000000000000A0000000000" call="MPI_Irecv" bytes="2560" orank="0" region="0" commid="0" count="4" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000009" call="MPI_Irecv" bytes="2560" orank="9" region="0" commid="0" count="8" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A000000000F" call="MPI_Irecv" bytes="2560" orank="15" region="0" commid="0" count="6" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000010" call="MPI_Irecv" bytes="2560" orank="16" region="0" commid="0" count="7" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000048" call="MPI_Irecv" bytes="2560" orank="72" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.6212e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002C8" call="MPI_Irecv" bytes="2560" orank="712" region="0" commid="0" count="67" tid="0" op="" dtype="" >2.7180e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000000" call="MPI_Isend" bytes="1792" orank="0" region="0" commid="0" count="36" tid="0" op="" dtype="" >1.1587e-04 9.5367e-07 1.5020e-05</hent>
<hent key="02400100000000000000070000000009" call="MPI_Isend" bytes="1792" orank="9" region="0" commid="0" count="43" tid="0" op="" dtype="" >1.6785e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000007000000000F" call="MPI_Isend" bytes="1792" orank="15" region="0" commid="0" count="42" tid="0" op="" dtype="" >9.4891e-05 1.9073e-06 6.9141e-06</hent>
<hent key="02400100000000000000070000000010" call="MPI_Isend" bytes="1792" orank="16" region="0" commid="0" count="28" tid="0" op="" dtype="" >5.8174e-05 9.5367e-07 1.2875e-05</hent>
<hent key="02400100000000000000070000000048" call="MPI_Isend" bytes="1792" orank="72" region="0" commid="0" count="146" tid="0" op="" dtype="" >9.3198e-04 9.5367e-07 3.5048e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6146e-02 3.6146e-02 3.6146e-02</hent>
<hent key="024001000000000000000700000002C8" call="MPI_Isend" bytes="1792" orank="712" region="0" commid="0" count="221" tid="0" op="" dtype="" >1.0917e-03 9.5367e-07 2.3127e-05</hent>
<hent key="024001000000000000000A0000000000" call="MPI_Isend" bytes="2560" orank="0" region="0" commid="0" count="8" tid="0" op="" dtype="" >3.8147e-05 9.5367e-07 1.2875e-05</hent>
<hent key="024001000000000000000A0000000009" call="MPI_Isend" bytes="2560" orank="9" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.6226e-05 4.0531e-06 5.0068e-06</hent>
<hent key="024001000000000000000A000000000F" call="MPI_Isend" bytes="2560" orank="15" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.3828e-05 1.9073e-06 2.8610e-06</hent>
<hent key="024001000000000000000A0000000010" call="MPI_Isend" bytes="2560" orank="16" region="0" commid="0" count="10" tid="0" op="" dtype="" >6.1512e-05 9.5367e-07 1.4782e-05</hent>
<hent key="024001000000000000000A0000000048" call="MPI_Isend" bytes="2560" orank="72" region="0" commid="0" count="45" tid="0" op="" dtype="" >3.1614e-04 5.0068e-06 1.6928e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.3484e-04 3.3855e-05 1.0800e-04</hent>
<hent key="024001000000000000000A00000002C8" call="MPI_Isend" bytes="2560" orank="712" region="0" commid="0" count="58" tid="0" op="" dtype="" >3.4976e-04 3.8147e-06 1.2875e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >6.4373e-04 2.6584e-04 3.7789e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.0729e-05 1.9073e-06 8.8215e-06</hent>
<hent key="03800100000000000000000400000000" call="MPI_Irecv" bytes="4" orank="0" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.5037e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000000400000009" call="MPI_Irecv" bytes="4" orank="9" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.2094e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000000F" call="MPI_Irecv" bytes="4" orank="15" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1528e-03 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000000400000010" call="MPI_Irecv" bytes="4" orank="16" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.9523e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000000400000048" call="MPI_Irecv" bytes="4" orank="72" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0600e-03 0.0000e+00 1.5020e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.3535e-03 1.8311e-04 4.9019e-04</hent>
<hent key="038001000000000000001C0000000009" call="MPI_Irecv" bytes="7168" orank="9" region="0" commid="0" count="8851" tid="0" op="" dtype="" >1.5683e-03 0.0000e+00 1.5020e-05</hent>
<hent key="038001000000000000001C000000000F" call="MPI_Irecv" bytes="7168" orank="15" region="0" commid="0" count="9573" tid="0" op="" dtype="" >3.9515e-03 0.0000e+00 2.5034e-05</hent>
<hent key="038001000000000000000004000002C8" call="MPI_Irecv" bytes="4" orank="712" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2832e-03 0.0000e+00 3.5048e-05</hent>
<hent key="02400100000000000000000400000000" call="MPI_Isend" bytes="4" orank="0" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.5872e-03 0.0000e+00 1.1206e-05</hent>
<hent key="02400100000000000000000400000009" call="MPI_Isend" bytes="4" orank="9" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.7986e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000000040000000F" call="MPI_Isend" bytes="4" orank="15" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.5543e-03 0.0000e+00 4.0054e-05</hent>
<hent key="02400100000000000000000400000010" call="MPI_Isend" bytes="4" orank="16" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2910e-03 0.0000e+00 2.3842e-05</hent>
<hent key="02400100000000000000000400000048" call="MPI_Isend" bytes="4" orank="72" region="0" commid="0" count="3398" tid="0" op="" dtype="" >3.2768e-02 4.7684e-06 1.0729e-03</hent>
<hent key="024001000000000000001C0000000009" call="MPI_Isend" bytes="7168" orank="9" region="0" commid="0" count="9346" tid="0" op="" dtype="" >2.9958e-02 9.5367e-07 1.6928e-05</hent>
<hent key="024001000000000000001C000000000F" call="MPI_Isend" bytes="7168" orank="15" region="0" commid="0" count="8985" tid="0" op="" dtype="" >2.0972e-02 9.5367e-07 3.0994e-05</hent>
<hent key="024001000000000000000004000002C8" call="MPI_Isend" bytes="4" orank="712" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3936e-02 3.8147e-06 1.2112e-04</hent>
<hent key="03800100000000000000050000000000" call="MPI_Irecv" bytes="1280" orank="0" region="0" commid="0" count="269" tid="0" op="" dtype="" >5.1498e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000009" call="MPI_Irecv" bytes="1280" orank="9" region="0" commid="0" count="213" tid="0" op="" dtype="" >4.0531e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000F" call="MPI_Irecv" bytes="1280" orank="15" region="0" commid="0" count="171" tid="0" op="" dtype="" >6.0558e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000050000000010" call="MPI_Irecv" bytes="1280" orank="16" region="0" commid="0" count="238" tid="0" op="" dtype="" >3.8624e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000048" call="MPI_Irecv" bytes="1280" orank="72" region="0" commid="0" count="303" tid="0" op="" dtype="" >1.2827e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0843e+01 1.0967e-05 1.2808e-01</hent>
<hent key="03800100000000000000280000000000" call="MPI_Irecv" bytes="10240" orank="0" region="0" commid="0" count="288" tid="0" op="" dtype="" >3.7909e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000280000000010" call="MPI_Irecv" bytes="10240" orank="16" region="0" commid="0" count="69" tid="0" op="" dtype="" >1.1206e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002C8" call="MPI_Irecv" bytes="1280" orank="712" region="0" commid="0" count="282" tid="0" op="" dtype="" >1.3089e-04 0.0000e+00 1.1921e-05</hent>
<hent key="03800100000000000000080000000000" call="MPI_Irecv" bytes="2048" orank="0" region="0" commid="0" count="12" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000009" call="MPI_Irecv" bytes="2048" orank="9" region="0" commid="0" count="21" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000F" call="MPI_Irecv" bytes="2048" orank="15" region="0" commid="0" count="20" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000010" call="MPI_Irecv" bytes="2048" orank="16" region="0" commid="0" count="21" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000048" call="MPI_Irecv" bytes="2048" orank="72" region="0" commid="0" count="3405" tid="0" op="" dtype="" >7.4339e-04 0.0000e+00 2.0981e-05</hent>
<hent key="038001000000000000000800000002C8" call="MPI_Irecv" bytes="2048" orank="712" region="0" commid="0" count="3455" tid="0" op="" dtype="" >1.1766e-03 0.0000e+00 5.4836e-05</hent>
<hent key="02400100000000000000050000000000" call="MPI_Isend" bytes="1280" orank="0" region="0" commid="0" count="230" tid="0" op="" dtype="" >3.6263e-04 0.0000e+00 1.8120e-05</hent>
<hent key="02400100000000000000050000000009" call="MPI_Isend" bytes="1280" orank="9" region="0" commid="0" count="201" tid="0" op="" dtype="" >7.3433e-04 1.9073e-06 1.3113e-05</hent>
<hent key="0240010000000000000005000000000F" call="MPI_Isend" bytes="1280" orank="15" region="0" commid="0" count="202" tid="0" op="" dtype="" >3.7289e-04 9.5367e-07 7.1526e-06</hent>
<hent key="02400100000000000000050000000010" call="MPI_Isend" bytes="1280" orank="16" region="0" commid="0" count="236" tid="0" op="" dtype="" >3.1900e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000050000000048" call="MPI_Isend" bytes="1280" orank="72" region="0" commid="0" count="293" tid="0" op="" dtype="" >1.9736e-03 3.8147e-06 2.0981e-05</hent>
<hent key="02400100000000000000280000000000" call="MPI_Isend" bytes="10240" orank="0" region="0" commid="0" count="149" tid="0" op="" dtype="" >1.1086e-04 0.0000e+00 1.2159e-05</hent>
<hent key="02400100000000000000280000000010" call="MPI_Isend" bytes="10240" orank="16" region="0" commid="0" count="75" tid="0" op="" dtype="" >3.5763e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000500000002C8" call="MPI_Isend" bytes="1280" orank="712" region="0" commid="0" count="268" tid="0" op="" dtype="" >1.4687e-03 2.8610e-06 1.9789e-05</hent>
</hash>
<internal rank="8" log_i="1723713849.510183" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="9" mpi_size="768" stamp_init="1723713791.101398" stamp_final="1723713849.503008" username="apac4" allocationname="unknown" flags="0" pid="1717072" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.84016e+01" utime="5.02293e+01" stime="6.81679e+00" mtime="3.23269e+01" gflop="0.00000e+00" gbyte="3.76797e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23269e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4471448144a143f564a144914a5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83308e+01" utime="5.02012e+01" stime="6.80501e+00" mtime="3.23269e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23269e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4856e+08" > 3.4574e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4867e+08" > 2.9720e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6801e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4544e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1696e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4094e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6133e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.2151e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8181e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6181e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9164e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="194" >
<hent key="02400100000000000000080000000001" call="MPI_Isend" bytes="2048" orank="1" region="0" commid="0" count="12" tid="0" op="" dtype="" >2.0742e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000080000000008" call="MPI_Isend" bytes="2048" orank="8" region="0" commid="0" count="21" tid="0" op="" dtype="" >4.3392e-05 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000008000000000A" call="MPI_Isend" bytes="2048" orank="10" region="0" commid="0" count="14" tid="0" op="" dtype="" >5.2214e-05 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000080000000011" call="MPI_Isend" bytes="2048" orank="17" region="0" commid="0" count="12" tid="0" op="" dtype="" >3.1948e-05 9.5367e-07 1.1206e-05</hent>
<hent key="02400100000000000000080000000049" call="MPI_Isend" bytes="2048" orank="73" region="0" commid="0" count="3465" tid="0" op="" dtype="" >1.1358e-02 9.5367e-07 4.1962e-05</hent>
<hent key="024001000000000000000800000002C9" call="MPI_Isend" bytes="2048" orank="713" region="0" commid="0" count="3473" tid="0" op="" dtype="" >1.0773e-02 9.5367e-07 1.5020e-05</hent>
<hent key="03800100000000000000028000000001" call="MPI_Irecv" bytes="640" orank="1" region="0" commid="0" count="390" tid="0" op="" dtype="" >1.6427e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000008" call="MPI_Irecv" bytes="640" orank="8" region="0" commid="0" count="395" tid="0" op="" dtype="" >1.7023e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000A" call="MPI_Irecv" bytes="640" orank="10" region="0" commid="0" count="401" tid="0" op="" dtype="" >1.0848e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000011" call="MPI_Irecv" bytes="640" orank="17" region="0" commid="0" count="432" tid="0" op="" dtype="" >7.8201e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000049" call="MPI_Irecv" bytes="640" orank="73" region="0" commid="0" count="265" tid="0" op="" dtype="" >7.3433e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000000280000002C9" call="MPI_Irecv" bytes="640" orank="713" region="0" commid="0" count="301" tid="0" op="" dtype="" >1.0538e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000001" call="MPI_Isend" bytes="640" orank="1" region="0" commid="0" count="392" tid="0" op="" dtype="" >4.4227e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000028000000008" call="MPI_Isend" bytes="640" orank="8" region="0" commid="0" count="406" tid="0" op="" dtype="" >5.0354e-04 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000002800000000A" call="MPI_Isend" bytes="640" orank="10" region="0" commid="0" count="412" tid="0" op="" dtype="" >1.1873e-03 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000028000000011" call="MPI_Isend" bytes="640" orank="17" region="0" commid="0" count="417" tid="0" op="" dtype="" >5.1570e-04 0.0000e+00 4.0054e-05</hent>
<hent key="02400100000000000000028000000049" call="MPI_Isend" bytes="640" orank="73" region="0" commid="0" count="284" tid="0" op="" dtype="" >1.3454e-03 3.8147e-06 8.8215e-06</hent>
<hent key="024001000000000000000280000002C9" call="MPI_Isend" bytes="640" orank="713" region="0" commid="0" count="258" tid="0" op="" dtype="" >1.0600e-03 2.8610e-06 6.1989e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1206e-05 1.1206e-05 1.1206e-05</hent>
<hent key="03800100000000000000014000000001" call="MPI_Irecv" bytes="320" orank="1" region="0" commid="0" count="355" tid="0" op="" dtype="" >1.4567e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000008" call="MPI_Irecv" bytes="320" orank="8" region="0" commid="0" count="382" tid="0" op="" dtype="" >1.4997e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000A" call="MPI_Irecv" bytes="320" orank="10" region="0" commid="0" count="342" tid="0" op="" dtype="" >7.6294e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000011" call="MPI_Irecv" bytes="320" orank="17" region="0" commid="0" count="359" tid="0" op="" dtype="" >7.0810e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000014000000049" call="MPI_Irecv" bytes="320" orank="73" region="0" commid="0" count="171" tid="0" op="" dtype="" >4.1246e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002C9" call="MPI_Irecv" bytes="320" orank="713" region="0" commid="0" count="193" tid="0" op="" dtype="" >6.6519e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.8147e-06 0.0000e+00 2.8610e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.6801e+00 0.0000e+00 1.3105e-01</hent>
<hent key="03800100000000000000400000000049" call="MPI_Irecv" bytes="16384" orank="73" region="0" commid="0" count="12676" tid="0" op="" dtype="" >7.4918e-03 0.0000e+00 8.1062e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.8835e-05 0.0000e+00 1.7881e-05</hent>
<hent key="038001000000000000004000000002C9" call="MPI_Irecv" bytes="16384" orank="713" region="0" commid="0" count="12699" tid="0" op="" dtype="" >2.0168e-03 0.0000e+00 1.0014e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000001" call="MPI_Isend" bytes="320" orank="1" region="0" commid="0" count="370" tid="0" op="" dtype="" >3.4428e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000014000000008" call="MPI_Isend" bytes="320" orank="8" region="0" commid="0" count="365" tid="0" op="" dtype="" >3.5262e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000001400000000A" call="MPI_Isend" bytes="320" orank="10" region="0" commid="0" count="344" tid="0" op="" dtype="" >8.5521e-04 9.5367e-07 7.1526e-06</hent>
<hent key="02400100000000000000014000000011" call="MPI_Isend" bytes="320" orank="17" region="0" commid="0" count="368" tid="0" op="" dtype="" >3.5167e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000014000000049" call="MPI_Isend" bytes="320" orank="73" region="0" commid="0" count="166" tid="0" op="" dtype="" >7.7748e-04 3.0994e-06 1.7166e-05</hent>
<hent key="03800100000000000000200000000001" call="MPI_Irecv" bytes="8192" orank="1" region="0" commid="0" count="12632" tid="0" op="" dtype="" >2.8396e-03 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000200000000008" call="MPI_Irecv" bytes="8192" orank="8" region="0" commid="0" count="3353" tid="0" op="" dtype="" >1.1127e-03 0.0000e+00 3.0994e-06</hent>
<hent key="0380010000000000000020000000000A" call="MPI_Irecv" bytes="8192" orank="10" region="0" commid="0" count="3181" tid="0" op="" dtype="" >4.3941e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000200000000011" call="MPI_Irecv" bytes="8192" orank="17" region="0" commid="0" count="12626" tid="0" op="" dtype="" >1.3607e-03 0.0000e+00 4.0531e-06</hent>
<hent key="024001000000000000000140000002C9" call="MPI_Isend" bytes="320" orank="713" region="0" commid="0" count="191" tid="0" op="" dtype="" >7.7152e-04 2.8610e-06 6.1989e-06</hent>
<hent key="03800100000000000000000000000001" call="MPI_Irecv" bytes="0" orank="1" region="0" commid="0" count="265" tid="0" op="" dtype="" >7.9870e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000008" call="MPI_Irecv" bytes="0" orank="8" region="0" commid="0" count="247" tid="0" op="" dtype="" >7.8440e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000000000000000A" call="MPI_Irecv" bytes="0" orank="10" region="0" commid="0" count="242" tid="0" op="" dtype="" >5.1260e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000011" call="MPI_Irecv" bytes="0" orank="17" region="0" commid="0" count="254" tid="0" op="" dtype="" >3.9816e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000049" call="MPI_Irecv" bytes="0" orank="73" region="0" commid="0" count="165" tid="0" op="" dtype="" >3.4809e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000049" call="MPI_Isend" bytes="16384" orank="73" region="0" commid="0" count="12693" tid="0" op="" dtype="" >1.0215e-01 3.8147e-06 5.8889e-05</hent>
<hent key="038001000000000000000000000002C9" call="MPI_Irecv" bytes="0" orank="713" region="0" commid="0" count="140" tid="0" op="" dtype="" >3.2425e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002C9" call="MPI_Isend" bytes="16384" orank="713" region="0" commid="0" count="12699" tid="0" op="" dtype="" >7.3334e-02 2.8610e-06 1.7166e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.9417e-04 0.0000e+00 6.2609e-04</hent>
<hent key="02400100000000000000200000000001" call="MPI_Isend" bytes="8192" orank="1" region="0" commid="0" count="12684" tid="0" op="" dtype="" >2.9738e-03 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000200000000008" call="MPI_Isend" bytes="8192" orank="8" region="0" commid="0" count="3848" tid="0" op="" dtype="" >4.1099e-03 0.0000e+00 1.5020e-05</hent>
<hent key="0240010000000000000020000000000A" call="MPI_Isend" bytes="8192" orank="10" region="0" commid="0" count="2784" tid="0" op="" dtype="" >5.3926e-03 0.0000e+00 1.4997e-04</hent>
<hent key="02400100000000000000200000000011" call="MPI_Isend" bytes="8192" orank="17" region="0" commid="0" count="12691" tid="0" op="" dtype="" >7.4174e-03 0.0000e+00 5.6982e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1921e-05 1.1921e-05 1.1921e-05</hent>
<hent key="02400100000000000000000000000001" call="MPI_Isend" bytes="0" orank="1" region="0" commid="0" count="289" tid="0" op="" dtype="" >1.9431e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000000000000008" call="MPI_Isend" bytes="0" orank="8" region="0" commid="0" count="258" tid="0" op="" dtype="" >1.5306e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000000000000000A" call="MPI_Isend" bytes="0" orank="10" region="0" commid="0" count="259" tid="0" op="" dtype="" >5.0306e-04 9.5367e-07 5.6982e-05</hent>
<hent key="02400100000000000000000000000011" call="MPI_Isend" bytes="0" orank="17" region="0" commid="0" count="242" tid="0" op="" dtype="" >1.7357e-04 0.0000e+00 8.8215e-06</hent>
<hent key="02400100000000000000000000000049" call="MPI_Isend" bytes="0" orank="73" region="0" commid="0" count="148" tid="0" op="" dtype="" >5.6624e-04 9.5367e-07 9.0599e-06</hent>
<hent key="024001000000000000000000000002C9" call="MPI_Isend" bytes="0" orank="713" region="0" commid="0" count="149" tid="0" op="" dtype="" >5.0902e-04 9.5367e-07 5.0068e-06</hent>
<hent key="03800100000000000000060000000001" call="MPI_Irecv" bytes="1536" orank="1" region="0" commid="0" count="104" tid="0" op="" dtype="" >4.8876e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000060000000008" call="MPI_Irecv" bytes="1536" orank="8" region="0" commid="0" count="92" tid="0" op="" dtype="" >3.7432e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000A" call="MPI_Irecv" bytes="1536" orank="10" region="0" commid="0" count="104" tid="0" op="" dtype="" >2.6941e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000011" call="MPI_Irecv" bytes="1536" orank="17" region="0" commid="0" count="94" tid="0" op="" dtype="" >2.2888e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000049" call="MPI_Irecv" bytes="1536" orank="73" region="0" commid="0" count="225" tid="0" op="" dtype="" >6.6042e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002C9" call="MPI_Irecv" bytes="1536" orank="713" region="0" commid="0" count="218" tid="0" op="" dtype="" >6.6280e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000001" call="MPI_Isend" bytes="1536" orank="1" region="0" commid="0" count="113" tid="0" op="" dtype="" >2.0409e-04 9.5367e-07 1.1921e-05</hent>
<hent key="02400100000000000000060000000008" call="MPI_Isend" bytes="1536" orank="8" region="0" commid="0" count="94" tid="0" op="" dtype="" >1.8048e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000006000000000A" call="MPI_Isend" bytes="1536" orank="10" region="0" commid="0" count="95" tid="0" op="" dtype="" >3.3236e-04 2.8610e-06 7.1526e-06</hent>
<hent key="02400100000000000000060000000011" call="MPI_Isend" bytes="1536" orank="17" region="0" commid="0" count="99" tid="0" op="" dtype="" >2.1315e-04 9.5367e-07 1.2875e-05</hent>
<hent key="02400100000000000000060000000049" call="MPI_Isend" bytes="1536" orank="73" region="0" commid="0" count="214" tid="0" op="" dtype="" >1.1113e-03 3.8147e-06 1.1921e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C9" call="MPI_Isend" bytes="1536" orank="713" region="0" commid="0" count="214" tid="0" op="" dtype="" >9.7370e-04 2.8610e-06 1.0014e-05</hent>
<hent key="038001000000000000000C000000000A" call="MPI_Irecv" bytes="3072" orank="10" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C0000000011" call="MPI_Irecv" bytes="3072" orank="17" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C0000000049" call="MPI_Irecv" bytes="3072" orank="73" region="0" commid="0" count="5" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C9" call="MPI_Irecv" bytes="3072" orank="713" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C0000000008" call="MPI_Isend" bytes="3072" orank="8" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000C0000000049" call="MPI_Isend" bytes="3072" orank="73" region="0" commid="0" count="6" tid="0" op="" dtype="" >4.0054e-05 5.0068e-06 1.0967e-05</hent>
<hent key="024001000000000000000C00000002C9" call="MPI_Isend" bytes="3072" orank="713" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.5034e-05 5.0068e-06 5.0068e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.0599e-04 2.0599e-04 2.0599e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.7588e-04 1.5593e-04 1.5998e-04</hent>
<hent key="03800100000000000000038000000001" call="MPI_Irecv" bytes="896" orank="1" region="0" commid="0" count="353" tid="0" op="" dtype="" >1.4949e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000008" call="MPI_Irecv" bytes="896" orank="8" region="0" commid="0" count="2833" tid="0" op="" dtype="" >5.3692e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000003800000000A" call="MPI_Irecv" bytes="896" orank="10" region="0" commid="0" count="2913" tid="0" op="" dtype="" >5.8436e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000011" call="MPI_Irecv" bytes="896" orank="17" region="0" commid="0" count="315" tid="0" op="" dtype="" >5.9605e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000049" call="MPI_Irecv" bytes="896" orank="73" region="0" commid="0" count="332" tid="0" op="" dtype="" >8.2254e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002C9" call="MPI_Irecv" bytes="896" orank="713" region="0" commid="0" count="326" tid="0" op="" dtype="" >1.1587e-04 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.0081e-05 6.0081e-05 6.0081e-05</hent>
<hent key="03800100000000000000380000000049" call="MPI_Irecv" bytes="14336" orank="73" region="0" commid="0" count="23" tid="0" op="" dtype="" >1.4067e-05 0.0000e+00 3.0994e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.1921e-06 1.1921e-06 1.1921e-06</hent>
<hent key="02400100000000000000038000000001" call="MPI_Isend" bytes="896" orank="1" region="0" commid="0" count="311" tid="0" op="" dtype="" >4.4513e-04 9.5367e-07 7.8678e-06</hent>
<hent key="02400100000000000000038000000008" call="MPI_Isend" bytes="896" orank="8" region="0" commid="0" count="2662" tid="0" op="" dtype="" >1.7688e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000003800000000A" call="MPI_Isend" bytes="896" orank="10" region="0" commid="0" count="2965" tid="0" op="" dtype="" >2.7804e-03 0.0000e+00 1.3828e-05</hent>
<hent key="02400100000000000000038000000011" call="MPI_Isend" bytes="896" orank="17" region="0" commid="0" count="349" tid="0" op="" dtype="" >4.7779e-04 9.5367e-07 1.0014e-05</hent>
<hent key="02400100000000000000038000000049" call="MPI_Isend" bytes="896" orank="73" region="0" commid="0" count="324" tid="0" op="" dtype="" >1.6062e-03 3.8147e-06 1.2875e-05</hent>
<hent key="024001000000000000000380000002C9" call="MPI_Isend" bytes="896" orank="713" region="0" commid="0" count="330" tid="0" op="" dtype="" >1.3885e-03 2.8610e-06 6.9141e-06</hent>
<hent key="02400100000000000000380000000049" call="MPI_Isend" bytes="14336" orank="73" region="0" commid="0" count="6" tid="0" op="" dtype="" >4.5300e-05 5.0068e-06 1.2159e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.1906e+00 6.9141e-06 1.3921e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.2810e-04 5.2810e-04 5.2810e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >2.8181e-04 2.8181e-04 2.8181e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >1.7591e-03 1.7591e-03 1.7591e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >6.6384e-02 3.3498e-04 6.1447e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.8889e-04 5.8889e-04 5.8889e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6133e+00 4.5395e-04 2.5657e-01</hent>
<hent key="03800100000000000000040000000001" call="MPI_Irecv" bytes="1024" orank="1" region="0" commid="0" count="3378" tid="0" op="" dtype="" >4.7612e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000008" call="MPI_Irecv" bytes="1024" orank="8" region="0" commid="0" count="886" tid="0" op="" dtype="" >1.4472e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000000A" call="MPI_Irecv" bytes="1024" orank="10" region="0" commid="0" count="830" tid="0" op="" dtype="" >1.6427e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000011" call="MPI_Irecv" bytes="1024" orank="17" region="0" commid="0" count="3380" tid="0" op="" dtype="" >4.5633e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.1696e-05 0.0000e+00 9.0599e-06</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0014e-05 1.0014e-05 1.0014e-05</hent>
<hent key="03800100000000000000070000000001" call="MPI_Irecv" bytes="1792" orank="1" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.1458e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000008" call="MPI_Irecv" bytes="1792" orank="8" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.4557e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000A" call="MPI_Irecv" bytes="1792" orank="10" region="0" commid="0" count="42" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000011" call="MPI_Irecv" bytes="1792" orank="17" region="0" commid="0" count="44" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000049" call="MPI_Irecv" bytes="1792" orank="73" region="0" commid="0" count="139" tid="0" op="" dtype="" >3.8862e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002C9" call="MPI_Irecv" bytes="1792" orank="713" region="0" commid="0" count="116" tid="0" op="" dtype="" >3.8147e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >9.7036e-05 0.0000e+00 5.1022e-05</hent>
<hent key="02400100000000000000040000000001" call="MPI_Isend" bytes="1024" orank="1" region="0" commid="0" count="3394" tid="0" op="" dtype="" >1.6348e-03 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000040000000008" call="MPI_Isend" bytes="1024" orank="8" region="0" commid="0" count="1026" tid="0" op="" dtype="" >6.2418e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000004000000000A" call="MPI_Isend" bytes="1024" orank="10" region="0" commid="0" count="758" tid="0" op="" dtype="" >4.8018e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000040000000011" call="MPI_Isend" bytes="1024" orank="17" region="0" commid="0" count="3396" tid="0" op="" dtype="" >1.7307e-03 0.0000e+00 1.0014e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >3.0041e-05 4.0531e-06 2.5988e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4534e+00 0.0000e+00 3.2505e+00</hent>
<hent key="038001000000000000000A0000000001" call="MPI_Irecv" bytes="2560" orank="1" region="0" commid="0" count="5" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000008" call="MPI_Irecv" bytes="2560" orank="8" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000000A" call="MPI_Irecv" bytes="2560" orank="10" region="0" commid="0" count="8" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000011" call="MPI_Irecv" bytes="2560" orank="17" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000049" call="MPI_Irecv" bytes="2560" orank="73" region="0" commid="0" count="44" tid="0" op="" dtype="" >1.2875e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002C9" call="MPI_Irecv" bytes="2560" orank="713" region="0" commid="0" count="41" tid="0" op="" dtype="" >7.3910e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000001" call="MPI_Isend" bytes="1792" orank="1" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.0300e-04 9.5367e-07 1.2159e-05</hent>
<hent key="02400100000000000000070000000008" call="MPI_Isend" bytes="1792" orank="8" region="0" commid="0" count="43" tid="0" op="" dtype="" >8.8692e-05 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000007000000000A" call="MPI_Isend" bytes="1792" orank="10" region="0" commid="0" count="45" tid="0" op="" dtype="" >1.5855e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000070000000011" call="MPI_Isend" bytes="1792" orank="17" region="0" commid="0" count="34" tid="0" op="" dtype="" >5.9843e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000049" call="MPI_Isend" bytes="1792" orank="73" region="0" commid="0" count="137" tid="0" op="" dtype="" >7.2885e-04 2.1458e-06 1.0967e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6181e-02 3.6181e-02 3.6181e-02</hent>
<hent key="024001000000000000000700000002C9" call="MPI_Isend" bytes="1792" orank="713" region="0" commid="0" count="133" tid="0" op="" dtype="" >5.9867e-04 3.8147e-06 8.8215e-06</hent>
<hent key="024001000000000000000A0000000001" call="MPI_Isend" bytes="2560" orank="1" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.0967e-05 1.9073e-06 6.9141e-06</hent>
<hent key="024001000000000000000A0000000008" call="MPI_Isend" bytes="2560" orank="8" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.6689e-05 1.9073e-06 2.8610e-06</hent>
<hent key="024001000000000000000A000000000A" call="MPI_Isend" bytes="2560" orank="10" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.2902e-05 2.8610e-06 4.0531e-06</hent>
<hent key="024001000000000000000A0000000011" call="MPI_Isend" bytes="2560" orank="17" region="0" commid="0" count="2" tid="0" op="" dtype="" >2.3127e-05 1.0967e-05 1.2159e-05</hent>
<hent key="024001000000000000000A0000000049" call="MPI_Isend" bytes="2560" orank="73" region="0" commid="0" count="46" tid="0" op="" dtype="" >2.5439e-04 4.7684e-06 1.0014e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.5463e-04 3.4809e-05 1.1992e-04</hent>
<hent key="024001000000000000000A00000002C9" call="MPI_Isend" bytes="2560" orank="713" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.9479e-04 3.8147e-06 6.1989e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >7.4792e-04 2.9802e-04 4.4990e-04</hent>
<hent key="024001000000000000001000000002C9" call="MPI_Isend" bytes="4096" orank="713" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0041e-05 3.0041e-05 3.0041e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="03800100000000000000000400000001" call="MPI_Irecv" bytes="4" orank="1" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.0221e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000008" call="MPI_Irecv" bytes="4" orank="8" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.4530e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000000040000000A" call="MPI_Irecv" bytes="4" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.1856e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000011" call="MPI_Irecv" bytes="4" orank="17" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.9843e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000000400000049" call="MPI_Irecv" bytes="4" orank="73" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.7592e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.5259e-03 2.0289e-04 5.4407e-04</hent>
<hent key="038001000000000000001C0000000008" call="MPI_Irecv" bytes="7168" orank="8" region="0" commid="0" count="9346" tid="0" op="" dtype="" >3.1526e-03 0.0000e+00 3.0994e-06</hent>
<hent key="038001000000000000001C000000000A" call="MPI_Irecv" bytes="7168" orank="10" region="0" commid="0" count="9518" tid="0" op="" dtype="" >1.3585e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000004000002C9" call="MPI_Irecv" bytes="4" orank="713" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.3252e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000400000001" call="MPI_Isend" bytes="4" orank="1" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0276e-03 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000400000008" call="MPI_Isend" bytes="4" orank="8" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2655e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000000040000000A" call="MPI_Isend" bytes="4" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.8448e-03 0.0000e+00 1.0300e-04</hent>
<hent key="02400100000000000000000400000011" call="MPI_Isend" bytes="4" orank="17" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6403e-03 0.0000e+00 2.7895e-05</hent>
<hent key="02400100000000000000000400000049" call="MPI_Isend" bytes="4" orank="73" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3244e-02 4.7684e-06 2.5034e-05</hent>
<hent key="024001000000000000001C0000000008" call="MPI_Isend" bytes="7168" orank="8" region="0" commid="0" count="8851" tid="0" op="" dtype="" >1.8308e-02 9.5367e-07 1.3113e-05</hent>
<hent key="024001000000000000001C000000000A" call="MPI_Isend" bytes="7168" orank="10" region="0" commid="0" count="9915" tid="0" op="" dtype="" >2.9168e-02 9.5367e-07 4.6015e-05</hent>
<hent key="024001000000000000000004000002C9" call="MPI_Isend" bytes="4" orank="713" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7193e-02 2.8610e-06 1.0896e-04</hent>
<hent key="03800100000000000000050000000001" call="MPI_Irecv" bytes="1280" orank="1" region="0" commid="0" count="194" tid="0" op="" dtype="" >8.5831e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000008" call="MPI_Irecv" bytes="1280" orank="8" region="0" commid="0" count="201" tid="0" op="" dtype="" >8.0585e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000A" call="MPI_Irecv" bytes="1280" orank="10" region="0" commid="0" count="196" tid="0" op="" dtype="" >4.1008e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000011" call="MPI_Irecv" bytes="1280" orank="17" region="0" commid="0" count="201" tid="0" op="" dtype="" >3.9101e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000049" call="MPI_Irecv" bytes="1280" orank="73" region="0" commid="0" count="291" tid="0" op="" dtype="" >7.3195e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0904e+01 8.1062e-06 1.2809e-01</hent>
<hent key="03800100000000000000280000000001" call="MPI_Irecv" bytes="10240" orank="1" region="0" commid="0" count="67" tid="0" op="" dtype="" >2.1696e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000280000000011" call="MPI_Irecv" bytes="10240" orank="17" region="0" commid="0" count="73" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002C9" call="MPI_Irecv" bytes="1280" orank="713" region="0" commid="0" count="272" tid="0" op="" dtype="" >9.2030e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000001" call="MPI_Irecv" bytes="2048" orank="1" region="0" commid="0" count="13" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000080000000008" call="MPI_Irecv" bytes="2048" orank="8" region="0" commid="0" count="12" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000A" call="MPI_Irecv" bytes="2048" orank="10" region="0" commid="0" count="18" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000011" call="MPI_Irecv" bytes="2048" orank="17" region="0" commid="0" count="15" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000049" call="MPI_Irecv" bytes="2048" orank="73" region="0" commid="0" count="3460" tid="0" op="" dtype="" >5.3763e-04 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000800000002C9" call="MPI_Irecv" bytes="2048" orank="713" region="0" commid="0" count="3484" tid="0" op="" dtype="" >5.8818e-04 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000050000000001" call="MPI_Isend" bytes="1280" orank="1" region="0" commid="0" count="164" tid="0" op="" dtype="" >2.5272e-04 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000050000000008" call="MPI_Isend" bytes="1280" orank="8" region="0" commid="0" count="213" tid="0" op="" dtype="" >3.8290e-04 9.5367e-07 6.1989e-06</hent>
<hent key="0240010000000000000005000000000A" call="MPI_Isend" bytes="1280" orank="10" region="0" commid="0" count="196" tid="0" op="" dtype="" >6.4588e-04 1.9073e-06 5.0068e-06</hent>
<hent key="02400100000000000000050000000011" call="MPI_Isend" bytes="1280" orank="17" region="0" commid="0" count="178" tid="0" op="" dtype="" >2.6608e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000050000000049" call="MPI_Isend" bytes="1280" orank="73" region="0" commid="0" count="307" tid="0" op="" dtype="" >1.5635e-03 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000280000000001" call="MPI_Isend" bytes="10240" orank="1" region="0" commid="0" count="15" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000280000000011" call="MPI_Isend" bytes="10240" orank="17" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000500000002C9" call="MPI_Isend" bytes="1280" orank="713" region="0" commid="0" count="302" tid="0" op="" dtype="" >1.3309e-03 2.8610e-06 2.0027e-05</hent>
</hash>
<internal rank="9" log_i="1723713849.503008" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="10" mpi_size="768" stamp_init="1723713791.104485" stamp_final="1723713849.507450" username="apac4" allocationname="unknown" flags="0" pid="1717073" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.84030e+01" utime="4.83595e+01" stime="7.60460e+00" mtime="3.21506e+01" gflop="0.00000e+00" gbyte="3.76217e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21506e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83306e+01" utime="4.83306e+01" stime="7.59271e+00" mtime="3.21506e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21506e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.5763e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4883e+08" > 4.4463e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4877e+08" > 4.6067e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6158e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4362e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.7224e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4094e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6129e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.4704e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4189e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6147e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8954e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="190" >
<hent key="02400100000000000000080000000002" call="MPI_Isend" bytes="2048" orank="2" region="0" commid="0" count="11" tid="0" op="" dtype="" >4.2200e-05 9.5367e-07 1.4067e-05</hent>
<hent key="02400100000000000000080000000009" call="MPI_Isend" bytes="2048" orank="9" region="0" commid="0" count="18" tid="0" op="" dtype="" >4.1008e-05 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000008000000000B" call="MPI_Isend" bytes="2048" orank="11" region="0" commid="0" count="15" tid="0" op="" dtype="" >6.4373e-05 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000080000000012" call="MPI_Isend" bytes="2048" orank="18" region="0" commid="0" count="17" tid="0" op="" dtype="" >6.7949e-05 9.5367e-07 2.3127e-05</hent>
<hent key="0240010000000000000008000000004A" call="MPI_Isend" bytes="2048" orank="74" region="0" commid="0" count="3460" tid="0" op="" dtype="" >1.8135e-02 9.5367e-07 4.3154e-05</hent>
<hent key="024001000000000000000800000002CA" call="MPI_Isend" bytes="2048" orank="714" region="0" commid="0" count="3467" tid="0" op="" dtype="" >1.5172e-02 9.5367e-07 3.6001e-05</hent>
<hent key="038001000000000000000E000000004A" call="MPI_Irecv" bytes="3584" orank="74" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000E00000002CA" call="MPI_Irecv" bytes="3584" orank="714" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="03800100000000000000028000000002" call="MPI_Irecv" bytes="640" orank="2" region="0" commid="0" count="407" tid="0" op="" dtype="" >9.9182e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000009" call="MPI_Irecv" bytes="640" orank="9" region="0" commid="0" count="412" tid="0" op="" dtype="" >1.6809e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000B" call="MPI_Irecv" bytes="640" orank="11" region="0" commid="0" count="364" tid="0" op="" dtype="" >1.5450e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000028000000012" call="MPI_Irecv" bytes="640" orank="18" region="0" commid="0" count="424" tid="0" op="" dtype="" >9.2983e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000004A" call="MPI_Irecv" bytes="640" orank="74" region="0" commid="0" count="257" tid="0" op="" dtype="" >2.4128e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1921e-06 1.1921e-06 1.1921e-06</hent>
<hent key="038001000000000000000280000002CA" call="MPI_Irecv" bytes="640" orank="714" region="0" commid="0" count="271" tid="0" op="" dtype="" >5.8079e-04 0.0000e+00 1.9073e-05</hent>
<hent key="02400100000000000000028000000002" call="MPI_Isend" bytes="640" orank="2" region="0" commid="0" count="412" tid="0" op="" dtype="" >4.8375e-04 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000028000000009" call="MPI_Isend" bytes="640" orank="9" region="0" commid="0" count="401" tid="0" op="" dtype="" >5.7507e-04 9.5367e-07 6.1989e-06</hent>
<hent key="0240010000000000000002800000000B" call="MPI_Isend" bytes="640" orank="11" region="0" commid="0" count="424" tid="0" op="" dtype="" >1.2596e-03 1.9073e-06 9.0599e-06</hent>
<hent key="02400100000000000000028000000012" call="MPI_Isend" bytes="640" orank="18" region="0" commid="0" count="392" tid="0" op="" dtype="" >4.3082e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000002800000004A" call="MPI_Isend" bytes="640" orank="74" region="0" commid="0" count="295" tid="0" op="" dtype="" >1.7192e-03 4.0531e-06 1.3828e-05</hent>
<hent key="024001000000000000000280000002CA" call="MPI_Isend" bytes="640" orank="714" region="0" commid="0" count="299" tid="0" op="" dtype="" >1.3225e-03 2.8610e-06 1.1206e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >7.1526e-06 7.1526e-06 7.1526e-06</hent>
<hent key="03800100000000000000014000000002" call="MPI_Irecv" bytes="320" orank="2" region="0" commid="0" count="358" tid="0" op="" dtype="" >1.0586e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000014000000009" call="MPI_Irecv" bytes="320" orank="9" region="0" commid="0" count="344" tid="0" op="" dtype="" >1.3232e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000B" call="MPI_Irecv" bytes="320" orank="11" region="0" commid="0" count="391" tid="0" op="" dtype="" >1.3137e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000014000000012" call="MPI_Irecv" bytes="320" orank="18" region="0" commid="0" count="345" tid="0" op="" dtype="" >7.1764e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000004A" call="MPI_Irecv" bytes="320" orank="74" region="0" commid="0" count="181" tid="0" op="" dtype="" >1.8311e-04 0.0000e+00 6.9141e-06</hent>
<hent key="038001000000000000000140000002CA" call="MPI_Irecv" bytes="320" orank="714" region="0" commid="0" count="180" tid="0" op="" dtype="" >3.8314e-04 0.0000e+00 1.4067e-05</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >4.7684e-06 9.5367e-07 1.9073e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.6158e+00 0.0000e+00 1.1633e-01</hent>
<hent key="0380010000000000000040000000004A" call="MPI_Irecv" bytes="16384" orank="74" region="0" commid="0" count="12699" tid="0" op="" dtype="" >8.9407e-03 0.0000e+00 2.4080e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6212e-05 0.0000e+00 1.5020e-05</hent>
<hent key="038001000000000000004000000002CA" call="MPI_Irecv" bytes="16384" orank="714" region="0" commid="0" count="12691" tid="0" op="" dtype="" >3.8211e-03 0.0000e+00 2.1935e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000002" call="MPI_Isend" bytes="320" orank="2" region="0" commid="0" count="357" tid="0" op="" dtype="" >3.3021e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000014000000009" call="MPI_Isend" bytes="320" orank="9" region="0" commid="0" count="342" tid="0" op="" dtype="" >3.8028e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000001400000000B" call="MPI_Isend" bytes="320" orank="11" region="0" commid="0" count="382" tid="0" op="" dtype="" >1.0209e-03 9.5367e-07 7.1526e-06</hent>
<hent key="02400100000000000000014000000012" call="MPI_Isend" bytes="320" orank="18" region="0" commid="0" count="385" tid="0" op="" dtype="" >3.3069e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000001400000004A" call="MPI_Isend" bytes="320" orank="74" region="0" commid="0" count="172" tid="0" op="" dtype="" >9.5987e-04 3.8147e-06 1.2159e-05</hent>
<hent key="03800100000000000000200000000002" call="MPI_Irecv" bytes="8192" orank="2" region="0" commid="0" count="12699" tid="0" op="" dtype="" >3.7971e-03 0.0000e+00 2.7180e-05</hent>
<hent key="03800100000000000000200000000009" call="MPI_Irecv" bytes="8192" orank="9" region="0" commid="0" count="2784" tid="0" op="" dtype="" >1.1191e-03 0.0000e+00 1.0014e-05</hent>
<hent key="0380010000000000000020000000000B" call="MPI_Irecv" bytes="8192" orank="11" region="0" commid="0" count="4063" tid="0" op="" dtype="" >7.1573e-04 0.0000e+00 1.3113e-05</hent>
<hent key="03800100000000000000200000000012" call="MPI_Irecv" bytes="8192" orank="18" region="0" commid="0" count="12699" tid="0" op="" dtype="" >2.8830e-03 0.0000e+00 1.9073e-05</hent>
<hent key="024001000000000000000140000002CA" call="MPI_Isend" bytes="320" orank="714" region="0" commid="0" count="163" tid="0" op="" dtype="" >7.1263e-04 2.8610e-06 9.0599e-06</hent>
<hent key="03800100000000000000000000000002" call="MPI_Irecv" bytes="0" orank="2" region="0" commid="0" count="252" tid="0" op="" dtype="" >4.9353e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000009" call="MPI_Irecv" bytes="0" orank="9" region="0" commid="0" count="259" tid="0" op="" dtype="" >9.2983e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000B" call="MPI_Irecv" bytes="0" orank="11" region="0" commid="0" count="257" tid="0" op="" dtype="" >8.0824e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000012" call="MPI_Irecv" bytes="0" orank="18" region="0" commid="0" count="264" tid="0" op="" dtype="" >5.2214e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000004A" call="MPI_Irecv" bytes="0" orank="74" region="0" commid="0" count="143" tid="0" op="" dtype="" >1.3733e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000040000000004A" call="MPI_Isend" bytes="16384" orank="74" region="0" commid="0" count="12667" tid="0" op="" dtype="" >1.3584e-01 3.8147e-06 5.2214e-05</hent>
<hent key="038001000000000000000000000002CA" call="MPI_Irecv" bytes="0" orank="714" region="0" commid="0" count="140" tid="0" op="" dtype="" >2.0075e-04 0.0000e+00 1.0014e-05</hent>
<hent key="024001000000000000004000000002CA" call="MPI_Isend" bytes="16384" orank="714" region="0" commid="0" count="12699" tid="0" op="" dtype="" >9.7612e-02 2.8610e-06 6.2943e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.3218e-04 0.0000e+00 5.6386e-04</hent>
<hent key="02400100000000000000200000000002" call="MPI_Isend" bytes="8192" orank="2" region="0" commid="0" count="12699" tid="0" op="" dtype="" >8.1000e-03 0.0000e+00 2.5988e-05</hent>
<hent key="02400100000000000000200000000009" call="MPI_Isend" bytes="8192" orank="9" region="0" commid="0" count="3181" tid="0" op="" dtype="" >4.3669e-03 0.0000e+00 5.2929e-05</hent>
<hent key="0240010000000000000020000000000B" call="MPI_Isend" bytes="8192" orank="11" region="0" commid="0" count="3681" tid="0" op="" dtype="" >7.6816e-03 0.0000e+00 5.1975e-05</hent>
<hent key="02400100000000000000200000000012" call="MPI_Isend" bytes="8192" orank="18" region="0" commid="0" count="12647" tid="0" op="" dtype="" >1.1956e-02 0.0000e+00 5.1975e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-05 5.0068e-05 5.0068e-05</hent>
<hent key="02400100000000000000000000000002" call="MPI_Isend" bytes="0" orank="2" region="0" commid="0" count="260" tid="0" op="" dtype="" >1.7405e-04 0.0000e+00 8.8215e-06</hent>
<hent key="02400100000000000000000000000009" call="MPI_Isend" bytes="0" orank="9" region="0" commid="0" count="242" tid="0" op="" dtype="" >1.6212e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000000000000B" call="MPI_Isend" bytes="0" orank="11" region="0" commid="0" count="264" tid="0" op="" dtype="" >5.0879e-04 9.5367e-07 2.3842e-05</hent>
<hent key="02400100000000000000000000000012" call="MPI_Isend" bytes="0" orank="18" region="0" commid="0" count="242" tid="0" op="" dtype="" >1.5378e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0240010000000000000000000000004A" call="MPI_Isend" bytes="0" orank="74" region="0" commid="0" count="144" tid="0" op="" dtype="" >6.7306e-04 2.8610e-06 2.5988e-05</hent>
<hent key="024001000000000000000000000002CA" call="MPI_Isend" bytes="0" orank="714" region="0" commid="0" count="140" tid="0" op="" dtype="" >5.2953e-04 2.8610e-06 1.4067e-05</hent>
<hent key="03800100000000000000060000000002" call="MPI_Irecv" bytes="1536" orank="2" region="0" commid="0" count="99" tid="0" op="" dtype="" >3.2425e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000009" call="MPI_Irecv" bytes="1536" orank="9" region="0" commid="0" count="95" tid="0" op="" dtype="" >4.2677e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000006000000000B" call="MPI_Irecv" bytes="1536" orank="11" region="0" commid="0" count="85" tid="0" op="" dtype="" >4.0293e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000060000000012" call="MPI_Irecv" bytes="1536" orank="18" region="0" commid="0" count="115" tid="0" op="" dtype="" >2.7180e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000004A" call="MPI_Irecv" bytes="1536" orank="74" region="0" commid="0" count="215" tid="0" op="" dtype="" >2.0552e-04 0.0000e+00 3.8147e-06</hent>
<hent key="038001000000000000000600000002CA" call="MPI_Irecv" bytes="1536" orank="714" region="0" commid="0" count="230" tid="0" op="" dtype="" >4.5347e-04 0.0000e+00 1.7881e-05</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.5763e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000002" call="MPI_Isend" bytes="1536" orank="2" region="0" commid="0" count="130" tid="0" op="" dtype="" >2.6989e-04 9.5367e-07 1.1921e-05</hent>
<hent key="02400100000000000000060000000009" call="MPI_Isend" bytes="1536" orank="9" region="0" commid="0" count="104" tid="0" op="" dtype="" >2.1577e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000006000000000B" call="MPI_Isend" bytes="1536" orank="11" region="0" commid="0" count="82" tid="0" op="" dtype="" >3.1662e-04 2.8610e-06 1.6928e-05</hent>
<hent key="02400100000000000000060000000012" call="MPI_Isend" bytes="1536" orank="18" region="0" commid="0" count="100" tid="0" op="" dtype="" >2.0432e-04 9.5367e-07 2.0027e-05</hent>
<hent key="0240010000000000000006000000004A" call="MPI_Isend" bytes="1536" orank="74" region="0" commid="0" count="232" tid="0" op="" dtype="" >1.4384e-03 5.0068e-06 1.3113e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002CA" call="MPI_Isend" bytes="1536" orank="714" region="0" commid="0" count="216" tid="0" op="" dtype="" >1.0185e-03 3.8147e-06 1.4067e-05</hent>
<hent key="038001000000000000000C000000004A" call="MPI_Irecv" bytes="3072" orank="74" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 4.0531e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002CA" call="MPI_Irecv" bytes="3072" orank="714" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.9073e-05 1.1921e-06 3.0994e-06</hent>
<hent key="024001000000000000000C0000000009" call="MPI_Isend" bytes="3072" orank="9" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000C000000004A" call="MPI_Isend" bytes="3072" orank="74" region="0" commid="0" count="8" tid="0" op="" dtype="" >6.8426e-05 5.9605e-06 1.4067e-05</hent>
<hent key="024001000000000000000C00000002CA" call="MPI_Isend" bytes="3072" orank="714" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.8133e-05 5.0068e-06 1.1206e-05</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.3103e-04 2.3103e-04 2.3103e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >5.1689e-04 1.6499e-04 1.7786e-04</hent>
<hent key="03800100000000000000038000000002" call="MPI_Irecv" bytes="896" orank="2" region="0" commid="0" count="321" tid="0" op="" dtype="" >8.9884e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000009" call="MPI_Irecv" bytes="896" orank="9" region="0" commid="0" count="2965" tid="0" op="" dtype="" >6.8879e-04 0.0000e+00 8.8215e-06</hent>
<hent key="0380010000000000000003800000000B" call="MPI_Irecv" bytes="896" orank="11" region="0" commid="0" count="2618" tid="0" op="" dtype="" >6.3443e-04 0.0000e+00 1.2159e-05</hent>
<hent key="03800100000000000000038000000012" call="MPI_Irecv" bytes="896" orank="18" region="0" commid="0" count="294" tid="0" op="" dtype="" >7.4625e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000004A" call="MPI_Irecv" bytes="896" orank="74" region="0" commid="0" count="323" tid="0" op="" dtype="" >2.9874e-04 0.0000e+00 6.9141e-06</hent>
<hent key="038001000000000000000380000002CA" call="MPI_Irecv" bytes="896" orank="714" region="0" commid="0" count="347" tid="0" op="" dtype="" >7.3838e-04 0.0000e+00 2.9087e-05</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6001e-05 3.6001e-05 3.6001e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000003800000002CA" call="MPI_Irecv" bytes="14336" orank="714" region="0" commid="0" count="8" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000038000000002" call="MPI_Isend" bytes="896" orank="2" region="0" commid="0" count="300" tid="0" op="" dtype="" >3.9244e-04 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000038000000009" call="MPI_Isend" bytes="896" orank="9" region="0" commid="0" count="2913" tid="0" op="" dtype="" >2.2113e-03 0.0000e+00 1.3113e-05</hent>
<hent key="0240010000000000000003800000000B" call="MPI_Isend" bytes="896" orank="11" region="0" commid="0" count="2746" tid="0" op="" dtype="" >2.8703e-03 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000038000000012" call="MPI_Isend" bytes="896" orank="18" region="0" commid="0" count="324" tid="0" op="" dtype="" >4.6182e-04 9.5367e-07 1.3113e-05</hent>
<hent key="0240010000000000000003800000004A" call="MPI_Isend" bytes="896" orank="74" region="0" commid="0" count="335" tid="0" op="" dtype="" >1.9960e-03 4.0531e-06 1.4067e-05</hent>
<hent key="024001000000000000000380000002CA" call="MPI_Isend" bytes="896" orank="714" region="0" commid="0" count="331" tid="0" op="" dtype="" >1.4911e-03 3.0994e-06 1.0014e-05</hent>
<hent key="0240010000000000000038000000004A" call="MPI_Isend" bytes="14336" orank="74" region="0" commid="0" count="32" tid="0" op="" dtype="" >3.0446e-04 5.9605e-06 1.5974e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.8907e+00 2.0027e-05 1.3915e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.2214e-04 5.2214e-04 5.2214e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >3.4189e-04 3.4189e-04 3.4189e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >1.8721e-03 1.8721e-03 1.8721e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >9.6564e-02 3.4499e-04 9.0450e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.8103e-04 5.8103e-04 5.8103e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6129e+00 4.3297e-04 2.5650e-01</hent>
<hent key="03800100000000000000040000000002" call="MPI_Irecv" bytes="1024" orank="2" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.4216e-04 0.0000e+00 1.2875e-05</hent>
<hent key="03800100000000000000040000000009" call="MPI_Irecv" bytes="1024" orank="9" region="0" commid="0" count="758" tid="0" op="" dtype="" >1.7190e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000004000000000B" call="MPI_Irecv" bytes="1024" orank="11" region="0" commid="0" count="1114" tid="0" op="" dtype="" >2.2602e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000040000000012" call="MPI_Irecv" bytes="1024" orank="18" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.0225e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >7.7224e-04 1.9073e-06 7.0500e-04</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="03800100000000000000070000000002" call="MPI_Irecv" bytes="1792" orank="2" region="0" commid="0" count="37" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000009" call="MPI_Irecv" bytes="1792" orank="9" region="0" commid="0" count="45" tid="0" op="" dtype="" >1.2398e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000B" call="MPI_Irecv" bytes="1792" orank="11" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.7643e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000070000000012" call="MPI_Irecv" bytes="1792" orank="18" region="0" commid="0" count="28" tid="0" op="" dtype="" >8.5831e-06 0.0000e+00 3.8147e-06</hent>
<hent key="0380010000000000000007000000004A" call="MPI_Irecv" bytes="1792" orank="74" region="0" commid="0" count="128" tid="0" op="" dtype="" >1.3256e-04 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000700000002CA" call="MPI_Irecv" bytes="1792" orank="714" region="0" commid="0" count="127" tid="0" op="" dtype="" >2.7847e-04 0.0000e+00 9.0599e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >9.5606e-05 0.0000e+00 5.1022e-05</hent>
<hent key="02400100000000000000040000000002" call="MPI_Isend" bytes="1024" orank="2" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.1868e-03 0.0000e+00 2.1935e-05</hent>
<hent key="02400100000000000000040000000009" call="MPI_Isend" bytes="1024" orank="9" region="0" commid="0" count="830" tid="0" op="" dtype="" >5.4789e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000004000000000B" call="MPI_Isend" bytes="1024" orank="11" region="0" commid="0" count="958" tid="0" op="" dtype="" >7.8654e-04 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000040000000012" call="MPI_Isend" bytes="1024" orank="18" region="0" commid="0" count="3380" tid="0" op="" dtype="" >2.0802e-03 0.0000e+00 2.2173e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >3.0994e-06 9.5367e-07 2.1458e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4352e+00 0.0000e+00 3.2501e+00</hent>
<hent key="038001000000000000000A0000000002" call="MPI_Irecv" bytes="2560" orank="2" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000009" call="MPI_Irecv" bytes="2560" orank="9" region="0" commid="0" count="9" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000000B" call="MPI_Irecv" bytes="2560" orank="11" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000012" call="MPI_Irecv" bytes="2560" orank="18" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000004A" call="MPI_Irecv" bytes="2560" orank="74" region="0" commid="0" count="50" tid="0" op="" dtype="" >6.6042e-05 0.0000e+00 9.0599e-06</hent>
<hent key="038001000000000000000A00000002CA" call="MPI_Irecv" bytes="2560" orank="714" region="0" commid="0" count="46" tid="0" op="" dtype="" >1.0228e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000070000000002" call="MPI_Isend" bytes="1792" orank="2" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.0824e-04 9.5367e-07 1.4067e-05</hent>
<hent key="02400100000000000000070000000009" call="MPI_Isend" bytes="1792" orank="9" region="0" commid="0" count="42" tid="0" op="" dtype="" >8.4639e-05 9.5367e-07 2.8610e-06</hent>
<hent key="0240010000000000000007000000000B" call="MPI_Isend" bytes="1792" orank="11" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.1396e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000070000000012" call="MPI_Isend" bytes="1792" orank="18" region="0" commid="0" count="44" tid="0" op="" dtype="" >1.0157e-04 9.5367e-07 1.1921e-05</hent>
<hent key="0240010000000000000007000000004A" call="MPI_Isend" bytes="1792" orank="74" region="0" commid="0" count="114" tid="0" op="" dtype="" >6.9094e-04 1.9073e-06 1.3113e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6147e-02 3.6147e-02 3.6147e-02</hent>
<hent key="024001000000000000000700000002CA" call="MPI_Isend" bytes="1792" orank="714" region="0" commid="0" count="126" tid="0" op="" dtype="" >6.3419e-04 3.8147e-06 1.0967e-05</hent>
<hent key="024001000000000000000A0000000002" call="MPI_Isend" bytes="2560" orank="2" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.7418e-05 2.1458e-06 1.4067e-05</hent>
<hent key="024001000000000000000A0000000009" call="MPI_Isend" bytes="2560" orank="9" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.8835e-05 1.9073e-06 3.8147e-06</hent>
<hent key="024001000000000000000A000000000B" call="MPI_Isend" bytes="2560" orank="11" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.7166e-05 4.0531e-06 5.0068e-06</hent>
<hent key="024001000000000000000A0000000012" call="MPI_Isend" bytes="2560" orank="18" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.6689e-05 9.5367e-07 1.1921e-05</hent>
<hent key="024001000000000000000A000000004A" call="MPI_Isend" bytes="2560" orank="74" region="0" commid="0" count="51" tid="0" op="" dtype="" >3.4451e-04 5.9605e-06 1.1921e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.6989e-04 3.6001e-05 1.2684e-04</hent>
<hent key="024001000000000000000A00000002CA" call="MPI_Isend" bytes="2560" orank="714" region="0" commid="0" count="46" tid="0" op="" dtype="" >2.5606e-04 4.0531e-06 1.0014e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >8.0085e-04 3.2091e-04 4.7994e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.1948e-05 3.1948e-05 3.1948e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >7.8678e-06 1.9073e-06 5.9605e-06</hent>
<hent key="03800100000000000000000400000002" call="MPI_Irecv" bytes="4" orank="2" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.6076e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000000400000009" call="MPI_Irecv" bytes="4" orank="9" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2701e-03 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000000040000000B" call="MPI_Irecv" bytes="4" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.1141e-04 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000000400000012" call="MPI_Irecv" bytes="4" orank="18" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.2956e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0380010000000000000000040000004A" call="MPI_Irecv" bytes="4" orank="74" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.1198e-03 0.0000e+00 2.0027e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.6460e-03 2.1911e-04 5.8603e-04</hent>
<hent key="038001000000000000001C0000000009" call="MPI_Irecv" bytes="7168" orank="9" region="0" commid="0" count="9915" tid="0" op="" dtype="" >3.7804e-03 0.0000e+00 1.0967e-05</hent>
<hent key="038001000000000000001C000000000B" call="MPI_Irecv" bytes="7168" orank="11" region="0" commid="0" count="8636" tid="0" op="" dtype="" >1.5438e-03 0.0000e+00 1.5020e-05</hent>
<hent key="038001000000000000000004000002CA" call="MPI_Irecv" bytes="4" orank="714" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.0833e-03 0.0000e+00 2.5034e-05</hent>
<hent key="02400100000000000000000400000002" call="MPI_Isend" bytes="4" orank="2" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4484e-03 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000000400000009" call="MPI_Isend" bytes="4" orank="9" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1678e-03 0.0000e+00 4.1962e-05</hent>
<hent key="0240010000000000000000040000000B" call="MPI_Isend" bytes="4" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.6870e-03 0.0000e+00 8.8215e-06</hent>
<hent key="02400100000000000000000400000012" call="MPI_Isend" bytes="4" orank="18" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.5652e-03 0.0000e+00 2.2888e-05</hent>
<hent key="0240010000000000000000040000004A" call="MPI_Isend" bytes="4" orank="74" region="0" commid="0" count="3398" tid="0" op="" dtype="" >3.1251e-02 3.8147e-06 3.0041e-05</hent>
<hent key="024001000000000000001C0000000009" call="MPI_Isend" bytes="7168" orank="9" region="0" commid="0" count="9518" tid="0" op="" dtype="" >2.1498e-02 9.5367e-07 1.5974e-05</hent>
<hent key="024001000000000000001C000000000B" call="MPI_Isend" bytes="7168" orank="11" region="0" commid="0" count="9018" tid="0" op="" dtype="" >2.8550e-02 9.5367e-07 2.1935e-05</hent>
<hent key="024001000000000000000004000002CA" call="MPI_Isend" bytes="4" orank="714" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.9426e-02 2.8610e-06 1.2517e-04</hent>
<hent key="03800100000000000000050000000002" call="MPI_Irecv" bytes="1280" orank="2" region="0" commid="0" count="207" tid="0" op="" dtype="" >5.1498e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000009" call="MPI_Irecv" bytes="1280" orank="9" region="0" commid="0" count="196" tid="0" op="" dtype="" >7.1764e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000B" call="MPI_Irecv" bytes="1280" orank="11" region="0" commid="0" count="213" tid="0" op="" dtype="" >9.9182e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000050000000012" call="MPI_Irecv" bytes="1280" orank="18" region="0" commid="0" count="209" tid="0" op="" dtype="" >4.0531e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000004A" call="MPI_Irecv" bytes="1280" orank="74" region="0" commid="0" count="331" tid="0" op="" dtype="" >3.3808e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0964e+01 1.0014e-05 1.3899e-01</hent>
<hent key="038001000000000000000500000002CA" call="MPI_Irecv" bytes="1280" orank="714" region="0" commid="0" count="295" tid="0" op="" dtype="" >5.7292e-04 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000080000000002" call="MPI_Irecv" bytes="2048" orank="2" region="0" commid="0" count="15" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000009" call="MPI_Irecv" bytes="2048" orank="9" region="0" commid="0" count="14" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000B" call="MPI_Irecv" bytes="2048" orank="11" region="0" commid="0" count="16" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000080000000012" call="MPI_Irecv" bytes="2048" orank="18" region="0" commid="0" count="13" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000004A" call="MPI_Irecv" bytes="2048" orank="74" region="0" commid="0" count="3464" tid="0" op="" dtype="" >1.0140e-03 0.0000e+00 2.0981e-05</hent>
<hent key="038001000000000000000800000002CA" call="MPI_Irecv" bytes="2048" orank="714" region="0" commid="0" count="3453" tid="0" op="" dtype="" >1.4908e-03 0.0000e+00 2.0981e-05</hent>
<hent key="02400100000000000000050000000002" call="MPI_Isend" bytes="1280" orank="2" region="0" commid="0" count="178" tid="0" op="" dtype="" >2.6917e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000050000000009" call="MPI_Isend" bytes="1280" orank="9" region="0" commid="0" count="196" tid="0" op="" dtype="" >3.7718e-04 9.5367e-07 7.1526e-06</hent>
<hent key="0240010000000000000005000000000B" call="MPI_Isend" bytes="1280" orank="11" region="0" commid="0" count="193" tid="0" op="" dtype="" >7.0834e-04 2.8610e-06 2.0981e-05</hent>
<hent key="02400100000000000000050000000012" call="MPI_Isend" bytes="1280" orank="18" region="0" commid="0" count="209" tid="0" op="" dtype="" >2.8729e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000005000000004A" call="MPI_Isend" bytes="1280" orank="74" region="0" commid="0" count="286" tid="0" op="" dtype="" >1.7667e-03 4.7684e-06 1.5020e-05</hent>
<hent key="02400100000000000000280000000012" call="MPI_Isend" bytes="10240" orank="18" region="0" commid="0" count="52" tid="0" op="" dtype="" >4.0770e-05 0.0000e+00 7.8678e-06</hent>
<hent key="024001000000000000000500000002CA" call="MPI_Isend" bytes="1280" orank="714" region="0" commid="0" count="305" tid="0" op="" dtype="" >1.4732e-03 3.0994e-06 2.3127e-05</hent>
</hash>
<internal rank="10" log_i="1723713849.507450" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="11" mpi_size="768" stamp_init="1723713791.106640" stamp_final="1723713849.513292" username="apac4" allocationname="unknown" flags="0" pid="1717074" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.84067e+01" utime="5.00295e+01" stime="6.72934e+00" mtime="3.25720e+01" gflop="0.00000e+00" gbyte="3.77419e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25720e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008514215685148514a2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83333e+01" utime="5.00002e+01" stime="6.71812e+00" mtime="3.25720e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25720e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4919e+08" > 3.4448e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4846e+08" > 2.7966e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4789e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4373e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.3855e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6128e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.7961e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0398e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6142e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9630e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="195" >
<hent key="02400100000000000000080000000003" call="MPI_Isend" bytes="2048" orank="3" region="0" commid="0" count="10" tid="0" op="" dtype="" >4.9353e-05 9.5367e-07 1.2875e-05</hent>
<hent key="0240010000000000000008000000000A" call="MPI_Isend" bytes="2048" orank="10" region="0" commid="0" count="16" tid="0" op="" dtype="" >3.4571e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000008000000000C" call="MPI_Isend" bytes="2048" orank="12" region="0" commid="0" count="13" tid="0" op="" dtype="" >5.6028e-05 3.0994e-06 5.0068e-06</hent>
<hent key="02400100000000000000080000000013" call="MPI_Isend" bytes="2048" orank="19" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.8133e-05 9.5367e-07 1.0967e-05</hent>
<hent key="0240010000000000000008000000004B" call="MPI_Isend" bytes="2048" orank="75" region="0" commid="0" count="3454" tid="0" op="" dtype="" >1.1047e-02 9.5367e-07 2.3127e-05</hent>
<hent key="024001000000000000000800000002CB" call="MPI_Isend" bytes="2048" orank="715" region="0" commid="0" count="3473" tid="0" op="" dtype="" >1.0577e-02 9.5367e-07 2.0981e-05</hent>
<hent key="038001000000000000000E000000004B" call="MPI_Irecv" bytes="3584" orank="75" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000E00000002CB" call="MPI_Irecv" bytes="3584" orank="715" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.1921e-06 1.1921e-06 1.1921e-06</hent>
<hent key="024001000000000000000E000000004B" call="MPI_Isend" bytes="3584" orank="75" region="0" commid="0" count="1" tid="0" op="" dtype="" >6.9141e-06 6.9141e-06 6.9141e-06</hent>
<hent key="03800100000000000000028000000003" call="MPI_Irecv" bytes="640" orank="3" region="0" commid="0" count="396" tid="0" op="" dtype="" >7.6532e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000A" call="MPI_Irecv" bytes="640" orank="10" region="0" commid="0" count="424" tid="0" op="" dtype="" >1.7858e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000C" call="MPI_Irecv" bytes="640" orank="12" region="0" commid="0" count="419" tid="0" op="" dtype="" >1.0467e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000013" call="MPI_Irecv" bytes="640" orank="19" region="0" commid="0" count="394" tid="0" op="" dtype="" >7.8440e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000004B" call="MPI_Irecv" bytes="640" orank="75" region="0" commid="0" count="298" tid="0" op="" dtype="" >1.1492e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002CB" call="MPI_Irecv" bytes="640" orank="715" region="0" commid="0" count="278" tid="0" op="" dtype="" >8.7976e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000003" call="MPI_Isend" bytes="640" orank="3" region="0" commid="0" count="402" tid="0" op="" dtype="" >4.4775e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000002800000000A" call="MPI_Isend" bytes="640" orank="10" region="0" commid="0" count="364" tid="0" op="" dtype="" >5.0449e-04 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000002800000000C" call="MPI_Isend" bytes="640" orank="12" region="0" commid="0" count="433" tid="0" op="" dtype="" >1.3547e-03 1.9073e-06 2.0981e-05</hent>
<hent key="02400100000000000000028000000013" call="MPI_Isend" bytes="640" orank="19" region="0" commid="0" count="410" tid="0" op="" dtype="" >4.8447e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000002800000004B" call="MPI_Isend" bytes="640" orank="75" region="0" commid="0" count="281" tid="0" op="" dtype="" >1.3700e-03 3.8147e-06 1.0014e-05</hent>
<hent key="024001000000000000000280000002CB" call="MPI_Isend" bytes="640" orank="715" region="0" commid="0" count="282" tid="0" op="" dtype="" >1.1594e-03 2.8610e-06 9.0599e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0014e-05 1.0014e-05 1.0014e-05</hent>
<hent key="03800100000000000000014000000003" call="MPI_Irecv" bytes="320" orank="3" region="0" commid="0" count="348" tid="0" op="" dtype="" >6.2227e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000A" call="MPI_Irecv" bytes="320" orank="10" region="0" commid="0" count="382" tid="0" op="" dtype="" >1.6952e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000C" call="MPI_Irecv" bytes="320" orank="12" region="0" commid="0" count="321" tid="0" op="" dtype="" >8.2970e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000013" call="MPI_Irecv" bytes="320" orank="19" region="0" commid="0" count="352" tid="0" op="" dtype="" >6.5565e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000004B" call="MPI_Irecv" bytes="320" orank="75" region="0" commid="0" count="182" tid="0" op="" dtype="" >5.9366e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002CB" call="MPI_Irecv" bytes="320" orank="715" region="0" commid="0" count="175" tid="0" op="" dtype="" >5.0306e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.4789e+00 0.0000e+00 1.1626e-01</hent>
<hent key="0380010000000000000040000000004B" call="MPI_Irecv" bytes="16384" orank="75" region="0" commid="0" count="12620" tid="0" op="" dtype="" >5.6260e-03 0.0000e+00 8.1062e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9789e-05 9.5367e-07 1.7881e-05</hent>
<hent key="038001000000000000004000000002CB" call="MPI_Irecv" bytes="16384" orank="715" region="0" commid="0" count="12624" tid="0" op="" dtype="" >2.2063e-03 0.0000e+00 6.9141e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000003" call="MPI_Isend" bytes="320" orank="3" region="0" commid="0" count="363" tid="0" op="" dtype="" >3.3641e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000001400000000A" call="MPI_Isend" bytes="320" orank="10" region="0" commid="0" count="391" tid="0" op="" dtype="" >4.4870e-04 0.0000e+00 8.8215e-06</hent>
<hent key="0240010000000000000001400000000C" call="MPI_Isend" bytes="320" orank="12" region="0" commid="0" count="341" tid="0" op="" dtype="" >9.3269e-04 1.9073e-06 4.0531e-06</hent>
<hent key="02400100000000000000014000000013" call="MPI_Isend" bytes="320" orank="19" region="0" commid="0" count="341" tid="0" op="" dtype="" >3.3426e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0240010000000000000001400000004B" call="MPI_Isend" bytes="320" orank="75" region="0" commid="0" count="178" tid="0" op="" dtype="" >8.4138e-04 3.8147e-06 1.0967e-05</hent>
<hent key="03800100000000000000200000000003" call="MPI_Irecv" bytes="8192" orank="3" region="0" commid="0" count="12683" tid="0" op="" dtype="" >2.6853e-03 0.0000e+00 8.8215e-06</hent>
<hent key="0380010000000000000020000000000A" call="MPI_Irecv" bytes="8192" orank="10" region="0" commid="0" count="3681" tid="0" op="" dtype="" >1.1797e-03 0.0000e+00 1.1921e-05</hent>
<hent key="0380010000000000000020000000000C" call="MPI_Irecv" bytes="8192" orank="12" region="0" commid="0" count="3108" tid="0" op="" dtype="" >4.0865e-04 0.0000e+00 2.8610e-06</hent>
<hent key="03800100000000000000200000000013" call="MPI_Irecv" bytes="8192" orank="19" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.3096e-03 0.0000e+00 5.0068e-06</hent>
<hent key="024001000000000000000140000002CB" call="MPI_Isend" bytes="320" orank="715" region="0" commid="0" count="178" tid="0" op="" dtype="" >7.1883e-04 2.8610e-06 7.1526e-06</hent>
<hent key="03800100000000000000000000000003" call="MPI_Irecv" bytes="0" orank="3" region="0" commid="0" count="265" tid="0" op="" dtype="" >5.6028e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000A" call="MPI_Irecv" bytes="0" orank="10" region="0" commid="0" count="264" tid="0" op="" dtype="" >9.6560e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000C" call="MPI_Irecv" bytes="0" orank="12" region="0" commid="0" count="228" tid="0" op="" dtype="" >6.4135e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000000000000013" call="MPI_Irecv" bytes="0" orank="19" region="0" commid="0" count="224" tid="0" op="" dtype="" >4.5061e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000004B" call="MPI_Irecv" bytes="0" orank="75" region="0" commid="0" count="145" tid="0" op="" dtype="" >4.4584e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000040000000004B" call="MPI_Isend" bytes="16384" orank="75" region="0" commid="0" count="12648" tid="0" op="" dtype="" >1.0536e-01 4.0531e-06 1.2302e-04</hent>
<hent key="038001000000000000000000000002CB" call="MPI_Irecv" bytes="0" orank="715" region="0" commid="0" count="145" tid="0" op="" dtype="" >2.5511e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002CB" call="MPI_Isend" bytes="16384" orank="715" region="0" commid="0" count="12699" tid="0" op="" dtype="" >7.1396e-02 2.8610e-06 2.4080e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.8893e-04 0.0000e+00 6.2609e-04</hent>
<hent key="02400100000000000000200000000003" call="MPI_Isend" bytes="8192" orank="3" region="0" commid="0" count="12699" tid="0" op="" dtype="" >3.7873e-03 0.0000e+00 1.2159e-05</hent>
<hent key="0240010000000000000020000000000A" call="MPI_Isend" bytes="8192" orank="10" region="0" commid="0" count="4063" tid="0" op="" dtype="" >4.4451e-03 0.0000e+00 1.2875e-05</hent>
<hent key="0240010000000000000020000000000C" call="MPI_Isend" bytes="8192" orank="12" region="0" commid="0" count="3201" tid="0" op="" dtype="" >5.9078e-03 0.0000e+00 6.6996e-05</hent>
<hent key="02400100000000000000200000000013" call="MPI_Isend" bytes="8192" orank="19" region="0" commid="0" count="12674" tid="0" op="" dtype="" >6.6915e-03 0.0000e+00 6.1989e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.2888e-05 2.2888e-05 2.2888e-05</hent>
<hent key="02400100000000000000000000000003" call="MPI_Isend" bytes="0" orank="3" region="0" commid="0" count="269" tid="0" op="" dtype="" >1.9217e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000000000000000A" call="MPI_Isend" bytes="0" orank="10" region="0" commid="0" count="257" tid="0" op="" dtype="" >2.0051e-04 0.0000e+00 6.1989e-06</hent>
<hent key="0240010000000000000000000000000C" call="MPI_Isend" bytes="0" orank="12" region="0" commid="0" count="235" tid="0" op="" dtype="" >4.9686e-04 9.5367e-07 5.5075e-05</hent>
<hent key="02400100000000000000000000000013" call="MPI_Isend" bytes="0" orank="19" region="0" commid="0" count="237" tid="0" op="" dtype="" >1.5807e-04 0.0000e+00 8.8215e-06</hent>
<hent key="0240010000000000000000000000004B" call="MPI_Isend" bytes="0" orank="75" region="0" commid="0" count="149" tid="0" op="" dtype="" >6.1178e-04 1.9073e-06 9.0599e-06</hent>
<hent key="024001000000000000000000000002CB" call="MPI_Isend" bytes="0" orank="715" region="0" commid="0" count="153" tid="0" op="" dtype="" >5.5075e-04 9.5367e-07 4.7684e-06</hent>
<hent key="03800100000000000000060000000003" call="MPI_Irecv" bytes="1536" orank="3" region="0" commid="0" count="91" tid="0" op="" dtype="" >1.7643e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000A" call="MPI_Irecv" bytes="1536" orank="10" region="0" commid="0" count="82" tid="0" op="" dtype="" >3.6716e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000C" call="MPI_Irecv" bytes="1536" orank="12" region="0" commid="0" count="101" tid="0" op="" dtype="" >2.6703e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000013" call="MPI_Irecv" bytes="1536" orank="19" region="0" commid="0" count="110" tid="0" op="" dtype="" >2.5034e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000004B" call="MPI_Irecv" bytes="1536" orank="75" region="0" commid="0" count="220" tid="0" op="" dtype="" >8.9645e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002CB" call="MPI_Irecv" bytes="1536" orank="715" region="0" commid="0" count="221" tid="0" op="" dtype="" >7.8917e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000003" call="MPI_Isend" bytes="1536" orank="3" region="0" commid="0" count="90" tid="0" op="" dtype="" >1.6618e-04 9.5367e-07 1.1921e-05</hent>
<hent key="0240010000000000000006000000000A" call="MPI_Isend" bytes="1536" orank="10" region="0" commid="0" count="85" tid="0" op="" dtype="" >1.6332e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000006000000000C" call="MPI_Isend" bytes="1536" orank="12" region="0" commid="0" count="127" tid="0" op="" dtype="" >4.9329e-04 2.8610e-06 5.9605e-06</hent>
<hent key="02400100000000000000060000000013" call="MPI_Isend" bytes="1536" orank="19" region="0" commid="0" count="110" tid="0" op="" dtype="" >1.9956e-04 9.5367e-07 1.2875e-05</hent>
<hent key="0240010000000000000006000000004B" call="MPI_Isend" bytes="1536" orank="75" region="0" commid="0" count="240" tid="0" op="" dtype="" >1.2772e-03 3.8147e-06 6.1989e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002CB" call="MPI_Isend" bytes="1536" orank="715" region="0" commid="0" count="203" tid="0" op="" dtype="" >9.2030e-04 2.8610e-06 8.1062e-06</hent>
<hent key="038001000000000000000C0000000003" call="MPI_Irecv" bytes="3072" orank="3" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C000000000C" call="MPI_Irecv" bytes="3072" orank="12" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C000000004B" call="MPI_Irecv" bytes="3072" orank="75" region="0" commid="0" count="12" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 2.1458e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002CB" call="MPI_Irecv" bytes="3072" orank="715" region="0" commid="0" count="6" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000013" call="MPI_Isend" bytes="3072" orank="19" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000C000000004B" call="MPI_Isend" bytes="3072" orank="75" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.5974e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000C00000002CB" call="MPI_Isend" bytes="3072" orank="715" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.4094e-05 4.0531e-06 5.0068e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.5702e-04 2.5702e-04 2.5702e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >5.5909e-04 1.8501e-04 1.8907e-04</hent>
<hent key="03800100000000000000038000000003" call="MPI_Irecv" bytes="896" orank="3" region="0" commid="0" count="341" tid="0" op="" dtype="" >6.1512e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000A" call="MPI_Irecv" bytes="896" orank="10" region="0" commid="0" count="2746" tid="0" op="" dtype="" >4.7207e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000C" call="MPI_Irecv" bytes="896" orank="12" region="0" commid="0" count="2908" tid="0" op="" dtype="" >6.4659e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000013" call="MPI_Irecv" bytes="896" orank="19" region="0" commid="0" count="319" tid="0" op="" dtype="" >5.1975e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000004B" call="MPI_Irecv" bytes="896" orank="75" region="0" commid="0" count="311" tid="0" op="" dtype="" >1.4806e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002CB" call="MPI_Irecv" bytes="896" orank="715" region="0" commid="0" count="349" tid="0" op="" dtype="" >1.1539e-04 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >7.2002e-05 7.2002e-05 7.2002e-05</hent>
<hent key="0380010000000000000038000000004B" call="MPI_Irecv" bytes="14336" orank="75" region="0" commid="0" count="79" tid="0" op="" dtype="" >3.9101e-05 0.0000e+00 2.1458e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000003800000002CB" call="MPI_Irecv" bytes="14336" orank="715" region="0" commid="0" count="75" tid="0" op="" dtype="" >1.9073e-05 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000038000000003" call="MPI_Isend" bytes="896" orank="3" region="0" commid="0" count="323" tid="0" op="" dtype="" >4.2725e-04 9.5367e-07 1.2159e-05</hent>
<hent key="0240010000000000000003800000000A" call="MPI_Isend" bytes="896" orank="10" region="0" commid="0" count="2618" tid="0" op="" dtype="" >1.9114e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000003800000000C" call="MPI_Isend" bytes="896" orank="12" region="0" commid="0" count="2832" tid="0" op="" dtype="" >2.9025e-03 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000038000000013" call="MPI_Isend" bytes="896" orank="19" region="0" commid="0" count="325" tid="0" op="" dtype="" >4.3893e-04 9.5367e-07 5.9605e-06</hent>
<hent key="0240010000000000000003800000004B" call="MPI_Isend" bytes="896" orank="75" region="0" commid="0" count="329" tid="0" op="" dtype="" >1.6627e-03 3.8147e-06 1.1921e-05</hent>
<hent key="024001000000000000000380000002CB" call="MPI_Isend" bytes="896" orank="715" region="0" commid="0" count="345" tid="0" op="" dtype="" >1.4744e-03 2.8610e-06 1.5974e-05</hent>
<hent key="0240010000000000000038000000004B" call="MPI_Isend" bytes="14336" orank="75" region="0" commid="0" count="51" tid="0" op="" dtype="" >4.3011e-04 6.9141e-06 1.2159e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.4733e+00 2.0981e-05 1.3926e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.1498e-04 5.1498e-04 5.1498e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >3.0398e-04 3.0398e-04 3.0398e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >9.2602e-04 9.2602e-04 9.2602e-04</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >8.9097e-02 3.6001e-04 8.3487e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.6005e-04 5.6005e-04 5.6005e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6128e+00 4.5681e-04 2.5646e-01</hent>
<hent key="03800100000000000000040000000003" call="MPI_Irecv" bytes="1024" orank="3" region="0" commid="0" count="3394" tid="0" op="" dtype="" >5.8842e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000000A" call="MPI_Irecv" bytes="1024" orank="10" region="0" commid="0" count="958" tid="0" op="" dtype="" >1.1945e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000000C" call="MPI_Irecv" bytes="1024" orank="12" region="0" commid="0" count="848" tid="0" op="" dtype="" >2.1505e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000013" call="MPI_Irecv" bytes="1024" orank="19" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.1294e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.5736e-05 0.0000e+00 6.9141e-06</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="03800100000000000000070000000003" call="MPI_Irecv" bytes="1792" orank="3" region="0" commid="0" count="42" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000A" call="MPI_Irecv" bytes="1792" orank="10" region="0" commid="0" count="29" tid="0" op="" dtype="" >9.5367e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000007000000000C" call="MPI_Irecv" bytes="1792" orank="12" region="0" commid="0" count="38" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000013" call="MPI_Irecv" bytes="1792" orank="19" region="0" commid="0" count="47" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000004B" call="MPI_Irecv" bytes="1792" orank="75" region="0" commid="0" count="144" tid="0" op="" dtype="" >6.8665e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002CB" call="MPI_Irecv" bytes="1792" orank="715" region="0" commid="0" count="163" tid="0" op="" dtype="" >4.8399e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >7.7009e-05 0.0000e+00 4.9114e-05</hent>
<hent key="02400100000000000000040000000003" call="MPI_Isend" bytes="1024" orank="3" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7934e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000004000000000A" call="MPI_Isend" bytes="1024" orank="10" region="0" commid="0" count="1114" tid="0" op="" dtype="" >7.0643e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000004000000000C" call="MPI_Isend" bytes="1024" orank="12" region="0" commid="0" count="886" tid="0" op="" dtype="" >6.7759e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000040000000013" call="MPI_Isend" bytes="1024" orank="19" region="0" commid="0" count="3392" tid="0" op="" dtype="" >1.6518e-03 0.0000e+00 9.0599e-06</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >4.7684e-06 1.9073e-06 2.8610e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4363e+00 0.0000e+00 3.2515e+00</hent>
<hent key="038001000000000000000A0000000003" call="MPI_Irecv" bytes="2560" orank="3" region="0" commid="0" count="5" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000000A" call="MPI_Irecv" bytes="2560" orank="10" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000000C" call="MPI_Irecv" bytes="2560" orank="12" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000013" call="MPI_Irecv" bytes="2560" orank="19" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000004B" call="MPI_Irecv" bytes="2560" orank="75" region="0" commid="0" count="33" tid="0" op="" dtype="" >1.3828e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002CB" call="MPI_Irecv" bytes="2560" orank="715" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.3590e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000003" call="MPI_Isend" bytes="1792" orank="3" region="0" commid="0" count="49" tid="0" op="" dtype="" >8.4400e-05 9.5367e-07 6.9141e-06</hent>
<hent key="0240010000000000000007000000000A" call="MPI_Isend" bytes="1792" orank="10" region="0" commid="0" count="37" tid="0" op="" dtype="" >8.7976e-05 9.5367e-07 8.1062e-06</hent>
<hent key="0240010000000000000007000000000C" call="MPI_Isend" bytes="1792" orank="12" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.6093e-04 2.8610e-06 5.9605e-06</hent>
<hent key="02400100000000000000070000000013" call="MPI_Isend" bytes="1792" orank="19" region="0" commid="0" count="48" tid="0" op="" dtype="" >1.2183e-04 9.5367e-07 1.2159e-05</hent>
<hent key="0240010000000000000007000000004B" call="MPI_Isend" bytes="1792" orank="75" region="0" commid="0" count="142" tid="0" op="" dtype="" >7.3647e-04 9.5367e-07 1.1921e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6142e-02 3.6142e-02 3.6142e-02</hent>
<hent key="024001000000000000000700000002CB" call="MPI_Isend" bytes="1792" orank="715" region="0" commid="0" count="128" tid="0" op="" dtype="" >5.8460e-04 3.8147e-06 7.8678e-06</hent>
<hent key="024001000000000000000A0000000003" call="MPI_Isend" bytes="2560" orank="3" region="0" commid="0" count="3" tid="0" op="" dtype="" >5.9605e-06 9.5367e-07 3.0994e-06</hent>
<hent key="024001000000000000000A000000000A" call="MPI_Isend" bytes="2560" orank="10" region="0" commid="0" count="2" tid="0" op="" dtype="" >5.2452e-06 2.1458e-06 3.0994e-06</hent>
<hent key="024001000000000000000A000000000C" call="MPI_Isend" bytes="2560" orank="12" region="0" commid="0" count="8" tid="0" op="" dtype="" >3.5286e-05 4.0531e-06 5.0068e-06</hent>
<hent key="024001000000000000000A0000000013" call="MPI_Isend" bytes="2560" orank="19" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.0252e-05 9.5367e-07 2.1458e-06</hent>
<hent key="024001000000000000000A000000004B" call="MPI_Isend" bytes="2560" orank="75" region="0" commid="0" count="51" tid="0" op="" dtype="" >2.9683e-04 5.0068e-06 6.9141e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.8801e-04 3.8147e-05 1.3685e-04</hent>
<hent key="024001000000000000000A00000002CB" call="MPI_Isend" bytes="2560" orank="715" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.9479e-04 3.8147e-06 5.9605e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >8.8787e-04 3.4904e-04 5.3883e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.1948e-05 3.1948e-05 3.1948e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 1.9073e-06 3.0994e-06</hent>
<hent key="03800100000000000000000400000003" call="MPI_Irecv" bytes="4" orank="3" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.2595e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000000040000000A" call="MPI_Irecv" bytes="4" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.2336e-04 0.0000e+00 3.0994e-06</hent>
<hent key="0380010000000000000000040000000C" call="MPI_Irecv" bytes="4" orank="12" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.3777e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000013" call="MPI_Irecv" bytes="4" orank="19" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.4206e-04 0.0000e+00 2.8610e-06</hent>
<hent key="0380010000000000000000040000004B" call="MPI_Irecv" bytes="4" orank="75" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.5017e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.8029e-03 2.4199e-04 6.3300e-04</hent>
<hent key="038001000000000000001C000000000A" call="MPI_Irecv" bytes="7168" orank="10" region="0" commid="0" count="9018" tid="0" op="" dtype="" >3.0112e-03 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000001C000000000C" call="MPI_Irecv" bytes="7168" orank="12" region="0" commid="0" count="9591" tid="0" op="" dtype="" >1.3206e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000004000002CB" call="MPI_Irecv" bytes="4" orank="715" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.0930e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000400000003" call="MPI_Isend" bytes="4" orank="3" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2424e-03 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000000040000000A" call="MPI_Isend" bytes="4" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0197e-03 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000000040000000C" call="MPI_Isend" bytes="4" orank="12" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.2020e-03 0.0000e+00 7.7963e-05</hent>
<hent key="02400100000000000000000400000013" call="MPI_Isend" bytes="4" orank="19" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0021e-03 0.0000e+00 2.6941e-05</hent>
<hent key="0240010000000000000000040000004B" call="MPI_Isend" bytes="4" orank="75" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.2691e-02 3.8147e-06 1.4067e-05</hent>
<hent key="024001000000000000001C000000000A" call="MPI_Isend" bytes="7168" orank="10" region="0" commid="0" count="8636" tid="0" op="" dtype="" >1.7900e-02 9.5367e-07 2.0981e-05</hent>
<hent key="024001000000000000001C000000000C" call="MPI_Isend" bytes="7168" orank="12" region="0" commid="0" count="9498" tid="0" op="" dtype="" >2.7248e-02 9.5367e-07 2.2888e-05</hent>
<hent key="024001000000000000000004000002CB" call="MPI_Isend" bytes="4" orank="715" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6641e-02 2.8610e-06 1.8001e-04</hent>
<hent key="03800100000000000000050000000003" call="MPI_Irecv" bytes="1280" orank="3" region="0" commid="0" count="197" tid="0" op="" dtype="" >4.3631e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000A" call="MPI_Irecv" bytes="1280" orank="10" region="0" commid="0" count="193" tid="0" op="" dtype="" >7.2718e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000C" call="MPI_Irecv" bytes="1280" orank="12" region="0" commid="0" count="210" tid="0" op="" dtype="" >4.1485e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000013" call="MPI_Irecv" bytes="1280" orank="19" region="0" commid="0" count="239" tid="0" op="" dtype="" >5.4598e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000004B" call="MPI_Irecv" bytes="1280" orank="75" region="0" commid="0" count="306" tid="0" op="" dtype="" >1.3089e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.1066e+01 7.8678e-06 1.3898e-01</hent>
<hent key="03800100000000000000280000000003" call="MPI_Irecv" bytes="10240" orank="3" region="0" commid="0" count="16" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002CB" call="MPI_Irecv" bytes="1280" orank="715" region="0" commid="0" count="289" tid="0" op="" dtype="" >9.5606e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000003" call="MPI_Irecv" bytes="2048" orank="3" region="0" commid="0" count="17" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000A" call="MPI_Irecv" bytes="2048" orank="10" region="0" commid="0" count="15" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000C" call="MPI_Irecv" bytes="2048" orank="12" region="0" commid="0" count="21" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000013" call="MPI_Irecv" bytes="2048" orank="19" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000004B" call="MPI_Irecv" bytes="2048" orank="75" region="0" commid="0" count="3443" tid="0" op="" dtype="" >5.0998e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002CB" call="MPI_Irecv" bytes="2048" orank="715" region="0" commid="0" count="3429" tid="0" op="" dtype="" >5.2762e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000050000000003" call="MPI_Isend" bytes="1280" orank="3" region="0" commid="0" count="190" tid="0" op="" dtype="" >3.0899e-04 9.5367e-07 1.0014e-05</hent>
<hent key="0240010000000000000005000000000A" call="MPI_Isend" bytes="1280" orank="10" region="0" commid="0" count="213" tid="0" op="" dtype="" >3.8242e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000005000000000C" call="MPI_Isend" bytes="1280" orank="12" region="0" commid="0" count="181" tid="0" op="" dtype="" >6.7306e-04 2.8610e-06 1.0967e-05</hent>
<hent key="02400100000000000000050000000013" call="MPI_Isend" bytes="1280" orank="19" region="0" commid="0" count="216" tid="0" op="" dtype="" >3.0756e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000005000000004B" call="MPI_Isend" bytes="1280" orank="75" region="0" commid="0" count="269" tid="0" op="" dtype="" >1.4033e-03 4.0531e-06 1.0014e-05</hent>
<hent key="02400100000000000000280000000013" call="MPI_Isend" bytes="10240" orank="19" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.3351e-05 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000500000002CB" call="MPI_Isend" bytes="1280" orank="715" region="0" commid="0" count="288" tid="0" op="" dtype="" >1.2450e-03 2.8610e-06 9.0599e-06</hent>
</hash>
<internal rank="11" log_i="1723713849.513292" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="12" mpi_size="768" stamp_init="1723713791.110258" stamp_final="1723713849.508386" username="apac4" allocationname="unknown" flags="0" pid="1717075" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.83981e+01" utime="4.80371e+01" stime="7.70431e+00" mtime="3.20971e+01" gflop="0.00000e+00" gbyte="3.77377e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20971e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a515a4152f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83277e+01" utime="4.80101e+01" stime="7.69084e+00" mtime="3.20971e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20971e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4937e+08" > 4.5369e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4744e+08" > 4.2764e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6407e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4481e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.7830e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.2915e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6122e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.0703e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1996e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6152e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8858e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="197" >
<hent key="02400100000000000000080000000004" call="MPI_Isend" bytes="2048" orank="4" region="0" commid="0" count="12" tid="0" op="" dtype="" >2.1458e-05 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000008000000000B" call="MPI_Isend" bytes="2048" orank="11" region="0" commid="0" count="21" tid="0" op="" dtype="" >4.9114e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000008000000000D" call="MPI_Isend" bytes="2048" orank="13" region="0" commid="0" count="11" tid="0" op="" dtype="" >4.4823e-05 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000080000000014" call="MPI_Isend" bytes="2048" orank="20" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.2469e-04 9.5367e-07 1.8120e-05</hent>
<hent key="0240010000000000000008000000004C" call="MPI_Isend" bytes="2048" orank="76" region="0" commid="0" count="3457" tid="0" op="" dtype="" >1.4302e-02 9.5367e-07 3.2902e-05</hent>
<hent key="024001000000000000000800000002CC" call="MPI_Isend" bytes="2048" orank="716" region="0" commid="0" count="3457" tid="0" op="" dtype="" >1.2554e-02 9.5367e-07 2.7895e-05</hent>
<hent key="038001000000000000000E000000004C" call="MPI_Irecv" bytes="3584" orank="76" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000E000000004C" call="MPI_Isend" bytes="3584" orank="76" region="0" commid="0" count="1" tid="0" op="" dtype="" >6.1989e-06 6.1989e-06 6.1989e-06</hent>
<hent key="03800100000000000000028000000004" call="MPI_Irecv" bytes="640" orank="4" region="0" commid="0" count="410" tid="0" op="" dtype="" >1.5974e-04 0.0000e+00 6.1989e-06</hent>
<hent key="0380010000000000000002800000000B" call="MPI_Irecv" bytes="640" orank="11" region="0" commid="0" count="433" tid="0" op="" dtype="" >1.6856e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000D" call="MPI_Irecv" bytes="640" orank="13" region="0" commid="0" count="382" tid="0" op="" dtype="" >8.7500e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000014" call="MPI_Irecv" bytes="640" orank="20" region="0" commid="0" count="389" tid="0" op="" dtype="" >9.6798e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000002800000004C" call="MPI_Irecv" bytes="640" orank="76" region="0" commid="0" count="275" tid="0" op="" dtype="" >1.0657e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.8147e-06 3.8147e-06 3.8147e-06</hent>
<hent key="038001000000000000000280000002CC" call="MPI_Irecv" bytes="640" orank="716" region="0" commid="0" count="313" tid="0" op="" dtype="" >1.0061e-04 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000028000000004" call="MPI_Isend" bytes="640" orank="4" region="0" commid="0" count="409" tid="0" op="" dtype="" >5.0712e-04 9.5367e-07 1.0967e-05</hent>
<hent key="0240010000000000000002800000000B" call="MPI_Isend" bytes="640" orank="11" region="0" commid="0" count="419" tid="0" op="" dtype="" >5.7173e-04 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000002800000000D" call="MPI_Isend" bytes="640" orank="13" region="0" commid="0" count="422" tid="0" op="" dtype="" >1.3373e-03 1.9073e-06 1.0967e-05</hent>
<hent key="02400100000000000000028000000014" call="MPI_Isend" bytes="640" orank="20" region="0" commid="0" count="399" tid="0" op="" dtype="" >6.0678e-04 9.5367e-07 1.1921e-05</hent>
<hent key="0240010000000000000002800000004C" call="MPI_Isend" bytes="640" orank="76" region="0" commid="0" count="260" tid="0" op="" dtype="" >1.5280e-03 3.8147e-06 3.0994e-05</hent>
<hent key="024001000000000000000280000002CC" call="MPI_Isend" bytes="640" orank="716" region="0" commid="0" count="285" tid="0" op="" dtype="" >1.2496e-03 2.8610e-06 1.0014e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000014000000004" call="MPI_Irecv" bytes="320" orank="4" region="0" commid="0" count="361" tid="0" op="" dtype="" >1.3876e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000001400000000B" call="MPI_Irecv" bytes="320" orank="11" region="0" commid="0" count="341" tid="0" op="" dtype="" >1.5259e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000D" call="MPI_Irecv" bytes="320" orank="13" region="0" commid="0" count="335" tid="0" op="" dtype="" >8.3923e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000014" call="MPI_Irecv" bytes="320" orank="20" region="0" commid="0" count="364" tid="0" op="" dtype="" >6.5565e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000004C" call="MPI_Irecv" bytes="320" orank="76" region="0" commid="0" count="194" tid="0" op="" dtype="" >5.9366e-05 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000140000002CC" call="MPI_Irecv" bytes="320" orank="716" region="0" commid="0" count="165" tid="0" op="" dtype="" >6.5804e-05 0.0000e+00 3.0994e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >6.9141e-06 9.5367e-07 3.0994e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.6407e+00 0.0000e+00 1.3116e-01</hent>
<hent key="0380010000000000000040000000004C" call="MPI_Irecv" bytes="16384" orank="76" region="0" commid="0" count="12423" tid="0" op="" dtype="" >8.6584e-03 0.0000e+00 2.5034e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.7881e-05 0.0000e+00 1.6928e-05</hent>
<hent key="038001000000000000004000000002CC" call="MPI_Irecv" bytes="16384" orank="716" region="0" commid="0" count="12524" tid="0" op="" dtype="" >5.1620e-03 0.0000e+00 2.7895e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.0531e-06 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000014000000004" call="MPI_Isend" bytes="320" orank="4" region="0" commid="0" count="343" tid="0" op="" dtype="" >3.6502e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000001400000000B" call="MPI_Isend" bytes="320" orank="11" region="0" commid="0" count="321" tid="0" op="" dtype="" >3.4618e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000001400000000D" call="MPI_Isend" bytes="320" orank="13" region="0" commid="0" count="353" tid="0" op="" dtype="" >1.0126e-03 1.9073e-06 1.1921e-05</hent>
<hent key="02400100000000000000014000000014" call="MPI_Isend" bytes="320" orank="20" region="0" commid="0" count="390" tid="0" op="" dtype="" >4.3416e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000001400000004C" call="MPI_Isend" bytes="320" orank="76" region="0" commid="0" count="160" tid="0" op="" dtype="" >8.2541e-04 3.8147e-06 1.0967e-05</hent>
<hent key="03800100000000000000200000000004" call="MPI_Irecv" bytes="8192" orank="4" region="0" commid="0" count="12659" tid="0" op="" dtype="" >4.9136e-03 0.0000e+00 3.4094e-05</hent>
<hent key="0380010000000000000020000000000B" call="MPI_Irecv" bytes="8192" orank="11" region="0" commid="0" count="3201" tid="0" op="" dtype="" >1.2770e-03 0.0000e+00 7.8678e-06</hent>
<hent key="0380010000000000000020000000000D" call="MPI_Irecv" bytes="8192" orank="13" region="0" commid="0" count="3149" tid="0" op="" dtype="" >8.4448e-04 0.0000e+00 1.2159e-05</hent>
<hent key="03800100000000000000200000000014" call="MPI_Irecv" bytes="8192" orank="20" region="0" commid="0" count="12676" tid="0" op="" dtype="" >2.4445e-03 0.0000e+00 1.8120e-05</hent>
<hent key="024001000000000000000140000002CC" call="MPI_Isend" bytes="320" orank="716" region="0" commid="0" count="181" tid="0" op="" dtype="" >7.2575e-04 2.8610e-06 1.0014e-05</hent>
<hent key="03800100000000000000000000000004" call="MPI_Irecv" bytes="0" orank="4" region="0" commid="0" count="266" tid="0" op="" dtype="" >8.2493e-05 0.0000e+00 2.8610e-06</hent>
<hent key="0380010000000000000000000000000B" call="MPI_Irecv" bytes="0" orank="11" region="0" commid="0" count="235" tid="0" op="" dtype="" >8.3208e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000000000000000D" call="MPI_Irecv" bytes="0" orank="13" region="0" commid="0" count="263" tid="0" op="" dtype="" >6.2466e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000014" call="MPI_Irecv" bytes="0" orank="20" region="0" commid="0" count="264" tid="0" op="" dtype="" >6.3896e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000004C" call="MPI_Irecv" bytes="0" orank="76" region="0" commid="0" count="150" tid="0" op="" dtype="" >4.8637e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000040000000004C" call="MPI_Isend" bytes="16384" orank="76" region="0" commid="0" count="12646" tid="0" op="" dtype="" >1.6463e-01 3.8147e-06 7.5102e-05</hent>
<hent key="038001000000000000000000000002CC" call="MPI_Irecv" bytes="0" orank="716" region="0" commid="0" count="144" tid="0" op="" dtype="" >4.0531e-05 0.0000e+00 5.0068e-06</hent>
<hent key="024001000000000000004000000002CC" call="MPI_Isend" bytes="16384" orank="716" region="0" commid="0" count="12692" tid="0" op="" dtype="" >9.3722e-02 2.8610e-06 2.1315e-04</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.3385e-04 0.0000e+00 5.6601e-04</hent>
<hent key="02400100000000000000200000000004" call="MPI_Isend" bytes="8192" orank="4" region="0" commid="0" count="12639" tid="0" op="" dtype="" >7.5557e-03 0.0000e+00 2.3127e-05</hent>
<hent key="0240010000000000000020000000000B" call="MPI_Isend" bytes="8192" orank="11" region="0" commid="0" count="3108" tid="0" op="" dtype="" >3.5210e-03 0.0000e+00 1.6928e-05</hent>
<hent key="0240010000000000000020000000000D" call="MPI_Isend" bytes="8192" orank="13" region="0" commid="0" count="4251" tid="0" op="" dtype="" >8.5280e-03 0.0000e+00 2.0981e-05</hent>
<hent key="02400100000000000000200000000014" call="MPI_Isend" bytes="8192" orank="20" region="0" commid="0" count="12682" tid="0" op="" dtype="" >1.2140e-02 0.0000e+00 5.5075e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6001e-05 3.6001e-05 3.6001e-05</hent>
<hent key="02400100000000000000000000000004" call="MPI_Isend" bytes="0" orank="4" region="0" commid="0" count="276" tid="0" op="" dtype="" >2.1100e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000000000000000B" call="MPI_Isend" bytes="0" orank="11" region="0" commid="0" count="228" tid="0" op="" dtype="" >1.5473e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000000000000D" call="MPI_Isend" bytes="0" orank="13" region="0" commid="0" count="238" tid="0" op="" dtype="" >5.3072e-04 9.5367e-07 4.3869e-05</hent>
<hent key="02400100000000000000000000000014" call="MPI_Isend" bytes="0" orank="20" region="0" commid="0" count="252" tid="0" op="" dtype="" >1.8620e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000000000000004C" call="MPI_Isend" bytes="0" orank="76" region="0" commid="0" count="155" tid="0" op="" dtype="" >6.8069e-04 1.9073e-06 9.0599e-06</hent>
<hent key="024001000000000000000000000002CC" call="MPI_Isend" bytes="0" orank="716" region="0" commid="0" count="145" tid="0" op="" dtype="" >5.3620e-04 9.5367e-07 1.3113e-05</hent>
<hent key="03800100000000000000060000000004" call="MPI_Irecv" bytes="1536" orank="4" region="0" commid="0" count="103" tid="0" op="" dtype="" >4.9114e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000006000000000B" call="MPI_Irecv" bytes="1536" orank="11" region="0" commid="0" count="127" tid="0" op="" dtype="" >5.3167e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000D" call="MPI_Irecv" bytes="1536" orank="13" region="0" commid="0" count="95" tid="0" op="" dtype="" >2.5988e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000014" call="MPI_Irecv" bytes="1536" orank="20" region="0" commid="0" count="89" tid="0" op="" dtype="" >2.4080e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000004C" call="MPI_Irecv" bytes="1536" orank="76" region="0" commid="0" count="206" tid="0" op="" dtype="" >7.8201e-05 0.0000e+00 3.0994e-06</hent>
<hent key="038001000000000000000600000002CC" call="MPI_Irecv" bytes="1536" orank="716" region="0" commid="0" count="219" tid="0" op="" dtype="" >8.8930e-05 0.0000e+00 5.0068e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000004" call="MPI_Isend" bytes="1536" orank="4" region="0" commid="0" count="105" tid="0" op="" dtype="" >2.3413e-04 9.5367e-07 1.2159e-05</hent>
<hent key="0240010000000000000006000000000B" call="MPI_Isend" bytes="1536" orank="11" region="0" commid="0" count="101" tid="0" op="" dtype="" >2.0194e-04 9.5367e-07 4.0531e-06</hent>
<hent key="0240010000000000000006000000000D" call="MPI_Isend" bytes="1536" orank="13" region="0" commid="0" count="88" tid="0" op="" dtype="" >3.5119e-04 2.8610e-06 8.8215e-06</hent>
<hent key="02400100000000000000060000000014" call="MPI_Isend" bytes="1536" orank="20" region="0" commid="0" count="89" tid="0" op="" dtype="" >1.8120e-04 9.5367e-07 1.5020e-05</hent>
<hent key="0240010000000000000006000000004C" call="MPI_Isend" bytes="1536" orank="76" region="0" commid="0" count="221" tid="0" op="" dtype="" >1.3769e-03 4.0531e-06 1.4782e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002CC" call="MPI_Isend" bytes="1536" orank="716" region="0" commid="0" count="209" tid="0" op="" dtype="" >9.9659e-04 2.8610e-06 1.5020e-05</hent>
<hent key="038001000000000000000C0000000014" call="MPI_Irecv" bytes="3072" orank="20" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000C000000004C" call="MPI_Irecv" bytes="3072" orank="76" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002CC" call="MPI_Irecv" bytes="3072" orank="716" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C000000000B" call="MPI_Isend" bytes="3072" orank="11" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000C000000004C" call="MPI_Isend" bytes="3072" orank="76" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.6928e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000C00000002CC" call="MPI_Isend" bytes="3072" orank="716" region="0" commid="0" count="5" tid="0" op="" dtype="" >3.3617e-05 4.7684e-06 1.0014e-05</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8014e-04 2.8014e-04 2.8014e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >6.1703e-04 2.0313e-04 2.0885e-04</hent>
<hent key="03800100000000000000038000000004" call="MPI_Irecv" bytes="896" orank="4" region="0" commid="0" count="320" tid="0" op="" dtype="" >1.2326e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000003800000000B" call="MPI_Irecv" bytes="896" orank="11" region="0" commid="0" count="2832" tid="0" op="" dtype="" >4.9758e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000003800000000D" call="MPI_Irecv" bytes="896" orank="13" region="0" commid="0" count="2884" tid="0" op="" dtype="" >8.3876e-04 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000038000000014" call="MPI_Irecv" bytes="896" orank="20" region="0" commid="0" count="338" tid="0" op="" dtype="" >7.3195e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000004C" call="MPI_Irecv" bytes="896" orank="76" region="0" commid="0" count="326" tid="0" op="" dtype="" >1.2898e-04 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000380000002CC" call="MPI_Irecv" bytes="896" orank="716" region="0" commid="0" count="340" tid="0" op="" dtype="" >1.4758e-04 0.0000e+00 5.0068e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.7949e-05 6.7949e-05 6.7949e-05</hent>
<hent key="0380010000000000000038000000004C" call="MPI_Irecv" bytes="14336" orank="76" region="0" commid="0" count="276" tid="0" op="" dtype="" >2.0170e-04 0.0000e+00 1.5020e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="038001000000000000003800000002CC" call="MPI_Irecv" bytes="14336" orank="716" region="0" commid="0" count="175" tid="0" op="" dtype="" >7.2718e-05 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000038000000004" call="MPI_Isend" bytes="896" orank="4" region="0" commid="0" count="307" tid="0" op="" dtype="" >4.5729e-04 9.5367e-07 1.0967e-05</hent>
<hent key="0240010000000000000003800000000B" call="MPI_Isend" bytes="896" orank="11" region="0" commid="0" count="2908" tid="0" op="" dtype="" >2.2209e-03 0.0000e+00 1.3113e-05</hent>
<hent key="0240010000000000000003800000000D" call="MPI_Isend" bytes="896" orank="13" region="0" commid="0" count="2580" tid="0" op="" dtype="" >2.8760e-03 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000038000000014" call="MPI_Isend" bytes="896" orank="20" region="0" commid="0" count="308" tid="0" op="" dtype="" >5.3477e-04 9.5367e-07 1.3113e-05</hent>
<hent key="0240010000000000000003800000004C" call="MPI_Isend" bytes="896" orank="76" region="0" commid="0" count="350" tid="0" op="" dtype="" >2.0947e-03 3.8147e-06 1.6928e-05</hent>
<hent key="024001000000000000000380000002CC" call="MPI_Isend" bytes="896" orank="716" region="0" commid="0" count="342" tid="0" op="" dtype="" >1.4920e-03 2.8610e-06 1.3113e-05</hent>
<hent key="0240010000000000000038000000004C" call="MPI_Isend" bytes="14336" orank="76" region="0" commid="0" count="53" tid="0" op="" dtype="" >7.3361e-04 5.9605e-06 3.4094e-05</hent>
<hent key="024001000000000000003800000002CC" call="MPI_Isend" bytes="14336" orank="716" region="0" commid="0" count="7" tid="0" op="" dtype="" >5.2929e-05 5.0068e-06 1.3113e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.8518e+00 6.9141e-06 1.3915e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.1188e-04 5.1188e-04 5.1188e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >3.1996e-04 3.1996e-04 3.1996e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.2860e-03 2.2860e-03 2.2860e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >8.1091e-02 3.5810e-04 7.5171e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.6815e-04 5.6815e-04 5.6815e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 0.0000e+00 5.0068e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6122e+00 4.5395e-04 2.5641e-01</hent>
<hent key="03800100000000000000040000000004" call="MPI_Irecv" bytes="1024" orank="4" region="0" commid="0" count="3388" tid="0" op="" dtype="" >5.1308e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0380010000000000000004000000000B" call="MPI_Irecv" bytes="1024" orank="11" region="0" commid="0" count="886" tid="0" op="" dtype="" >1.1706e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000000D" call="MPI_Irecv" bytes="1024" orank="13" region="0" commid="0" count="864" tid="0" op="" dtype="" >2.1410e-04 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000040000000014" call="MPI_Irecv" bytes="1024" orank="20" region="0" commid="0" count="3392" tid="0" op="" dtype="" >5.4193e-04 0.0000e+00 1.1206e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >6.7830e-04 9.5367e-07 2.5892e-04</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="03800100000000000000070000000004" call="MPI_Irecv" bytes="1792" orank="4" region="0" commid="0" count="46" tid="0" op="" dtype="" >1.6451e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000B" call="MPI_Irecv" bytes="1792" orank="11" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.5736e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000D" call="MPI_Irecv" bytes="1792" orank="13" region="0" commid="0" count="34" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000014" call="MPI_Irecv" bytes="1792" orank="20" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.0014e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000004C" call="MPI_Irecv" bytes="1792" orank="76" region="0" commid="0" count="226" tid="0" op="" dtype="" >7.2479e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002CC" call="MPI_Irecv" bytes="1792" orank="716" region="0" commid="0" count="172" tid="0" op="" dtype="" >6.2704e-05 0.0000e+00 5.9605e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >1.3041e-04 0.0000e+00 6.7949e-05</hent>
<hent key="02400100000000000000040000000004" call="MPI_Isend" bytes="1024" orank="4" region="0" commid="0" count="3382" tid="0" op="" dtype="" >2.1839e-03 0.0000e+00 2.0027e-05</hent>
<hent key="0240010000000000000004000000000B" call="MPI_Isend" bytes="1024" orank="11" region="0" commid="0" count="848" tid="0" op="" dtype="" >5.6958e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000004000000000D" call="MPI_Isend" bytes="1024" orank="13" region="0" commid="0" count="1174" tid="0" op="" dtype="" >9.2125e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000040000000014" call="MPI_Isend" bytes="1024" orank="20" region="0" commid="0" count="3394" tid="0" op="" dtype="" >2.0189e-03 0.0000e+00 1.9073e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >2.1935e-05 5.0068e-06 1.6928e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4471e+00 0.0000e+00 3.2495e+00</hent>
<hent key="038001000000000000000A0000000004" call="MPI_Irecv" bytes="2560" orank="4" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000000B" call="MPI_Irecv" bytes="2560" orank="11" region="0" commid="0" count="8" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000000D" call="MPI_Irecv" bytes="2560" orank="13" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000014" call="MPI_Irecv" bytes="2560" orank="20" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000004C" call="MPI_Irecv" bytes="2560" orank="76" region="0" commid="0" count="45" tid="0" op="" dtype="" >1.7643e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002CC" call="MPI_Irecv" bytes="2560" orank="716" region="0" commid="0" count="55" tid="0" op="" dtype="" >1.5974e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000004" call="MPI_Isend" bytes="1792" orank="4" region="0" commid="0" count="43" tid="0" op="" dtype="" >8.9407e-05 9.5367e-07 8.1062e-06</hent>
<hent key="0240010000000000000007000000000B" call="MPI_Isend" bytes="1792" orank="11" region="0" commid="0" count="38" tid="0" op="" dtype="" >7.8917e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000007000000000D" call="MPI_Isend" bytes="1792" orank="13" region="0" commid="0" count="34" tid="0" op="" dtype="" >1.4091e-04 2.8610e-06 1.0014e-05</hent>
<hent key="02400100000000000000070000000014" call="MPI_Isend" bytes="1792" orank="20" region="0" commid="0" count="42" tid="0" op="" dtype="" >1.0085e-04 9.5367e-07 8.1062e-06</hent>
<hent key="0240010000000000000007000000004C" call="MPI_Isend" bytes="1792" orank="76" region="0" commid="0" count="133" tid="0" op="" dtype="" >7.9036e-04 9.5367e-07 1.6928e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6152e-02 3.6152e-02 3.6152e-02</hent>
<hent key="024001000000000000000700000002CC" call="MPI_Isend" bytes="1792" orank="716" region="0" commid="0" count="120" tid="0" op="" dtype="" >5.5575e-04 1.9073e-06 1.2875e-05</hent>
<hent key="024001000000000000000A0000000004" call="MPI_Isend" bytes="2560" orank="4" region="0" commid="0" count="9" tid="0" op="" dtype="" >4.5061e-05 9.5367e-07 1.3113e-05</hent>
<hent key="024001000000000000000A000000000B" call="MPI_Isend" bytes="2560" orank="11" region="0" commid="0" count="2" tid="0" op="" dtype="" >5.0068e-06 2.1458e-06 2.8610e-06</hent>
<hent key="024001000000000000000A000000000D" call="MPI_Isend" bytes="2560" orank="13" region="0" commid="0" count="4" tid="0" op="" dtype="" >3.2902e-05 4.0531e-06 1.8835e-05</hent>
<hent key="024001000000000000000A0000000014" call="MPI_Isend" bytes="2560" orank="20" region="0" commid="0" count="4" tid="0" op="" dtype="" >3.8385e-05 1.1921e-06 1.9073e-05</hent>
<hent key="024001000000000000000A000000004C" call="MPI_Isend" bytes="2560" orank="76" region="0" commid="0" count="35" tid="0" op="" dtype="" >2.3055e-04 4.7684e-06 1.4067e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.0494e-04 3.7909e-05 1.4305e-04</hent>
<hent key="024001000000000000000A00000002CC" call="MPI_Isend" bytes="2560" orank="716" region="0" commid="0" count="50" tid="0" op="" dtype="" >2.5940e-04 3.8147e-06 1.0967e-05</hent>
<hent key="0240010000000000000010000000004C" call="MPI_Isend" bytes="4096" orank="76" region="0" commid="0" count="1" tid="0" op="" dtype="" >7.1526e-06 7.1526e-06 7.1526e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.4795e-04 3.6693e-04 5.8103e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.5048e-05 3.5048e-05 3.5048e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.1921e-05 1.9073e-06 1.0014e-05</hent>
<hent key="03800100000000000000000400000004" call="MPI_Irecv" bytes="4" orank="4" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.5265e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000000040000000B" call="MPI_Irecv" bytes="4" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1804e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000000040000000D" call="MPI_Irecv" bytes="4" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.6233e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000014" call="MPI_Irecv" bytes="4" orank="20" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.1859e-04 0.0000e+00 1.5020e-05</hent>
<hent key="0380010000000000000000040000004C" call="MPI_Irecv" bytes="4" orank="76" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.2827e-04 0.0000e+00 1.1206e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.9083e-03 2.5606e-04 6.5517e-04</hent>
<hent key="038001000000000000001C000000000B" call="MPI_Irecv" bytes="7168" orank="11" region="0" commid="0" count="9498" tid="0" op="" dtype="" >3.8571e-03 0.0000e+00 8.1062e-06</hent>
<hent key="038001000000000000001C000000000D" call="MPI_Irecv" bytes="7168" orank="13" region="0" commid="0" count="9550" tid="0" op="" dtype="" >2.8753e-03 0.0000e+00 4.1008e-05</hent>
<hent key="038001000000000000000004000002CC" call="MPI_Irecv" bytes="4" orank="716" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0095e-03 0.0000e+00 1.6928e-05</hent>
<hent key="02400100000000000000000400000004" call="MPI_Isend" bytes="4" orank="4" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.5306e-03 0.0000e+00 1.2875e-05</hent>
<hent key="0240010000000000000000040000000B" call="MPI_Isend" bytes="4" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3051e-03 0.0000e+00 6.1989e-05</hent>
<hent key="0240010000000000000000040000000D" call="MPI_Isend" bytes="4" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.6879e-03 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000000400000014" call="MPI_Isend" bytes="4" orank="20" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4830e-03 0.0000e+00 2.5034e-05</hent>
<hent key="0240010000000000000000040000004C" call="MPI_Isend" bytes="4" orank="76" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.5803e-02 3.8147e-06 4.1008e-05</hent>
<hent key="024001000000000000001C000000000B" call="MPI_Isend" bytes="7168" orank="11" region="0" commid="0" count="9591" tid="0" op="" dtype="" >2.0426e-02 9.5367e-07 1.5974e-05</hent>
<hent key="024001000000000000001C000000000D" call="MPI_Isend" bytes="7168" orank="13" region="0" commid="0" count="8448" tid="0" op="" dtype="" >2.6153e-02 9.5367e-07 2.3127e-05</hent>
<hent key="024001000000000000000004000002CC" call="MPI_Isend" bytes="4" orank="716" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.8330e-02 2.8610e-06 1.1301e-04</hent>
<hent key="03800100000000000000050000000004" call="MPI_Irecv" bytes="1280" orank="4" region="0" commid="0" count="188" tid="0" op="" dtype="" >8.2493e-05 0.0000e+00 9.0599e-06</hent>
<hent key="0380010000000000000005000000000B" call="MPI_Irecv" bytes="1280" orank="11" region="0" commid="0" count="181" tid="0" op="" dtype="" >6.6042e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000D" call="MPI_Irecv" bytes="1280" orank="13" region="0" commid="0" count="218" tid="0" op="" dtype="" >4.7207e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000014" call="MPI_Irecv" bytes="1280" orank="20" region="0" commid="0" count="195" tid="0" op="" dtype="" >4.8161e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000004C" call="MPI_Irecv" bytes="1280" orank="76" region="0" commid="0" count="274" tid="0" op="" dtype="" >1.1015e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0922e+01 7.8678e-06 1.2807e-01</hent>
<hent key="03800100000000000000280000000004" call="MPI_Irecv" bytes="10240" orank="4" region="0" commid="0" count="40" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000280000000014" call="MPI_Irecv" bytes="10240" orank="20" region="0" commid="0" count="23" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002CC" call="MPI_Irecv" bytes="1280" orank="716" region="0" commid="0" count="275" tid="0" op="" dtype="" >1.2565e-04 0.0000e+00 1.3113e-05</hent>
<hent key="03800100000000000000080000000004" call="MPI_Irecv" bytes="2048" orank="4" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000B" call="MPI_Irecv" bytes="2048" orank="11" region="0" commid="0" count="13" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000D" call="MPI_Irecv" bytes="2048" orank="13" region="0" commid="0" count="17" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000014" call="MPI_Irecv" bytes="2048" orank="20" region="0" commid="0" count="14" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000004C" call="MPI_Irecv" bytes="2048" orank="76" region="0" commid="0" count="3396" tid="0" op="" dtype="" >7.1692e-04 0.0000e+00 1.0014e-05</hent>
<hent key="038001000000000000000800000002CC" call="MPI_Irecv" bytes="2048" orank="716" region="0" commid="0" count="3408" tid="0" op="" dtype="" >8.6713e-04 0.0000e+00 1.5974e-05</hent>
<hent key="02400100000000000000050000000004" call="MPI_Isend" bytes="1280" orank="4" region="0" commid="0" count="211" tid="0" op="" dtype="" >2.9898e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000005000000000B" call="MPI_Isend" bytes="1280" orank="11" region="0" commid="0" count="210" tid="0" op="" dtype="" >3.8838e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000005000000000D" call="MPI_Isend" bytes="1280" orank="13" region="0" commid="0" count="193" tid="0" op="" dtype="" >7.0691e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000050000000014" call="MPI_Isend" bytes="1280" orank="20" region="0" commid="0" count="194" tid="0" op="" dtype="" >3.4952e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000005000000004C" call="MPI_Isend" bytes="1280" orank="76" region="0" commid="0" count="321" tid="0" op="" dtype="" >1.9028e-03 3.8147e-06 1.3113e-05</hent>
<hent key="02400100000000000000280000000004" call="MPI_Isend" bytes="10240" orank="4" region="0" commid="0" count="60" tid="0" op="" dtype="" >2.5511e-05 0.0000e+00 3.0994e-06</hent>
<hent key="02400100000000000000280000000014" call="MPI_Isend" bytes="10240" orank="20" region="0" commid="0" count="17" tid="0" op="" dtype="" >1.2398e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000500000002CC" call="MPI_Isend" bytes="1280" orank="716" region="0" commid="0" count="303" tid="0" op="" dtype="" >1.3566e-03 2.8610e-06 1.6928e-05</hent>
</hash>
<internal rank="12" log_i="1723713849.508386" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="13" mpi_size="768" stamp_init="1723713791.112115" stamp_final="1723713849.502236" username="apac4" allocationname="unknown" flags="0" pid="1717076" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.83901e+01" utime="5.02690e+01" stime="6.72738e+00" mtime="3.21126e+01" gflop="0.00000e+00" gbyte="3.76766e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21126e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83185e+01" utime="5.02380e+01" stime="6.71829e+00" mtime="3.21126e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21126e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4895e+08" > 3.5400e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4938e+08" > 2.4900e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7639e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4510e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4121e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6117e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.4498e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0994e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6132e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9866e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="195" >
<hent key="02400100000000000000080000000005" call="MPI_Isend" bytes="2048" orank="5" region="0" commid="0" count="14" tid="0" op="" dtype="" >5.2929e-05 9.5367e-07 1.1921e-05</hent>
<hent key="0240010000000000000008000000000C" call="MPI_Isend" bytes="2048" orank="12" region="0" commid="0" count="17" tid="0" op="" dtype="" >4.2200e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000008000000000E" call="MPI_Isend" bytes="2048" orank="14" region="0" commid="0" count="11" tid="0" op="" dtype="" >4.4107e-05 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000080000000015" call="MPI_Isend" bytes="2048" orank="21" region="0" commid="0" count="18" tid="0" op="" dtype="" >5.9366e-05 9.5367e-07 1.2875e-05</hent>
<hent key="0240010000000000000008000000004D" call="MPI_Isend" bytes="2048" orank="77" region="0" commid="0" count="3449" tid="0" op="" dtype="" >1.1446e-02 9.5367e-07 2.5988e-05</hent>
<hent key="024001000000000000000800000002CD" call="MPI_Isend" bytes="2048" orank="717" region="0" commid="0" count="3450" tid="0" op="" dtype="" >1.0345e-02 9.5367e-07 1.4067e-05</hent>
<hent key="038001000000000000000E000000004D" call="MPI_Irecv" bytes="3584" orank="77" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000E000000004D" call="MPI_Isend" bytes="3584" orank="77" region="0" commid="0" count="1" tid="0" op="" dtype="" >6.9141e-06 6.9141e-06 6.9141e-06</hent>
<hent key="03800100000000000000028000000005" call="MPI_Irecv" bytes="640" orank="5" region="0" commid="0" count="447" tid="0" op="" dtype="" >8.5831e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000C" call="MPI_Irecv" bytes="640" orank="12" region="0" commid="0" count="422" tid="0" op="" dtype="" >1.4329e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000E" call="MPI_Irecv" bytes="640" orank="14" region="0" commid="0" count="395" tid="0" op="" dtype="" >7.7724e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000015" call="MPI_Irecv" bytes="640" orank="21" region="0" commid="0" count="406" tid="0" op="" dtype="" >6.9380e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000004D" call="MPI_Irecv" bytes="640" orank="77" region="0" commid="0" count="267" tid="0" op="" dtype="" >6.4850e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="038001000000000000000280000002CD" call="MPI_Irecv" bytes="640" orank="717" region="0" commid="0" count="275" tid="0" op="" dtype="" >6.4135e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000005" call="MPI_Isend" bytes="640" orank="5" region="0" commid="0" count="426" tid="0" op="" dtype="" >4.6611e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000002800000000C" call="MPI_Isend" bytes="640" orank="12" region="0" commid="0" count="382" tid="0" op="" dtype="" >5.2738e-04 9.5367e-07 7.1526e-06</hent>
<hent key="0240010000000000000002800000000E" call="MPI_Isend" bytes="640" orank="14" region="0" commid="0" count="395" tid="0" op="" dtype="" >1.1992e-03 1.9073e-06 8.1062e-06</hent>
<hent key="02400100000000000000028000000015" call="MPI_Isend" bytes="640" orank="21" region="0" commid="0" count="374" tid="0" op="" dtype="" >4.5466e-04 9.5367e-07 1.0014e-05</hent>
<hent key="0240010000000000000002800000004D" call="MPI_Isend" bytes="640" orank="77" region="0" commid="0" count="290" tid="0" op="" dtype="" >1.3826e-03 3.8147e-06 1.0014e-05</hent>
<hent key="024001000000000000000280000002CD" call="MPI_Isend" bytes="640" orank="717" region="0" commid="0" count="280" tid="0" op="" dtype="" >1.2105e-03 2.8610e-06 9.0599e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1206e-05 1.1206e-05 1.1206e-05</hent>
<hent key="03800100000000000000014000000005" call="MPI_Irecv" bytes="320" orank="5" region="0" commid="0" count="377" tid="0" op="" dtype="" >8.0585e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000001400000000C" call="MPI_Irecv" bytes="320" orank="12" region="0" commid="0" count="353" tid="0" op="" dtype="" >1.2493e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000E" call="MPI_Irecv" bytes="320" orank="14" region="0" commid="0" count="368" tid="0" op="" dtype="" >8.4162e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000015" call="MPI_Irecv" bytes="320" orank="21" region="0" commid="0" count="346" tid="0" op="" dtype="" >6.4850e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000001400000004D" call="MPI_Irecv" bytes="320" orank="77" region="0" commid="0" count="188" tid="0" op="" dtype="" >5.5075e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002CD" call="MPI_Irecv" bytes="320" orank="717" region="0" commid="0" count="181" tid="0" op="" dtype="" >3.9101e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.7639e+00 0.0000e+00 1.3115e-01</hent>
<hent key="0380010000000000000040000000004D" call="MPI_Irecv" bytes="16384" orank="77" region="0" commid="0" count="12516" tid="0" op="" dtype="" >4.7042e-03 0.0000e+00 8.1062e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.7881e-05 0.0000e+00 1.6928e-05</hent>
<hent key="038001000000000000004000000002CD" call="MPI_Irecv" bytes="16384" orank="717" region="0" commid="0" count="12588" tid="0" op="" dtype="" >1.7369e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 0.0000e+00 3.0994e-06</hent>
<hent key="02400100000000000000014000000005" call="MPI_Isend" bytes="320" orank="5" region="0" commid="0" count="324" tid="0" op="" dtype="" >3.1352e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000001400000000C" call="MPI_Isend" bytes="320" orank="12" region="0" commid="0" count="335" tid="0" op="" dtype="" >3.6693e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000001400000000E" call="MPI_Isend" bytes="320" orank="14" region="0" commid="0" count="368" tid="0" op="" dtype="" >1.0102e-03 1.1921e-06 9.0599e-06</hent>
<hent key="02400100000000000000014000000015" call="MPI_Isend" bytes="320" orank="21" region="0" commid="0" count="361" tid="0" op="" dtype="" >3.4356e-04 0.0000e+00 6.1989e-06</hent>
<hent key="0240010000000000000001400000004D" call="MPI_Isend" bytes="320" orank="77" region="0" commid="0" count="164" tid="0" op="" dtype="" >7.3600e-04 3.8147e-06 5.9605e-06</hent>
<hent key="03800100000000000000200000000005" call="MPI_Irecv" bytes="8192" orank="5" region="0" commid="0" count="12672" tid="0" op="" dtype="" >2.8162e-03 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000020000000000C" call="MPI_Irecv" bytes="8192" orank="12" region="0" commid="0" count="4251" tid="0" op="" dtype="" >1.1926e-03 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000020000000000E" call="MPI_Irecv" bytes="8192" orank="14" region="0" commid="0" count="3730" tid="0" op="" dtype="" >5.3382e-04 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000200000000015" call="MPI_Irecv" bytes="8192" orank="21" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.2872e-03 0.0000e+00 4.0531e-06</hent>
<hent key="024001000000000000000140000002CD" call="MPI_Isend" bytes="320" orank="717" region="0" commid="0" count="192" tid="0" op="" dtype="" >8.1682e-04 2.8610e-06 8.1062e-06</hent>
<hent key="03800100000000000000000000000005" call="MPI_Irecv" bytes="0" orank="5" region="0" commid="0" count="245" tid="0" op="" dtype="" >4.8399e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000C" call="MPI_Irecv" bytes="0" orank="12" region="0" commid="0" count="238" tid="0" op="" dtype="" >7.7009e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000000000000000E" call="MPI_Irecv" bytes="0" orank="14" region="0" commid="0" count="265" tid="0" op="" dtype="" >6.7711e-05 0.0000e+00 3.8147e-06</hent>
<hent key="03800100000000000000000000000015" call="MPI_Irecv" bytes="0" orank="21" region="0" commid="0" count="253" tid="0" op="" dtype="" >2.1815e-04 0.0000e+00 1.6713e-04</hent>
<hent key="0380010000000000000000000000004D" call="MPI_Irecv" bytes="0" orank="77" region="0" commid="0" count="144" tid="0" op="" dtype="" >3.1710e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000040000000004D" call="MPI_Isend" bytes="16384" orank="77" region="0" commid="0" count="12624" tid="0" op="" dtype="" >1.0260e-01 3.8147e-06 2.2888e-05</hent>
<hent key="038001000000000000000000000002CD" call="MPI_Irecv" bytes="0" orank="717" region="0" commid="0" count="157" tid="0" op="" dtype="" >3.2425e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002CD" call="MPI_Isend" bytes="16384" orank="717" region="0" commid="0" count="12660" tid="0" op="" dtype="" >7.7119e-02 2.8610e-06 4.9114e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.2432e-04 0.0000e+00 5.6291e-04</hent>
<hent key="02400100000000000000200000000005" call="MPI_Isend" bytes="8192" orank="5" region="0" commid="0" count="12597" tid="0" op="" dtype="" >3.9871e-03 0.0000e+00 4.7684e-06</hent>
<hent key="0240010000000000000020000000000C" call="MPI_Isend" bytes="8192" orank="12" region="0" commid="0" count="3149" tid="0" op="" dtype="" >3.7608e-03 0.0000e+00 4.9829e-05</hent>
<hent key="0240010000000000000020000000000E" call="MPI_Isend" bytes="8192" orank="14" region="0" commid="0" count="3881" tid="0" op="" dtype="" >7.3090e-03 0.0000e+00 7.7009e-05</hent>
<hent key="02400100000000000000200000000015" call="MPI_Isend" bytes="8192" orank="21" region="0" commid="0" count="12679" tid="0" op="" dtype="" >8.2648e-03 0.0000e+00 6.0081e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0041e-05 3.0041e-05 3.0041e-05</hent>
<hent key="02400100000000000000000000000005" call="MPI_Isend" bytes="0" orank="5" region="0" commid="0" count="279" tid="0" op="" dtype="" >2.0504e-04 0.0000e+00 6.1989e-06</hent>
<hent key="0240010000000000000000000000000C" call="MPI_Isend" bytes="0" orank="12" region="0" commid="0" count="263" tid="0" op="" dtype="" >2.0146e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000000000000E" call="MPI_Isend" bytes="0" orank="14" region="0" commid="0" count="272" tid="0" op="" dtype="" >5.4789e-04 0.0000e+00 5.2929e-05</hent>
<hent key="02400100000000000000000000000015" call="MPI_Isend" bytes="0" orank="21" region="0" commid="0" count="278" tid="0" op="" dtype="" >1.9145e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000000000004D" call="MPI_Isend" bytes="0" orank="77" region="0" commid="0" count="140" tid="0" op="" dtype="" >5.3382e-04 9.5367e-07 8.1062e-06</hent>
<hent key="024001000000000000000000000002CD" call="MPI_Isend" bytes="0" orank="717" region="0" commid="0" count="155" tid="0" op="" dtype="" >5.5075e-04 9.5367e-07 7.8678e-06</hent>
<hent key="03800100000000000000060000000005" call="MPI_Irecv" bytes="1536" orank="5" region="0" commid="0" count="87" tid="0" op="" dtype="" >1.9550e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000C" call="MPI_Irecv" bytes="1536" orank="12" region="0" commid="0" count="88" tid="0" op="" dtype="" >2.4319e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000E" call="MPI_Irecv" bytes="1536" orank="14" region="0" commid="0" count="94" tid="0" op="" dtype="" >1.7881e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000015" call="MPI_Irecv" bytes="1536" orank="21" region="0" commid="0" count="107" tid="0" op="" dtype="" >2.2173e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000004D" call="MPI_Irecv" bytes="1536" orank="77" region="0" commid="0" count="213" tid="0" op="" dtype="" >5.7459e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002CD" call="MPI_Irecv" bytes="1536" orank="717" region="0" commid="0" count="200" tid="0" op="" dtype="" >6.3181e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000005" call="MPI_Isend" bytes="1536" orank="5" region="0" commid="0" count="76" tid="0" op="" dtype="" >1.4472e-04 9.5367e-07 1.2159e-05</hent>
<hent key="0240010000000000000006000000000C" call="MPI_Isend" bytes="1536" orank="12" region="0" commid="0" count="95" tid="0" op="" dtype="" >1.8096e-04 9.5367e-07 5.0068e-06</hent>
<hent key="0240010000000000000006000000000E" call="MPI_Isend" bytes="1536" orank="14" region="0" commid="0" count="89" tid="0" op="" dtype="" >3.4380e-04 2.8610e-06 1.7881e-05</hent>
<hent key="02400100000000000000060000000015" call="MPI_Isend" bytes="1536" orank="21" region="0" commid="0" count="111" tid="0" op="" dtype="" >2.2316e-04 9.5367e-07 1.1921e-05</hent>
<hent key="0240010000000000000006000000004D" call="MPI_Isend" bytes="1536" orank="77" region="0" commid="0" count="207" tid="0" op="" dtype="" >1.1051e-03 3.8147e-06 1.1206e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002CD" call="MPI_Isend" bytes="1536" orank="717" region="0" commid="0" count="211" tid="0" op="" dtype="" >9.9277e-04 3.8147e-06 6.1989e-06</hent>
<hent key="038001000000000000000C0000000015" call="MPI_Irecv" bytes="3072" orank="21" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000C000000004D" call="MPI_Irecv" bytes="3072" orank="77" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002CD" call="MPI_Irecv" bytes="3072" orank="717" region="0" commid="0" count="8" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C000000000E" call="MPI_Isend" bytes="3072" orank="14" region="0" commid="0" count="1" tid="0" op="" dtype="" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="024001000000000000000C000000004D" call="MPI_Isend" bytes="3072" orank="77" region="0" commid="0" count="5" tid="0" op="" dtype="" >3.0756e-05 5.0068e-06 6.9141e-06</hent>
<hent key="024001000000000000000C00000002CD" call="MPI_Isend" bytes="3072" orank="717" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.3127e-05 5.0068e-06 6.1989e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0184e-04 3.0184e-04 3.0184e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >6.5827e-04 2.1505e-04 2.2316e-04</hent>
<hent key="03800100000000000000038000000005" call="MPI_Irecv" bytes="896" orank="5" region="0" commid="0" count="297" tid="0" op="" dtype="" >5.6505e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000003800000000C" call="MPI_Irecv" bytes="896" orank="12" region="0" commid="0" count="2580" tid="0" op="" dtype="" >4.0388e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000E" call="MPI_Irecv" bytes="896" orank="14" region="0" commid="0" count="2714" tid="0" op="" dtype="" >5.1093e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000038000000015" call="MPI_Irecv" bytes="896" orank="21" region="0" commid="0" count="314" tid="0" op="" dtype="" >5.2452e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000004D" call="MPI_Irecv" bytes="896" orank="77" region="0" commid="0" count="327" tid="0" op="" dtype="" >9.7036e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002CD" call="MPI_Irecv" bytes="896" orank="717" region="0" commid="0" count="338" tid="0" op="" dtype="" >1.1301e-04 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.6015e-05 4.6015e-05 4.6015e-05</hent>
<hent key="0380010000000000000038000000004D" call="MPI_Irecv" bytes="14336" orank="77" region="0" commid="0" count="183" tid="0" op="" dtype="" >8.6546e-05 0.0000e+00 3.0994e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000003800000002CD" call="MPI_Irecv" bytes="14336" orank="717" region="0" commid="0" count="111" tid="0" op="" dtype="" >2.3603e-05 0.0000e+00 3.0994e-06</hent>
<hent key="02400100000000000000038000000005" call="MPI_Isend" bytes="896" orank="5" region="0" commid="0" count="317" tid="0" op="" dtype="" >4.3106e-04 9.5367e-07 6.1989e-06</hent>
<hent key="0240010000000000000003800000000C" call="MPI_Isend" bytes="896" orank="12" region="0" commid="0" count="2884" tid="0" op="" dtype="" >2.0969e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000003800000000E" call="MPI_Isend" bytes="896" orank="14" region="0" commid="0" count="2666" tid="0" op="" dtype="" >2.6219e-03 0.0000e+00 3.4094e-05</hent>
<hent key="02400100000000000000038000000015" call="MPI_Isend" bytes="896" orank="21" region="0" commid="0" count="313" tid="0" op="" dtype="" >4.3678e-04 9.5367e-07 7.8678e-06</hent>
<hent key="0240010000000000000003800000004D" call="MPI_Isend" bytes="896" orank="77" region="0" commid="0" count="319" tid="0" op="" dtype="" >1.5709e-03 3.8147e-06 8.8215e-06</hent>
<hent key="024001000000000000000380000002CD" call="MPI_Isend" bytes="896" orank="717" region="0" commid="0" count="339" tid="0" op="" dtype="" >1.5140e-03 2.8610e-06 2.0027e-05</hent>
<hent key="0240010000000000000038000000004D" call="MPI_Isend" bytes="14336" orank="77" region="0" commid="0" count="75" tid="0" op="" dtype="" >6.1083e-04 5.0068e-06 1.3113e-05</hent>
<hent key="024001000000000000003800000002CD" call="MPI_Isend" bytes="14336" orank="717" region="0" commid="0" count="39" tid="0" op="" dtype="" >2.2411e-04 4.0531e-06 9.0599e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.4389e+00 1.6928e-05 1.3923e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.8389e-04 5.8389e-04 5.8389e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >3.0994e-04 3.0994e-04 3.0994e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >1.8249e-03 1.8249e-03 1.8249e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >8.4204e-02 3.2401e-04 7.9213e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.6005e-04 5.6005e-04 5.6005e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6117e+00 4.3797e-04 2.5639e-01</hent>
<hent key="03800100000000000000040000000005" call="MPI_Irecv" bytes="1024" orank="5" region="0" commid="0" count="3392" tid="0" op="" dtype="" >4.6825e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000000C" call="MPI_Irecv" bytes="1024" orank="12" region="0" commid="0" count="1174" tid="0" op="" dtype="" >1.5974e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000000E" call="MPI_Irecv" bytes="1024" orank="14" region="0" commid="0" count="1008" tid="0" op="" dtype="" >2.0051e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000040000000015" call="MPI_Irecv" bytes="1024" orank="21" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.6372e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.2888e-05 0.0000e+00 9.0599e-06</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="03800100000000000000070000000005" call="MPI_Irecv" bytes="1792" orank="5" region="0" commid="0" count="29" tid="0" op="" dtype="" >6.4373e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000C" call="MPI_Irecv" bytes="1792" orank="12" region="0" commid="0" count="34" tid="0" op="" dtype="" >1.3590e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000E" call="MPI_Irecv" bytes="1792" orank="14" region="0" commid="0" count="26" tid="0" op="" dtype="" >8.8215e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000015" call="MPI_Irecv" bytes="1792" orank="21" region="0" commid="0" count="36" tid="0" op="" dtype="" >4.5300e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000004D" call="MPI_Irecv" bytes="1792" orank="77" region="0" commid="0" count="187" tid="0" op="" dtype="" >3.6001e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002CD" call="MPI_Irecv" bytes="1792" orank="717" region="0" commid="0" count="165" tid="0" op="" dtype="" >5.8174e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >7.8917e-05 0.0000e+00 5.0068e-05</hent>
<hent key="02400100000000000000040000000005" call="MPI_Isend" bytes="1024" orank="5" region="0" commid="0" count="3372" tid="0" op="" dtype="" >1.7295e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000004000000000C" call="MPI_Isend" bytes="1024" orank="12" region="0" commid="0" count="864" tid="0" op="" dtype="" >5.3334e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000004000000000E" call="MPI_Isend" bytes="1024" orank="14" region="0" commid="0" count="1050" tid="0" op="" dtype="" >7.3171e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000040000000015" call="MPI_Isend" bytes="1024" orank="21" region="0" commid="0" count="3392" tid="0" op="" dtype="" >1.8623e-03 0.0000e+00 1.0014e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4501e+00 0.0000e+00 3.2516e+00</hent>
<hent key="038001000000000000000A0000000005" call="MPI_Irecv" bytes="2560" orank="5" region="0" commid="0" count="8" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000000C" call="MPI_Irecv" bytes="2560" orank="12" region="0" commid="0" count="4" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A000000000E" call="MPI_Irecv" bytes="2560" orank="14" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000015" call="MPI_Irecv" bytes="2560" orank="21" region="0" commid="0" count="10" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000004D" call="MPI_Irecv" bytes="2560" orank="77" region="0" commid="0" count="52" tid="0" op="" dtype="" >1.1921e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002CD" call="MPI_Irecv" bytes="2560" orank="717" region="0" commid="0" count="34" tid="0" op="" dtype="" >1.0014e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000005" call="MPI_Isend" bytes="1792" orank="5" region="0" commid="0" count="32" tid="0" op="" dtype="" >5.3167e-05 9.5367e-07 6.9141e-06</hent>
<hent key="0240010000000000000007000000000C" call="MPI_Isend" bytes="1792" orank="12" region="0" commid="0" count="34" tid="0" op="" dtype="" >7.9393e-05 9.5367e-07 9.0599e-06</hent>
<hent key="0240010000000000000007000000000E" call="MPI_Isend" bytes="1792" orank="14" region="0" commid="0" count="35" tid="0" op="" dtype="" >1.3232e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000070000000015" call="MPI_Isend" bytes="1792" orank="21" region="0" commid="0" count="37" tid="0" op="" dtype="" >9.8944e-05 9.5367e-07 1.2159e-05</hent>
<hent key="0240010000000000000007000000004D" call="MPI_Isend" bytes="1792" orank="77" region="0" commid="0" count="158" tid="0" op="" dtype="" >7.9155e-04 9.5367e-07 1.0967e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6132e-02 3.6132e-02 3.6132e-02</hent>
<hent key="024001000000000000000700000002CD" call="MPI_Isend" bytes="1792" orank="717" region="0" commid="0" count="143" tid="0" op="" dtype="" >6.7377e-04 9.5367e-07 1.0014e-05</hent>
<hent key="024001000000000000000A0000000005" call="MPI_Isend" bytes="2560" orank="5" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.9087e-05 9.5367e-07 1.1921e-05</hent>
<hent key="024001000000000000000A000000000C" call="MPI_Isend" bytes="2560" orank="12" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.3828e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A000000000E" call="MPI_Isend" bytes="2560" orank="14" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.5034e-05 3.0994e-06 5.0068e-06</hent>
<hent key="024001000000000000000A0000000015" call="MPI_Isend" bytes="2560" orank="21" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.3113e-05 1.9073e-06 7.1526e-06</hent>
<hent key="024001000000000000000A000000004D" call="MPI_Isend" bytes="2560" orank="77" region="0" commid="0" count="52" tid="0" op="" dtype="" >2.9993e-04 5.0068e-06 6.9141e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.3212e-04 4.2200e-05 1.5688e-04</hent>
<hent key="024001000000000000000A00000002CD" call="MPI_Isend" bytes="2560" orank="717" region="0" commid="0" count="47" tid="0" op="" dtype="" >2.4486e-04 4.0531e-06 6.1989e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.0507e-03 3.9387e-04 6.5684e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.7922e-05 4.7922e-05 4.7922e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >7.1526e-06 2.1458e-06 5.0068e-06</hent>
<hent key="03800100000000000000000400000005" call="MPI_Irecv" bytes="4" orank="5" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.2939e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000040000000C" call="MPI_Irecv" bytes="4" orank="12" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.3682e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0380010000000000000000040000000E" call="MPI_Irecv" bytes="4" orank="14" region="0" commid="0" count="3398" tid="0" op="" dtype="" >3.8934e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000015" call="MPI_Irecv" bytes="4" orank="21" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.9973e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000004D" call="MPI_Irecv" bytes="4" orank="77" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.0582e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.1060e-03 2.8110e-04 7.2694e-04</hent>
<hent key="038001000000000000001C000000000C" call="MPI_Irecv" bytes="7168" orank="12" region="0" commid="0" count="8448" tid="0" op="" dtype="" >2.3851e-03 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000001C000000000E" call="MPI_Irecv" bytes="7168" orank="14" region="0" commid="0" count="8969" tid="0" op="" dtype="" >1.2934e-03 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000004000002CD" call="MPI_Irecv" bytes="4" orank="717" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.9949e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000400000005" call="MPI_Isend" bytes="4" orank="5" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2720e-03 0.0000e+00 3.0994e-06</hent>
<hent key="0240010000000000000000040000000C" call="MPI_Isend" bytes="4" orank="12" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4486e-03 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000040000000E" call="MPI_Isend" bytes="4" orank="14" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3425e-03 0.0000e+00 6.0081e-05</hent>
<hent key="02400100000000000000000400000015" call="MPI_Isend" bytes="4" orank="21" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2815e-03 0.0000e+00 2.7895e-05</hent>
<hent key="0240010000000000000000040000004D" call="MPI_Isend" bytes="4" orank="77" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.2277e-02 3.8147e-06 2.1935e-05</hent>
<hent key="024001000000000000001C000000000C" call="MPI_Isend" bytes="7168" orank="12" region="0" commid="0" count="9550" tid="0" op="" dtype="" >2.0836e-02 9.5367e-07 1.1921e-05</hent>
<hent key="024001000000000000001C000000000E" call="MPI_Isend" bytes="7168" orank="14" region="0" commid="0" count="8818" tid="0" op="" dtype="" >2.6931e-02 9.5367e-07 1.5974e-05</hent>
<hent key="024001000000000000000004000002CD" call="MPI_Isend" bytes="4" orank="717" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6925e-02 2.8610e-06 1.2398e-04</hent>
<hent key="03800100000000000000050000000005" call="MPI_Irecv" bytes="1280" orank="5" region="0" commid="0" count="206" tid="0" op="" dtype="" >4.5538e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000C" call="MPI_Irecv" bytes="1280" orank="12" region="0" commid="0" count="193" tid="0" op="" dtype="" >7.1526e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000005000000000E" call="MPI_Irecv" bytes="1280" orank="14" region="0" commid="0" count="205" tid="0" op="" dtype="" >4.8876e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000015" call="MPI_Irecv" bytes="1280" orank="21" region="0" commid="0" count="207" tid="0" op="" dtype="" >3.4571e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000004D" call="MPI_Irecv" bytes="1280" orank="77" region="0" commid="0" count="283" tid="0" op="" dtype="" >6.9857e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.1340e+01 6.9141e-06 1.2809e-01</hent>
<hent key="03800100000000000000280000000005" call="MPI_Irecv" bytes="10240" orank="5" region="0" commid="0" count="27" tid="0" op="" dtype="" >6.4373e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002CD" call="MPI_Irecv" bytes="1280" orank="717" region="0" commid="0" count="307" tid="0" op="" dtype="" >9.5606e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000005" call="MPI_Irecv" bytes="2048" orank="5" region="0" commid="0" count="9" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000C" call="MPI_Irecv" bytes="2048" orank="12" region="0" commid="0" count="11" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000E" call="MPI_Irecv" bytes="2048" orank="14" region="0" commid="0" count="17" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000015" call="MPI_Irecv" bytes="2048" orank="21" region="0" commid="0" count="19" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000004D" call="MPI_Irecv" bytes="2048" orank="77" region="0" commid="0" count="3428" tid="0" op="" dtype="" >5.2929e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002CD" call="MPI_Irecv" bytes="2048" orank="717" region="0" commid="0" count="3432" tid="0" op="" dtype="" >5.5194e-04 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000050000000005" call="MPI_Isend" bytes="1280" orank="5" region="0" commid="0" count="250" tid="0" op="" dtype="" >3.4904e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0240010000000000000005000000000C" call="MPI_Isend" bytes="1280" orank="12" region="0" commid="0" count="218" tid="0" op="" dtype="" >4.0030e-04 9.5367e-07 9.0599e-06</hent>
<hent key="0240010000000000000005000000000E" call="MPI_Isend" bytes="1280" orank="14" region="0" commid="0" count="204" tid="0" op="" dtype="" >7.4029e-04 2.8610e-06 8.1062e-06</hent>
<hent key="02400100000000000000050000000015" call="MPI_Isend" bytes="1280" orank="21" region="0" commid="0" count="209" tid="0" op="" dtype="" >3.3998e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000005000000004D" call="MPI_Isend" bytes="1280" orank="77" region="0" commid="0" count="312" tid="0" op="" dtype="" >1.5993e-03 3.8147e-06 8.8215e-06</hent>
<hent key="02400100000000000000280000000005" call="MPI_Isend" bytes="10240" orank="5" region="0" commid="0" count="102" tid="0" op="" dtype="" >3.3140e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000280000000015" call="MPI_Isend" bytes="10240" orank="21" region="0" commid="0" count="20" tid="0" op="" dtype="" >1.1921e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000500000002CD" call="MPI_Isend" bytes="1280" orank="717" region="0" commid="0" count="276" tid="0" op="" dtype="" >1.2524e-03 3.8147e-06 9.0599e-06</hent>
</hash>
<internal rank="13" log_i="1723713849.502236" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="14" mpi_size="768" stamp_init="1723713791.115477" stamp_final="1723713849.502994" username="apac4" allocationname="unknown" flags="0" pid="1717077" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.83875e+01" utime="4.80965e+01" stime="7.74093e+00" mtime="3.23419e+01" gflop="0.00000e+00" gbyte="3.77968e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23419e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83169e+01" utime="4.80639e+01" stime="7.73331e+00" mtime="3.23419e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23419e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4941e+08" > 4.3756e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4857e+08" > 3.5581e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9613e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4514e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.6420e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.5916e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6117e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.6754e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4809e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6162e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8803e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="196" >
<hent key="02400100000000000000080000000006" call="MPI_Isend" bytes="2048" orank="6" region="0" commid="0" count="12" tid="0" op="" dtype="" >5.7697e-05 9.5367e-07 1.5020e-05</hent>
<hent key="0240010000000000000008000000000D" call="MPI_Isend" bytes="2048" orank="13" region="0" commid="0" count="17" tid="0" op="" dtype="" >3.9339e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000008000000000F" call="MPI_Isend" bytes="2048" orank="15" region="0" commid="0" count="13" tid="0" op="" dtype="" >4.9114e-05 2.8610e-06 4.0531e-06</hent>
<hent key="02400100000000000000080000000016" call="MPI_Isend" bytes="2048" orank="22" region="0" commid="0" count="14" tid="0" op="" dtype="" >5.0068e-05 9.5367e-07 1.4067e-05</hent>
<hent key="0240010000000000000008000000004E" call="MPI_Isend" bytes="2048" orank="78" region="0" commid="0" count="3443" tid="0" op="" dtype="" >1.4616e-02 9.5367e-07 3.6001e-05</hent>
<hent key="024001000000000000000800000002CE" call="MPI_Isend" bytes="2048" orank="718" region="0" commid="0" count="3403" tid="0" op="" dtype="" >1.3057e-02 9.5367e-07 3.8862e-05</hent>
<hent key="024001000000000000000E000000004E" call="MPI_Isend" bytes="3584" orank="78" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000006" call="MPI_Irecv" bytes="640" orank="6" region="0" commid="0" count="412" tid="0" op="" dtype="" >1.4472e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0380010000000000000002800000000D" call="MPI_Irecv" bytes="640" orank="13" region="0" commid="0" count="395" tid="0" op="" dtype="" >1.4877e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000002800000000F" call="MPI_Irecv" bytes="640" orank="15" region="0" commid="0" count="421" tid="0" op="" dtype="" >1.1683e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000028000000016" call="MPI_Irecv" bytes="640" orank="22" region="0" commid="0" count="390" tid="0" op="" dtype="" >7.1526e-05 0.0000e+00 2.8610e-06</hent>
<hent key="0380010000000000000002800000004E" call="MPI_Irecv" bytes="640" orank="78" region="0" commid="0" count="306" tid="0" op="" dtype="" >1.0467e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="038001000000000000000280000002CE" call="MPI_Irecv" bytes="640" orank="718" region="0" commid="0" count="305" tid="0" op="" dtype="" >1.5330e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000028000000006" call="MPI_Isend" bytes="640" orank="6" region="0" commid="0" count="406" tid="0" op="" dtype="" >5.5313e-04 0.0000e+00 1.1921e-05</hent>
<hent key="0240010000000000000002800000000D" call="MPI_Isend" bytes="640" orank="13" region="0" commid="0" count="395" tid="0" op="" dtype="" >5.3716e-04 9.5367e-07 5.9605e-06</hent>
<hent key="0240010000000000000002800000000F" call="MPI_Isend" bytes="640" orank="15" region="0" commid="0" count="386" tid="0" op="" dtype="" >1.1313e-03 1.9073e-06 6.9141e-06</hent>
<hent key="02400100000000000000028000000016" call="MPI_Isend" bytes="640" orank="22" region="0" commid="0" count="386" tid="0" op="" dtype="" >4.7040e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000002800000004E" call="MPI_Isend" bytes="640" orank="78" region="0" commid="0" count="301" tid="0" op="" dtype="" >1.6248e-03 2.8610e-06 1.9789e-05</hent>
<hent key="024001000000000000000280000002CE" call="MPI_Isend" bytes="640" orank="718" region="0" commid="0" count="276" tid="0" op="" dtype="" >1.2217e-03 2.8610e-06 1.5020e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.1989e-06 6.1989e-06 6.1989e-06</hent>
<hent key="03800100000000000000014000000006" call="MPI_Irecv" bytes="320" orank="6" region="0" commid="0" count="364" tid="0" op="" dtype="" >8.6784e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000D" call="MPI_Irecv" bytes="320" orank="13" region="0" commid="0" count="368" tid="0" op="" dtype="" >1.3137e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000F" call="MPI_Irecv" bytes="320" orank="15" region="0" commid="0" count="342" tid="0" op="" dtype="" >9.2030e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000016" call="MPI_Irecv" bytes="320" orank="22" region="0" commid="0" count="370" tid="0" op="" dtype="" >7.6294e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000001400000004E" call="MPI_Irecv" bytes="320" orank="78" region="0" commid="0" count="203" tid="0" op="" dtype="" >6.2704e-05 0.0000e+00 2.8610e-06</hent>
<hent key="038001000000000000000140000002CE" call="MPI_Irecv" bytes="320" orank="718" region="0" commid="0" count="156" tid="0" op="" dtype="" >4.7922e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.9613e+00 0.0000e+00 1.3107e-01</hent>
<hent key="0380010000000000000040000000004E" call="MPI_Irecv" bytes="16384" orank="78" region="0" commid="0" count="12550" tid="0" op="" dtype="" >8.8573e-03 0.0000e+00 4.1008e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.5020e-05 0.0000e+00 1.5020e-05</hent>
<hent key="038001000000000000004000000002CE" call="MPI_Irecv" bytes="16384" orank="718" region="0" commid="0" count="12396" tid="0" op="" dtype="" >3.5779e-03 0.0000e+00 2.9087e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.1195e-04 0.0000e+00 2.1195e-04</hent>
<hent key="02400100000000000000014000000006" call="MPI_Isend" bytes="320" orank="6" region="0" commid="0" count="375" tid="0" op="" dtype="" >3.8218e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0240010000000000000001400000000D" call="MPI_Isend" bytes="320" orank="13" region="0" commid="0" count="368" tid="0" op="" dtype="" >3.8171e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000001400000000F" call="MPI_Isend" bytes="320" orank="15" region="0" commid="0" count="341" tid="0" op="" dtype="" >8.7690e-04 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000014000000016" call="MPI_Isend" bytes="320" orank="22" region="0" commid="0" count="396" tid="0" op="" dtype="" >4.3058e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000001400000004E" call="MPI_Isend" bytes="320" orank="78" region="0" commid="0" count="174" tid="0" op="" dtype="" >9.0265e-04 2.8610e-06 1.8120e-05</hent>
<hent key="03800100000000000000200000000006" call="MPI_Irecv" bytes="8192" orank="6" region="0" commid="0" count="12691" tid="0" op="" dtype="" >2.2447e-03 0.0000e+00 3.2902e-05</hent>
<hent key="0380010000000000000020000000000D" call="MPI_Irecv" bytes="8192" orank="13" region="0" commid="0" count="3881" tid="0" op="" dtype="" >1.3912e-03 0.0000e+00 2.0981e-05</hent>
<hent key="0380010000000000000020000000000F" call="MPI_Irecv" bytes="8192" orank="15" region="0" commid="0" count="3557" tid="0" op="" dtype="" >5.9891e-04 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000200000000016" call="MPI_Irecv" bytes="8192" orank="22" region="0" commid="0" count="12611" tid="0" op="" dtype="" >2.2266e-03 0.0000e+00 1.3113e-05</hent>
<hent key="024001000000000000000140000002CE" call="MPI_Isend" bytes="320" orank="718" region="0" commid="0" count="171" tid="0" op="" dtype="" >7.5150e-04 2.8610e-06 1.2875e-05</hent>
<hent key="03800100000000000000000000000006" call="MPI_Irecv" bytes="0" orank="6" region="0" commid="0" count="254" tid="0" op="" dtype="" >6.1750e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000000000000000D" call="MPI_Irecv" bytes="0" orank="13" region="0" commid="0" count="272" tid="0" op="" dtype="" >8.8692e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000000000000000F" call="MPI_Irecv" bytes="0" orank="15" region="0" commid="0" count="228" tid="0" op="" dtype="" >5.8651e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000016" call="MPI_Irecv" bytes="0" orank="22" region="0" commid="0" count="280" tid="0" op="" dtype="" >6.7711e-05 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000000000000004E" call="MPI_Irecv" bytes="0" orank="78" region="0" commid="0" count="152" tid="0" op="" dtype="" >4.0293e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000040000000004E" call="MPI_Isend" bytes="16384" orank="78" region="0" commid="0" count="12673" tid="0" op="" dtype="" >1.4632e-01 4.0531e-06 5.8174e-05</hent>
<hent key="038001000000000000000000000002CE" call="MPI_Irecv" bytes="0" orank="718" region="0" commid="0" count="146" tid="0" op="" dtype="" >5.0306e-05 0.0000e+00 3.0994e-06</hent>
<hent key="024001000000000000004000000002CE" call="MPI_Isend" bytes="16384" orank="718" region="0" commid="0" count="12509" tid="0" op="" dtype="" >1.0098e-01 2.8610e-06 1.0395e-04</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.2646e-04 0.0000e+00 5.6505e-04</hent>
<hent key="02400100000000000000200000000006" call="MPI_Isend" bytes="8192" orank="6" region="0" commid="0" count="12501" tid="0" op="" dtype="" >4.8218e-03 0.0000e+00 2.5034e-05</hent>
<hent key="0240010000000000000020000000000D" call="MPI_Isend" bytes="8192" orank="13" region="0" commid="0" count="3730" tid="0" op="" dtype="" >4.3347e-03 0.0000e+00 1.2875e-05</hent>
<hent key="0240010000000000000020000000000F" call="MPI_Isend" bytes="8192" orank="15" region="0" commid="0" count="3759" tid="0" op="" dtype="" >6.6032e-03 0.0000e+00 8.5831e-05</hent>
<hent key="02400100000000000000200000000016" call="MPI_Isend" bytes="8192" orank="22" region="0" commid="0" count="12668" tid="0" op="" dtype="" >7.7102e-03 0.0000e+00 4.9829e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-05 5.0068e-05 5.0068e-05</hent>
<hent key="02400100000000000000000000000006" call="MPI_Isend" bytes="0" orank="6" region="0" commid="0" count="267" tid="0" op="" dtype="" >2.1553e-04 0.0000e+00 1.5974e-05</hent>
<hent key="0240010000000000000000000000000D" call="MPI_Isend" bytes="0" orank="13" region="0" commid="0" count="265" tid="0" op="" dtype="" >1.7214e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000000000000000F" call="MPI_Isend" bytes="0" orank="15" region="0" commid="0" count="246" tid="0" op="" dtype="" >4.6563e-04 9.5367e-07 3.0994e-05</hent>
<hent key="02400100000000000000000000000016" call="MPI_Isend" bytes="0" orank="22" region="0" commid="0" count="274" tid="0" op="" dtype="" >2.2721e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000000000000004E" call="MPI_Isend" bytes="0" orank="78" region="0" commid="0" count="150" tid="0" op="" dtype="" >6.3992e-04 1.1921e-06 1.5020e-05</hent>
<hent key="024001000000000000000000000002CE" call="MPI_Isend" bytes="0" orank="718" region="0" commid="0" count="151" tid="0" op="" dtype="" >5.7411e-04 9.5367e-07 9.0599e-06</hent>
<hent key="03800100000000000000060000000006" call="MPI_Irecv" bytes="1536" orank="6" region="0" commid="0" count="93" tid="0" op="" dtype="" >1.9789e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000D" call="MPI_Irecv" bytes="1536" orank="13" region="0" commid="0" count="89" tid="0" op="" dtype="" >2.2888e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000F" call="MPI_Irecv" bytes="1536" orank="15" region="0" commid="0" count="90" tid="0" op="" dtype="" >2.7657e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000016" call="MPI_Irecv" bytes="1536" orank="22" region="0" commid="0" count="98" tid="0" op="" dtype="" >1.9789e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000004E" call="MPI_Irecv" bytes="1536" orank="78" region="0" commid="0" count="204" tid="0" op="" dtype="" >6.8188e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002CE" call="MPI_Irecv" bytes="1536" orank="718" region="0" commid="0" count="202" tid="0" op="" dtype="" >7.8440e-05 0.0000e+00 7.8678e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000006" call="MPI_Isend" bytes="1536" orank="6" region="0" commid="0" count="97" tid="0" op="" dtype="" >2.2936e-04 9.5367e-07 1.5020e-05</hent>
<hent key="0240010000000000000006000000000D" call="MPI_Isend" bytes="1536" orank="13" region="0" commid="0" count="94" tid="0" op="" dtype="" >1.7190e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000006000000000F" call="MPI_Isend" bytes="1536" orank="15" region="0" commid="0" count="105" tid="0" op="" dtype="" >3.9625e-04 2.8610e-06 2.0027e-05</hent>
<hent key="02400100000000000000060000000016" call="MPI_Isend" bytes="1536" orank="22" region="0" commid="0" count="93" tid="0" op="" dtype="" >2.0242e-04 9.5367e-07 1.4067e-05</hent>
<hent key="0240010000000000000006000000004E" call="MPI_Isend" bytes="1536" orank="78" region="0" commid="0" count="219" tid="0" op="" dtype="" >1.2493e-03 3.8147e-06 1.2875e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002CE" call="MPI_Isend" bytes="1536" orank="718" region="0" commid="0" count="230" tid="0" op="" dtype="" >1.1427e-03 2.8610e-06 1.2159e-05</hent>
<hent key="038001000000000000000C000000000D" call="MPI_Irecv" bytes="3072" orank="13" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C000000000F" call="MPI_Irecv" bytes="3072" orank="15" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000C000000004E" call="MPI_Irecv" bytes="3072" orank="78" region="0" commid="0" count="8" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002CE" call="MPI_Irecv" bytes="3072" orank="718" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C000000000F" call="MPI_Isend" bytes="3072" orank="15" region="0" commid="0" count="1" tid="0" op="" dtype="" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="024001000000000000000C000000004E" call="MPI_Isend" bytes="3072" orank="78" region="0" commid="0" count="9" tid="0" op="" dtype="" >5.7697e-05 5.9605e-06 6.9141e-06</hent>
<hent key="024001000000000000000C00000002CE" call="MPI_Isend" bytes="3072" orank="718" region="0" commid="0" count="9" tid="0" op="" dtype="" >5.0068e-05 5.0068e-06 6.1989e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.1209e-04 3.1209e-04 3.1209e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >6.8116e-04 2.2602e-04 2.2793e-04</hent>
<hent key="03800100000000000000038000000006" call="MPI_Irecv" bytes="896" orank="6" region="0" commid="0" count="336" tid="0" op="" dtype="" >1.1134e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0380010000000000000003800000000D" call="MPI_Irecv" bytes="896" orank="13" region="0" commid="0" count="2666" tid="0" op="" dtype="" >4.7636e-04 0.0000e+00 6.1989e-06</hent>
<hent key="0380010000000000000003800000000F" call="MPI_Irecv" bytes="896" orank="15" region="0" commid="0" count="2773" tid="0" op="" dtype="" >5.6648e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000038000000016" call="MPI_Irecv" bytes="896" orank="22" region="0" commid="0" count="319" tid="0" op="" dtype="" >4.9353e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000004E" call="MPI_Irecv" bytes="896" orank="78" region="0" commid="0" count="327" tid="0" op="" dtype="" >1.3781e-04 0.0000e+00 3.0994e-05</hent>
<hent key="038001000000000000000380000002CE" call="MPI_Irecv" bytes="896" orank="718" region="0" commid="0" count="332" tid="0" op="" dtype="" >1.3041e-04 0.0000e+00 5.9605e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.2902e-05 3.2902e-05 3.2902e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0380010000000000000038000000004E" call="MPI_Irecv" bytes="14336" orank="78" region="0" commid="0" count="149" tid="0" op="" dtype="" >1.3161e-04 0.0000e+00 1.0014e-05</hent>
<hent key="038001000000000000003800000002CE" call="MPI_Irecv" bytes="14336" orank="718" region="0" commid="0" count="303" tid="0" op="" dtype="" >7.9155e-05 0.0000e+00 3.0994e-06</hent>
<hent key="02400100000000000000038000000006" call="MPI_Isend" bytes="896" orank="6" region="0" commid="0" count="281" tid="0" op="" dtype="" >4.0650e-04 9.5367e-07 7.8678e-06</hent>
<hent key="0240010000000000000003800000000D" call="MPI_Isend" bytes="896" orank="13" region="0" commid="0" count="2714" tid="0" op="" dtype="" >2.0149e-03 0.0000e+00 2.3842e-05</hent>
<hent key="0240010000000000000003800000000F" call="MPI_Isend" bytes="896" orank="15" region="0" commid="0" count="2755" tid="0" op="" dtype="" >2.8934e-03 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000038000000016" call="MPI_Isend" bytes="896" orank="22" region="0" commid="0" count="308" tid="0" op="" dtype="" >4.9090e-04 9.5367e-07 1.8835e-05</hent>
<hent key="0240010000000000000003800000004E" call="MPI_Isend" bytes="896" orank="78" region="0" commid="0" count="332" tid="0" op="" dtype="" >1.7784e-03 3.8147e-06 2.3127e-05</hent>
<hent key="024001000000000000000380000002CE" call="MPI_Isend" bytes="896" orank="718" region="0" commid="0" count="332" tid="0" op="" dtype="" >1.5295e-03 2.8610e-06 1.7881e-05</hent>
<hent key="0240010000000000000038000000004E" call="MPI_Isend" bytes="14336" orank="78" region="0" commid="0" count="26" tid="0" op="" dtype="" >3.7837e-04 5.9605e-06 2.6941e-05</hent>
<hent key="024001000000000000003800000002CE" call="MPI_Isend" bytes="14336" orank="718" region="0" commid="0" count="190" tid="0" op="" dtype="" >1.4570e-03 3.8147e-06 3.2902e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.6678e+00 1.1921e-05 1.3911e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.3501e-04 5.3501e-04 5.3501e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >3.4809e-04 3.4809e-04 3.4809e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.0599e-03 2.0599e-03 2.0599e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >8.2722e-02 3.1614e-04 7.7261e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.8198e-04 5.8198e-04 5.8198e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6117e+00 4.5490e-04 2.5631e-01</hent>
<hent key="03800100000000000000040000000006" call="MPI_Irecv" bytes="1024" orank="6" region="0" commid="0" count="3396" tid="0" op="" dtype="" >4.8542e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0380010000000000000004000000000D" call="MPI_Irecv" bytes="1024" orank="13" region="0" commid="0" count="1050" tid="0" op="" dtype="" >1.7524e-04 0.0000e+00 6.1989e-06</hent>
<hent key="0380010000000000000004000000000F" call="MPI_Irecv" bytes="1024" orank="15" region="0" commid="0" count="966" tid="0" op="" dtype="" >2.1410e-04 0.0000e+00 1.6928e-05</hent>
<hent key="03800100000000000000040000000016" call="MPI_Irecv" bytes="1024" orank="22" region="0" commid="0" count="3376" tid="0" op="" dtype="" >4.6825e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >4.6420e-04 9.5367e-07 3.5214e-04</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000070000000006" call="MPI_Irecv" bytes="1792" orank="6" region="0" commid="0" count="42" tid="0" op="" dtype="" >1.3590e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000D" call="MPI_Irecv" bytes="1792" orank="13" region="0" commid="0" count="35" tid="0" op="" dtype="" >1.5736e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000F" call="MPI_Irecv" bytes="1792" orank="15" region="0" commid="0" count="36" tid="0" op="" dtype="" >8.5831e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000016" call="MPI_Irecv" bytes="1792" orank="22" region="0" commid="0" count="29" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000007000000004E" call="MPI_Irecv" bytes="1792" orank="78" region="0" commid="0" count="173" tid="0" op="" dtype="" >6.2227e-05 0.0000e+00 6.1989e-06</hent>
<hent key="038001000000000000000700000002CE" call="MPI_Irecv" bytes="1792" orank="718" region="0" commid="0" count="226" tid="0" op="" dtype="" >7.9393e-05 0.0000e+00 9.0599e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >1.2422e-04 0.0000e+00 5.5075e-05</hent>
<hent key="02400100000000000000040000000006" call="MPI_Isend" bytes="1024" orank="6" region="0" commid="0" count="3344" tid="0" op="" dtype="" >2.1789e-03 0.0000e+00 2.0027e-05</hent>
<hent key="0240010000000000000004000000000D" call="MPI_Isend" bytes="1024" orank="13" region="0" commid="0" count="1008" tid="0" op="" dtype="" >6.8855e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000004000000000F" call="MPI_Isend" bytes="1024" orank="15" region="0" commid="0" count="1014" tid="0" op="" dtype="" >7.6771e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000040000000016" call="MPI_Isend" bytes="1024" orank="22" region="0" commid="0" count="3390" tid="0" op="" dtype="" >2.1296e-03 0.0000e+00 1.3113e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >5.9605e-06 2.8610e-06 3.0994e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4504e+00 0.0000e+00 3.2510e+00</hent>
<hent key="038001000000000000000A0000000006" call="MPI_Irecv" bytes="2560" orank="6" region="0" commid="0" count="7" tid="0" op="" dtype="" >9.7752e-06 0.0000e+00 8.8215e-06</hent>
<hent key="038001000000000000000A000000000D" call="MPI_Irecv" bytes="2560" orank="13" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000000F" call="MPI_Irecv" bytes="2560" orank="15" region="0" commid="0" count="11" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000016" call="MPI_Irecv" bytes="2560" orank="22" region="0" commid="0" count="14" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000004E" call="MPI_Irecv" bytes="2560" orank="78" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.4782e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002CE" call="MPI_Irecv" bytes="2560" orank="718" region="0" commid="0" count="53" tid="0" op="" dtype="" >1.6451e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000006" call="MPI_Isend" bytes="1792" orank="6" region="0" commid="0" count="50" tid="0" op="" dtype="" >1.3256e-04 9.5367e-07 1.5974e-05</hent>
<hent key="0240010000000000000007000000000D" call="MPI_Isend" bytes="1792" orank="13" region="0" commid="0" count="26" tid="0" op="" dtype="" >5.4359e-05 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000007000000000F" call="MPI_Isend" bytes="1792" orank="15" region="0" commid="0" count="45" tid="0" op="" dtype="" >1.6475e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000070000000016" call="MPI_Isend" bytes="1792" orank="22" region="0" commid="0" count="28" tid="0" op="" dtype="" >6.6757e-05 9.5367e-07 1.3828e-05</hent>
<hent key="0240010000000000000007000000004E" call="MPI_Isend" bytes="1792" orank="78" region="0" commid="0" count="131" tid="0" op="" dtype="" >7.7534e-04 1.9073e-06 1.6928e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6162e-02 3.6162e-02 3.6162e-02</hent>
<hent key="024001000000000000000700000002CE" call="MPI_Isend" bytes="1792" orank="718" region="0" commid="0" count="193" tid="0" op="" dtype="" >8.4758e-04 9.5367e-07 1.7881e-05</hent>
<hent key="024001000000000000000A0000000006" call="MPI_Isend" bytes="2560" orank="6" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.5020e-05 1.1921e-06 1.1921e-05</hent>
<hent key="024001000000000000000A000000000D" call="MPI_Isend" bytes="2560" orank="13" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.0729e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A000000000F" call="MPI_Isend" bytes="2560" orank="15" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.0967e-05 2.8610e-06 4.0531e-06</hent>
<hent key="024001000000000000000A0000000016" call="MPI_Isend" bytes="2560" orank="22" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000A000000004E" call="MPI_Isend" bytes="2560" orank="78" region="0" commid="0" count="49" tid="0" op="" dtype="" >3.2902e-04 5.0068e-06 2.0027e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.4595e-04 4.1008e-05 1.6689e-04</hent>
<hent key="024001000000000000000A00000002CE" call="MPI_Isend" bytes="2560" orank="718" region="0" commid="0" count="47" tid="0" op="" dtype="" >2.6250e-04 4.0531e-06 1.0014e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.1032e-03 4.2796e-04 6.7520e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.5061e-05 4.5061e-05 4.5061e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.0967e-05 9.5367e-07 1.0014e-05</hent>
<hent key="03800100000000000000000400000006" call="MPI_Irecv" bytes="4" orank="6" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.1083e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0380010000000000000000040000000D" call="MPI_Irecv" bytes="4" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0672e-03 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000040000000F" call="MPI_Irecv" bytes="4" orank="15" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.1737e-04 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000000400000016" call="MPI_Irecv" bytes="4" orank="22" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.7353e-04 0.0000e+00 1.1921e-05</hent>
<hent key="0380010000000000000000040000004E" call="MPI_Irecv" bytes="4" orank="78" region="0" commid="0" count="3398" tid="0" op="" dtype="" >9.8491e-04 0.0000e+00 1.4067e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.2299e-03 2.8992e-04 7.7605e-04</hent>
<hent key="038001000000000000001C000000000D" call="MPI_Irecv" bytes="7168" orank="13" region="0" commid="0" count="8818" tid="0" op="" dtype="" >3.2792e-03 0.0000e+00 7.8678e-06</hent>
<hent key="038001000000000000001C000000000F" call="MPI_Irecv" bytes="7168" orank="15" region="0" commid="0" count="9142" tid="0" op="" dtype="" >1.6263e-03 0.0000e+00 2.1935e-05</hent>
<hent key="038001000000000000000004000002CE" call="MPI_Irecv" bytes="4" orank="718" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.3804e-04 0.0000e+00 1.5974e-05</hent>
<hent key="02400100000000000000000400000006" call="MPI_Isend" bytes="4" orank="6" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3342e-03 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000000040000000D" call="MPI_Isend" bytes="4" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2977e-03 0.0000e+00 4.3154e-05</hent>
<hent key="0240010000000000000000040000000F" call="MPI_Isend" bytes="4" orank="15" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.7301e-03 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000000400000016" call="MPI_Isend" bytes="4" orank="22" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.5836e-03 0.0000e+00 2.3842e-05</hent>
<hent key="0240010000000000000000040000004E" call="MPI_Isend" bytes="4" orank="78" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.6349e-02 3.8147e-06 5.6982e-05</hent>
<hent key="024001000000000000001C000000000D" call="MPI_Isend" bytes="7168" orank="13" region="0" commid="0" count="8969" tid="0" op="" dtype="" >1.9554e-02 9.5367e-07 1.1921e-05</hent>
<hent key="024001000000000000001C000000000F" call="MPI_Isend" bytes="7168" orank="15" region="0" commid="0" count="8940" tid="0" op="" dtype="" >2.6381e-02 9.5367e-07 2.2888e-05</hent>
<hent key="024001000000000000000004000002CE" call="MPI_Isend" bytes="4" orank="718" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.0541e-02 2.8610e-06 1.2112e-04</hent>
<hent key="03800100000000000000050000000006" call="MPI_Irecv" bytes="1280" orank="6" region="0" commid="0" count="184" tid="0" op="" dtype="" >5.2691e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000D" call="MPI_Irecv" bytes="1280" orank="13" region="0" commid="0" count="204" tid="0" op="" dtype="" >7.1526e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000F" call="MPI_Irecv" bytes="1280" orank="15" region="0" commid="0" count="213" tid="0" op="" dtype="" >7.4148e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000050000000016" call="MPI_Irecv" bytes="1280" orank="22" region="0" commid="0" count="209" tid="0" op="" dtype="" >3.5524e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000004E" call="MPI_Irecv" bytes="1280" orank="78" region="0" commid="0" count="264" tid="0" op="" dtype="" >1.0443e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.1049e+01 9.0599e-06 1.2808e-01</hent>
<hent key="03800100000000000000280000000006" call="MPI_Irecv" bytes="10240" orank="6" region="0" commid="0" count="8" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000280000000016" call="MPI_Irecv" bytes="10240" orank="22" region="0" commid="0" count="88" tid="0" op="" dtype="" >1.9073e-05 0.0000e+00 9.0599e-06</hent>
<hent key="038001000000000000000500000002CE" call="MPI_Irecv" bytes="1280" orank="718" region="0" commid="0" count="290" tid="0" op="" dtype="" >9.8467e-05 0.0000e+00 1.3113e-05</hent>
<hent key="03800100000000000000080000000006" call="MPI_Irecv" bytes="2048" orank="6" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000D" call="MPI_Irecv" bytes="2048" orank="13" region="0" commid="0" count="11" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000F" call="MPI_Irecv" bytes="2048" orank="15" region="0" commid="0" count="15" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000016" call="MPI_Irecv" bytes="2048" orank="22" region="0" commid="0" count="12" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000004E" call="MPI_Irecv" bytes="2048" orank="78" region="0" commid="0" count="3420" tid="0" op="" dtype="" >7.1931e-04 0.0000e+00 1.3113e-05</hent>
<hent key="038001000000000000000800000002CE" call="MPI_Irecv" bytes="2048" orank="718" region="0" commid="0" count="3383" tid="0" op="" dtype="" >7.9679e-04 0.0000e+00 2.4080e-05</hent>
<hent key="02400100000000000000050000000006" call="MPI_Isend" bytes="1280" orank="6" region="0" commid="0" count="262" tid="0" op="" dtype="" >3.6597e-04 0.0000e+00 1.3828e-05</hent>
<hent key="0240010000000000000005000000000D" call="MPI_Isend" bytes="1280" orank="13" region="0" commid="0" count="205" tid="0" op="" dtype="" >3.6335e-04 9.5367e-07 4.0531e-06</hent>
<hent key="0240010000000000000005000000000F" call="MPI_Isend" bytes="1280" orank="15" region="0" commid="0" count="188" tid="0" op="" dtype="" >6.3705e-04 1.9073e-06 9.0599e-06</hent>
<hent key="02400100000000000000050000000016" call="MPI_Isend" bytes="1280" orank="22" region="0" commid="0" count="207" tid="0" op="" dtype="" >3.1161e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0240010000000000000005000000004E" call="MPI_Isend" bytes="1280" orank="78" region="0" commid="0" count="288" tid="0" op="" dtype="" >1.5795e-03 3.8147e-06 1.8835e-05</hent>
<hent key="02400100000000000000280000000006" call="MPI_Isend" bytes="10240" orank="6" region="0" commid="0" count="198" tid="0" op="" dtype="" >7.4863e-05 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000280000000016" call="MPI_Isend" bytes="10240" orank="22" region="0" commid="0" count="31" tid="0" op="" dtype="" >1.1921e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000500000002CE" call="MPI_Isend" bytes="1280" orank="718" region="0" commid="0" count="285" tid="0" op="" dtype="" >1.3437e-03 2.8610e-06 1.3113e-05</hent>
</hash>
<internal rank="14" log_i="1723713849.502994" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="15" mpi_size="768" stamp_init="1723713791.117122" stamp_final="1723713849.517431" username="apac4" allocationname="unknown" flags="0" pid="1717078" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.84003e+01" utime="5.00238e+01" stime="6.71565e+00" mtime="3.22895e+01" gflop="0.00000e+00" gbyte="3.77125e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22895e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ab15ac15ad153456ad15ad1500" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83302e+01" utime="4.99963e+01" stime="6.70310e+00" mtime="3.22895e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22895e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4796e+08" > 3.4949e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4912e+08" > 2.6316e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4455e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4257e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3365e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9101e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6115e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.0325e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0708e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6136e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9390e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="199" >
<hent key="02400100000000000000080000000007" call="MPI_Isend" bytes="2048" orank="7" region="0" commid="0" count="16" tid="0" op="" dtype="" >6.2943e-05 9.5367e-07 1.2159e-05</hent>
<hent key="02400100000000000000080000000008" call="MPI_Isend" bytes="2048" orank="8" region="0" commid="0" count="20" tid="0" op="" dtype="" >7.8678e-05 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000008000000000E" call="MPI_Isend" bytes="2048" orank="14" region="0" commid="0" count="15" tid="0" op="" dtype="" >3.6478e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000017" call="MPI_Isend" bytes="2048" orank="23" region="0" commid="0" count="15" tid="0" op="" dtype="" >4.7207e-05 9.5367e-07 1.3828e-05</hent>
<hent key="0240010000000000000008000000004F" call="MPI_Isend" bytes="2048" orank="79" region="0" commid="0" count="3392" tid="0" op="" dtype="" >1.0960e-02 9.5367e-07 2.3842e-05</hent>
<hent key="024001000000000000000800000002CF" call="MPI_Isend" bytes="2048" orank="719" region="0" commid="0" count="3348" tid="0" op="" dtype="" >1.0124e-02 9.5367e-07 1.5020e-05</hent>
<hent key="038001000000000000000E000000004F" call="MPI_Irecv" bytes="3584" orank="79" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000028000000007" call="MPI_Irecv" bytes="640" orank="7" region="0" commid="0" count="396" tid="0" op="" dtype="" >9.4414e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000008" call="MPI_Irecv" bytes="640" orank="8" region="0" commid="0" count="425" tid="0" op="" dtype="" >1.1206e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000E" call="MPI_Irecv" bytes="640" orank="14" region="0" commid="0" count="386" tid="0" op="" dtype="" >1.4663e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000017" call="MPI_Irecv" bytes="640" orank="23" region="0" commid="0" count="410" tid="0" op="" dtype="" >1.0276e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000004F" call="MPI_Irecv" bytes="640" orank="79" region="0" commid="0" count="271" tid="0" op="" dtype="" >9.3699e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000014000000004F" call="MPI_Irecv" bytes="5120" orank="79" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="038001000000000000000280000002CF" call="MPI_Irecv" bytes="640" orank="719" region="0" commid="0" count="269" tid="0" op="" dtype="" >7.0572e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000007" call="MPI_Isend" bytes="640" orank="7" region="0" commid="0" count="396" tid="0" op="" dtype="" >4.5109e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000028000000008" call="MPI_Isend" bytes="640" orank="8" region="0" commid="0" count="411" tid="0" op="" dtype="" >1.2765e-03 9.5367e-07 1.0014e-05</hent>
<hent key="0240010000000000000002800000000E" call="MPI_Isend" bytes="640" orank="14" region="0" commid="0" count="421" tid="0" op="" dtype="" >5.9819e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000028000000017" call="MPI_Isend" bytes="640" orank="23" region="0" commid="0" count="386" tid="0" op="" dtype="" >4.6849e-04 9.5367e-07 9.0599e-06</hent>
<hent key="0240010000000000000002800000004F" call="MPI_Isend" bytes="640" orank="79" region="0" commid="0" count="264" tid="0" op="" dtype="" >1.2333e-03 3.8147e-06 8.1062e-06</hent>
<hent key="024001000000000000000280000002CF" call="MPI_Isend" bytes="640" orank="719" region="0" commid="0" count="279" tid="0" op="" dtype="" >1.1482e-03 2.8610e-06 7.8678e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.4067e-05 1.4067e-05 1.4067e-05</hent>
<hent key="03800100000000000000014000000007" call="MPI_Irecv" bytes="320" orank="7" region="0" commid="0" count="360" tid="0" op="" dtype="" >8.2493e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000008" call="MPI_Irecv" bytes="320" orank="8" region="0" commid="0" count="362" tid="0" op="" dtype="" >8.6308e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000E" call="MPI_Irecv" bytes="320" orank="14" region="0" commid="0" count="341" tid="0" op="" dtype="" >1.2636e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000017" call="MPI_Irecv" bytes="320" orank="23" region="0" commid="0" count="349" tid="0" op="" dtype="" >8.4877e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000004F" call="MPI_Irecv" bytes="320" orank="79" region="0" commid="0" count="193" tid="0" op="" dtype="" >7.5102e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002CF" call="MPI_Irecv" bytes="320" orank="719" region="0" commid="0" count="168" tid="0" op="" dtype="" >4.8637e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.4455e+00 0.0000e+00 1.3901e-01</hent>
<hent key="0380010000000000000040000000004F" call="MPI_Irecv" bytes="16384" orank="79" region="0" commid="0" count="12670" tid="0" op="" dtype="" >5.7764e-03 0.0000e+00 7.1526e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.7881e-05 0.0000e+00 1.6928e-05</hent>
<hent key="038001000000000000004000000002CF" call="MPI_Irecv" bytes="16384" orank="719" region="0" commid="0" count="12187" tid="0" op="" dtype="" >1.9472e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000014000000007" call="MPI_Isend" bytes="320" orank="7" region="0" commid="0" count="357" tid="0" op="" dtype="" >3.3760e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000014000000008" call="MPI_Isend" bytes="320" orank="8" region="0" commid="0" count="372" tid="0" op="" dtype="" >9.8228e-04 9.5367e-07 5.9605e-06</hent>
<hent key="0240010000000000000001400000000E" call="MPI_Isend" bytes="320" orank="14" region="0" commid="0" count="342" tid="0" op="" dtype="" >3.6764e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000017" call="MPI_Isend" bytes="320" orank="23" region="0" commid="0" count="346" tid="0" op="" dtype="" >3.7217e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0240010000000000000001400000004F" call="MPI_Isend" bytes="320" orank="79" region="0" commid="0" count="187" tid="0" op="" dtype="" >8.2779e-04 3.8147e-06 8.1062e-06</hent>
<hent key="03800100000000000000200000000007" call="MPI_Irecv" bytes="8192" orank="7" region="0" commid="0" count="12373" tid="0" op="" dtype="" >2.2218e-03 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000200000000008" call="MPI_Irecv" bytes="8192" orank="8" region="0" commid="0" count="3714" tid="0" op="" dtype="" >5.9366e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000020000000000E" call="MPI_Irecv" bytes="8192" orank="14" region="0" commid="0" count="3759" tid="0" op="" dtype="" >1.0061e-03 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000200000000017" call="MPI_Irecv" bytes="8192" orank="23" region="0" commid="0" count="12674" tid="0" op="" dtype="" >1.3211e-03 0.0000e+00 3.0994e-06</hent>
<hent key="024001000000000000000140000002CF" call="MPI_Isend" bytes="320" orank="719" region="0" commid="0" count="168" tid="0" op="" dtype="" >6.7234e-04 2.8610e-06 5.0068e-06</hent>
<hent key="03800100000000000000000000000007" call="MPI_Irecv" bytes="0" orank="7" region="0" commid="0" count="262" tid="0" op="" dtype="" >6.7472e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000008" call="MPI_Irecv" bytes="0" orank="8" region="0" commid="0" count="250" tid="0" op="" dtype="" >6.5565e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000E" call="MPI_Irecv" bytes="0" orank="14" region="0" commid="0" count="246" tid="0" op="" dtype="" >7.7963e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000017" call="MPI_Irecv" bytes="0" orank="23" region="0" commid="0" count="252" tid="0" op="" dtype="" >5.6744e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000004F" call="MPI_Irecv" bytes="0" orank="79" region="0" commid="0" count="145" tid="0" op="" dtype="" >4.8637e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000040000000004F" call="MPI_Isend" bytes="16384" orank="79" region="0" commid="0" count="12402" tid="0" op="" dtype="" >1.0143e-01 3.8147e-06 5.0068e-05</hent>
<hent key="038001000000000000000000000002CF" call="MPI_Irecv" bytes="0" orank="719" region="0" commid="0" count="140" tid="0" op="" dtype="" >3.8624e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002CF" call="MPI_Isend" bytes="16384" orank="719" region="0" commid="0" count="12247" tid="0" op="" dtype="" >7.4016e-02 2.8610e-06 2.5988e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.2002e-04 0.0000e+00 5.6100e-04</hent>
<hent key="02400100000000000000200000000007" call="MPI_Isend" bytes="8192" orank="7" region="0" commid="0" count="12321" tid="0" op="" dtype="" >4.1895e-03 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000200000000008" call="MPI_Isend" bytes="8192" orank="8" region="0" commid="0" count="3126" tid="0" op="" dtype="" >4.6678e-03 0.0000e+00 1.2875e-05</hent>
<hent key="0240010000000000000020000000000E" call="MPI_Isend" bytes="8192" orank="14" region="0" commid="0" count="3557" tid="0" op="" dtype="" >3.8555e-03 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000200000000017" call="MPI_Isend" bytes="8192" orank="23" region="0" commid="0" count="12667" tid="0" op="" dtype="" >7.1049e-03 0.0000e+00 4.1008e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.7909e-05 3.7909e-05 3.7909e-05</hent>
<hent key="02400100000000000000000000000007" call="MPI_Isend" bytes="0" orank="7" region="0" commid="0" count="268" tid="0" op="" dtype="" >1.7357e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000008" call="MPI_Isend" bytes="0" orank="8" region="0" commid="0" count="253" tid="0" op="" dtype="" >4.9305e-04 0.0000e+00 5.1022e-05</hent>
<hent key="0240010000000000000000000000000E" call="MPI_Isend" bytes="0" orank="14" region="0" commid="0" count="228" tid="0" op="" dtype="" >1.4806e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000017" call="MPI_Isend" bytes="0" orank="23" region="0" commid="0" count="245" tid="0" op="" dtype="" >1.6999e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000000000000004F" call="MPI_Isend" bytes="0" orank="79" region="0" commid="0" count="150" tid="0" op="" dtype="" >5.5408e-04 9.5367e-07 7.1526e-06</hent>
<hent key="024001000000000000000000000002CF" call="MPI_Isend" bytes="0" orank="719" region="0" commid="0" count="149" tid="0" op="" dtype="" >5.3096e-04 9.5367e-07 7.8678e-06</hent>
<hent key="03800100000000000000060000000007" call="MPI_Irecv" bytes="1536" orank="7" region="0" commid="0" count="107" tid="0" op="" dtype="" >2.1696e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000008" call="MPI_Irecv" bytes="1536" orank="8" region="0" commid="0" count="94" tid="0" op="" dtype="" >2.2411e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000E" call="MPI_Irecv" bytes="1536" orank="14" region="0" commid="0" count="105" tid="0" op="" dtype="" >3.3617e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000060000000017" call="MPI_Irecv" bytes="1536" orank="23" region="0" commid="0" count="95" tid="0" op="" dtype="" >1.9789e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000004F" call="MPI_Irecv" bytes="1536" orank="79" region="0" commid="0" count="203" tid="0" op="" dtype="" >8.4877e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002CF" call="MPI_Irecv" bytes="1536" orank="719" region="0" commid="0" count="240" tid="0" op="" dtype="" >6.5088e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 5.9605e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000007" call="MPI_Isend" bytes="1536" orank="7" region="0" commid="0" count="99" tid="0" op="" dtype="" >1.7142e-04 9.5367e-07 1.1921e-05</hent>
<hent key="02400100000000000000060000000008" call="MPI_Isend" bytes="1536" orank="8" region="0" commid="0" count="82" tid="0" op="" dtype="" >2.9469e-04 2.8610e-06 4.0531e-06</hent>
<hent key="0240010000000000000006000000000E" call="MPI_Isend" bytes="1536" orank="14" region="0" commid="0" count="90" tid="0" op="" dtype="" >1.7691e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000017" call="MPI_Isend" bytes="1536" orank="23" region="0" commid="0" count="93" tid="0" op="" dtype="" >2.0623e-04 9.5367e-07 1.3113e-05</hent>
<hent key="0240010000000000000006000000004F" call="MPI_Isend" bytes="1536" orank="79" region="0" commid="0" count="229" tid="0" op="" dtype="" >1.1594e-03 3.8147e-06 8.1062e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002CF" call="MPI_Isend" bytes="1536" orank="719" region="0" commid="0" count="245" tid="0" op="" dtype="" >1.0819e-03 2.8610e-06 8.1062e-06</hent>
<hent key="038001000000000000000C000000000E" call="MPI_Irecv" bytes="3072" orank="14" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C0000000017" call="MPI_Irecv" bytes="3072" orank="23" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C000000004F" call="MPI_Irecv" bytes="3072" orank="79" region="0" commid="0" count="8" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002CF" call="MPI_Irecv" bytes="3072" orank="719" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C000000000E" call="MPI_Isend" bytes="3072" orank="14" region="0" commid="0" count="2" tid="0" op="" dtype="" >5.0068e-06 2.1458e-06 2.8610e-06</hent>
<hent key="024001000000000000000C000000004F" call="MPI_Isend" bytes="3072" orank="79" region="0" commid="0" count="9" tid="0" op="" dtype="" >4.9829e-05 5.0068e-06 6.9141e-06</hent>
<hent key="024001000000000000000C00000002CF" call="MPI_Isend" bytes="3072" orank="719" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.5034e-05 5.0068e-06 5.0068e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.3784e-04 3.3784e-04 3.3784e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >7.2885e-04 2.3985e-04 2.4605e-04</hent>
<hent key="03800100000000000000038000000007" call="MPI_Irecv" bytes="896" orank="7" region="0" commid="0" count="334" tid="0" op="" dtype="" >8.5354e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000008" call="MPI_Irecv" bytes="896" orank="8" region="0" commid="0" count="2695" tid="0" op="" dtype="" >4.6182e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000003800000000E" call="MPI_Irecv" bytes="896" orank="14" region="0" commid="0" count="2755" tid="0" op="" dtype="" >4.1771e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000017" call="MPI_Irecv" bytes="896" orank="23" region="0" commid="0" count="310" tid="0" op="" dtype="" >6.7711e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000003800000004F" call="MPI_Irecv" bytes="896" orank="79" region="0" commid="0" count="319" tid="0" op="" dtype="" >1.3804e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002CF" call="MPI_Irecv" bytes="896" orank="719" region="0" commid="0" count="303" tid="0" op="" dtype="" >9.2268e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.6941e-05 2.6941e-05 2.6941e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.1921e-06 1.1921e-06 1.1921e-06</hent>
<hent key="0380010000000000000038000000004F" call="MPI_Irecv" bytes="14336" orank="79" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.6212e-05 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000003800000002CF" call="MPI_Irecv" bytes="14336" orank="719" region="0" commid="0" count="512" tid="0" op="" dtype="" >7.5579e-05 0.0000e+00 2.8610e-06</hent>
<hent key="02400100000000000000038000000007" call="MPI_Isend" bytes="896" orank="7" region="0" commid="0" count="314" tid="0" op="" dtype="" >4.3726e-04 9.5367e-07 1.1921e-05</hent>
<hent key="02400100000000000000038000000008" call="MPI_Isend" bytes="896" orank="8" region="0" commid="0" count="2896" tid="0" op="" dtype="" >2.9788e-03 0.0000e+00 1.2875e-05</hent>
<hent key="0240010000000000000003800000000E" call="MPI_Isend" bytes="896" orank="14" region="0" commid="0" count="2773" tid="0" op="" dtype="" >1.9968e-03 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000038000000017" call="MPI_Isend" bytes="896" orank="23" region="0" commid="0" count="360" tid="0" op="" dtype="" >5.3644e-04 9.5367e-07 1.0014e-05</hent>
<hent key="0240010000000000000003800000004F" call="MPI_Isend" bytes="896" orank="79" region="0" commid="0" count="334" tid="0" op="" dtype="" >1.6289e-03 3.8147e-06 1.0014e-05</hent>
<hent key="024001000000000000000380000002CF" call="MPI_Isend" bytes="896" orank="719" region="0" commid="0" count="311" tid="0" op="" dtype="" >1.2996e-03 2.8610e-06 7.8678e-06</hent>
<hent key="0240010000000000000038000000004F" call="MPI_Isend" bytes="14336" orank="79" region="0" commid="0" count="297" tid="0" op="" dtype="" >2.4328e-03 5.0068e-06 1.4067e-05</hent>
<hent key="024001000000000000003800000002CF" call="MPI_Isend" bytes="14336" orank="719" region="0" commid="0" count="452" tid="0" op="" dtype="" >2.6643e-03 3.8147e-06 1.2159e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.2932e+00 1.7166e-05 1.3410e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.1594e-04 5.1594e-04 5.1594e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >3.0708e-04 3.0708e-04 3.0708e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >1.8210e-03 1.8210e-03 1.8210e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >8.0888e-02 3.4094e-04 7.5857e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.9009e-04 5.9009e-04 5.9009e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6115e+00 4.4394e-04 2.5626e-01</hent>
<hent key="03800100000000000000040000000007" call="MPI_Irecv" bytes="1024" orank="7" region="0" commid="0" count="3296" tid="0" op="" dtype="" >5.6624e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000008" call="MPI_Irecv" bytes="1024" orank="8" region="0" commid="0" count="1008" tid="0" op="" dtype="" >1.5950e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000000E" call="MPI_Irecv" bytes="1024" orank="14" region="0" commid="0" count="1014" tid="0" op="" dtype="" >9.8228e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000017" call="MPI_Irecv" bytes="1024" orank="23" region="0" commid="0" count="3392" tid="0" op="" dtype="" >4.3726e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.3365e-05 0.0000e+00 1.2159e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="03800100000000000000070000000007" call="MPI_Irecv" bytes="1792" orank="7" region="0" commid="0" count="32" tid="0" op="" dtype="" >8.8215e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000008" call="MPI_Irecv" bytes="1792" orank="8" region="0" commid="0" count="42" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000E" call="MPI_Irecv" bytes="1792" orank="14" region="0" commid="0" count="45" tid="0" op="" dtype="" >2.0981e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000017" call="MPI_Irecv" bytes="1792" orank="23" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.1206e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000004F" call="MPI_Irecv" bytes="1792" orank="79" region="0" commid="0" count="157" tid="0" op="" dtype="" >7.9870e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002CF" call="MPI_Irecv" bytes="1792" orank="719" region="0" commid="0" count="290" tid="0" op="" dtype="" >5.6267e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >8.8692e-05 0.0000e+00 5.1022e-05</hent>
<hent key="02400100000000000000040000000007" call="MPI_Isend" bytes="1024" orank="7" region="0" commid="0" count="3292" tid="0" op="" dtype="" >1.5895e-03 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000040000000008" call="MPI_Isend" bytes="1024" orank="8" region="0" commid="0" count="846" tid="0" op="" dtype="" >6.0391e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000004000000000E" call="MPI_Isend" bytes="1024" orank="14" region="0" commid="0" count="966" tid="0" op="" dtype="" >5.8246e-04 0.0000e+00 3.8147e-06</hent>
<hent key="02400100000000000000040000000017" call="MPI_Isend" bytes="1024" orank="23" region="0" commid="0" count="3390" tid="0" op="" dtype="" >1.7579e-03 0.0000e+00 1.0014e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >1.0014e-05 1.9073e-06 8.1062e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4248e+00 0.0000e+00 3.2510e+00</hent>
<hent key="038001000000000000000A0000000007" call="MPI_Irecv" bytes="2560" orank="7" region="0" commid="0" count="4" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000008" call="MPI_Irecv" bytes="2560" orank="8" region="0" commid="0" count="6" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A000000000E" call="MPI_Irecv" bytes="2560" orank="14" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000017" call="MPI_Irecv" bytes="2560" orank="23" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A000000004F" call="MPI_Irecv" bytes="2560" orank="79" region="0" commid="0" count="50" tid="0" op="" dtype="" >2.5034e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002CF" call="MPI_Irecv" bytes="2560" orank="719" region="0" commid="0" count="50" tid="0" op="" dtype="" >1.7881e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000007" call="MPI_Isend" bytes="1792" orank="7" region="0" commid="0" count="41" tid="0" op="" dtype="" >9.1553e-05 9.5367e-07 1.2159e-05</hent>
<hent key="02400100000000000000070000000008" call="MPI_Isend" bytes="1792" orank="8" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.5163e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000007000000000E" call="MPI_Isend" bytes="1792" orank="14" region="0" commid="0" count="36" tid="0" op="" dtype="" >8.0824e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000017" call="MPI_Isend" bytes="1792" orank="23" region="0" commid="0" count="39" tid="0" op="" dtype="" >9.4652e-05 9.5367e-07 1.0014e-05</hent>
<hent key="0240010000000000000007000000004F" call="MPI_Isend" bytes="1792" orank="79" region="0" commid="0" count="215" tid="0" op="" dtype="" >9.3865e-04 9.5367e-07 6.1989e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6136e-02 3.6136e-02 3.6136e-02</hent>
<hent key="024001000000000000000700000002CF" call="MPI_Isend" bytes="1792" orank="719" region="0" commid="0" count="248" tid="0" op="" dtype="" >9.2864e-04 9.5367e-07 5.9605e-06</hent>
<hent key="024001000000000000000A0000000007" call="MPI_Isend" bytes="2560" orank="7" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.7881e-05 9.5367e-07 1.1921e-05</hent>
<hent key="024001000000000000000A0000000008" call="MPI_Isend" bytes="2560" orank="8" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.4080e-05 3.0994e-06 5.0068e-06</hent>
<hent key="024001000000000000000A000000000E" call="MPI_Isend" bytes="2560" orank="14" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.6226e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000017" call="MPI_Isend" bytes="2560" orank="23" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.5020e-05 9.5367e-07 1.0014e-05</hent>
<hent key="024001000000000000000A000000004F" call="MPI_Isend" bytes="2560" orank="79" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.2173e-04 5.0068e-06 6.9141e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.6120e-04 4.4107e-05 1.7118e-04</hent>
<hent key="024001000000000000000A00000002CF" call="MPI_Isend" bytes="2560" orank="719" region="0" commid="0" count="49" tid="0" op="" dtype="" >2.3389e-04 3.8147e-06 6.1989e-06</hent>
<hent key="0240010000000000000010000000004F" call="MPI_Isend" bytes="4096" orank="79" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.1921e-03 4.6206e-04 7.3004e-04</hent>
<hent key="024001000000000000001000000002CF" call="MPI_Isend" bytes="4096" orank="719" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.5048e-05 3.5048e-05 3.5048e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 2.1458e-06 3.8147e-06</hent>
<hent key="03800100000000000000000400000007" call="MPI_Irecv" bytes="4" orank="7" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.2786e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000008" call="MPI_Irecv" bytes="4" orank="8" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.4012e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000040000000E" call="MPI_Irecv" bytes="4" orank="14" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.1512e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000017" call="MPI_Irecv" bytes="4" orank="23" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.9543e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000004F" call="MPI_Irecv" bytes="4" orank="79" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.1679e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.4126e-03 3.0994e-04 8.4782e-04</hent>
<hent key="038001000000000000001C0000000008" call="MPI_Irecv" bytes="7168" orank="8" region="0" commid="0" count="8985" tid="0" op="" dtype="" >1.3895e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000001C000000000E" call="MPI_Irecv" bytes="7168" orank="14" region="0" commid="0" count="8940" tid="0" op="" dtype="" >2.7127e-03 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000004000002CF" call="MPI_Irecv" bytes="4" orank="719" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.9162e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000400000007" call="MPI_Isend" bytes="4" orank="7" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0114e-03 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000400000008" call="MPI_Isend" bytes="4" orank="8" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.1520e-03 0.0000e+00 6.0081e-05</hent>
<hent key="0240010000000000000000040000000E" call="MPI_Isend" bytes="4" orank="14" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4181e-03 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000400000017" call="MPI_Isend" bytes="4" orank="23" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3561e-03 0.0000e+00 2.5988e-05</hent>
<hent key="0240010000000000000000040000004F" call="MPI_Isend" bytes="4" orank="79" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.2309e-02 3.8147e-06 1.5020e-05</hent>
<hent key="024001000000000000001C0000000008" call="MPI_Isend" bytes="7168" orank="8" region="0" commid="0" count="9573" tid="0" op="" dtype="" >2.7181e-02 9.5367e-07 1.7881e-05</hent>
<hent key="024001000000000000001C000000000E" call="MPI_Isend" bytes="7168" orank="14" region="0" commid="0" count="9142" tid="0" op="" dtype="" >2.0637e-02 9.5367e-07 1.4067e-05</hent>
<hent key="024001000000000000000004000002CF" call="MPI_Isend" bytes="4" orank="719" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6994e-02 2.8610e-06 1.1206e-04</hent>
<hent key="03800100000000000000050000000007" call="MPI_Irecv" bytes="1280" orank="7" region="0" commid="0" count="291" tid="0" op="" dtype="" >5.5313e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000008" call="MPI_Irecv" bytes="1280" orank="8" region="0" commid="0" count="202" tid="0" op="" dtype="" >4.5300e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000E" call="MPI_Irecv" bytes="1280" orank="14" region="0" commid="0" count="188" tid="0" op="" dtype="" >7.6294e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000017" call="MPI_Irecv" bytes="1280" orank="23" region="0" commid="0" count="224" tid="0" op="" dtype="" >4.0054e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000004F" call="MPI_Irecv" bytes="1280" orank="79" region="0" commid="0" count="295" tid="0" op="" dtype="" >1.2827e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.1013e+01 6.9141e-06 1.2808e-01</hent>
<hent key="03800100000000000000280000000007" call="MPI_Irecv" bytes="10240" orank="7" region="0" commid="0" count="326" tid="0" op="" dtype="" >5.2452e-05 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000280000000017" call="MPI_Irecv" bytes="10240" orank="23" region="0" commid="0" count="25" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002CF" call="MPI_Irecv" bytes="1280" orank="719" region="0" commid="0" count="321" tid="0" op="" dtype="" >9.1791e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000007" call="MPI_Irecv" bytes="2048" orank="7" region="0" commid="0" count="15" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000008" call="MPI_Irecv" bytes="2048" orank="8" region="0" commid="0" count="13" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000008000000000E" call="MPI_Irecv" bytes="2048" orank="14" region="0" commid="0" count="13" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000017" call="MPI_Irecv" bytes="2048" orank="23" region="0" commid="0" count="12" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000004F" call="MPI_Irecv" bytes="2048" orank="79" region="0" commid="0" count="3454" tid="0" op="" dtype="" >5.7507e-04 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000800000002CF" call="MPI_Irecv" bytes="2048" orank="719" region="0" commid="0" count="3312" tid="0" op="" dtype="" >4.9996e-04 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000050000000007" call="MPI_Isend" bytes="1280" orank="7" region="0" commid="0" count="309" tid="0" op="" dtype="" >3.7384e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000050000000008" call="MPI_Isend" bytes="1280" orank="8" region="0" commid="0" count="171" tid="0" op="" dtype="" >6.2776e-04 2.1458e-06 7.8678e-06</hent>
<hent key="0240010000000000000005000000000E" call="MPI_Isend" bytes="1280" orank="14" region="0" commid="0" count="213" tid="0" op="" dtype="" >4.0221e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000017" call="MPI_Isend" bytes="1280" orank="23" region="0" commid="0" count="219" tid="0" op="" dtype="" >3.4142e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000005000000004F" call="MPI_Isend" bytes="1280" orank="79" region="0" commid="0" count="276" tid="0" op="" dtype="" >1.3833e-03 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000280000000007" call="MPI_Isend" bytes="10240" orank="7" region="0" commid="0" count="378" tid="0" op="" dtype="" >1.1635e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000280000000017" call="MPI_Isend" bytes="10240" orank="23" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.6451e-05 0.0000e+00 1.9073e-06</hent>
<hent key="024001000000000000000500000002CF" call="MPI_Isend" bytes="1280" orank="719" region="0" commid="0" count="294" tid="0" op="" dtype="" >1.2770e-03 2.8610e-06 8.8215e-06</hent>
</hash>
<internal rank="15" log_i="1723713849.517431" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="16" mpi_size="768" stamp_init="1723713791.122017" stamp_final="1723713849.510253" username="apac4" allocationname="unknown" flags="0" pid="1717079" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.83882e+01" utime="4.79809e+01" stime="7.73315e+00" mtime="3.20279e+01" gflop="0.00000e+00" gbyte="3.77365e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20279e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000361431149b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83203e+01" utime="4.79506e+01" stime="7.72375e+00" mtime="3.20279e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20279e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4921e+08" > 6.4668e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5021e+08" > 4.3508e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4549e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4103e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1032e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.0994e-06 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6109e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.4598e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3498e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6160e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8819e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="196" >
<hent key="02400100000000000000080000000008" call="MPI_Isend" bytes="2048" orank="8" region="0" commid="0" count="21" tid="0" op="" dtype="" >4.5061e-05 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000080000000011" call="MPI_Isend" bytes="2048" orank="17" region="0" commid="0" count="11" tid="0" op="" dtype="" >4.6730e-05 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000080000000017" call="MPI_Isend" bytes="2048" orank="23" region="0" commid="0" count="13" tid="0" op="" dtype="" >2.8133e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000080000000018" call="MPI_Isend" bytes="2048" orank="24" region="0" commid="0" count="13" tid="0" op="" dtype="" >7.8201e-05 5.0068e-06 1.2159e-05</hent>
<hent key="02400100000000000000080000000050" call="MPI_Isend" bytes="2048" orank="80" region="0" commid="0" count="3469" tid="0" op="" dtype="" >1.4470e-02 9.5367e-07 1.3900e-04</hent>
<hent key="024001000000000000000800000002D0" call="MPI_Isend" bytes="2048" orank="720" region="0" commid="0" count="3474" tid="0" op="" dtype="" >1.3183e-02 9.5367e-07 6.0081e-05</hent>
<hent key="038001000000000000000E0000000050" call="MPI_Irecv" bytes="3584" orank="80" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.9073e-06 9.5367e-07 9.5367e-07</hent>
<hent key="03800100000000000000028000000008" call="MPI_Irecv" bytes="640" orank="8" region="0" commid="0" count="382" tid="0" op="" dtype="" >2.0266e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000028000000011" call="MPI_Irecv" bytes="640" orank="17" region="0" commid="0" count="382" tid="0" op="" dtype="" >7.9393e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000017" call="MPI_Irecv" bytes="640" orank="23" region="0" commid="0" count="383" tid="0" op="" dtype="" >1.6212e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000018" call="MPI_Irecv" bytes="640" orank="24" region="0" commid="0" count="419" tid="0" op="" dtype="" >1.4472e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000028000000050" call="MPI_Irecv" bytes="640" orank="80" region="0" commid="0" count="272" tid="0" op="" dtype="" >1.3900e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002D0" call="MPI_Irecv" bytes="640" orank="720" region="0" commid="0" count="267" tid="0" op="" dtype="" >9.2983e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000008" call="MPI_Isend" bytes="640" orank="8" region="0" commid="0" count="422" tid="0" op="" dtype="" >5.9819e-04 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000028000000011" call="MPI_Isend" bytes="640" orank="17" region="0" commid="0" count="405" tid="0" op="" dtype="" >1.2929e-03 1.9073e-06 1.8835e-05</hent>
<hent key="02400100000000000000028000000017" call="MPI_Isend" bytes="640" orank="23" region="0" commid="0" count="401" tid="0" op="" dtype="" >5.2643e-04 9.5367e-07 6.1989e-06</hent>
<hent key="02400100000000000000028000000018" call="MPI_Isend" bytes="640" orank="24" region="0" commid="0" count="384" tid="0" op="" dtype="" >1.8277e-03 3.8147e-06 1.2159e-05</hent>
<hent key="02400100000000000000028000000050" call="MPI_Isend" bytes="640" orank="80" region="0" commid="0" count="268" tid="0" op="" dtype="" >1.3733e-03 3.0994e-06 1.2159e-05</hent>
<hent key="024001000000000000000280000002D0" call="MPI_Isend" bytes="640" orank="720" region="0" commid="0" count="258" tid="0" op="" dtype="" >1.2662e-03 2.8610e-06 1.3113e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.8147e-06 3.8147e-06 3.8147e-06</hent>
<hent key="03800100000000000000014000000008" call="MPI_Irecv" bytes="320" orank="8" region="0" commid="0" count="339" tid="0" op="" dtype="" >1.8787e-04 0.0000e+00 1.6928e-05</hent>
<hent key="03800100000000000000014000000011" call="MPI_Irecv" bytes="320" orank="17" region="0" commid="0" count="384" tid="0" op="" dtype="" >9.1791e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000017" call="MPI_Irecv" bytes="320" orank="23" region="0" commid="0" count="348" tid="0" op="" dtype="" >1.3900e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000018" call="MPI_Irecv" bytes="320" orank="24" region="0" commid="0" count="354" tid="0" op="" dtype="" >1.2684e-04 0.0000e+00 1.3113e-05</hent>
<hent key="03800100000000000000014000000050" call="MPI_Irecv" bytes="320" orank="80" region="0" commid="0" count="172" tid="0" op="" dtype="" >7.2002e-05 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000140000002D0" call="MPI_Irecv" bytes="320" orank="720" region="0" commid="0" count="196" tid="0" op="" dtype="" >8.3208e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 9.5367e-07 1.9073e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.4549e+00 0.0000e+00 1.1644e-01</hent>
<hent key="03800100000000000000400000000050" call="MPI_Irecv" bytes="16384" orank="80" region="0" commid="0" count="12671" tid="0" op="" dtype="" >6.5432e-03 0.0000e+00 2.8849e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.7881e-05 0.0000e+00 1.7881e-05</hent>
<hent key="038001000000000000004000000002D0" call="MPI_Irecv" bytes="16384" orank="720" region="0" commid="0" count="12655" tid="0" op="" dtype="" >7.2999e-03 0.0000e+00 1.3494e-04</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000014000000008" call="MPI_Isend" bytes="320" orank="8" region="0" commid="0" count="352" tid="0" op="" dtype="" >4.6086e-04 0.0000e+00 1.2159e-05</hent>
<hent key="02400100000000000000014000000011" call="MPI_Isend" bytes="320" orank="17" region="0" commid="0" count="321" tid="0" op="" dtype="" >9.4032e-04 1.9073e-06 7.8678e-06</hent>
<hent key="02400100000000000000014000000017" call="MPI_Isend" bytes="320" orank="23" region="0" commid="0" count="368" tid="0" op="" dtype="" >3.6764e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000014000000018" call="MPI_Isend" bytes="320" orank="24" region="0" commid="0" count="386" tid="0" op="" dtype="" >1.8027e-03 3.8147e-06 1.7881e-05</hent>
<hent key="02400100000000000000014000000050" call="MPI_Isend" bytes="320" orank="80" region="0" commid="0" count="176" tid="0" op="" dtype="" >9.1839e-04 3.0994e-06 1.5020e-05</hent>
<hent key="03800100000000000000200000000008" call="MPI_Irecv" bytes="8192" orank="8" region="0" commid="0" count="12624" tid="0" op="" dtype="" >5.9261e-03 0.0000e+00 3.4094e-05</hent>
<hent key="03800100000000000000200000000011" call="MPI_Irecv" bytes="8192" orank="17" region="0" commid="0" count="4074" tid="0" op="" dtype="" >8.0514e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000200000000017" call="MPI_Irecv" bytes="8192" orank="23" region="0" commid="0" count="4033" tid="0" op="" dtype="" >1.4505e-03 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000200000000018" call="MPI_Irecv" bytes="8192" orank="24" region="0" commid="0" count="12684" tid="0" op="" dtype="" >2.8918e-03 0.0000e+00 1.7881e-05</hent>
<hent key="024001000000000000000140000002D0" call="MPI_Isend" bytes="320" orank="720" region="0" commid="0" count="189" tid="0" op="" dtype="" >8.9455e-04 2.8610e-06 3.1948e-05</hent>
<hent key="03800100000000000000000000000008" call="MPI_Irecv" bytes="0" orank="8" region="0" commid="0" count="234" tid="0" op="" dtype="" >7.9155e-05 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000000000000011" call="MPI_Irecv" bytes="0" orank="17" region="0" commid="0" count="234" tid="0" op="" dtype="" >6.0081e-05 0.0000e+00 3.8147e-06</hent>
<hent key="03800100000000000000000000000017" call="MPI_Irecv" bytes="0" orank="23" region="0" commid="0" count="258" tid="0" op="" dtype="" >7.9632e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000018" call="MPI_Irecv" bytes="0" orank="24" region="0" commid="0" count="240" tid="0" op="" dtype="" >7.8678e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000000000050" call="MPI_Irecv" bytes="0" orank="80" region="0" commid="0" count="159" tid="0" op="" dtype="" >6.4611e-05 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000400000000050" call="MPI_Isend" bytes="16384" orank="80" region="0" commid="0" count="12683" tid="0" op="" dtype="" >1.2596e-01 3.8147e-06 1.4114e-04</hent>
<hent key="038001000000000000000000000002D0" call="MPI_Irecv" bytes="0" orank="720" region="0" commid="0" count="154" tid="0" op="" dtype="" >5.9128e-05 0.0000e+00 4.0531e-06</hent>
<hent key="024001000000000000004000000002D0" call="MPI_Isend" bytes="16384" orank="720" region="0" commid="0" count="12682" tid="0" op="" dtype="" >1.3235e-01 3.0994e-06 1.3804e-04</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.8368e-04 0.0000e+00 6.1011e-04</hent>
<hent key="02400100000000000000200000000008" call="MPI_Isend" bytes="8192" orank="8" region="0" commid="0" count="12630" tid="0" op="" dtype="" >1.0497e-02 0.0000e+00 3.6001e-05</hent>
<hent key="02400100000000000000200000000011" call="MPI_Isend" bytes="8192" orank="17" region="0" commid="0" count="3654" tid="0" op="" dtype="" >7.2687e-03 0.0000e+00 1.1897e-04</hent>
<hent key="02400100000000000000200000000017" call="MPI_Isend" bytes="8192" orank="23" region="0" commid="0" count="3421" tid="0" op="" dtype="" >4.0641e-03 0.0000e+00 6.9857e-05</hent>
<hent key="02400100000000000000200000000018" call="MPI_Isend" bytes="8192" orank="24" region="0" commid="0" count="12671" tid="0" op="" dtype="" >1.5605e-01 3.8147e-06 2.5296e-04</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1935e-05 2.1935e-05 2.1935e-05</hent>
<hent key="02400100000000000000000000000008" call="MPI_Isend" bytes="0" orank="8" region="0" commid="0" count="228" tid="0" op="" dtype="" >2.1434e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000000000000011" call="MPI_Isend" bytes="0" orank="17" region="0" commid="0" count="272" tid="0" op="" dtype="" >6.1345e-04 9.5367e-07 7.5102e-05</hent>
<hent key="02400100000000000000000000000017" call="MPI_Isend" bytes="0" orank="23" region="0" commid="0" count="250" tid="0" op="" dtype="" >1.7476e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000018" call="MPI_Isend" bytes="0" orank="24" region="0" commid="0" count="255" tid="0" op="" dtype="" >1.0560e-03 1.1921e-06 2.5034e-05</hent>
<hent key="02400100000000000000000000000050" call="MPI_Isend" bytes="0" orank="80" region="0" commid="0" count="156" tid="0" op="" dtype="" >7.0763e-04 9.5367e-07 1.5020e-05</hent>
<hent key="024001000000000000000000000002D0" call="MPI_Isend" bytes="0" orank="720" region="0" commid="0" count="145" tid="0" op="" dtype="" >6.0582e-04 2.1458e-06 1.1206e-05</hent>
<hent key="03800100000000000000060000000008" call="MPI_Irecv" bytes="1536" orank="8" region="0" commid="0" count="110" tid="0" op="" dtype="" >7.6294e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000060000000011" call="MPI_Irecv" bytes="1536" orank="17" region="0" commid="0" count="105" tid="0" op="" dtype="" >1.4544e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000017" call="MPI_Irecv" bytes="1536" orank="23" region="0" commid="0" count="109" tid="0" op="" dtype="" >3.6955e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000018" call="MPI_Irecv" bytes="1536" orank="24" region="0" commid="0" count="80" tid="0" op="" dtype="" >2.4796e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000050" call="MPI_Irecv" bytes="1536" orank="80" region="0" commid="0" count="185" tid="0" op="" dtype="" >9.0599e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002D0" call="MPI_Irecv" bytes="1536" orank="720" region="0" commid="0" count="208" tid="0" op="" dtype="" >6.5565e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000008" call="MPI_Isend" bytes="1536" orank="8" region="0" commid="0" count="111" tid="0" op="" dtype="" >2.6035e-04 9.5367e-07 1.8835e-05</hent>
<hent key="02400100000000000000060000000011" call="MPI_Isend" bytes="1536" orank="17" region="0" commid="0" count="108" tid="0" op="" dtype="" >4.1294e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000060000000017" call="MPI_Isend" bytes="1536" orank="23" region="0" commid="0" count="100" tid="0" op="" dtype="" >2.0194e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000060000000018" call="MPI_Isend" bytes="1536" orank="24" region="0" commid="0" count="98" tid="0" op="" dtype="" >5.5766e-04 3.8147e-06 1.6928e-05</hent>
<hent key="02400100000000000000060000000050" call="MPI_Isend" bytes="1536" orank="80" region="0" commid="0" count="211" tid="0" op="" dtype="" >1.2026e-03 3.8147e-06 2.0027e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002D0" call="MPI_Isend" bytes="1536" orank="720" region="0" commid="0" count="197" tid="0" op="" dtype="" >1.0273e-03 3.0994e-06 3.4094e-05</hent>
<hent key="038001000000000000000C0000000050" call="MPI_Irecv" bytes="3072" orank="80" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002D0" call="MPI_Irecv" bytes="3072" orank="720" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C0000000018" call="MPI_Isend" bytes="3072" orank="24" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.2398e-05 6.1989e-06 6.1989e-06</hent>
<hent key="024001000000000000000C0000000050" call="MPI_Isend" bytes="3072" orank="80" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.8835e-05 5.9605e-06 6.9141e-06</hent>
<hent key="024001000000000000000C00000002D0" call="MPI_Isend" bytes="3072" orank="720" region="0" commid="0" count="13" tid="0" op="" dtype="" >8.2731e-05 5.0068e-06 1.0014e-05</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.5191e-04 3.5191e-04 3.5191e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >7.8368e-04 2.6083e-04 2.6202e-04</hent>
<hent key="03800100000000000000038000000008" call="MPI_Irecv" bytes="896" orank="8" region="0" commid="0" count="365" tid="0" op="" dtype="" >2.2221e-04 0.0000e+00 1.2159e-05</hent>
<hent key="03800100000000000000038000000011" call="MPI_Irecv" bytes="896" orank="17" region="0" commid="0" count="2648" tid="0" op="" dtype="" >5.4979e-04 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000038000000017" call="MPI_Irecv" bytes="896" orank="23" region="0" commid="0" count="2667" tid="0" op="" dtype="" >3.9172e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000018" call="MPI_Irecv" bytes="896" orank="24" region="0" commid="0" count="352" tid="0" op="" dtype="" >1.1277e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000038000000050" call="MPI_Irecv" bytes="896" orank="80" region="0" commid="0" count="355" tid="0" op="" dtype="" >1.8191e-04 0.0000e+00 7.8678e-06</hent>
<hent key="038001000000000000000380000002D0" call="MPI_Irecv" bytes="896" orank="720" region="0" commid="0" count="304" tid="0" op="" dtype="" >1.2875e-04 0.0000e+00 7.8678e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.0599e-06 9.0599e-06 9.0599e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="03800100000000000000380000000050" call="MPI_Irecv" bytes="14336" orank="80" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000003800000002D0" call="MPI_Irecv" bytes="14336" orank="720" region="0" commid="0" count="44" tid="0" op="" dtype="" >1.5974e-05 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000038000000008" call="MPI_Isend" bytes="896" orank="8" region="0" commid="0" count="311" tid="0" op="" dtype="" >5.7602e-04 9.5367e-07 1.1921e-05</hent>
<hent key="02400100000000000000038000000011" call="MPI_Isend" bytes="896" orank="17" region="0" commid="0" count="2778" tid="0" op="" dtype="" >2.8346e-03 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000038000000017" call="MPI_Isend" bytes="896" orank="23" region="0" commid="0" count="2802" tid="0" op="" dtype="" >1.9267e-03 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000038000000018" call="MPI_Isend" bytes="896" orank="24" region="0" commid="0" count="304" tid="0" op="" dtype="" >1.5483e-03 3.8147e-06 1.7166e-05</hent>
<hent key="02400100000000000000038000000050" call="MPI_Isend" bytes="896" orank="80" region="0" commid="0" count="339" tid="0" op="" dtype="" >1.8153e-03 2.8610e-06 1.7166e-05</hent>
<hent key="024001000000000000000380000002D0" call="MPI_Isend" bytes="896" orank="720" region="0" commid="0" count="330" tid="0" op="" dtype="" >1.6074e-03 3.0994e-06 2.2888e-05</hent>
<hent key="02400100000000000000380000000050" call="MPI_Isend" bytes="14336" orank="80" region="0" commid="0" count="16" tid="0" op="" dtype="" >2.2960e-04 5.9605e-06 2.8849e-05</hent>
<hent key="024001000000000000003800000002D0" call="MPI_Isend" bytes="14336" orank="720" region="0" commid="0" count="17" tid="0" op="" dtype="" >1.7333e-04 3.8147e-06 2.3842e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.8492e+00 2.3127e-05 1.3918e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.0712e-04 5.0712e-04 5.0712e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >3.3498e-04 3.3498e-04 3.3498e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >1.2369e-03 1.2369e-03 1.2369e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >5.8840e-02 3.3402e-04 5.3598e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >1.0204e-04 1.0204e-04 1.0204e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6109e+00 4.7898e-04 2.5621e-01</hent>
<hent key="03800100000000000000040000000008" call="MPI_Irecv" bytes="1024" orank="8" region="0" commid="0" count="3378" tid="0" op="" dtype="" >6.5827e-04 0.0000e+00 1.2159e-05</hent>
<hent key="03800100000000000000040000000011" call="MPI_Irecv" bytes="1024" orank="17" region="0" commid="0" count="1096" tid="0" op="" dtype="" >2.1553e-04 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000040000000017" call="MPI_Irecv" bytes="1024" orank="23" region="0" commid="0" count="1074" tid="0" op="" dtype="" >1.1158e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000040000000018" call="MPI_Irecv" bytes="1024" orank="24" region="0" commid="0" count="3394" tid="0" op="" dtype="" >7.0167e-04 0.0000e+00 1.5974e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.1032e-03 1.9073e-06 6.5708e-04</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="03800100000000000000070000000008" call="MPI_Irecv" bytes="1792" orank="8" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.7166e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000070000000011" call="MPI_Irecv" bytes="1792" orank="17" region="0" commid="0" count="37" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000017" call="MPI_Irecv" bytes="1792" orank="23" region="0" commid="0" count="44" tid="0" op="" dtype="" >2.1696e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000018" call="MPI_Irecv" bytes="1792" orank="24" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.0490e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000050" call="MPI_Irecv" bytes="1792" orank="80" region="0" commid="0" count="133" tid="0" op="" dtype="" >6.8665e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000700000002D0" call="MPI_Irecv" bytes="1792" orank="720" region="0" commid="0" count="159" tid="0" op="" dtype="" >6.2704e-05 0.0000e+00 4.0531e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >8.8692e-05 0.0000e+00 5.4121e-05</hent>
<hent key="02400100000000000000040000000008" call="MPI_Isend" bytes="1024" orank="8" region="0" commid="0" count="3380" tid="0" op="" dtype="" >2.7385e-03 0.0000e+00 2.8849e-05</hent>
<hent key="02400100000000000000040000000011" call="MPI_Isend" bytes="1024" orank="17" region="0" commid="0" count="956" tid="0" op="" dtype="" >6.6710e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000040000000017" call="MPI_Isend" bytes="1024" orank="23" region="0" commid="0" count="932" tid="0" op="" dtype="" >5.8746e-04 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000040000000018" call="MPI_Isend" bytes="1024" orank="24" region="0" commid="0" count="3392" tid="0" op="" dtype="" >1.7332e-02 9.5367e-07 4.1590e-03</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >1.3113e-05 3.0994e-06 1.0014e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4093e+00 0.0000e+00 3.2480e+00</hent>
<hent key="038001000000000000000A0000000008" call="MPI_Irecv" bytes="2560" orank="8" region="0" commid="0" count="10" tid="0" op="" dtype="" >6.6757e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000011" call="MPI_Irecv" bytes="2560" orank="17" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000017" call="MPI_Irecv" bytes="2560" orank="23" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000018" call="MPI_Irecv" bytes="2560" orank="24" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000050" call="MPI_Irecv" bytes="2560" orank="80" region="0" commid="0" count="50" tid="0" op="" dtype="" >2.9802e-05 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000A00000002D0" call="MPI_Irecv" bytes="2560" orank="720" region="0" commid="0" count="58" tid="0" op="" dtype="" >3.2425e-05 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000070000000008" call="MPI_Isend" bytes="1792" orank="8" region="0" commid="0" count="27" tid="0" op="" dtype="" >8.7023e-05 9.5367e-07 1.7881e-05</hent>
<hent key="02400100000000000000070000000011" call="MPI_Isend" bytes="1792" orank="17" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.1516e-04 3.0994e-06 5.0068e-06</hent>
<hent key="02400100000000000000070000000017" call="MPI_Isend" bytes="1792" orank="23" region="0" commid="0" count="41" tid="0" op="" dtype="" >8.8215e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000018" call="MPI_Isend" bytes="1792" orank="24" region="0" commid="0" count="38" tid="0" op="" dtype="" >2.1005e-04 5.0068e-06 1.7166e-05</hent>
<hent key="02400100000000000000070000000050" call="MPI_Isend" bytes="1792" orank="80" region="0" commid="0" count="128" tid="0" op="" dtype="" >7.2432e-04 2.1458e-06 2.2888e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6160e-02 3.6160e-02 3.6160e-02</hent>
<hent key="024001000000000000000700000002D0" call="MPI_Isend" bytes="1792" orank="720" region="0" commid="0" count="137" tid="0" op="" dtype="" >7.2932e-04 1.9073e-06 1.4067e-05</hent>
<hent key="024001000000000000000A0000000008" call="MPI_Isend" bytes="2560" orank="8" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.3140e-05 9.5367e-07 2.0027e-05</hent>
<hent key="024001000000000000000A0000000011" call="MPI_Isend" bytes="2560" orank="17" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.5034e-05 3.8147e-06 5.0068e-06</hent>
<hent key="024001000000000000000A0000000017" call="MPI_Isend" bytes="2560" orank="23" region="0" commid="0" count="2" tid="0" op="" dtype="" >4.2915e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000A0000000018" call="MPI_Isend" bytes="2560" orank="24" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.7657e-05 4.7684e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000050" call="MPI_Isend" bytes="2560" orank="80" region="0" commid="0" count="51" tid="0" op="" dtype="" >3.0494e-04 4.0531e-06 1.3113e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.9196e-04 4.7922e-05 1.8215e-04</hent>
<hent key="024001000000000000000A00000002D0" call="MPI_Isend" bytes="2560" orank="720" region="0" commid="0" count="51" tid="0" op="" dtype="" >2.8801e-04 4.0531e-06 1.4067e-05</hent>
<hent key="02400100000000000000100000000050" call="MPI_Isend" bytes="4096" orank="80" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.3330e-03 4.9806e-04 8.3494e-04</hent>
<hent key="024001000000000000001000000002D0" call="MPI_Isend" bytes="4096" orank="720" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 9.5367e-07 4.0531e-06</hent>
<hent key="03800100000000000000000400000008" call="MPI_Irecv" bytes="4" orank="8" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.8143e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000000400000011" call="MPI_Irecv" bytes="4" orank="17" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.6553e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000017" call="MPI_Irecv" bytes="4" orank="23" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2424e-03 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000000400000018" call="MPI_Irecv" bytes="4" orank="24" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.7650e-04 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000000400000050" call="MPI_Irecv" bytes="4" orank="80" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1773e-03 0.0000e+00 1.7166e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.5892e-03 3.3498e-04 9.1720e-04</hent>
<hent key="038001000000000000001C0000000011" call="MPI_Irecv" bytes="7168" orank="17" region="0" commid="0" count="8625" tid="0" op="" dtype="" >1.7502e-03 0.0000e+00 1.3113e-05</hent>
<hent key="038001000000000000001C0000000017" call="MPI_Irecv" bytes="7168" orank="23" region="0" commid="0" count="8666" tid="0" op="" dtype="" >3.0169e-03 0.0000e+00 1.0014e-05</hent>
<hent key="038001000000000000000004000002D0" call="MPI_Irecv" bytes="4" orank="720" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2858e-03 0.0000e+00 1.6928e-05</hent>
<hent key="02400100000000000000000400000008" call="MPI_Isend" bytes="4" orank="8" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4973e-03 0.0000e+00 2.9087e-05</hent>
<hent key="02400100000000000000000400000011" call="MPI_Isend" bytes="4" orank="17" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3425e-03 0.0000e+00 2.2888e-05</hent>
<hent key="02400100000000000000000400000017" call="MPI_Isend" bytes="4" orank="23" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.5211e-03 0.0000e+00 6.8903e-05</hent>
<hent key="02400100000000000000000400000018" call="MPI_Isend" bytes="4" orank="24" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.6734e-02 3.8147e-06 1.0705e-04</hent>
<hent key="02400100000000000000000400000050" call="MPI_Isend" bytes="4" orank="80" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.1979e-02 3.8147e-06 4.8161e-05</hent>
<hent key="024001000000000000001C0000000011" call="MPI_Isend" bytes="7168" orank="17" region="0" commid="0" count="9045" tid="0" op="" dtype="" >2.6838e-02 9.5367e-07 4.2915e-05</hent>
<hent key="024001000000000000001C0000000017" call="MPI_Isend" bytes="7168" orank="23" region="0" commid="0" count="9278" tid="0" op="" dtype="" >2.0123e-02 9.5367e-07 1.6928e-05</hent>
<hent key="024001000000000000000004000002D0" call="MPI_Isend" bytes="4" orank="720" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.1392e-02 3.8147e-06 1.3089e-04</hent>
<hent key="03800100000000000000050000000008" call="MPI_Irecv" bytes="1280" orank="8" region="0" commid="0" count="236" tid="0" op="" dtype="" >1.3590e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000050000000011" call="MPI_Irecv" bytes="1280" orank="17" region="0" commid="0" count="195" tid="0" op="" dtype="" >4.4584e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000017" call="MPI_Irecv" bytes="1280" orank="23" region="0" commid="0" count="197" tid="0" op="" dtype="" >8.7261e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000018" call="MPI_Irecv" bytes="1280" orank="24" region="0" commid="0" count="202" tid="0" op="" dtype="" >6.3896e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000050" call="MPI_Irecv" bytes="1280" orank="80" region="0" commid="0" count="310" tid="0" op="" dtype="" >1.5664e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0909e+01 9.0599e-06 1.3895e-01</hent>
<hent key="03800100000000000000280000000008" call="MPI_Irecv" bytes="10240" orank="8" region="0" commid="0" count="75" tid="0" op="" dtype="" >2.1696e-05 0.0000e+00 8.8215e-06</hent>
<hent key="03800100000000000000280000000018" call="MPI_Irecv" bytes="10240" orank="24" region="0" commid="0" count="15" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000500000002D0" call="MPI_Irecv" bytes="1280" orank="720" region="0" commid="0" count="278" tid="0" op="" dtype="" >1.3280e-04 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000080000000008" call="MPI_Irecv" bytes="2048" orank="8" region="0" commid="0" count="15" tid="0" op="" dtype="" >1.3590e-05 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000080000000011" call="MPI_Irecv" bytes="2048" orank="17" region="0" commid="0" count="9" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000017" call="MPI_Irecv" bytes="2048" orank="23" region="0" commid="0" count="13" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000018" call="MPI_Irecv" bytes="2048" orank="24" region="0" commid="0" count="12" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000050" call="MPI_Irecv" bytes="2048" orank="80" region="0" commid="0" count="3456" tid="0" op="" dtype="" >8.5330e-04 0.0000e+00 2.3842e-05</hent>
<hent key="038001000000000000000800000002D0" call="MPI_Irecv" bytes="2048" orank="720" region="0" commid="0" count="3464" tid="0" op="" dtype="" >7.5078e-04 0.0000e+00 2.1935e-05</hent>
<hent key="02400100000000000000050000000008" call="MPI_Isend" bytes="1280" orank="8" region="0" commid="0" count="238" tid="0" op="" dtype="" >4.3726e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000050000000011" call="MPI_Isend" bytes="1280" orank="17" region="0" commid="0" count="212" tid="0" op="" dtype="" >7.7033e-04 2.8610e-06 9.0599e-06</hent>
<hent key="02400100000000000000050000000017" call="MPI_Isend" bytes="1280" orank="23" region="0" commid="0" count="188" tid="0" op="" dtype="" >3.3641e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000018" call="MPI_Isend" bytes="1280" orank="24" region="0" commid="0" count="220" tid="0" op="" dtype="" >1.1764e-03 9.5367e-07 1.3828e-05</hent>
<hent key="02400100000000000000050000000050" call="MPI_Isend" bytes="1280" orank="80" region="0" commid="0" count="295" tid="0" op="" dtype="" >1.5888e-03 3.8147e-06 1.3113e-05</hent>
<hent key="02400100000000000000280000000008" call="MPI_Isend" bytes="10240" orank="8" region="0" commid="0" count="69" tid="0" op="" dtype="" >6.8426e-05 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000280000000018" call="MPI_Isend" bytes="10240" orank="24" region="0" commid="0" count="28" tid="0" op="" dtype="" >2.6488e-04 5.0068e-06 2.1935e-05</hent>
<hent key="024001000000000000000500000002D0" call="MPI_Isend" bytes="1280" orank="720" region="0" commid="0" count="302" tid="0" op="" dtype="" >1.4617e-03 2.8610e-06 2.3127e-05</hent>
</hash>
<internal rank="16" log_i="1723713849.510253" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="17" mpi_size="768" stamp_init="1723713791.123174" stamp_final="1723713849.510341" username="apac4" allocationname="unknown" flags="0" pid="1717080" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.83872e+01" utime="5.00339e+01" stime="6.91247e+00" mtime="3.19303e+01" gflop="0.00000e+00" gbyte="3.76991e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19303e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004c154c1536" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83194e+01" utime="5.00038e+01" stime="6.90290e+00" mtime="3.19303e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19303e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4957e+08" > 4.9289e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4970e+08" > 3.0600e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0426e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4416e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3842e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.7909e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6108e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.7342e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7084e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6141e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9270e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="195" >
<hent key="02400100000000000000080000000009" call="MPI_Isend" bytes="2048" orank="9" region="0" commid="0" count="15" tid="0" op="" dtype="" >4.7922e-05 1.9073e-06 1.1921e-05</hent>
<hent key="02400100000000000000080000000010" call="MPI_Isend" bytes="2048" orank="16" region="0" commid="0" count="9" tid="0" op="" dtype="" >2.2888e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000012" call="MPI_Isend" bytes="2048" orank="18" region="0" commid="0" count="12" tid="0" op="" dtype="" >4.9114e-05 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000080000000019" call="MPI_Isend" bytes="2048" orank="25" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.8862e-05 5.0068e-06 5.9605e-06</hent>
<hent key="02400100000000000000080000000051" call="MPI_Isend" bytes="2048" orank="81" region="0" commid="0" count="3464" tid="0" op="" dtype="" >1.0211e-02 9.5367e-07 4.6015e-05</hent>
<hent key="024001000000000000000800000002D1" call="MPI_Isend" bytes="2048" orank="721" region="0" commid="0" count="3467" tid="0" op="" dtype="" >1.0173e-02 9.5367e-07 1.6928e-05</hent>
<hent key="038001000000000000000E0000000051" call="MPI_Irecv" bytes="3584" orank="81" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000E0000000051" call="MPI_Isend" bytes="3584" orank="81" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.8120e-05 5.9605e-06 6.1989e-06</hent>
<hent key="03800100000000000000028000000009" call="MPI_Irecv" bytes="640" orank="9" region="0" commid="0" count="417" tid="0" op="" dtype="" >9.2506e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000010" call="MPI_Irecv" bytes="640" orank="16" region="0" commid="0" count="405" tid="0" op="" dtype="" >1.4949e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000012" call="MPI_Irecv" bytes="640" orank="18" region="0" commid="0" count="393" tid="0" op="" dtype="" >1.1873e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000019" call="MPI_Irecv" bytes="640" orank="25" region="0" commid="0" count="412" tid="0" op="" dtype="" >9.4414e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000E00000002D1" call="MPI_Isend" bytes="3584" orank="721" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="03800100000000000000028000000051" call="MPI_Irecv" bytes="640" orank="81" region="0" commid="0" count="276" tid="0" op="" dtype="" >1.2708e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000000280000002D1" call="MPI_Irecv" bytes="640" orank="721" region="0" commid="0" count="290" tid="0" op="" dtype="" >8.7738e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000009" call="MPI_Isend" bytes="640" orank="9" region="0" commid="0" count="432" tid="0" op="" dtype="" >5.9152e-04 9.5367e-07 1.4067e-05</hent>
<hent key="02400100000000000000028000000010" call="MPI_Isend" bytes="640" orank="16" region="0" commid="0" count="382" tid="0" op="" dtype="" >5.5933e-04 9.5367e-07 8.8215e-06</hent>
<hent key="02400100000000000000028000000012" call="MPI_Isend" bytes="640" orank="18" region="0" commid="0" count="438" tid="0" op="" dtype="" >1.2920e-03 1.9073e-06 1.9073e-05</hent>
<hent key="02400100000000000000028000000019" call="MPI_Isend" bytes="640" orank="25" region="0" commid="0" count="389" tid="0" op="" dtype="" >1.8561e-03 3.8147e-06 8.1062e-06</hent>
<hent key="02400100000000000000028000000051" call="MPI_Isend" bytes="640" orank="81" region="0" commid="0" count="254" tid="0" op="" dtype="" >1.1175e-03 3.0994e-06 5.9605e-06</hent>
<hent key="024001000000000000000280000002D1" call="MPI_Isend" bytes="640" orank="721" region="0" commid="0" count="292" tid="0" op="" dtype="" >1.2114e-03 2.8610e-06 8.8215e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.0599e-06 9.0599e-06 9.0599e-06</hent>
<hent key="03800100000000000000014000000009" call="MPI_Irecv" bytes="320" orank="9" region="0" commid="0" count="368" tid="0" op="" dtype="" >9.1076e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000010" call="MPI_Irecv" bytes="320" orank="16" region="0" commid="0" count="321" tid="0" op="" dtype="" >1.1182e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000014000000012" call="MPI_Irecv" bytes="320" orank="18" region="0" commid="0" count="358" tid="0" op="" dtype="" >1.1182e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000014000000019" call="MPI_Irecv" bytes="320" orank="25" region="0" commid="0" count="377" tid="0" op="" dtype="" >7.8678e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000051" call="MPI_Irecv" bytes="320" orank="81" region="0" commid="0" count="151" tid="0" op="" dtype="" >4.7922e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002D1" call="MPI_Irecv" bytes="320" orank="721" region="0" commid="0" count="197" tid="0" op="" dtype="" >6.7711e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.0426e+00 0.0000e+00 1.1631e-01</hent>
<hent key="03800100000000000000400000000051" call="MPI_Irecv" bytes="16384" orank="81" region="0" commid="0" count="12691" tid="0" op="" dtype="" >4.1044e-03 0.0000e+00 6.1989e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.7881e-05 0.0000e+00 1.7881e-05</hent>
<hent key="038001000000000000004000000002D1" call="MPI_Irecv" bytes="16384" orank="721" region="0" commid="0" count="12690" tid="0" op="" dtype="" >5.8730e-03 0.0000e+00 8.1062e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000014000000009" call="MPI_Isend" bytes="320" orank="9" region="0" commid="0" count="359" tid="0" op="" dtype="" >4.1699e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000014000000010" call="MPI_Isend" bytes="320" orank="16" region="0" commid="0" count="384" tid="0" op="" dtype="" >4.3440e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000012" call="MPI_Isend" bytes="320" orank="18" region="0" commid="0" count="359" tid="0" op="" dtype="" >8.9931e-04 9.5367e-07 4.0531e-06</hent>
<hent key="02400100000000000000014000000019" call="MPI_Isend" bytes="320" orank="25" region="0" commid="0" count="349" tid="0" op="" dtype="" >1.5950e-03 3.8147e-06 9.0599e-06</hent>
<hent key="02400100000000000000014000000051" call="MPI_Isend" bytes="320" orank="81" region="0" commid="0" count="162" tid="0" op="" dtype="" >7.0333e-04 2.8610e-06 9.0599e-06</hent>
<hent key="03800100000000000000200000000009" call="MPI_Irecv" bytes="8192" orank="9" region="0" commid="0" count="12691" tid="0" op="" dtype="" >3.4134e-03 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000200000000010" call="MPI_Irecv" bytes="8192" orank="16" region="0" commid="0" count="3654" tid="0" op="" dtype="" >1.0552e-03 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000200000000012" call="MPI_Irecv" bytes="8192" orank="18" region="0" commid="0" count="4049" tid="0" op="" dtype="" >6.9761e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000200000000019" call="MPI_Irecv" bytes="8192" orank="25" region="0" commid="0" count="12683" tid="0" op="" dtype="" >1.2450e-03 0.0000e+00 4.0531e-06</hent>
<hent key="024001000000000000000140000002D1" call="MPI_Isend" bytes="320" orank="721" region="0" commid="0" count="174" tid="0" op="" dtype="" >7.0810e-04 2.8610e-06 9.0599e-06</hent>
<hent key="03800100000000000000000000000009" call="MPI_Irecv" bytes="0" orank="9" region="0" commid="0" count="242" tid="0" op="" dtype="" >5.2452e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000010" call="MPI_Irecv" bytes="0" orank="16" region="0" commid="0" count="272" tid="0" op="" dtype="" >9.6798e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000000000000012" call="MPI_Irecv" bytes="0" orank="18" region="0" commid="0" count="262" tid="0" op="" dtype="" >6.8188e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000019" call="MPI_Irecv" bytes="0" orank="25" region="0" commid="0" count="240" tid="0" op="" dtype="" >5.6982e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000051" call="MPI_Irecv" bytes="0" orank="81" region="0" commid="0" count="168" tid="0" op="" dtype="" >5.9366e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000051" call="MPI_Isend" bytes="16384" orank="81" region="0" commid="0" count="12699" tid="0" op="" dtype="" >8.0525e-02 3.8147e-06 2.7895e-05</hent>
<hent key="038001000000000000000000000002D1" call="MPI_Irecv" bytes="0" orank="721" region="0" commid="0" count="138" tid="0" op="" dtype="" >3.6240e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002D1" call="MPI_Isend" bytes="16384" orank="721" region="0" commid="0" count="12699" tid="0" op="" dtype="" >9.5295e-02 2.8610e-06 2.6941e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.6127e-04 0.0000e+00 5.9390e-04</hent>
<hent key="02400100000000000000200000000009" call="MPI_Isend" bytes="8192" orank="9" region="0" commid="0" count="12626" tid="0" op="" dtype="" >7.3655e-03 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000200000000010" call="MPI_Isend" bytes="8192" orank="16" region="0" commid="0" count="4074" tid="0" op="" dtype="" >5.0280e-03 0.0000e+00 9.7752e-06</hent>
<hent key="02400100000000000000200000000012" call="MPI_Isend" bytes="8192" orank="18" region="0" commid="0" count="3358" tid="0" op="" dtype="" >6.3446e-03 0.0000e+00 6.1989e-05</hent>
<hent key="02400100000000000000200000000019" call="MPI_Isend" bytes="8192" orank="25" region="0" commid="0" count="12691" tid="0" op="" dtype="" >1.0738e-01 3.8147e-06 2.7895e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.6941e-05 2.6941e-05 2.6941e-05</hent>
<hent key="02400100000000000000000000000009" call="MPI_Isend" bytes="0" orank="9" region="0" commid="0" count="254" tid="0" op="" dtype="" >2.2578e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000000000000010" call="MPI_Isend" bytes="0" orank="16" region="0" commid="0" count="234" tid="0" op="" dtype="" >1.7166e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000012" call="MPI_Isend" bytes="0" orank="18" region="0" commid="0" count="251" tid="0" op="" dtype="" >5.0068e-04 9.5367e-07 4.9114e-05</hent>
<hent key="02400100000000000000000000000019" call="MPI_Isend" bytes="0" orank="25" region="0" commid="0" count="269" tid="0" op="" dtype="" >1.0388e-03 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000000000000051" call="MPI_Isend" bytes="0" orank="81" region="0" commid="0" count="153" tid="0" op="" dtype="" >5.8174e-04 9.5367e-07 5.0068e-06</hent>
<hent key="024001000000000000000000000002D1" call="MPI_Isend" bytes="0" orank="721" region="0" commid="0" count="145" tid="0" op="" dtype="" >5.4026e-04 9.5367e-07 7.8678e-06</hent>
<hent key="03800100000000000000060000000009" call="MPI_Irecv" bytes="1536" orank="9" region="0" commid="0" count="99" tid="0" op="" dtype="" >2.8133e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000010" call="MPI_Irecv" bytes="1536" orank="16" region="0" commid="0" count="108" tid="0" op="" dtype="" >3.6478e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000012" call="MPI_Irecv" bytes="1536" orank="18" region="0" commid="0" count="98" tid="0" op="" dtype="" >3.3379e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000060000000019" call="MPI_Irecv" bytes="1536" orank="25" region="0" commid="0" count="110" tid="0" op="" dtype="" >2.9325e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000051" call="MPI_Irecv" bytes="1536" orank="81" region="0" commid="0" count="224" tid="0" op="" dtype="" >1.0705e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002D1" call="MPI_Irecv" bytes="1536" orank="721" region="0" commid="0" count="227" tid="0" op="" dtype="" >7.5817e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000009" call="MPI_Isend" bytes="1536" orank="9" region="0" commid="0" count="94" tid="0" op="" dtype="" >2.0146e-04 9.5367e-07 1.3828e-05</hent>
<hent key="02400100000000000000060000000010" call="MPI_Isend" bytes="1536" orank="16" region="0" commid="0" count="105" tid="0" op="" dtype="" >2.2054e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000012" call="MPI_Isend" bytes="1536" orank="18" region="0" commid="0" count="91" tid="0" op="" dtype="" >3.3116e-04 2.8610e-06 9.0599e-06</hent>
<hent key="02400100000000000000060000000019" call="MPI_Isend" bytes="1536" orank="25" region="0" commid="0" count="100" tid="0" op="" dtype="" >5.1284e-04 3.8147e-06 6.1989e-06</hent>
<hent key="02400100000000000000060000000051" call="MPI_Isend" bytes="1536" orank="81" region="0" commid="0" count="223" tid="0" op="" dtype="" >1.0529e-03 3.8147e-06 9.0599e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002D1" call="MPI_Isend" bytes="1536" orank="721" region="0" commid="0" count="205" tid="0" op="" dtype="" >9.1147e-04 2.8610e-06 8.1062e-06</hent>
<hent key="038001000000000000000C0000000051" call="MPI_Irecv" bytes="3072" orank="81" region="0" commid="0" count="4" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002D1" call="MPI_Irecv" bytes="3072" orank="721" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C0000000009" call="MPI_Isend" bytes="3072" orank="9" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.3828e-05 1.3828e-05 1.3828e-05</hent>
<hent key="024001000000000000000C0000000019" call="MPI_Isend" bytes="3072" orank="25" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="024001000000000000000C0000000051" call="MPI_Isend" bytes="3072" orank="81" region="0" commid="0" count="10" tid="0" op="" dtype="" >5.2929e-05 4.7684e-06 6.1989e-06</hent>
<hent key="024001000000000000000C00000002D1" call="MPI_Isend" bytes="3072" orank="721" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.5034e-05 4.0531e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.7289e-04 3.7289e-04 3.7289e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >8.3470e-04 2.7680e-04 2.7895e-04</hent>
<hent key="03800100000000000000038000000009" call="MPI_Irecv" bytes="896" orank="9" region="0" commid="0" count="349" tid="0" op="" dtype="" >9.0599e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000010" call="MPI_Irecv" bytes="896" orank="16" region="0" commid="0" count="2778" tid="0" op="" dtype="" >4.0960e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000012" call="MPI_Irecv" bytes="896" orank="18" region="0" commid="0" count="2663" tid="0" op="" dtype="" >5.9032e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000038000000019" call="MPI_Irecv" bytes="896" orank="25" region="0" commid="0" count="316" tid="0" op="" dtype="" >7.0095e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000051" call="MPI_Irecv" bytes="896" orank="81" region="0" commid="0" count="291" tid="0" op="" dtype="" >1.2350e-04 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.5034e-05 2.5034e-05 2.5034e-05</hent>
<hent key="038001000000000000000380000002D1" call="MPI_Irecv" bytes="896" orank="721" region="0" commid="0" count="320" tid="0" op="" dtype="" >9.8467e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="03800100000000000000380000000051" call="MPI_Irecv" bytes="14336" orank="81" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000003800000002D1" call="MPI_Irecv" bytes="14336" orank="721" region="0" commid="0" count="9" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000038000000009" call="MPI_Isend" bytes="896" orank="9" region="0" commid="0" count="315" tid="0" op="" dtype="" >5.2214e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000038000000010" call="MPI_Isend" bytes="896" orank="16" region="0" commid="0" count="2648" tid="0" op="" dtype="" >1.9448e-03 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000038000000012" call="MPI_Isend" bytes="896" orank="18" region="0" commid="0" count="2801" tid="0" op="" dtype="" >2.8093e-03 0.0000e+00 1.2159e-05</hent>
<hent key="02400100000000000000038000000019" call="MPI_Isend" bytes="896" orank="25" region="0" commid="0" count="339" tid="0" op="" dtype="" >1.6706e-03 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000038000000051" call="MPI_Isend" bytes="896" orank="81" region="0" commid="0" count="345" tid="0" op="" dtype="" >1.5643e-03 3.0994e-06 1.5974e-05</hent>
<hent key="024001000000000000000380000002D1" call="MPI_Isend" bytes="896" orank="721" region="0" commid="0" count="333" tid="0" op="" dtype="" >1.4281e-03 2.8610e-06 9.0599e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.1839e+00 8.8215e-06 1.3921e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.2810e-04 5.2810e-04 5.2810e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >2.7084e-04 2.7084e-04 2.7084e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >1.5719e-03 1.5719e-03 1.5719e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >7.2707e-02 3.3617e-04 6.7582e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.8103e-04 5.8103e-04 5.8103e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6108e+00 4.5109e-04 2.5617e-01</hent>
<hent key="03800100000000000000040000000009" call="MPI_Irecv" bytes="1024" orank="9" region="0" commid="0" count="3396" tid="0" op="" dtype="" >4.3869e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000010" call="MPI_Irecv" bytes="1024" orank="16" region="0" commid="0" count="956" tid="0" op="" dtype="" >1.0896e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000012" call="MPI_Irecv" bytes="1024" orank="18" region="0" commid="0" count="1068" tid="0" op="" dtype="" >2.2864e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000040000000019" call="MPI_Irecv" bytes="1024" orank="25" region="0" commid="0" count="3394" tid="0" op="" dtype="" >5.0735e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.3842e-05 0.0000e+00 1.0967e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.3113e-05 1.3113e-05 1.3113e-05</hent>
<hent key="03800100000000000000070000000009" call="MPI_Irecv" bytes="1792" orank="9" region="0" commid="0" count="34" tid="0" op="" dtype="" >1.0252e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000010" call="MPI_Irecv" bytes="1792" orank="16" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.3113e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000012" call="MPI_Irecv" bytes="1792" orank="18" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.5974e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000019" call="MPI_Irecv" bytes="1792" orank="25" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000051" call="MPI_Irecv" bytes="1792" orank="81" region="0" commid="0" count="163" tid="0" op="" dtype="" >8.7023e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002D1" call="MPI_Irecv" bytes="1792" orank="721" region="0" commid="0" count="138" tid="0" op="" dtype="" >2.8610e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >1.0991e-04 0.0000e+00 5.1022e-05</hent>
<hent key="02400100000000000000040000000009" call="MPI_Isend" bytes="1024" orank="9" region="0" commid="0" count="3380" tid="0" op="" dtype="" >1.9979e-03 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000040000000010" call="MPI_Isend" bytes="1024" orank="16" region="0" commid="0" count="1096" tid="0" op="" dtype="" >7.1383e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000040000000012" call="MPI_Isend" bytes="1024" orank="18" region="0" commid="0" count="920" tid="0" op="" dtype="" >6.8903e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000040000000019" call="MPI_Isend" bytes="1024" orank="25" region="0" commid="0" count="3396" tid="0" op="" dtype="" >8.6818e-03 9.5367e-07 1.6928e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >5.0068e-06 1.9073e-06 3.0994e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4406e+00 0.0000e+00 3.2514e+00</hent>
<hent key="038001000000000000000A0000000009" call="MPI_Irecv" bytes="2560" orank="9" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000010" call="MPI_Irecv" bytes="2560" orank="16" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000012" call="MPI_Irecv" bytes="2560" orank="18" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000019" call="MPI_Irecv" bytes="2560" orank="25" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000051" call="MPI_Irecv" bytes="2560" orank="81" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.5272e-05 0.0000e+00 6.1989e-06</hent>
<hent key="038001000000000000000A00000002D1" call="MPI_Irecv" bytes="2560" orank="721" region="0" commid="0" count="48" tid="0" op="" dtype="" >1.3828e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000009" call="MPI_Isend" bytes="1792" orank="9" region="0" commid="0" count="44" tid="0" op="" dtype="" >8.9884e-05 9.5367e-07 1.0014e-05</hent>
<hent key="02400100000000000000070000000010" call="MPI_Isend" bytes="1792" orank="16" region="0" commid="0" count="37" tid="0" op="" dtype="" >8.4162e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000012" call="MPI_Isend" bytes="1792" orank="18" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.5903e-04 2.8610e-06 7.8678e-06</hent>
<hent key="02400100000000000000070000000019" call="MPI_Isend" bytes="1792" orank="25" region="0" commid="0" count="36" tid="0" op="" dtype="" >1.9121e-04 4.7684e-06 6.1989e-06</hent>
<hent key="02400100000000000000070000000051" call="MPI_Isend" bytes="1792" orank="81" region="0" commid="0" count="139" tid="0" op="" dtype="" >6.7115e-04 3.8147e-06 6.1989e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6141e-02 3.6141e-02 3.6141e-02</hent>
<hent key="024001000000000000000700000002D1" call="MPI_Isend" bytes="1792" orank="721" region="0" commid="0" count="147" tid="0" op="" dtype="" >6.7616e-04 3.8147e-06 9.0599e-06</hent>
<hent key="024001000000000000000A0000000009" call="MPI_Isend" bytes="2560" orank="9" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.2159e-05 2.1458e-06 1.0014e-05</hent>
<hent key="024001000000000000000A0000000010" call="MPI_Isend" bytes="2560" orank="16" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.9073e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000012" call="MPI_Isend" bytes="2560" orank="18" region="0" commid="0" count="1" tid="0" op="" dtype="" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="024001000000000000000A0000000019" call="MPI_Isend" bytes="2560" orank="25" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.1948e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000051" call="MPI_Isend" bytes="2560" orank="81" region="0" commid="0" count="56" tid="0" op="" dtype="" >2.8896e-04 3.8147e-06 6.1989e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.0197e-04 5.1022e-05 1.9503e-04</hent>
<hent key="024001000000000000000A00000002D1" call="MPI_Isend" bytes="2560" orank="721" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.1815e-04 3.8147e-06 6.1989e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.4138e-03 5.2786e-04 8.8596e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.4094e-05 3.4094e-05 3.4094e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 2.8610e-06 3.0994e-06</hent>
<hent key="03800100000000000000000400000009" call="MPI_Irecv" bytes="4" orank="9" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.9295e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000010" call="MPI_Irecv" bytes="4" orank="16" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.4754e-04 0.0000e+00 3.8147e-06</hent>
<hent key="03800100000000000000000400000012" call="MPI_Irecv" bytes="4" orank="18" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.0793e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000000400000019" call="MPI_Irecv" bytes="4" orank="25" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.0150e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000000400000051" call="MPI_Irecv" bytes="4" orank="81" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.9908e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.7080e-03 3.5596e-04 9.3794e-04</hent>
<hent key="038001000000000000001C0000000010" call="MPI_Irecv" bytes="7168" orank="16" region="0" commid="0" count="9045" tid="0" op="" dtype="" >2.6386e-03 0.0000e+00 3.0994e-06</hent>
<hent key="038001000000000000001C0000000012" call="MPI_Irecv" bytes="7168" orank="18" region="0" commid="0" count="8650" tid="0" op="" dtype="" >1.5132e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000004000002D1" call="MPI_Irecv" bytes="4" orank="721" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.1822e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000400000009" call="MPI_Isend" bytes="4" orank="9" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3912e-03 0.0000e+00 2.9087e-05</hent>
<hent key="02400100000000000000000400000010" call="MPI_Isend" bytes="4" orank="16" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1585e-03 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000400000012" call="MPI_Isend" bytes="4" orank="18" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.5501e-03 0.0000e+00 5.7936e-05</hent>
<hent key="02400100000000000000000400000019" call="MPI_Isend" bytes="4" orank="25" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.2713e-02 3.8147e-06 1.9073e-05</hent>
<hent key="02400100000000000000000400000051" call="MPI_Isend" bytes="4" orank="81" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.9612e-02 3.8147e-06 1.1381e-02</hent>
<hent key="024001000000000000001C0000000010" call="MPI_Isend" bytes="7168" orank="16" region="0" commid="0" count="8625" tid="0" op="" dtype="" >1.8905e-02 9.5367e-07 2.8849e-05</hent>
<hent key="024001000000000000001C0000000012" call="MPI_Isend" bytes="7168" orank="18" region="0" commid="0" count="9341" tid="0" op="" dtype="" >2.8871e-02 9.5367e-07 1.4067e-05</hent>
<hent key="024001000000000000000004000002D1" call="MPI_Isend" bytes="4" orank="721" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7884e-02 2.8610e-06 1.0896e-04</hent>
<hent key="03800100000000000000050000000009" call="MPI_Irecv" bytes="1280" orank="9" region="0" commid="0" count="178" tid="0" op="" dtype="" >3.7670e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000010" call="MPI_Irecv" bytes="1280" orank="16" region="0" commid="0" count="212" tid="0" op="" dtype="" >9.0122e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000012" call="MPI_Irecv" bytes="1280" orank="18" region="0" commid="0" count="191" tid="0" op="" dtype="" >7.2956e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000019" call="MPI_Irecv" bytes="1280" orank="25" region="0" commid="0" count="194" tid="0" op="" dtype="" >4.3392e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000051" call="MPI_Irecv" bytes="1280" orank="81" region="0" commid="0" count="314" tid="0" op="" dtype="" >1.5163e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.1010e+01 1.2875e-05 1.3910e-01</hent>
<hent key="03800100000000000000280000000009" call="MPI_Irecv" bytes="10240" orank="9" region="0" commid="0" count="8" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000280000000019" call="MPI_Irecv" bytes="10240" orank="25" region="0" commid="0" count="16" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000500000002D1" call="MPI_Irecv" bytes="1280" orank="721" region="0" commid="0" count="268" tid="0" op="" dtype="" >7.5340e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000009" call="MPI_Irecv" bytes="2048" orank="9" region="0" commid="0" count="12" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000010" call="MPI_Irecv" bytes="2048" orank="16" region="0" commid="0" count="11" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000012" call="MPI_Irecv" bytes="2048" orank="18" region="0" commid="0" count="15" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000019" call="MPI_Irecv" bytes="2048" orank="25" region="0" commid="0" count="19" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000051" call="MPI_Irecv" bytes="2048" orank="81" region="0" commid="0" count="3461" tid="0" op="" dtype="" >7.1907e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002D1" call="MPI_Irecv" bytes="2048" orank="721" region="0" commid="0" count="3464" tid="0" op="" dtype="" >5.3310e-04 0.0000e+00 2.8610e-06</hent>
<hent key="02400100000000000000050000000009" call="MPI_Isend" bytes="1280" orank="9" region="0" commid="0" count="201" tid="0" op="" dtype="" >3.5548e-04 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000050000000010" call="MPI_Isend" bytes="1280" orank="16" region="0" commid="0" count="195" tid="0" op="" dtype="" >3.8123e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000050000000012" call="MPI_Isend" bytes="1280" orank="18" region="0" commid="0" count="183" tid="0" op="" dtype="" >6.3658e-04 1.9073e-06 1.0014e-05</hent>
<hent key="02400100000000000000050000000019" call="MPI_Isend" bytes="1280" orank="25" region="0" commid="0" count="205" tid="0" op="" dtype="" >1.0285e-03 2.1458e-06 6.1989e-06</hent>
<hent key="02400100000000000000050000000051" call="MPI_Isend" bytes="1280" orank="81" region="0" commid="0" count="288" tid="0" op="" dtype="" >1.3275e-03 3.8147e-06 1.1206e-05</hent>
<hent key="02400100000000000000280000000009" call="MPI_Isend" bytes="10240" orank="9" region="0" commid="0" count="73" tid="0" op="" dtype="" >3.1471e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000280000000019" call="MPI_Isend" bytes="10240" orank="25" region="0" commid="0" count="8" tid="0" op="" dtype="" >6.6042e-05 6.9141e-06 1.2159e-05</hent>
<hent key="024001000000000000000500000002D1" call="MPI_Isend" bytes="1280" orank="721" region="0" commid="0" count="285" tid="0" op="" dtype="" >1.2295e-03 2.8610e-06 1.5974e-05</hent>
</hash>
<internal rank="17" log_i="1723713849.510341" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="18" mpi_size="768" stamp_init="1723713791.126839" stamp_final="1723713849.504534" username="apac4" allocationname="unknown" flags="0" pid="1717081" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.83777e+01" utime="4.87114e+01" stime="7.20200e+00" mtime="3.20560e+01" gflop="0.00000e+00" gbyte="3.78048e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20560e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000052144d14c3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83128e+01" utime="4.86795e+01" stime="7.19469e+00" mtime="3.20560e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20560e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4894e+08" > 5.8340e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4987e+08" > 4.2157e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1948e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4390e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3855e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1683e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6104e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.0008e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8682e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6156e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9144e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="194" >
<hent key="0240010000000000000008000000000A" call="MPI_Isend" bytes="2048" orank="10" region="0" commid="0" count="13" tid="0" op="" dtype="" >4.5300e-05 9.5367e-07 1.3113e-05</hent>
<hent key="02400100000000000000080000000011" call="MPI_Isend" bytes="2048" orank="17" region="0" commid="0" count="15" tid="0" op="" dtype="" >3.3855e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000013" call="MPI_Isend" bytes="2048" orank="19" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.1253e-04 3.0994e-06 2.0027e-05</hent>
<hent key="0240010000000000000008000000001A" call="MPI_Isend" bytes="2048" orank="26" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.0293e-05 4.0531e-06 1.0014e-05</hent>
<hent key="02400100000000000000080000000052" call="MPI_Isend" bytes="2048" orank="82" region="0" commid="0" count="3443" tid="0" op="" dtype="" >1.2303e-02 9.5367e-07 2.8133e-05</hent>
<hent key="024001000000000000000800000002D2" call="MPI_Isend" bytes="2048" orank="722" region="0" commid="0" count="3464" tid="0" op="" dtype="" >1.1324e-02 9.5367e-07 7.4863e-05</hent>
<hent key="038001000000000000000E0000000052" call="MPI_Irecv" bytes="3584" orank="82" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000E00000002D2" call="MPI_Irecv" bytes="3584" orank="722" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000E0000000052" call="MPI_Isend" bytes="3584" orank="82" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0380010000000000000002800000000A" call="MPI_Irecv" bytes="640" orank="10" region="0" commid="0" count="392" tid="0" op="" dtype="" >2.3222e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000028000000011" call="MPI_Irecv" bytes="640" orank="17" region="0" commid="0" count="438" tid="0" op="" dtype="" >1.6165e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000028000000013" call="MPI_Irecv" bytes="640" orank="19" region="0" commid="0" count="407" tid="0" op="" dtype="" >8.7738e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000001A" call="MPI_Irecv" bytes="640" orank="26" region="0" commid="0" count="400" tid="0" op="" dtype="" >9.2745e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000052" call="MPI_Irecv" bytes="640" orank="82" region="0" commid="0" count="300" tid="0" op="" dtype="" >1.1635e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000000280000002D2" call="MPI_Irecv" bytes="640" orank="722" region="0" commid="0" count="296" tid="0" op="" dtype="" >1.0085e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000002800000000A" call="MPI_Isend" bytes="640" orank="10" region="0" commid="0" count="424" tid="0" op="" dtype="" >6.4564e-04 0.0000e+00 5.3883e-05</hent>
<hent key="02400100000000000000028000000011" call="MPI_Isend" bytes="640" orank="17" region="0" commid="0" count="393" tid="0" op="" dtype="" >5.3167e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000028000000013" call="MPI_Isend" bytes="640" orank="19" region="0" commid="0" count="411" tid="0" op="" dtype="" >1.2200e-03 1.9073e-06 7.8678e-06</hent>
<hent key="0240010000000000000002800000001A" call="MPI_Isend" bytes="640" orank="26" region="0" commid="0" count="415" tid="0" op="" dtype="" >1.9171e-03 3.8147e-06 1.0967e-05</hent>
<hent key="02400100000000000000028000000052" call="MPI_Isend" bytes="640" orank="82" region="0" commid="0" count="299" tid="0" op="" dtype="" >1.6665e-03 3.0994e-06 1.3828e-05</hent>
<hent key="024001000000000000000280000002D2" call="MPI_Isend" bytes="640" orank="722" region="0" commid="0" count="283" tid="0" op="" dtype="" >1.2462e-03 2.8610e-06 1.3113e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="0380010000000000000001400000000A" call="MPI_Irecv" bytes="320" orank="10" region="0" commid="0" count="385" tid="0" op="" dtype="" >2.1696e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000014000000011" call="MPI_Irecv" bytes="320" orank="17" region="0" commid="0" count="359" tid="0" op="" dtype="" >1.3328e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000013" call="MPI_Irecv" bytes="320" orank="19" region="0" commid="0" count="383" tid="0" op="" dtype="" >1.3638e-04 0.0000e+00 5.4836e-05</hent>
<hent key="0380010000000000000001400000001A" call="MPI_Irecv" bytes="320" orank="26" region="0" commid="0" count="366" tid="0" op="" dtype="" >1.0252e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000014000000052" call="MPI_Irecv" bytes="320" orank="82" region="0" commid="0" count="175" tid="0" op="" dtype="" >7.1526e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002D2" call="MPI_Irecv" bytes="320" orank="722" region="0" commid="0" count="175" tid="0" op="" dtype="" >7.3671e-05 0.0000e+00 6.1989e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.1948e+00 0.0000e+00 1.1640e-01</hent>
<hent key="03800100000000000000400000000052" call="MPI_Irecv" bytes="16384" orank="82" region="0" commid="0" count="12651" tid="0" op="" dtype="" >3.8431e-03 0.0000e+00 1.8120e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.0027e-05 9.5367e-07 1.7881e-05</hent>
<hent key="038001000000000000004000000002D2" call="MPI_Irecv" bytes="16384" orank="722" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.0503e-02 0.0000e+00 3.6955e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >7.9870e-05 0.0000e+00 7.8917e-05</hent>
<hent key="0240010000000000000001400000000A" call="MPI_Isend" bytes="320" orank="10" region="0" commid="0" count="345" tid="0" op="" dtype="" >3.8958e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000014000000011" call="MPI_Isend" bytes="320" orank="17" region="0" commid="0" count="358" tid="0" op="" dtype="" >3.7766e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000014000000013" call="MPI_Isend" bytes="320" orank="19" region="0" commid="0" count="353" tid="0" op="" dtype="" >9.4008e-04 1.9073e-06 5.9605e-06</hent>
<hent key="0240010000000000000001400000001A" call="MPI_Isend" bytes="320" orank="26" region="0" commid="0" count="363" tid="0" op="" dtype="" >1.6181e-03 2.8610e-06 1.0014e-05</hent>
<hent key="02400100000000000000014000000052" call="MPI_Isend" bytes="320" orank="82" region="0" commid="0" count="188" tid="0" op="" dtype="" >1.0097e-03 2.8610e-06 1.2159e-05</hent>
<hent key="0380010000000000000020000000000A" call="MPI_Irecv" bytes="8192" orank="10" region="0" commid="0" count="12647" tid="0" op="" dtype="" >5.8000e-03 0.0000e+00 4.1008e-05</hent>
<hent key="03800100000000000000200000000011" call="MPI_Irecv" bytes="8192" orank="17" region="0" commid="0" count="3358" tid="0" op="" dtype="" >1.2929e-03 0.0000e+00 1.1921e-05</hent>
<hent key="03800100000000000000200000000013" call="MPI_Irecv" bytes="8192" orank="19" region="0" commid="0" count="4625" tid="0" op="" dtype="" >1.0376e-03 0.0000e+00 1.0967e-05</hent>
<hent key="0380010000000000000020000000001A" call="MPI_Irecv" bytes="8192" orank="26" region="0" commid="0" count="12667" tid="0" op="" dtype="" >2.0697e-03 0.0000e+00 1.7166e-05</hent>
<hent key="024001000000000000000140000002D2" call="MPI_Isend" bytes="320" orank="722" region="0" commid="0" count="202" tid="0" op="" dtype="" >9.0528e-04 2.8610e-06 1.5974e-05</hent>
<hent key="0380010000000000000000000000000A" call="MPI_Irecv" bytes="0" orank="10" region="0" commid="0" count="242" tid="0" op="" dtype="" >1.0180e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000000000011" call="MPI_Irecv" bytes="0" orank="17" region="0" commid="0" count="251" tid="0" op="" dtype="" >8.6308e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000013" call="MPI_Irecv" bytes="0" orank="19" region="0" commid="0" count="282" tid="0" op="" dtype="" >7.1526e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000000000000001A" call="MPI_Irecv" bytes="0" orank="26" region="0" commid="0" count="267" tid="0" op="" dtype="" >6.3419e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000052" call="MPI_Irecv" bytes="0" orank="82" region="0" commid="0" count="162" tid="0" op="" dtype="" >2.5988e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000052" call="MPI_Isend" bytes="16384" orank="82" region="0" commid="0" count="12683" tid="0" op="" dtype="" >1.1464e-01 3.8147e-06 6.6042e-05</hent>
<hent key="038001000000000000000000000002D2" call="MPI_Irecv" bytes="0" orank="722" region="0" commid="0" count="157" tid="0" op="" dtype="" >4.4584e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002D2" call="MPI_Isend" bytes="16384" orank="722" region="0" commid="0" count="12692" tid="0" op="" dtype="" >1.0733e-01 2.8610e-06 1.0085e-04</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.3409e-04 0.0000e+00 5.6791e-04</hent>
<hent key="0240010000000000000020000000000A" call="MPI_Isend" bytes="8192" orank="10" region="0" commid="0" count="12699" tid="0" op="" dtype="" >8.7328e-03 0.0000e+00 3.1948e-05</hent>
<hent key="02400100000000000000200000000011" call="MPI_Isend" bytes="8192" orank="17" region="0" commid="0" count="4049" tid="0" op="" dtype="" >4.8831e-03 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000200000000013" call="MPI_Isend" bytes="8192" orank="19" region="0" commid="0" count="3047" tid="0" op="" dtype="" >5.5997e-03 0.0000e+00 7.8917e-05</hent>
<hent key="0240010000000000000020000000001A" call="MPI_Isend" bytes="8192" orank="26" region="0" commid="0" count="12631" tid="0" op="" dtype="" >1.5145e-01 3.0994e-06 1.0896e-04</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.6941e-05 2.6941e-05 2.6941e-05</hent>
<hent key="0240010000000000000000000000000A" call="MPI_Isend" bytes="0" orank="10" region="0" commid="0" count="264" tid="0" op="" dtype="" >2.0671e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000000000000011" call="MPI_Isend" bytes="0" orank="17" region="0" commid="0" count="262" tid="0" op="" dtype="" >1.5521e-04 0.0000e+00 3.8147e-06</hent>
<hent key="02400100000000000000000000000013" call="MPI_Isend" bytes="0" orank="19" region="0" commid="0" count="260" tid="0" op="" dtype="" >5.0330e-04 9.5367e-07 2.8133e-05</hent>
<hent key="0240010000000000000000000000001A" call="MPI_Isend" bytes="0" orank="26" region="0" commid="0" count="281" tid="0" op="" dtype="" >9.9373e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000000000000052" call="MPI_Isend" bytes="0" orank="82" region="0" commid="0" count="150" tid="0" op="" dtype="" >6.6018e-04 9.5367e-07 1.0967e-05</hent>
<hent key="024001000000000000000000000002D2" call="MPI_Isend" bytes="0" orank="722" region="0" commid="0" count="148" tid="0" op="" dtype="" >5.4836e-04 9.5367e-07 1.3113e-05</hent>
<hent key="0380010000000000000006000000000A" call="MPI_Irecv" bytes="1536" orank="10" region="0" commid="0" count="100" tid="0" op="" dtype="" >5.1975e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000060000000011" call="MPI_Irecv" bytes="1536" orank="17" region="0" commid="0" count="91" tid="0" op="" dtype="" >4.0770e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000013" call="MPI_Irecv" bytes="1536" orank="19" region="0" commid="0" count="65" tid="0" op="" dtype="" >1.4544e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000001A" call="MPI_Irecv" bytes="1536" orank="26" region="0" commid="0" count="98" tid="0" op="" dtype="" >2.5749e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000052" call="MPI_Irecv" bytes="1536" orank="82" region="0" commid="0" count="220" tid="0" op="" dtype="" >7.9632e-05 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000600000002D2" call="MPI_Irecv" bytes="1536" orank="722" region="0" commid="0" count="209" tid="0" op="" dtype="" >6.5088e-05 0.0000e+00 7.1526e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0240010000000000000006000000000A" call="MPI_Isend" bytes="1536" orank="10" region="0" commid="0" count="115" tid="0" op="" dtype="" >2.7013e-04 9.5367e-07 2.2888e-05</hent>
<hent key="02400100000000000000060000000011" call="MPI_Isend" bytes="1536" orank="17" region="0" commid="0" count="98" tid="0" op="" dtype="" >1.8930e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000013" call="MPI_Isend" bytes="1536" orank="19" region="0" commid="0" count="94" tid="0" op="" dtype="" >3.5405e-04 2.8610e-06 8.8215e-06</hent>
<hent key="0240010000000000000006000000001A" call="MPI_Isend" bytes="1536" orank="26" region="0" commid="0" count="90" tid="0" op="" dtype="" >4.8280e-04 3.8147e-06 2.6941e-05</hent>
<hent key="02400100000000000000060000000052" call="MPI_Isend" bytes="1536" orank="82" region="0" commid="0" count="236" tid="0" op="" dtype="" >1.4822e-03 3.8147e-06 6.1989e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002D2" call="MPI_Isend" bytes="1536" orank="722" region="0" commid="0" count="204" tid="0" op="" dtype="" >1.0343e-03 3.8147e-06 1.3113e-05</hent>
<hent key="038001000000000000000C0000000052" call="MPI_Irecv" bytes="3072" orank="82" region="0" commid="0" count="5" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002D2" call="MPI_Irecv" bytes="3072" orank="722" region="0" commid="0" count="6" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000C0000000052" call="MPI_Isend" bytes="3072" orank="82" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="024001000000000000000C00000002D2" call="MPI_Isend" bytes="3072" orank="722" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.3617e-05 3.8147e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.8815e-04 3.8815e-04 3.8815e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >8.7094e-04 2.8682e-04 2.9302e-04</hent>
<hent key="0380010000000000000003800000000A" call="MPI_Irecv" bytes="896" orank="10" region="0" commid="0" count="324" tid="0" op="" dtype="" >2.1482e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000038000000011" call="MPI_Irecv" bytes="896" orank="17" region="0" commid="0" count="2801" tid="0" op="" dtype="" >4.5037e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000013" call="MPI_Irecv" bytes="896" orank="19" region="0" commid="0" count="2469" tid="0" op="" dtype="" >5.3334e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000003800000001A" call="MPI_Irecv" bytes="896" orank="26" region="0" commid="0" count="301" tid="0" op="" dtype="" >7.9632e-05 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000038000000052" call="MPI_Irecv" bytes="896" orank="82" region="0" commid="0" count="323" tid="0" op="" dtype="" >1.1730e-04 0.0000e+00 7.8678e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.6015e-05 4.6015e-05 4.6015e-05</hent>
<hent key="038001000000000000000380000002D2" call="MPI_Irecv" bytes="896" orank="722" region="0" commid="0" count="318" tid="0" op="" dtype="" >1.1039e-04 0.0000e+00 4.0531e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000380000000052" call="MPI_Irecv" bytes="14336" orank="82" region="0" commid="0" count="48" tid="0" op="" dtype="" >2.6941e-05 0.0000e+00 1.2875e-05</hent>
<hent key="0240010000000000000003800000000A" call="MPI_Isend" bytes="896" orank="10" region="0" commid="0" count="294" tid="0" op="" dtype="" >4.5419e-04 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000038000000011" call="MPI_Isend" bytes="896" orank="17" region="0" commid="0" count="2663" tid="0" op="" dtype="" >1.9619e-03 0.0000e+00 1.2875e-05</hent>
<hent key="02400100000000000000038000000013" call="MPI_Isend" bytes="896" orank="19" region="0" commid="0" count="2894" tid="0" op="" dtype="" >2.9171e-03 0.0000e+00 2.4080e-05</hent>
<hent key="0240010000000000000003800000001A" call="MPI_Isend" bytes="896" orank="26" region="0" commid="0" count="336" tid="0" op="" dtype="" >1.6036e-03 3.8147e-06 2.0981e-05</hent>
<hent key="02400100000000000000038000000052" call="MPI_Isend" bytes="896" orank="82" region="0" commid="0" count="335" tid="0" op="" dtype="" >1.9476e-03 3.8147e-06 1.6212e-05</hent>
<hent key="024001000000000000000380000002D2" call="MPI_Isend" bytes="896" orank="722" region="0" commid="0" count="331" tid="0" op="" dtype="" >1.4954e-03 2.8610e-06 1.6928e-05</hent>
<hent key="02400100000000000000380000000052" call="MPI_Isend" bytes="14336" orank="82" region="0" commid="0" count="16" tid="0" op="" dtype="" >1.2517e-04 5.9605e-06 1.5020e-05</hent>
<hent key="024001000000000000003800000002D2" call="MPI_Isend" bytes="14336" orank="722" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.6240e-05 4.0531e-06 6.1989e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.0941e+00 1.9073e-05 1.3923e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.5194e-04 5.5194e-04 5.5194e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >2.8682e-04 2.8682e-04 2.8682e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >1.1868e-03 1.1868e-03 1.1868e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >8.0320e-02 3.2806e-04 7.3462e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.7316e-04 5.7316e-04 5.7316e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6104e+00 4.8089e-04 2.5610e-01</hent>
<hent key="0380010000000000000004000000000A" call="MPI_Irecv" bytes="1024" orank="10" region="0" commid="0" count="3380" tid="0" op="" dtype="" >4.7517e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000040000000011" call="MPI_Irecv" bytes="1024" orank="17" region="0" commid="0" count="920" tid="0" op="" dtype="" >1.1420e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000040000000013" call="MPI_Irecv" bytes="1024" orank="19" region="0" commid="0" count="1240" tid="0" op="" dtype="" >2.3913e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000001A" call="MPI_Irecv" bytes="1024" orank="26" region="0" commid="0" count="3390" tid="0" op="" dtype="" >4.9329e-04 0.0000e+00 1.9789e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >3.3855e-05 0.0000e+00 1.6928e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="0380010000000000000007000000000A" call="MPI_Irecv" bytes="1792" orank="10" region="0" commid="0" count="44" tid="0" op="" dtype="" >3.7670e-05 0.0000e+00 2.8610e-06</hent>
<hent key="03800100000000000000070000000011" call="MPI_Irecv" bytes="1792" orank="17" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000013" call="MPI_Irecv" bytes="1792" orank="19" region="0" commid="0" count="38" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000007000000001A" call="MPI_Irecv" bytes="1792" orank="26" region="0" commid="0" count="37" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000052" call="MPI_Irecv" bytes="1792" orank="82" region="0" commid="0" count="140" tid="0" op="" dtype="" >4.5300e-05 0.0000e+00 3.0994e-06</hent>
<hent key="038001000000000000000700000002D2" call="MPI_Irecv" bytes="1792" orank="722" region="0" commid="0" count="116" tid="0" op="" dtype="" >4.6968e-05 0.0000e+00 4.0531e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >1.2636e-04 0.0000e+00 6.1035e-05</hent>
<hent key="0240010000000000000004000000000A" call="MPI_Isend" bytes="1024" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.7554e-03 0.0000e+00 1.7881e-05</hent>
<hent key="02400100000000000000040000000011" call="MPI_Isend" bytes="1024" orank="17" region="0" commid="0" count="1068" tid="0" op="" dtype="" >6.7115e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000040000000013" call="MPI_Isend" bytes="1024" orank="19" region="0" commid="0" count="834" tid="0" op="" dtype="" >6.2823e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0240010000000000000004000000001A" call="MPI_Isend" bytes="1024" orank="26" region="0" commid="0" count="3380" tid="0" op="" dtype="" >9.2871e-03 9.5367e-07 3.1948e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >2.8610e-06 9.5367e-07 1.9073e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4380e+00 0.0000e+00 3.2485e+00</hent>
<hent key="038001000000000000000A000000000A" call="MPI_Irecv" bytes="2560" orank="10" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000011" call="MPI_Irecv" bytes="2560" orank="17" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000013" call="MPI_Irecv" bytes="2560" orank="19" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.5300e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000001A" call="MPI_Irecv" bytes="2560" orank="26" region="0" commid="0" count="3" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000A0000000052" call="MPI_Irecv" bytes="2560" orank="82" region="0" commid="0" count="29" tid="0" op="" dtype="" >9.7752e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002D2" call="MPI_Irecv" bytes="2560" orank="722" region="0" commid="0" count="43" tid="0" op="" dtype="" >1.0729e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000007000000000A" call="MPI_Isend" bytes="1792" orank="10" region="0" commid="0" count="28" tid="0" op="" dtype="" >7.1287e-05 9.5367e-07 1.2875e-05</hent>
<hent key="02400100000000000000070000000011" call="MPI_Isend" bytes="1792" orank="17" region="0" commid="0" count="47" tid="0" op="" dtype="" >9.5367e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000013" call="MPI_Isend" bytes="1792" orank="19" region="0" commid="0" count="27" tid="0" op="" dtype="" >9.9182e-05 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000007000000001A" call="MPI_Isend" bytes="1792" orank="26" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.2364e-04 3.8147e-06 1.0967e-05</hent>
<hent key="02400100000000000000070000000052" call="MPI_Isend" bytes="1792" orank="82" region="0" commid="0" count="108" tid="0" op="" dtype="" >6.5207e-04 9.5367e-07 1.7166e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6156e-02 3.6156e-02 3.6156e-02</hent>
<hent key="024001000000000000000700000002D2" call="MPI_Isend" bytes="1792" orank="722" region="0" commid="0" count="127" tid="0" op="" dtype="" >6.2585e-04 1.9073e-06 1.8120e-05</hent>
<hent key="024001000000000000000A000000000A" call="MPI_Isend" bytes="2560" orank="10" region="0" commid="0" count="7" tid="0" op="" dtype="" >5.3883e-05 1.9073e-06 2.2173e-05</hent>
<hent key="024001000000000000000A0000000011" call="MPI_Isend" bytes="2560" orank="17" region="0" commid="0" count="2" tid="0" op="" dtype="" >5.0068e-06 2.1458e-06 2.8610e-06</hent>
<hent key="024001000000000000000A0000000013" call="MPI_Isend" bytes="2560" orank="19" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.9312e-05 3.0994e-06 5.0068e-06</hent>
<hent key="024001000000000000000A000000001A" call="MPI_Isend" bytes="2560" orank="26" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.0027e-05 5.0068e-06 5.0068e-06</hent>
<hent key="024001000000000000000A0000000052" call="MPI_Isend" bytes="2560" orank="82" region="0" commid="0" count="33" tid="0" op="" dtype="" >2.1052e-04 4.7684e-06 1.1921e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.2081e-04 4.9829e-05 1.9813e-04</hent>
<hent key="024001000000000000000A00000002D2" call="MPI_Isend" bytes="2560" orank="722" region="0" commid="0" count="31" tid="0" op="" dtype="" >1.5402e-04 4.0531e-06 7.8678e-06</hent>
<hent key="03800100000000000000100000000052" call="MPI_Irecv" bytes="4096" orank="82" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.4539e-03 5.5289e-04 9.0098e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.5048e-05 3.5048e-05 3.5048e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 9.5367e-07 5.0068e-06</hent>
<hent key="0380010000000000000000040000000A" call="MPI_Irecv" bytes="4" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.9380e-04 0.0000e+00 1.0967e-05</hent>
<hent key="03800100000000000000000400000011" call="MPI_Irecv" bytes="4" orank="17" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1811e-03 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000013" call="MPI_Irecv" bytes="4" orank="19" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.3539e-04 0.0000e+00 1.1921e-05</hent>
<hent key="0380010000000000000000040000001A" call="MPI_Irecv" bytes="4" orank="26" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.3014e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000000400000052" call="MPI_Irecv" bytes="4" orank="82" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1151e-03 0.0000e+00 1.0014e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.8660e-03 3.6693e-04 1.0190e-03</hent>
<hent key="038001000000000000001C0000000011" call="MPI_Irecv" bytes="7168" orank="17" region="0" commid="0" count="9341" tid="0" op="" dtype="" >3.5181e-03 0.0000e+00 1.1921e-05</hent>
<hent key="038001000000000000001C0000000013" call="MPI_Irecv" bytes="7168" orank="19" region="0" commid="0" count="8074" tid="0" op="" dtype="" >1.6992e-03 0.0000e+00 2.4080e-05</hent>
<hent key="038001000000000000000004000002D2" call="MPI_Irecv" bytes="4" orank="722" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.7547e-04 0.0000e+00 1.5020e-05</hent>
<hent key="0240010000000000000000040000000A" call="MPI_Isend" bytes="4" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2202e-03 0.0000e+00 2.5988e-05</hent>
<hent key="02400100000000000000000400000011" call="MPI_Isend" bytes="4" orank="17" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4224e-03 0.0000e+00 4.1008e-05</hent>
<hent key="02400100000000000000000400000013" call="MPI_Isend" bytes="4" orank="19" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3835e-03 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000000040000001A" call="MPI_Isend" bytes="4" orank="26" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3237e-02 3.8147e-06 2.3127e-05</hent>
<hent key="02400100000000000000000400000052" call="MPI_Isend" bytes="4" orank="82" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.0200e-02 2.8610e-06 4.8161e-05</hent>
<hent key="024001000000000000001C0000000011" call="MPI_Isend" bytes="7168" orank="17" region="0" commid="0" count="8650" tid="0" op="" dtype="" >1.9544e-02 9.5367e-07 3.6001e-05</hent>
<hent key="024001000000000000001C0000000013" call="MPI_Isend" bytes="7168" orank="19" region="0" commid="0" count="9652" tid="0" op="" dtype="" >2.7829e-02 9.5367e-07 9.5129e-05</hent>
<hent key="024001000000000000000004000002D2" call="MPI_Isend" bytes="4" orank="722" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.9024e-02 2.8610e-06 1.2207e-04</hent>
<hent key="0380010000000000000005000000000A" call="MPI_Irecv" bytes="1280" orank="10" region="0" commid="0" count="209" tid="0" op="" dtype="" >1.2660e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000050000000011" call="MPI_Irecv" bytes="1280" orank="17" region="0" commid="0" count="183" tid="0" op="" dtype="" >7.2479e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000050000000013" call="MPI_Irecv" bytes="1280" orank="19" region="0" commid="0" count="197" tid="0" op="" dtype="" >4.0770e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000001A" call="MPI_Irecv" bytes="1280" orank="26" region="0" commid="0" count="220" tid="0" op="" dtype="" >6.3419e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000052" call="MPI_Irecv" bytes="1280" orank="82" region="0" commid="0" count="303" tid="0" op="" dtype="" >1.3041e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0967e+01 7.8678e-06 1.3909e-01</hent>
<hent key="0380010000000000000028000000000A" call="MPI_Irecv" bytes="10240" orank="10" region="0" commid="0" count="52" tid="0" op="" dtype="" >1.8120e-05 0.0000e+00 3.0994e-06</hent>
<hent key="0380010000000000000028000000001A" call="MPI_Irecv" bytes="10240" orank="26" region="0" commid="0" count="32" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000500000002D2" call="MPI_Irecv" bytes="1280" orank="722" region="0" commid="0" count="311" tid="0" op="" dtype="" >1.0753e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000008000000000A" call="MPI_Irecv" bytes="2048" orank="10" region="0" commid="0" count="17" tid="0" op="" dtype="" >1.5259e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000080000000011" call="MPI_Irecv" bytes="2048" orank="17" region="0" commid="0" count="12" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000013" call="MPI_Irecv" bytes="2048" orank="19" region="0" commid="0" count="9" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000008000000001A" call="MPI_Irecv" bytes="2048" orank="26" region="0" commid="0" count="15" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000052" call="MPI_Irecv" bytes="2048" orank="82" region="0" commid="0" count="3437" tid="0" op="" dtype="" >7.9870e-04 0.0000e+00 2.1935e-05</hent>
<hent key="038001000000000000000800000002D2" call="MPI_Irecv" bytes="2048" orank="722" region="0" commid="0" count="3465" tid="0" op="" dtype="" >6.4325e-04 0.0000e+00 1.5020e-05</hent>
<hent key="0240010000000000000005000000000A" call="MPI_Isend" bytes="1280" orank="10" region="0" commid="0" count="209" tid="0" op="" dtype="" >3.7265e-04 9.5367e-07 1.0014e-05</hent>
<hent key="02400100000000000000050000000011" call="MPI_Isend" bytes="1280" orank="17" region="0" commid="0" count="191" tid="0" op="" dtype="" >3.4046e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000013" call="MPI_Isend" bytes="1280" orank="19" region="0" commid="0" count="197" tid="0" op="" dtype="" >6.9594e-04 1.9073e-06 5.0068e-06</hent>
<hent key="0240010000000000000005000000001A" call="MPI_Isend" bytes="1280" orank="26" region="0" commid="0" count="178" tid="0" op="" dtype="" >8.9598e-04 9.5367e-07 1.3113e-05</hent>
<hent key="02400100000000000000050000000052" call="MPI_Isend" bytes="1280" orank="82" region="0" commid="0" count="303" tid="0" op="" dtype="" >1.7400e-03 3.8147e-06 1.3828e-05</hent>
<hent key="0240010000000000000028000000001A" call="MPI_Isend" bytes="10240" orank="26" region="0" commid="0" count="68" tid="0" op="" dtype="" >1.0822e-03 5.0068e-06 3.4094e-05</hent>
<hent key="024001000000000000000500000002D2" call="MPI_Isend" bytes="1280" orank="722" region="0" commid="0" count="300" tid="0" op="" dtype="" >1.4079e-03 2.8610e-06 1.2159e-05</hent>
</hash>
<internal rank="18" log_i="1723713849.504534" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="19" mpi_size="768" stamp_init="1723713791.128377" stamp_final="1723713849.509822" username="apac4" allocationname="unknown" flags="0" pid="1717082" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.83814e+01" utime="5.01081e+01" stime="6.91822e+00" mtime="3.24201e+01" gflop="0.00000e+00" gbyte="3.77064e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24201e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f414f31487" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83163e+01" utime="5.00782e+01" stime="6.90883e+00" mtime="3.24201e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24201e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.5043e+08" > 4.7095e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4880e+08" > 2.9871e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6419e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4392e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6703e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1008e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6099e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.3651e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.6216e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6119e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9185e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="191" >
<hent key="0240010000000000000008000000000B" call="MPI_Isend" bytes="2048" orank="11" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.2411e-05 9.5367e-07 1.3113e-05</hent>
<hent key="02400100000000000000080000000012" call="MPI_Isend" bytes="2048" orank="18" region="0" commid="0" count="9" tid="0" op="" dtype="" >2.1458e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000014" call="MPI_Isend" bytes="2048" orank="20" region="0" commid="0" count="10" tid="0" op="" dtype="" >4.1008e-05 3.0994e-06 5.0068e-06</hent>
<hent key="0240010000000000000008000000001B" call="MPI_Isend" bytes="2048" orank="27" region="0" commid="0" count="13" tid="0" op="" dtype="" >7.3195e-05 5.0068e-06 6.1989e-06</hent>
<hent key="02400100000000000000080000000053" call="MPI_Isend" bytes="2048" orank="83" region="0" commid="0" count="3481" tid="0" op="" dtype="" >1.0038e-02 9.5367e-07 2.2888e-05</hent>
<hent key="024001000000000000000800000002D3" call="MPI_Isend" bytes="2048" orank="723" region="0" commid="0" count="3463" tid="0" op="" dtype="" >1.0256e-02 9.5367e-07 2.5988e-05</hent>
<hent key="0380010000000000000002800000000B" call="MPI_Irecv" bytes="640" orank="11" region="0" commid="0" count="410" tid="0" op="" dtype="" >8.8930e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000012" call="MPI_Irecv" bytes="640" orank="18" region="0" commid="0" count="411" tid="0" op="" dtype="" >1.5116e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000014" call="MPI_Irecv" bytes="640" orank="20" region="0" commid="0" count="417" tid="0" op="" dtype="" >8.8453e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000001B" call="MPI_Irecv" bytes="640" orank="27" region="0" commid="0" count="416" tid="0" op="" dtype="" >8.4400e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000053" call="MPI_Irecv" bytes="640" orank="83" region="0" commid="0" count="263" tid="0" op="" dtype="" >7.4387e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="038001000000000000000280000002D3" call="MPI_Irecv" bytes="640" orank="723" region="0" commid="0" count="260" tid="0" op="" dtype="" >6.3896e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000002800000000B" call="MPI_Isend" bytes="640" orank="11" region="0" commid="0" count="394" tid="0" op="" dtype="" >4.4322e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000028000000012" call="MPI_Isend" bytes="640" orank="18" region="0" commid="0" count="407" tid="0" op="" dtype="" >5.5408e-04 9.5367e-07 9.7752e-06</hent>
<hent key="02400100000000000000028000000014" call="MPI_Isend" bytes="640" orank="20" region="0" commid="0" count="419" tid="0" op="" dtype="" >1.2333e-03 1.9073e-06 4.0531e-06</hent>
<hent key="0240010000000000000002800000001B" call="MPI_Isend" bytes="640" orank="27" region="0" commid="0" count="410" tid="0" op="" dtype="" >1.9829e-03 3.8147e-06 1.3113e-05</hent>
<hent key="02400100000000000000028000000053" call="MPI_Isend" bytes="640" orank="83" region="0" commid="0" count="247" tid="0" op="" dtype="" >1.0870e-03 3.8147e-06 9.7752e-06</hent>
<hent key="024001000000000000000280000002D3" call="MPI_Isend" bytes="640" orank="723" region="0" commid="0" count="273" tid="0" op="" dtype="" >1.1733e-03 3.8147e-06 9.0599e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0014e-05 1.0014e-05 1.0014e-05</hent>
<hent key="0380010000000000000001400000000B" call="MPI_Irecv" bytes="320" orank="11" region="0" commid="0" count="341" tid="0" op="" dtype="" >7.1049e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000012" call="MPI_Irecv" bytes="320" orank="18" region="0" commid="0" count="353" tid="0" op="" dtype="" >1.2326e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000014" call="MPI_Irecv" bytes="320" orank="20" region="0" commid="0" count="378" tid="0" op="" dtype="" >9.2745e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000001400000001B" call="MPI_Irecv" bytes="320" orank="27" region="0" commid="0" count="365" tid="0" op="" dtype="" >8.0824e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000053" call="MPI_Irecv" bytes="320" orank="83" region="0" commid="0" count="172" tid="0" op="" dtype="" >5.3406e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002D3" call="MPI_Irecv" bytes="320" orank="723" region="0" commid="0" count="197" tid="0" op="" dtype="" >5.3167e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.6419e+00 0.0000e+00 1.1629e-01</hent>
<hent key="03800100000000000000400000000053" call="MPI_Irecv" bytes="16384" orank="83" region="0" commid="0" count="12691" tid="0" op="" dtype="" >2.6047e-03 0.0000e+00 8.1062e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6928e-05 0.0000e+00 1.6928e-05</hent>
<hent key="038001000000000000004000000002D3" call="MPI_Irecv" bytes="16384" orank="723" region="0" commid="0" count="12699" tid="0" op="" dtype="" >7.3440e-03 0.0000e+00 1.1206e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.8147e-06 0.0000e+00 2.8610e-06</hent>
<hent key="0240010000000000000001400000000B" call="MPI_Isend" bytes="320" orank="11" region="0" commid="0" count="352" tid="0" op="" dtype="" >3.3665e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000014000000012" call="MPI_Isend" bytes="320" orank="18" region="0" commid="0" count="383" tid="0" op="" dtype="" >4.0078e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000014000000014" call="MPI_Isend" bytes="320" orank="20" region="0" commid="0" count="383" tid="0" op="" dtype="" >1.0223e-03 1.9073e-06 7.1526e-06</hent>
<hent key="0240010000000000000001400000001B" call="MPI_Isend" bytes="320" orank="27" region="0" commid="0" count="352" tid="0" op="" dtype="" >1.6141e-03 3.8147e-06 9.0599e-06</hent>
<hent key="02400100000000000000014000000053" call="MPI_Isend" bytes="320" orank="83" region="0" commid="0" count="165" tid="0" op="" dtype="" >6.8784e-04 2.8610e-06 5.9605e-06</hent>
<hent key="0380010000000000000020000000000B" call="MPI_Irecv" bytes="8192" orank="11" region="0" commid="0" count="12674" tid="0" op="" dtype="" >3.8245e-03 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000200000000012" call="MPI_Irecv" bytes="8192" orank="18" region="0" commid="0" count="3047" tid="0" op="" dtype="" >8.5521e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000200000000014" call="MPI_Irecv" bytes="8192" orank="20" region="0" commid="0" count="3669" tid="0" op="" dtype="" >5.5528e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0380010000000000000020000000001B" call="MPI_Irecv" bytes="8192" orank="27" region="0" commid="0" count="12637" tid="0" op="" dtype="" >1.2746e-03 0.0000e+00 3.8147e-06</hent>
<hent key="024001000000000000000140000002D3" call="MPI_Isend" bytes="320" orank="723" region="0" commid="0" count="191" tid="0" op="" dtype="" >7.9513e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0380010000000000000000000000000B" call="MPI_Irecv" bytes="0" orank="11" region="0" commid="0" count="237" tid="0" op="" dtype="" >5.7459e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000012" call="MPI_Irecv" bytes="0" orank="18" region="0" commid="0" count="260" tid="0" op="" dtype="" >7.9870e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000014" call="MPI_Irecv" bytes="0" orank="20" region="0" commid="0" count="247" tid="0" op="" dtype="" >5.3883e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000001B" call="MPI_Irecv" bytes="0" orank="27" region="0" commid="0" count="267" tid="0" op="" dtype="" >5.0068e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000053" call="MPI_Irecv" bytes="0" orank="83" region="0" commid="0" count="131" tid="0" op="" dtype="" >2.5511e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000053" call="MPI_Isend" bytes="16384" orank="83" region="0" commid="0" count="12683" tid="0" op="" dtype="" >8.0076e-02 3.8147e-06 1.0440e-03</hent>
<hent key="038001000000000000000000000002D3" call="MPI_Irecv" bytes="0" orank="723" region="0" commid="0" count="145" tid="0" op="" dtype="" >2.6941e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002D3" call="MPI_Isend" bytes="16384" orank="723" region="0" commid="0" count="12699" tid="0" op="" dtype="" >8.9769e-02 2.8610e-06 2.3127e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.6604e-04 0.0000e+00 5.8699e-04</hent>
<hent key="0240010000000000000020000000000B" call="MPI_Isend" bytes="8192" orank="11" region="0" commid="0" count="12699" tid="0" op="" dtype="" >6.2311e-03 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000200000000012" call="MPI_Isend" bytes="8192" orank="18" region="0" commid="0" count="4625" tid="0" op="" dtype="" >5.5683e-03 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000200000000014" call="MPI_Isend" bytes="8192" orank="20" region="0" commid="0" count="3819" tid="0" op="" dtype="" >6.7716e-03 0.0000e+00 1.7166e-05</hent>
<hent key="0240010000000000000020000000001B" call="MPI_Isend" bytes="8192" orank="27" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.0629e-01 3.8147e-06 2.8133e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.5061e-05 4.5061e-05 4.5061e-05</hent>
<hent key="0240010000000000000000000000000B" call="MPI_Isend" bytes="0" orank="11" region="0" commid="0" count="224" tid="0" op="" dtype="" >1.4472e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000012" call="MPI_Isend" bytes="0" orank="18" region="0" commid="0" count="282" tid="0" op="" dtype="" >1.9670e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000014" call="MPI_Isend" bytes="0" orank="20" region="0" commid="0" count="259" tid="0" op="" dtype="" >5.0974e-04 9.5367e-07 5.1975e-05</hent>
<hent key="0240010000000000000000000000001B" call="MPI_Isend" bytes="0" orank="27" region="0" commid="0" count="250" tid="0" op="" dtype="" >9.9111e-04 9.5367e-07 1.8120e-05</hent>
<hent key="02400100000000000000000000000053" call="MPI_Isend" bytes="0" orank="83" region="0" commid="0" count="149" tid="0" op="" dtype="" >5.3859e-04 9.5367e-07 7.8678e-06</hent>
<hent key="024001000000000000000000000002D3" call="MPI_Isend" bytes="0" orank="723" region="0" commid="0" count="149" tid="0" op="" dtype="" >5.5528e-04 1.1921e-06 6.9141e-06</hent>
<hent key="0380010000000000000006000000000B" call="MPI_Irecv" bytes="1536" orank="11" region="0" commid="0" count="110" tid="0" op="" dtype="" >2.5272e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000012" call="MPI_Irecv" bytes="1536" orank="18" region="0" commid="0" count="94" tid="0" op="" dtype="" >3.7432e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000014" call="MPI_Irecv" bytes="1536" orank="20" region="0" commid="0" count="96" tid="0" op="" dtype="" >1.5974e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000001B" call="MPI_Irecv" bytes="1536" orank="27" region="0" commid="0" count="80" tid="0" op="" dtype="" >1.3590e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000053" call="MPI_Irecv" bytes="1536" orank="83" region="0" commid="0" count="205" tid="0" op="" dtype="" >5.3167e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002D3" call="MPI_Irecv" bytes="1536" orank="723" region="0" commid="0" count="235" tid="0" op="" dtype="" >6.7711e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0240010000000000000006000000000B" call="MPI_Isend" bytes="1536" orank="11" region="0" commid="0" count="110" tid="0" op="" dtype="" >2.0576e-04 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000060000000012" call="MPI_Isend" bytes="1536" orank="18" region="0" commid="0" count="65" tid="0" op="" dtype="" >1.3185e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000060000000014" call="MPI_Isend" bytes="1536" orank="20" region="0" commid="0" count="86" tid="0" op="" dtype="" >3.1281e-04 1.9073e-06 1.0014e-05</hent>
<hent key="0240010000000000000006000000001B" call="MPI_Isend" bytes="1536" orank="27" region="0" commid="0" count="103" tid="0" op="" dtype="" >5.5194e-04 4.7684e-06 6.1989e-06</hent>
<hent key="02400100000000000000060000000053" call="MPI_Isend" bytes="1536" orank="83" region="0" commid="0" count="188" tid="0" op="" dtype="" >8.6641e-04 3.8147e-06 5.9605e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002D3" call="MPI_Isend" bytes="1536" orank="723" region="0" commid="0" count="222" tid="0" op="" dtype="" >1.0326e-03 3.8147e-06 9.0599e-06</hent>
<hent key="038001000000000000000C000000000B" call="MPI_Irecv" bytes="3072" orank="11" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000C0000000014" call="MPI_Irecv" bytes="3072" orank="20" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C0000000053" call="MPI_Irecv" bytes="3072" orank="83" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002D3" call="MPI_Irecv" bytes="3072" orank="723" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000053" call="MPI_Isend" bytes="3072" orank="83" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.6212e-05 5.0068e-06 6.1989e-06</hent>
<hent key="024001000000000000000C00000002D3" call="MPI_Isend" bytes="3072" orank="723" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.2187e-05 5.0068e-06 6.1989e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.0698e-04 4.0698e-04 4.0698e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.2006e-04 3.0303e-04 3.0899e-04</hent>
<hent key="0380010000000000000003800000000B" call="MPI_Irecv" bytes="896" orank="11" region="0" commid="0" count="325" tid="0" op="" dtype="" >6.9141e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000012" call="MPI_Irecv" bytes="896" orank="18" region="0" commid="0" count="2894" tid="0" op="" dtype="" >4.5490e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000014" call="MPI_Irecv" bytes="896" orank="20" region="0" commid="0" count="2727" tid="0" op="" dtype="" >5.1761e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000001B" call="MPI_Irecv" bytes="896" orank="27" region="0" commid="0" count="339" tid="0" op="" dtype="" >7.1526e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000053" call="MPI_Irecv" bytes="896" orank="83" region="0" commid="0" count="340" tid="0" op="" dtype="" >9.8467e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1921e-05 1.1921e-05 1.1921e-05</hent>
<hent key="038001000000000000000380000002D3" call="MPI_Irecv" bytes="896" orank="723" region="0" commid="0" count="336" tid="0" op="" dtype="" >7.0095e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000380000000053" call="MPI_Irecv" bytes="14336" orank="83" region="0" commid="0" count="8" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0240010000000000000003800000000B" call="MPI_Isend" bytes="896" orank="11" region="0" commid="0" count="319" tid="0" op="" dtype="" >4.3559e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000038000000012" call="MPI_Isend" bytes="896" orank="18" region="0" commid="0" count="2469" tid="0" op="" dtype="" >1.8029e-03 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000038000000014" call="MPI_Isend" bytes="896" orank="20" region="0" commid="0" count="2680" tid="0" op="" dtype="" >2.6402e-03 0.0000e+00 2.0981e-05</hent>
<hent key="0240010000000000000003800000001B" call="MPI_Isend" bytes="896" orank="27" region="0" commid="0" count="339" tid="0" op="" dtype="" >1.6811e-03 3.8147e-06 8.8215e-06</hent>
<hent key="02400100000000000000038000000053" call="MPI_Isend" bytes="896" orank="83" region="0" commid="0" count="345" tid="0" op="" dtype="" >1.5297e-03 2.8610e-06 1.0014e-05</hent>
<hent key="024001000000000000000380000002D3" call="MPI_Isend" bytes="896" orank="723" region="0" commid="0" count="321" tid="0" op="" dtype="" >1.3869e-03 3.0994e-06 8.8215e-06</hent>
<hent key="02400100000000000000380000000053" call="MPI_Isend" bytes="14336" orank="83" region="0" commid="0" count="16" tid="0" op="" dtype="" >9.6560e-05 3.8147e-06 6.9141e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.9039e+00 6.9141e-06 1.3919e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.4502e-04 5.4502e-04 5.4502e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >3.6216e-04 3.6216e-04 3.6216e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >1.5609e-03 1.5609e-03 1.5609e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >7.3351e-02 3.0112e-04 6.7773e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.8293e-04 5.8293e-04 5.8293e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6099e+00 4.4298e-04 2.5608e-01</hent>
<hent key="0380010000000000000004000000000B" call="MPI_Irecv" bytes="1024" orank="11" region="0" commid="0" count="3392" tid="0" op="" dtype="" >4.4227e-04 0.0000e+00 3.8147e-06</hent>
<hent key="03800100000000000000040000000012" call="MPI_Irecv" bytes="1024" orank="18" region="0" commid="0" count="834" tid="0" op="" dtype="" >9.7990e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000014" call="MPI_Irecv" bytes="1024" orank="20" region="0" commid="0" count="988" tid="0" op="" dtype="" >1.7667e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000004000000001B" call="MPI_Irecv" bytes="1024" orank="27" region="0" commid="0" count="3382" tid="0" op="" dtype="" >4.7350e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.6703e-05 0.0000e+00 1.2875e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="0380010000000000000007000000000B" call="MPI_Irecv" bytes="1792" orank="11" region="0" commid="0" count="48" tid="0" op="" dtype="" >1.3351e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000012" call="MPI_Irecv" bytes="1792" orank="18" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000014" call="MPI_Irecv" bytes="1792" orank="20" region="0" commid="0" count="44" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000001B" call="MPI_Irecv" bytes="1792" orank="27" region="0" commid="0" count="29" tid="0" op="" dtype="" >7.6294e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000053" call="MPI_Irecv" bytes="1792" orank="83" region="0" commid="0" count="138" tid="0" op="" dtype="" >3.2902e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002D3" call="MPI_Irecv" bytes="1792" orank="723" region="0" commid="0" count="123" tid="0" op="" dtype="" >2.5988e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >8.3923e-05 0.0000e+00 5.2929e-05</hent>
<hent key="0240010000000000000004000000000B" call="MPI_Isend" bytes="1024" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.9407e-03 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000040000000012" call="MPI_Isend" bytes="1024" orank="18" region="0" commid="0" count="1240" tid="0" op="" dtype="" >7.8344e-04 0.0000e+00 2.8610e-06</hent>
<hent key="02400100000000000000040000000014" call="MPI_Isend" bytes="1024" orank="20" region="0" commid="0" count="1034" tid="0" op="" dtype="" >7.2551e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0240010000000000000004000000001B" call="MPI_Isend" bytes="1024" orank="27" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.8632e-03 9.5367e-07 1.5020e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >5.0068e-06 2.1458e-06 2.8610e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4383e+00 0.0000e+00 3.2515e+00</hent>
<hent key="038001000000000000000A000000000B" call="MPI_Irecv" bytes="2560" orank="11" region="0" commid="0" count="6" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000012" call="MPI_Irecv" bytes="2560" orank="18" region="0" commid="0" count="5" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000014" call="MPI_Irecv" bytes="2560" orank="20" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000001B" call="MPI_Irecv" bytes="2560" orank="27" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000053" call="MPI_Irecv" bytes="2560" orank="83" region="0" commid="0" count="43" tid="0" op="" dtype="" >1.2636e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002D3" call="MPI_Irecv" bytes="2560" orank="723" region="0" commid="0" count="36" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000007000000000B" call="MPI_Isend" bytes="1792" orank="11" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.1659e-04 9.5367e-07 1.2875e-05</hent>
<hent key="02400100000000000000070000000012" call="MPI_Isend" bytes="1792" orank="18" region="0" commid="0" count="38" tid="0" op="" dtype="" >7.2479e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000014" call="MPI_Isend" bytes="1792" orank="20" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.5187e-04 2.8610e-06 5.9605e-06</hent>
<hent key="0240010000000000000007000000001B" call="MPI_Isend" bytes="1792" orank="27" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.7118e-04 5.0068e-06 6.1989e-06</hent>
<hent key="02400100000000000000070000000053" call="MPI_Isend" bytes="1792" orank="83" region="0" commid="0" count="141" tid="0" op="" dtype="" >6.5875e-04 9.5367e-07 6.1989e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6119e-02 3.6119e-02 3.6119e-02</hent>
<hent key="024001000000000000000700000002D3" call="MPI_Isend" bytes="1792" orank="723" region="0" commid="0" count="127" tid="0" op="" dtype="" >5.9295e-04 3.8147e-06 6.1989e-06</hent>
<hent key="024001000000000000000A000000000B" call="MPI_Isend" bytes="2560" orank="11" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.3127e-05 9.5367e-07 1.2875e-05</hent>
<hent key="024001000000000000000A0000000012" call="MPI_Isend" bytes="2560" orank="18" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.6451e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000014" call="MPI_Isend" bytes="2560" orank="20" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.2888e-05 2.8610e-06 5.0068e-06</hent>
<hent key="024001000000000000000A000000001B" call="MPI_Isend" bytes="2560" orank="27" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.5020e-05 5.0068e-06 5.0068e-06</hent>
<hent key="024001000000000000000A0000000053" call="MPI_Isend" bytes="2560" orank="83" region="0" commid="0" count="59" tid="0" op="" dtype="" >2.9945e-04 3.8147e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.5109e-04 5.2929e-05 2.1315e-04</hent>
<hent key="024001000000000000000A00000002D3" call="MPI_Isend" bytes="2560" orank="723" region="0" commid="0" count="56" tid="0" op="" dtype="" >2.8849e-04 3.8147e-06 1.0014e-05</hent>
<hent key="038001000000000000001000000002D3" call="MPI_Irecv" bytes="4096" orank="723" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.5550e-03 5.8484e-04 9.7013e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.4094e-05 3.4094e-05 3.4094e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 1.9073e-06 3.0994e-06</hent>
<hent key="0380010000000000000000040000000B" call="MPI_Irecv" bytes="4" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.3477e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000012" call="MPI_Irecv" bytes="4" orank="18" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.5054e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000014" call="MPI_Irecv" bytes="4" orank="20" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.5300e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000001B" call="MPI_Irecv" bytes="4" orank="27" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.3035e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000400000053" call="MPI_Irecv" bytes="4" orank="83" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.2479e-04 0.0000e+00 2.8610e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.0310e-03 3.8695e-04 1.0800e-03</hent>
<hent key="038001000000000000001C0000000012" call="MPI_Irecv" bytes="7168" orank="18" region="0" commid="0" count="9652" tid="0" op="" dtype="" >2.7349e-03 0.0000e+00 6.9141e-06</hent>
<hent key="038001000000000000001C0000000014" call="MPI_Irecv" bytes="7168" orank="20" region="0" commid="0" count="9030" tid="0" op="" dtype="" >1.4768e-03 0.0000e+00 1.8120e-05</hent>
<hent key="038001000000000000000004000002D3" call="MPI_Irecv" bytes="4" orank="723" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.3848e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000000040000000B" call="MPI_Isend" bytes="4" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3869e-03 0.0000e+00 3.0994e-05</hent>
<hent key="02400100000000000000000400000012" call="MPI_Isend" bytes="4" orank="18" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2748e-03 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000000400000014" call="MPI_Isend" bytes="4" orank="20" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3379e-03 0.0000e+00 6.2943e-05</hent>
<hent key="0240010000000000000000040000001B" call="MPI_Isend" bytes="4" orank="27" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3924e-02 4.0531e-06 1.5497e-04</hent>
<hent key="02400100000000000000000400000053" call="MPI_Isend" bytes="4" orank="83" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.8306e-02 3.8147e-06 1.2875e-05</hent>
<hent key="024001000000000000001C0000000012" call="MPI_Isend" bytes="7168" orank="18" region="0" commid="0" count="8074" tid="0" op="" dtype="" >1.8803e-02 9.5367e-07 1.0967e-05</hent>
<hent key="024001000000000000001C0000000014" call="MPI_Isend" bytes="7168" orank="20" region="0" commid="0" count="8880" tid="0" op="" dtype="" >2.5739e-02 9.5367e-07 1.3113e-05</hent>
<hent key="024001000000000000000004000002D3" call="MPI_Isend" bytes="4" orank="723" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7181e-02 2.8610e-06 1.0896e-04</hent>
<hent key="0380010000000000000005000000000B" call="MPI_Irecv" bytes="1280" orank="11" region="0" commid="0" count="216" tid="0" op="" dtype="" >4.4107e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000012" call="MPI_Irecv" bytes="1280" orank="18" region="0" commid="0" count="197" tid="0" op="" dtype="" >5.7220e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000014" call="MPI_Irecv" bytes="1280" orank="20" region="0" commid="0" count="171" tid="0" op="" dtype="" >4.6968e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000005000000001B" call="MPI_Irecv" bytes="1280" orank="27" region="0" commid="0" count="203" tid="0" op="" dtype="" >3.9339e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000053" call="MPI_Irecv" bytes="1280" orank="83" region="0" commid="0" count="329" tid="0" op="" dtype="" >1.1015e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.1205e+01 8.1062e-06 1.3905e-01</hent>
<hent key="0380010000000000000028000000000B" call="MPI_Irecv" bytes="10240" orank="11" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.3351e-05 0.0000e+00 3.0994e-06</hent>
<hent key="0380010000000000000028000000001B" call="MPI_Irecv" bytes="10240" orank="27" region="0" commid="0" count="62" tid="0" op="" dtype="" >7.3910e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002D3" call="MPI_Irecv" bytes="1280" orank="723" region="0" commid="0" count="299" tid="0" op="" dtype="" >6.9618e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000B" call="MPI_Irecv" bytes="2048" orank="11" region="0" commid="0" count="11" tid="0" op="" dtype="" >6.4373e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000012" call="MPI_Irecv" bytes="2048" orank="18" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000014" call="MPI_Irecv" bytes="2048" orank="20" region="0" commid="0" count="20" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000001B" call="MPI_Irecv" bytes="2048" orank="27" region="0" commid="0" count="13" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000053" call="MPI_Irecv" bytes="2048" orank="83" region="0" commid="0" count="3467" tid="0" op="" dtype="" >5.9032e-04 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000000800000002D3" call="MPI_Irecv" bytes="2048" orank="723" region="0" commid="0" count="3458" tid="0" op="" dtype="" >5.2786e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000005000000000B" call="MPI_Isend" bytes="1280" orank="11" region="0" commid="0" count="239" tid="0" op="" dtype="" >3.8195e-04 9.5367e-07 1.1921e-05</hent>
<hent key="02400100000000000000050000000012" call="MPI_Isend" bytes="1280" orank="18" region="0" commid="0" count="197" tid="0" op="" dtype="" >3.6097e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000050000000014" call="MPI_Isend" bytes="1280" orank="20" region="0" commid="0" count="181" tid="0" op="" dtype="" >6.2156e-04 2.1458e-06 7.1526e-06</hent>
<hent key="0240010000000000000005000000001B" call="MPI_Isend" bytes="1280" orank="27" region="0" commid="0" count="197" tid="0" op="" dtype="" >1.0157e-03 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000050000000053" call="MPI_Isend" bytes="1280" orank="83" region="0" commid="0" count="319" tid="0" op="" dtype="" >1.4524e-03 3.0994e-06 9.0599e-06</hent>
<hent key="024001000000000000000500000002D3" call="MPI_Isend" bytes="1280" orank="723" region="0" commid="0" count="289" tid="0" op="" dtype="" >1.2691e-03 3.8147e-06 1.0014e-05</hent>
</hash>
<internal rank="19" log_i="1723713849.509822" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="20" mpi_size="768" stamp_init="1723713791.133433" stamp_final="1723713849.507148" username="apac4" allocationname="unknown" flags="0" pid="1717083" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.83737e+01" utime="4.70440e+01" stime="7.94612e+00" mtime="3.18712e+01" gflop="0.00000e+00" gbyte="3.75870e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18712e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ed15ec153a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83113e+01" utime="4.70189e+01" stime="7.93256e+00" mtime="3.18712e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18712e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4964e+08" > 6.9766e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4957e+08" > 4.5865e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2586e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4137e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6488e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.7193e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6092e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.6998e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1114e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6174e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8803e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="198" >
<hent key="0240010000000000000008000000000C" call="MPI_Isend" bytes="2048" orank="12" region="0" commid="0" count="14" tid="0" op="" dtype="" >4.9591e-05 9.5367e-07 1.5974e-05</hent>
<hent key="02400100000000000000080000000013" call="MPI_Isend" bytes="2048" orank="19" region="0" commid="0" count="20" tid="0" op="" dtype="" >4.6015e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000015" call="MPI_Isend" bytes="2048" orank="21" region="0" commid="0" count="14" tid="0" op="" dtype="" >5.5313e-05 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000008000000001C" call="MPI_Isend" bytes="2048" orank="28" region="0" commid="0" count="11" tid="0" op="" dtype="" >6.1035e-05 5.0068e-06 8.1062e-06</hent>
<hent key="02400100000000000000080000000054" call="MPI_Isend" bytes="2048" orank="84" region="0" commid="0" count="3460" tid="0" op="" dtype="" >1.7183e-02 9.5367e-07 8.0109e-05</hent>
<hent key="024001000000000000000800000002D4" call="MPI_Isend" bytes="2048" orank="724" region="0" commid="0" count="3466" tid="0" op="" dtype="" >1.6516e-02 9.5367e-07 9.4175e-05</hent>
<hent key="024001000000000000000E0000000054" call="MPI_Isend" bytes="3584" orank="84" region="0" commid="0" count="1" tid="0" op="" dtype="" >7.1526e-06 7.1526e-06 7.1526e-06</hent>
<hent key="0380010000000000000002800000000C" call="MPI_Irecv" bytes="640" orank="12" region="0" commid="0" count="399" tid="0" op="" dtype="" >1.0371e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000028000000013" call="MPI_Irecv" bytes="640" orank="19" region="0" commid="0" count="419" tid="0" op="" dtype="" >1.7667e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000015" call="MPI_Irecv" bytes="640" orank="21" region="0" commid="0" count="421" tid="0" op="" dtype="" >9.6560e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000001C" call="MPI_Irecv" bytes="640" orank="28" region="0" commid="0" count="420" tid="0" op="" dtype="" >9.2030e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000E00000002D4" call="MPI_Isend" bytes="3584" orank="724" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.0967e-05 5.0068e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000054" call="MPI_Irecv" bytes="640" orank="84" region="0" commid="0" count="280" tid="0" op="" dtype="" >1.7667e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="038001000000000000000280000002D4" call="MPI_Irecv" bytes="640" orank="724" region="0" commid="0" count="286" tid="0" op="" dtype="" >1.4091e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0240010000000000000002800000000C" call="MPI_Isend" bytes="640" orank="12" region="0" commid="0" count="389" tid="0" op="" dtype="" >4.8399e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000028000000013" call="MPI_Isend" bytes="640" orank="19" region="0" commid="0" count="417" tid="0" op="" dtype="" >5.7006e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000028000000015" call="MPI_Isend" bytes="640" orank="21" region="0" commid="0" count="427" tid="0" op="" dtype="" >1.3595e-03 1.9073e-06 1.0967e-05</hent>
<hent key="0240010000000000000002800000001C" call="MPI_Isend" bytes="640" orank="28" region="0" commid="0" count="395" tid="0" op="" dtype="" >2.0034e-03 3.8147e-06 1.5020e-05</hent>
<hent key="02400100000000000000028000000054" call="MPI_Isend" bytes="640" orank="84" region="0" commid="0" count="268" tid="0" op="" dtype="" >1.5655e-03 3.8147e-06 2.4080e-05</hent>
<hent key="024001000000000000000280000002D4" call="MPI_Isend" bytes="640" orank="724" region="0" commid="0" count="279" tid="0" op="" dtype="" >1.5099e-03 2.8610e-06 2.0981e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="0380010000000000000001400000000C" call="MPI_Irecv" bytes="320" orank="12" region="0" commid="0" count="390" tid="0" op="" dtype="" >1.0824e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000014000000013" call="MPI_Irecv" bytes="320" orank="19" region="0" commid="0" count="383" tid="0" op="" dtype="" >1.6761e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000015" call="MPI_Irecv" bytes="320" orank="21" region="0" commid="0" count="357" tid="0" op="" dtype="" >7.5340e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000001C" call="MPI_Irecv" bytes="320" orank="28" region="0" commid="0" count="340" tid="0" op="" dtype="" >6.9141e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000014000000054" call="MPI_Irecv" bytes="320" orank="84" region="0" commid="0" count="178" tid="0" op="" dtype="" >9.4175e-05 0.0000e+00 1.0967e-05</hent>
<hent key="038001000000000000000140000002D4" call="MPI_Irecv" bytes="320" orank="724" region="0" commid="0" count="170" tid="0" op="" dtype="" >8.4162e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.2586e+00 0.0000e+00 1.1635e-01</hent>
<hent key="03800100000000000000400000000054" call="MPI_Irecv" bytes="16384" orank="84" region="0" commid="0" count="12624" tid="0" op="" dtype="" >8.5657e-03 0.0000e+00 6.1989e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6928e-05 0.0000e+00 1.6928e-05</hent>
<hent key="038001000000000000004000000002D4" call="MPI_Irecv" bytes="16384" orank="724" region="0" commid="0" count="12646" tid="0" op="" dtype="" >8.6877e-03 0.0000e+00 6.8188e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000001400000000C" call="MPI_Isend" bytes="320" orank="12" region="0" commid="0" count="364" tid="0" op="" dtype="" >3.4428e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000014000000013" call="MPI_Isend" bytes="320" orank="19" region="0" commid="0" count="378" tid="0" op="" dtype="" >3.9959e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000014000000015" call="MPI_Isend" bytes="320" orank="21" region="0" commid="0" count="361" tid="0" op="" dtype="" >1.0030e-03 9.5367e-07 9.0599e-06</hent>
<hent key="0240010000000000000001400000001C" call="MPI_Isend" bytes="320" orank="28" region="0" commid="0" count="354" tid="0" op="" dtype="" >1.7247e-03 3.8147e-06 1.5974e-05</hent>
<hent key="02400100000000000000014000000054" call="MPI_Isend" bytes="320" orank="84" region="0" commid="0" count="191" tid="0" op="" dtype="" >1.0974e-03 3.0994e-06 2.0981e-05</hent>
<hent key="0380010000000000000020000000000C" call="MPI_Irecv" bytes="8192" orank="12" region="0" commid="0" count="12682" tid="0" op="" dtype="" >4.7121e-03 0.0000e+00 1.3113e-05</hent>
<hent key="03800100000000000000200000000013" call="MPI_Irecv" bytes="8192" orank="19" region="0" commid="0" count="3819" tid="0" op="" dtype="" >1.6243e-03 0.0000e+00 2.8610e-06</hent>
<hent key="03800100000000000000200000000015" call="MPI_Irecv" bytes="8192" orank="21" region="0" commid="0" count="3778" tid="0" op="" dtype="" >7.0810e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000020000000001C" call="MPI_Irecv" bytes="8192" orank="28" region="0" commid="0" count="12578" tid="0" op="" dtype="" >2.3417e-03 0.0000e+00 2.4080e-05</hent>
<hent key="024001000000000000000140000002D4" call="MPI_Isend" bytes="320" orank="724" region="0" commid="0" count="178" tid="0" op="" dtype="" >8.6713e-04 2.8610e-06 1.6928e-05</hent>
<hent key="0380010000000000000000000000000C" call="MPI_Irecv" bytes="0" orank="12" region="0" commid="0" count="252" tid="0" op="" dtype="" >7.1287e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000013" call="MPI_Irecv" bytes="0" orank="19" region="0" commid="0" count="259" tid="0" op="" dtype="" >9.8228e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000015" call="MPI_Irecv" bytes="0" orank="21" region="0" commid="0" count="247" tid="0" op="" dtype="" >6.4373e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000001C" call="MPI_Irecv" bytes="0" orank="28" region="0" commid="0" count="269" tid="0" op="" dtype="" >6.8903e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000054" call="MPI_Irecv" bytes="0" orank="84" region="0" commid="0" count="148" tid="0" op="" dtype="" >5.3167e-05 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000400000000054" call="MPI_Isend" bytes="16384" orank="84" region="0" commid="0" count="12667" tid="0" op="" dtype="" >1.8847e-01 3.8147e-06 1.4687e-04</hent>
<hent key="038001000000000000000000000002D4" call="MPI_Irecv" bytes="0" orank="724" region="0" commid="0" count="137" tid="0" op="" dtype="" >4.2439e-05 0.0000e+00 5.9605e-06</hent>
<hent key="024001000000000000004000000002D4" call="MPI_Isend" bytes="16384" orank="724" region="0" commid="0" count="12691" tid="0" op="" dtype="" >1.4884e-01 2.8610e-06 1.0896e-04</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.7772e-04 0.0000e+00 6.0701e-04</hent>
<hent key="0240010000000000000020000000000C" call="MPI_Isend" bytes="8192" orank="12" region="0" commid="0" count="12676" tid="0" op="" dtype="" >8.2366e-03 0.0000e+00 3.0041e-05</hent>
<hent key="02400100000000000000200000000013" call="MPI_Isend" bytes="8192" orank="19" region="0" commid="0" count="3669" tid="0" op="" dtype="" >4.5850e-03 0.0000e+00 5.0068e-05</hent>
<hent key="02400100000000000000200000000015" call="MPI_Isend" bytes="8192" orank="21" region="0" commid="0" count="3531" tid="0" op="" dtype="" >7.1192e-03 0.0000e+00 7.6056e-05</hent>
<hent key="0240010000000000000020000000001C" call="MPI_Isend" bytes="8192" orank="28" region="0" commid="0" count="12439" tid="0" op="" dtype="" >1.1990e-01 3.8147e-06 4.6968e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.9087e-05 2.9087e-05 2.9087e-05</hent>
<hent key="0240010000000000000000000000000C" call="MPI_Isend" bytes="0" orank="12" region="0" commid="0" count="264" tid="0" op="" dtype="" >2.0504e-04 0.0000e+00 8.8215e-06</hent>
<hent key="02400100000000000000000000000013" call="MPI_Isend" bytes="0" orank="19" region="0" commid="0" count="247" tid="0" op="" dtype="" >1.6046e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000000000015" call="MPI_Isend" bytes="0" orank="21" region="0" commid="0" count="259" tid="0" op="" dtype="" >5.0783e-04 9.5367e-07 3.0041e-05</hent>
<hent key="0240010000000000000000000000001C" call="MPI_Isend" bytes="0" orank="28" region="0" commid="0" count="270" tid="0" op="" dtype="" >1.1156e-03 1.9073e-06 1.2875e-05</hent>
<hent key="02400100000000000000000000000054" call="MPI_Isend" bytes="0" orank="84" region="0" commid="0" count="145" tid="0" op="" dtype="" >7.4244e-04 1.9073e-06 2.2888e-05</hent>
<hent key="024001000000000000000000000002D4" call="MPI_Isend" bytes="0" orank="724" region="0" commid="0" count="150" tid="0" op="" dtype="" >6.6280e-04 9.5367e-07 1.4067e-05</hent>
<hent key="0380010000000000000006000000000C" call="MPI_Irecv" bytes="1536" orank="12" region="0" commid="0" count="89" tid="0" op="" dtype="" >2.0742e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000013" call="MPI_Irecv" bytes="1536" orank="19" region="0" commid="0" count="86" tid="0" op="" dtype="" >4.4107e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000015" call="MPI_Irecv" bytes="1536" orank="21" region="0" commid="0" count="72" tid="0" op="" dtype="" >1.5020e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000001C" call="MPI_Irecv" bytes="1536" orank="28" region="0" commid="0" count="89" tid="0" op="" dtype="" >1.8597e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000054" call="MPI_Irecv" bytes="1536" orank="84" region="0" commid="0" count="234" tid="0" op="" dtype="" >1.3542e-04 0.0000e+00 1.0014e-05</hent>
<hent key="038001000000000000000600000002D4" call="MPI_Irecv" bytes="1536" orank="724" region="0" commid="0" count="238" tid="0" op="" dtype="" >7.7009e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000006000000000C" call="MPI_Isend" bytes="1536" orank="12" region="0" commid="0" count="89" tid="0" op="" dtype="" >2.1195e-04 9.5367e-07 1.5020e-05</hent>
<hent key="02400100000000000000060000000013" call="MPI_Isend" bytes="1536" orank="19" region="0" commid="0" count="96" tid="0" op="" dtype="" >1.9145e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000015" call="MPI_Isend" bytes="1536" orank="21" region="0" commid="0" count="80" tid="0" op="" dtype="" >2.9731e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000006000000001C" call="MPI_Isend" bytes="1536" orank="28" region="0" commid="0" count="103" tid="0" op="" dtype="" >5.7650e-04 3.8147e-06 1.2875e-05</hent>
<hent key="02400100000000000000060000000054" call="MPI_Isend" bytes="1536" orank="84" region="0" commid="0" count="212" tid="0" op="" dtype="" >1.4267e-03 3.8147e-06 4.1008e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002D4" call="MPI_Isend" bytes="1536" orank="724" region="0" commid="0" count="227" tid="0" op="" dtype="" >1.2932e-03 3.0994e-06 2.5034e-05</hent>
<hent key="038001000000000000000C0000000054" call="MPI_Irecv" bytes="3072" orank="84" region="0" commid="0" count="8" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002D4" call="MPI_Irecv" bytes="3072" orank="724" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C000000000C" call="MPI_Isend" bytes="3072" orank="12" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000C0000000013" call="MPI_Isend" bytes="3072" orank="19" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000C0000000015" call="MPI_Isend" bytes="3072" orank="21" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="024001000000000000000C000000001C" call="MPI_Isend" bytes="3072" orank="28" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.1921e-05 5.9605e-06 5.9605e-06</hent>
<hent key="024001000000000000000C0000000054" call="MPI_Isend" bytes="3072" orank="84" region="0" commid="0" count="10" tid="0" op="" dtype="" >1.1206e-04 5.0068e-06 2.0027e-05</hent>
<hent key="024001000000000000000C00000002D4" call="MPI_Isend" bytes="3072" orank="724" region="0" commid="0" count="6" tid="0" op="" dtype="" >4.7207e-05 5.0068e-06 1.8120e-05</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.2486e-04 4.2486e-04 4.2486e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.6822e-04 3.2210e-04 3.2401e-04</hent>
<hent key="0380010000000000000003800000000C" call="MPI_Irecv" bytes="896" orank="12" region="0" commid="0" count="308" tid="0" op="" dtype="" >8.0585e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000013" call="MPI_Irecv" bytes="896" orank="19" region="0" commid="0" count="2680" tid="0" op="" dtype="" >4.2081e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000015" call="MPI_Irecv" bytes="896" orank="21" region="0" commid="0" count="2714" tid="0" op="" dtype="" >5.2977e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000003800000001C" call="MPI_Irecv" bytes="896" orank="28" region="0" commid="0" count="324" tid="0" op="" dtype="" >7.2956e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000038000000054" call="MPI_Irecv" bytes="896" orank="84" region="0" commid="0" count="310" tid="0" op="" dtype="" >1.8001e-04 0.0000e+00 1.2875e-05</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.2875e-05 1.2875e-05 1.2875e-05</hent>
<hent key="038001000000000000000380000002D4" call="MPI_Irecv" bytes="896" orank="724" region="0" commid="0" count="311" tid="0" op="" dtype="" >1.3518e-04 0.0000e+00 6.9141e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="03800100000000000000380000000054" call="MPI_Irecv" bytes="14336" orank="84" region="0" commid="0" count="75" tid="0" op="" dtype="" >2.1458e-05 0.0000e+00 3.0994e-06</hent>
<hent key="038001000000000000003800000002D4" call="MPI_Irecv" bytes="14336" orank="724" region="0" commid="0" count="53" tid="0" op="" dtype="" >3.2663e-05 0.0000e+00 1.5974e-05</hent>
<hent key="0240010000000000000003800000000C" call="MPI_Isend" bytes="896" orank="12" region="0" commid="0" count="338" tid="0" op="" dtype="" >5.1141e-04 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000038000000013" call="MPI_Isend" bytes="896" orank="19" region="0" commid="0" count="2727" tid="0" op="" dtype="" >2.0397e-03 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000038000000015" call="MPI_Isend" bytes="896" orank="21" region="0" commid="0" count="2770" tid="0" op="" dtype="" >2.9585e-03 0.0000e+00 3.0994e-05</hent>
<hent key="0240010000000000000003800000001C" call="MPI_Isend" bytes="896" orank="28" region="0" commid="0" count="312" tid="0" op="" dtype="" >1.6558e-03 3.8147e-06 2.7180e-05</hent>
<hent key="02400100000000000000038000000054" call="MPI_Isend" bytes="896" orank="84" region="0" commid="0" count="300" tid="0" op="" dtype="" >1.8651e-03 3.8147e-06 2.7895e-05</hent>
<hent key="024001000000000000000380000002D4" call="MPI_Isend" bytes="896" orank="724" region="0" commid="0" count="326" tid="0" op="" dtype="" >1.8132e-03 2.8610e-06 2.0027e-05</hent>
<hent key="02400100000000000000380000000054" call="MPI_Isend" bytes="14336" orank="84" region="0" commid="0" count="32" tid="0" op="" dtype="" >5.1928e-04 5.0068e-06 5.4836e-05</hent>
<hent key="024001000000000000003800000002D4" call="MPI_Isend" bytes="14336" orank="724" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.9829e-05 4.0531e-06 1.0967e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.7057e+00 1.0014e-05 1.3914e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.4502e-04 5.4502e-04 5.4502e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >3.1114e-04 3.1114e-04 3.1114e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.0099e-03 2.0099e-03 2.0099e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >7.4502e-02 2.9612e-04 6.9582e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.5289e-04 5.5289e-04 5.5289e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6092e+00 4.7803e-04 2.5601e-01</hent>
<hent key="0380010000000000000004000000000C" call="MPI_Irecv" bytes="1024" orank="12" region="0" commid="0" count="3394" tid="0" op="" dtype="" >5.2691e-04 0.0000e+00 1.3828e-05</hent>
<hent key="03800100000000000000040000000013" call="MPI_Irecv" bytes="1024" orank="19" region="0" commid="0" count="1034" tid="0" op="" dtype="" >1.2231e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000040000000015" call="MPI_Irecv" bytes="1024" orank="21" region="0" commid="0" count="1014" tid="0" op="" dtype="" >2.1100e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000004000000001C" call="MPI_Irecv" bytes="1024" orank="28" region="0" commid="0" count="3360" tid="0" op="" dtype="" >5.6028e-04 0.0000e+00 2.3127e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.6488e-04 0.0000e+00 1.6403e-04</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="0380010000000000000007000000000C" call="MPI_Irecv" bytes="1792" orank="12" region="0" commid="0" count="42" tid="0" op="" dtype="" >1.4067e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000013" call="MPI_Irecv" bytes="1792" orank="19" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.5259e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000015" call="MPI_Irecv" bytes="1792" orank="21" region="0" commid="0" count="35" tid="0" op="" dtype="" >1.3590e-05 0.0000e+00 7.8678e-06</hent>
<hent key="0380010000000000000007000000001C" call="MPI_Irecv" bytes="1792" orank="28" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.0252e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000054" call="MPI_Irecv" bytes="1792" orank="84" region="0" commid="0" count="139" tid="0" op="" dtype="" >6.5804e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002D4" call="MPI_Irecv" bytes="1792" orank="724" region="0" commid="0" count="179" tid="0" op="" dtype="" >6.9141e-05 0.0000e+00 5.0068e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >8.1301e-05 0.0000e+00 5.3167e-05</hent>
<hent key="0240010000000000000004000000000C" call="MPI_Isend" bytes="1024" orank="12" region="0" commid="0" count="3392" tid="0" op="" dtype="" >2.7096e-03 0.0000e+00 3.0041e-05</hent>
<hent key="02400100000000000000040000000013" call="MPI_Isend" bytes="1024" orank="19" region="0" commid="0" count="988" tid="0" op="" dtype="" >6.3753e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000040000000015" call="MPI_Isend" bytes="1024" orank="21" region="0" commid="0" count="956" tid="0" op="" dtype="" >7.4840e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000004000000001C" call="MPI_Isend" bytes="1024" orank="28" region="0" commid="0" count="3324" tid="0" op="" dtype="" >9.6681e-03 9.5367e-07 1.9073e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4127e+00 0.0000e+00 3.2485e+00</hent>
<hent key="038001000000000000000A000000000C" call="MPI_Irecv" bytes="2560" orank="12" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000013" call="MPI_Irecv" bytes="2560" orank="19" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000015" call="MPI_Irecv" bytes="2560" orank="21" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000001C" call="MPI_Irecv" bytes="2560" orank="28" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000054" call="MPI_Irecv" bytes="2560" orank="84" region="0" commid="0" count="45" tid="0" op="" dtype="" >3.8147e-05 0.0000e+00 8.1062e-06</hent>
<hent key="038001000000000000000A00000002D4" call="MPI_Irecv" bytes="2560" orank="724" region="0" commid="0" count="48" tid="0" op="" dtype="" >3.8385e-05 0.0000e+00 8.8215e-06</hent>
<hent key="0240010000000000000007000000000C" call="MPI_Isend" bytes="1792" orank="12" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.1253e-04 9.5367e-07 1.5974e-05</hent>
<hent key="02400100000000000000070000000013" call="MPI_Isend" bytes="1792" orank="19" region="0" commid="0" count="44" tid="0" op="" dtype="" >9.6798e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000015" call="MPI_Isend" bytes="1792" orank="21" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.1563e-04 2.8610e-06 5.9605e-06</hent>
<hent key="0240010000000000000007000000001C" call="MPI_Isend" bytes="1792" orank="28" region="0" commid="0" count="33" tid="0" op="" dtype="" >1.9884e-04 3.8147e-06 1.2159e-05</hent>
<hent key="02400100000000000000070000000054" call="MPI_Isend" bytes="1792" orank="84" region="0" commid="0" count="137" tid="0" op="" dtype="" >9.7251e-04 9.5367e-07 3.2902e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6174e-02 3.6174e-02 3.6174e-02</hent>
<hent key="024001000000000000000700000002D4" call="MPI_Isend" bytes="1792" orank="724" region="0" commid="0" count="133" tid="0" op="" dtype="" >7.8940e-04 9.5367e-07 1.6928e-05</hent>
<hent key="024001000000000000000A000000000C" call="MPI_Isend" bytes="2560" orank="12" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.2173e-05 9.5367e-07 1.5974e-05</hent>
<hent key="024001000000000000000A0000000013" call="MPI_Isend" bytes="2560" orank="19" region="0" commid="0" count="8" tid="0" op="" dtype="" >2.0027e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000015" call="MPI_Isend" bytes="2560" orank="21" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.1219e-05 3.0994e-06 5.0068e-06</hent>
<hent key="024001000000000000000A000000001C" call="MPI_Isend" bytes="2560" orank="28" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.0994e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000054" call="MPI_Isend" bytes="2560" orank="84" region="0" commid="0" count="51" tid="0" op="" dtype="" >3.6550e-04 5.0068e-06 1.8835e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.6277e-04 5.1975e-05 2.1601e-04</hent>
<hent key="024001000000000000000A00000002D4" call="MPI_Isend" bytes="2560" orank="724" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.7490e-04 3.8147e-06 2.3842e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.6429e-03 6.0987e-04 1.0331e-03</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-05 3.0994e-05 3.0994e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >7.1526e-06 2.1458e-06 5.0068e-06</hent>
<hent key="0380010000000000000000040000000C" call="MPI_Irecv" bytes="4" orank="12" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.7639e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000400000013" call="MPI_Irecv" bytes="4" orank="19" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3189e-03 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000000400000015" call="MPI_Irecv" bytes="4" orank="21" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.3120e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000040000001C" call="MPI_Irecv" bytes="4" orank="28" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.4669e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000000400000054" call="MPI_Irecv" bytes="4" orank="84" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4904e-03 0.0000e+00 7.6056e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.1989e-03 4.0102e-04 1.1649e-03</hent>
<hent key="038001000000000000001C0000000013" call="MPI_Irecv" bytes="7168" orank="19" region="0" commid="0" count="8880" tid="0" op="" dtype="" >3.7379e-03 0.0000e+00 1.0014e-05</hent>
<hent key="038001000000000000001C0000000015" call="MPI_Irecv" bytes="7168" orank="21" region="0" commid="0" count="8921" tid="0" op="" dtype="" >1.8682e-03 0.0000e+00 1.2159e-05</hent>
<hent key="038001000000000000000004000002D4" call="MPI_Irecv" bytes="4" orank="724" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2202e-03 0.0000e+00 5.4121e-05</hent>
<hent key="0240010000000000000000040000000C" call="MPI_Isend" bytes="4" orank="12" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3621e-03 0.0000e+00 2.7180e-05</hent>
<hent key="02400100000000000000000400000013" call="MPI_Isend" bytes="4" orank="19" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.5793e-03 0.0000e+00 4.4823e-05</hent>
<hent key="02400100000000000000000400000015" call="MPI_Isend" bytes="4" orank="21" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.9581e-03 0.0000e+00 1.2875e-05</hent>
<hent key="0240010000000000000000040000001C" call="MPI_Isend" bytes="4" orank="28" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.5413e-02 3.8147e-06 2.7180e-05</hent>
<hent key="02400100000000000000000400000054" call="MPI_Isend" bytes="4" orank="84" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3433e-02 3.8147e-06 6.9141e-05</hent>
<hent key="024001000000000000001C0000000013" call="MPI_Isend" bytes="7168" orank="19" region="0" commid="0" count="9030" tid="0" op="" dtype="" >2.0451e-02 9.5367e-07 1.6928e-05</hent>
<hent key="024001000000000000001C0000000015" call="MPI_Isend" bytes="7168" orank="21" region="0" commid="0" count="9168" tid="0" op="" dtype="" >2.8073e-02 9.5367e-07 2.5988e-05</hent>
<hent key="024001000000000000000004000002D4" call="MPI_Isend" bytes="4" orank="724" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.5527e-02 3.0994e-06 1.3423e-04</hent>
<hent key="0380010000000000000005000000000C" call="MPI_Irecv" bytes="1280" orank="12" region="0" commid="0" count="194" tid="0" op="" dtype="" >4.6492e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000013" call="MPI_Irecv" bytes="1280" orank="19" region="0" commid="0" count="181" tid="0" op="" dtype="" >7.0333e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000015" call="MPI_Irecv" bytes="1280" orank="21" region="0" commid="0" count="216" tid="0" op="" dtype="" >4.6492e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000001C" call="MPI_Irecv" bytes="1280" orank="28" region="0" commid="0" count="233" tid="0" op="" dtype="" >5.5552e-05 0.0000e+00 3.8147e-06</hent>
<hent key="03800100000000000000050000000054" call="MPI_Irecv" bytes="1280" orank="84" region="0" commid="0" count="291" tid="0" op="" dtype="" >1.8454e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.1019e+01 7.8678e-06 1.3893e-01</hent>
<hent key="0380010000000000000028000000000C" call="MPI_Irecv" bytes="10240" orank="12" region="0" commid="0" count="17" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000028000000001C" call="MPI_Irecv" bytes="10240" orank="28" region="0" commid="0" count="121" tid="0" op="" dtype="" >1.5974e-05 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000500000002D4" call="MPI_Irecv" bytes="1280" orank="724" region="0" commid="0" count="286" tid="0" op="" dtype="" >1.3804e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0380010000000000000008000000000C" call="MPI_Irecv" bytes="2048" orank="12" region="0" commid="0" count="25" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000013" call="MPI_Irecv" bytes="2048" orank="19" region="0" commid="0" count="10" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000015" call="MPI_Irecv" bytes="2048" orank="21" region="0" commid="0" count="12" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000001C" call="MPI_Irecv" bytes="2048" orank="28" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000054" call="MPI_Irecv" bytes="2048" orank="84" region="0" commid="0" count="3464" tid="0" op="" dtype="" >1.0891e-03 0.0000e+00 5.4836e-05</hent>
<hent key="038001000000000000000800000002D4" call="MPI_Irecv" bytes="2048" orank="724" region="0" commid="0" count="3440" tid="0" op="" dtype="" >8.9645e-04 0.0000e+00 3.0994e-05</hent>
<hent key="0240010000000000000005000000000C" call="MPI_Isend" bytes="1280" orank="12" region="0" commid="0" count="195" tid="0" op="" dtype="" >2.8610e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000050000000013" call="MPI_Isend" bytes="1280" orank="19" region="0" commid="0" count="171" tid="0" op="" dtype="" >3.2568e-04 9.5367e-07 1.9789e-05</hent>
<hent key="02400100000000000000050000000015" call="MPI_Isend" bytes="1280" orank="21" region="0" commid="0" count="196" tid="0" op="" dtype="" >7.5412e-04 1.9073e-06 3.8862e-05</hent>
<hent key="0240010000000000000005000000001C" call="MPI_Isend" bytes="1280" orank="28" region="0" commid="0" count="287" tid="0" op="" dtype="" >1.4086e-03 9.5367e-07 1.4067e-05</hent>
<hent key="02400100000000000000050000000054" call="MPI_Isend" bytes="1280" orank="84" region="0" commid="0" count="322" tid="0" op="" dtype="" >2.1026e-03 3.8147e-06 2.2173e-05</hent>
<hent key="0240010000000000000028000000000C" call="MPI_Isend" bytes="10240" orank="12" region="0" commid="0" count="23" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000028000000001C" call="MPI_Isend" bytes="10240" orank="28" region="0" commid="0" count="260" tid="0" op="" dtype="" >2.6233e-03 3.8147e-06 2.4080e-05</hent>
<hent key="024001000000000000000500000002D4" call="MPI_Isend" bytes="1280" orank="724" region="0" commid="0" count="287" tid="0" op="" dtype="" >1.5678e-03 2.8610e-06 2.2173e-05</hent>
</hash>
<internal rank="20" log_i="1723713849.507148" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="21" mpi_size="768" stamp_init="1723713791.134009" stamp_final="1723713849.503787" username="apac4" allocationname="unknown" flags="0" pid="1717084" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.83698e+01" utime="5.03528e+01" stime="6.71816e+00" mtime="3.22882e+01" gflop="0.00000e+00" gbyte="3.77762e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22882e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007314255673147314d3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83087e+01" utime="5.03229e+01" stime="6.70927e+00" mtime="3.22882e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22882e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4989e+08" > 4.6358e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4889e+08" > 2.7537e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8480e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4342e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0054e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6096e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.0136e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5381e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6128e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9862e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="194" >
<hent key="0240010000000000000008000000000D" call="MPI_Isend" bytes="2048" orank="13" region="0" commid="0" count="19" tid="0" op="" dtype="" >6.5804e-05 9.5367e-07 1.4067e-05</hent>
<hent key="02400100000000000000080000000014" call="MPI_Isend" bytes="2048" orank="20" region="0" commid="0" count="12" tid="0" op="" dtype="" >2.8372e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000016" call="MPI_Isend" bytes="2048" orank="22" region="0" commid="0" count="8" tid="0" op="" dtype="" >3.5763e-05 3.0994e-06 6.9141e-06</hent>
<hent key="0240010000000000000008000000001D" call="MPI_Isend" bytes="2048" orank="29" region="0" commid="0" count="9" tid="0" op="" dtype="" >4.9829e-05 5.0068e-06 5.9605e-06</hent>
<hent key="02400100000000000000080000000055" call="MPI_Isend" bytes="2048" orank="85" region="0" commid="0" count="3477" tid="0" op="" dtype="" >1.0024e-02 9.5367e-07 2.0027e-05</hent>
<hent key="024001000000000000000800000002D5" call="MPI_Isend" bytes="2048" orank="725" region="0" commid="0" count="3455" tid="0" op="" dtype="" >1.0278e-02 9.5367e-07 4.3869e-05</hent>
<hent key="038001000000000000000E00000002D5" call="MPI_Irecv" bytes="3584" orank="725" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="0380010000000000000002800000000D" call="MPI_Irecv" bytes="640" orank="13" region="0" commid="0" count="374" tid="0" op="" dtype="" >9.0361e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000014" call="MPI_Irecv" bytes="640" orank="20" region="0" commid="0" count="427" tid="0" op="" dtype="" >1.5736e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000016" call="MPI_Irecv" bytes="640" orank="22" region="0" commid="0" count="424" tid="0" op="" dtype="" >9.5367e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000001D" call="MPI_Irecv" bytes="640" orank="29" region="0" commid="0" count="411" tid="0" op="" dtype="" >6.4373e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000028000000055" call="MPI_Irecv" bytes="640" orank="85" region="0" commid="0" count="272" tid="0" op="" dtype="" >6.6042e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="038001000000000000000280000002D5" call="MPI_Irecv" bytes="640" orank="725" region="0" commid="0" count="296" tid="0" op="" dtype="" >1.3280e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000002800000000D" call="MPI_Isend" bytes="640" orank="13" region="0" commid="0" count="406" tid="0" op="" dtype="" >4.9949e-04 0.0000e+00 8.8215e-06</hent>
<hent key="02400100000000000000028000000014" call="MPI_Isend" bytes="640" orank="20" region="0" commid="0" count="421" tid="0" op="" dtype="" >5.7507e-04 9.5367e-07 2.8610e-06</hent>
<hent key="02400100000000000000028000000016" call="MPI_Isend" bytes="640" orank="22" region="0" commid="0" count="435" tid="0" op="" dtype="" >1.3630e-03 1.9073e-06 1.5974e-05</hent>
<hent key="0240010000000000000002800000001D" call="MPI_Isend" bytes="640" orank="29" region="0" commid="0" count="456" tid="0" op="" dtype="" >2.2373e-03 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000028000000055" call="MPI_Isend" bytes="640" orank="85" region="0" commid="0" count="251" tid="0" op="" dtype="" >1.0030e-03 2.8610e-06 5.0068e-06</hent>
<hent key="024001000000000000000280000002D5" call="MPI_Isend" bytes="640" orank="725" region="0" commid="0" count="281" tid="0" op="" dtype="" >1.1456e-03 2.8610e-06 9.0599e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.2159e-05 1.2159e-05 1.2159e-05</hent>
<hent key="0380010000000000000001400000000D" call="MPI_Irecv" bytes="320" orank="13" region="0" commid="0" count="361" tid="0" op="" dtype="" >1.0061e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000014" call="MPI_Irecv" bytes="320" orank="20" region="0" commid="0" count="361" tid="0" op="" dtype="" >1.3041e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000016" call="MPI_Irecv" bytes="320" orank="22" region="0" commid="0" count="339" tid="0" op="" dtype="" >9.0361e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000001D" call="MPI_Irecv" bytes="320" orank="29" region="0" commid="0" count="353" tid="0" op="" dtype="" >7.2002e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000055" call="MPI_Irecv" bytes="320" orank="85" region="0" commid="0" count="169" tid="0" op="" dtype="" >5.2452e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002D5" call="MPI_Irecv" bytes="320" orank="725" region="0" commid="0" count="168" tid="0" op="" dtype="" >6.4611e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.8480e+00 0.0000e+00 1.1635e-01</hent>
<hent key="03800100000000000000400000000055" call="MPI_Irecv" bytes="16384" orank="85" region="0" commid="0" count="12517" tid="0" op="" dtype="" >2.7473e-03 0.0000e+00 5.0068e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.0027e-05 9.5367e-07 1.7881e-05</hent>
<hent key="038001000000000000004000000002D5" call="MPI_Irecv" bytes="16384" orank="725" region="0" commid="0" count="12618" tid="0" op="" dtype="" >3.7107e-03 0.0000e+00 6.9141e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 1.9073e-06</hent>
<hent key="0240010000000000000001400000000D" call="MPI_Isend" bytes="320" orank="13" region="0" commid="0" count="346" tid="0" op="" dtype="" >3.3450e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000014000000014" call="MPI_Isend" bytes="320" orank="20" region="0" commid="0" count="357" tid="0" op="" dtype="" >3.9649e-04 0.0000e+00 4.7684e-06</hent>
<hent key="02400100000000000000014000000016" call="MPI_Isend" bytes="320" orank="22" region="0" commid="0" count="353" tid="0" op="" dtype="" >9.8443e-04 1.9073e-06 5.9605e-06</hent>
<hent key="0240010000000000000001400000001D" call="MPI_Isend" bytes="320" orank="29" region="0" commid="0" count="387" tid="0" op="" dtype="" >1.8122e-03 3.8147e-06 8.1062e-06</hent>
<hent key="02400100000000000000014000000055" call="MPI_Isend" bytes="320" orank="85" region="0" commid="0" count="175" tid="0" op="" dtype="" >6.8235e-04 2.8610e-06 1.0014e-05</hent>
<hent key="0380010000000000000020000000000D" call="MPI_Irecv" bytes="8192" orank="13" region="0" commid="0" count="12679" tid="0" op="" dtype="" >4.0932e-03 0.0000e+00 8.8215e-06</hent>
<hent key="03800100000000000000200000000014" call="MPI_Irecv" bytes="8192" orank="20" region="0" commid="0" count="3531" tid="0" op="" dtype="" >1.0786e-03 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000200000000016" call="MPI_Irecv" bytes="8192" orank="22" region="0" commid="0" count="3440" tid="0" op="" dtype="" >4.4918e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000020000000001D" call="MPI_Irecv" bytes="8192" orank="29" region="0" commid="0" count="12448" tid="0" op="" dtype="" >1.3506e-03 0.0000e+00 2.1458e-06</hent>
<hent key="024001000000000000000140000002D5" call="MPI_Isend" bytes="320" orank="725" region="0" commid="0" count="171" tid="0" op="" dtype="" >6.8307e-04 2.8610e-06 7.8678e-06</hent>
<hent key="0380010000000000000000000000000D" call="MPI_Irecv" bytes="0" orank="13" region="0" commid="0" count="278" tid="0" op="" dtype="" >7.6294e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000014" call="MPI_Irecv" bytes="0" orank="20" region="0" commid="0" count="259" tid="0" op="" dtype="" >8.2016e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000016" call="MPI_Irecv" bytes="0" orank="22" region="0" commid="0" count="270" tid="0" op="" dtype="" >5.6982e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000001D" call="MPI_Irecv" bytes="0" orank="29" region="0" commid="0" count="262" tid="0" op="" dtype="" >4.1485e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000055" call="MPI_Irecv" bytes="0" orank="85" region="0" commid="0" count="133" tid="0" op="" dtype="" >3.4571e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000055" call="MPI_Isend" bytes="16384" orank="85" region="0" commid="0" count="12699" tid="0" op="" dtype="" >7.7229e-02 3.8147e-06 2.1935e-05</hent>
<hent key="038001000000000000000000000002D5" call="MPI_Irecv" bytes="0" orank="725" region="0" commid="0" count="147" tid="0" op="" dtype="" >3.6478e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002D5" call="MPI_Isend" bytes="16384" orank="725" region="0" commid="0" count="12685" tid="0" op="" dtype="" >8.4921e-02 2.8610e-06 5.5075e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.5722e-04 0.0000e+00 5.9390e-04</hent>
<hent key="0240010000000000000020000000000D" call="MPI_Isend" bytes="8192" orank="13" region="0" commid="0" count="12699" tid="0" op="" dtype="" >5.0976e-03 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000200000000014" call="MPI_Isend" bytes="8192" orank="20" region="0" commid="0" count="3778" tid="0" op="" dtype="" >4.5896e-03 0.0000e+00 1.2159e-05</hent>
<hent key="02400100000000000000200000000016" call="MPI_Isend" bytes="8192" orank="22" region="0" commid="0" count="3840" tid="0" op="" dtype="" >6.3648e-03 0.0000e+00 1.2875e-05</hent>
<hent key="0240010000000000000020000000001D" call="MPI_Isend" bytes="8192" orank="29" region="0" commid="0" count="12527" tid="0" op="" dtype="" >1.0146e-01 3.8147e-06 6.9141e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.3140e-05 3.3140e-05 3.3140e-05</hent>
<hent key="0240010000000000000000000000000D" call="MPI_Isend" bytes="0" orank="13" region="0" commid="0" count="253" tid="0" op="" dtype="" >1.5378e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000014" call="MPI_Isend" bytes="0" orank="20" region="0" commid="0" count="247" tid="0" op="" dtype="" >1.8501e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000016" call="MPI_Isend" bytes="0" orank="22" region="0" commid="0" count="246" tid="0" op="" dtype="" >5.1117e-04 9.5367e-07 5.0783e-05</hent>
<hent key="0240010000000000000000000000001D" call="MPI_Isend" bytes="0" orank="29" region="0" commid="0" count="257" tid="0" op="" dtype="" >1.0281e-03 1.9073e-06 8.1062e-06</hent>
<hent key="02400100000000000000000000000055" call="MPI_Isend" bytes="0" orank="85" region="0" commid="0" count="149" tid="0" op="" dtype="" >5.3072e-04 9.5367e-07 6.1989e-06</hent>
<hent key="024001000000000000000000000002D5" call="MPI_Isend" bytes="0" orank="725" region="0" commid="0" count="149" tid="0" op="" dtype="" >5.2571e-04 9.5367e-07 8.1062e-06</hent>
<hent key="0380010000000000000006000000000D" call="MPI_Irecv" bytes="1536" orank="13" region="0" commid="0" count="111" tid="0" op="" dtype="" >2.5034e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000014" call="MPI_Irecv" bytes="1536" orank="20" region="0" commid="0" count="80" tid="0" op="" dtype="" >2.2650e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000016" call="MPI_Irecv" bytes="1536" orank="22" region="0" commid="0" count="100" tid="0" op="" dtype="" >2.7657e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000001D" call="MPI_Irecv" bytes="1536" orank="29" region="0" commid="0" count="103" tid="0" op="" dtype="" >1.8358e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000055" call="MPI_Irecv" bytes="1536" orank="85" region="0" commid="0" count="232" tid="0" op="" dtype="" >6.2943e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002D5" call="MPI_Irecv" bytes="1536" orank="725" region="0" commid="0" count="206" tid="0" op="" dtype="" >1.0753e-04 0.0000e+00 1.9073e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0240010000000000000006000000000D" call="MPI_Isend" bytes="1536" orank="13" region="0" commid="0" count="107" tid="0" op="" dtype="" >2.0981e-04 9.5367e-07 1.3113e-05</hent>
<hent key="02400100000000000000060000000014" call="MPI_Isend" bytes="1536" orank="20" region="0" commid="0" count="72" tid="0" op="" dtype="" >1.4925e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000060000000016" call="MPI_Isend" bytes="1536" orank="22" region="0" commid="0" count="110" tid="0" op="" dtype="" >4.3368e-04 2.8610e-06 7.1526e-06</hent>
<hent key="0240010000000000000006000000001D" call="MPI_Isend" bytes="1536" orank="29" region="0" commid="0" count="83" tid="0" op="" dtype="" >4.4131e-04 4.7684e-06 6.9141e-06</hent>
<hent key="02400100000000000000060000000055" call="MPI_Isend" bytes="1536" orank="85" region="0" commid="0" count="221" tid="0" op="" dtype="" >9.7609e-04 2.8610e-06 8.8215e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002D5" call="MPI_Isend" bytes="1536" orank="725" region="0" commid="0" count="218" tid="0" op="" dtype="" >9.4318e-04 3.0994e-06 5.0068e-06</hent>
<hent key="038001000000000000000C0000000014" call="MPI_Irecv" bytes="3072" orank="20" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C0000000055" call="MPI_Irecv" bytes="3072" orank="85" region="0" commid="0" count="5" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002D5" call="MPI_Irecv" bytes="3072" orank="725" region="0" commid="0" count="6" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C000000000D" call="MPI_Isend" bytes="3072" orank="13" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.0014e-05 1.0014e-05 1.0014e-05</hent>
<hent key="024001000000000000000C0000000016" call="MPI_Isend" bytes="3072" orank="22" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="024001000000000000000C0000000055" call="MPI_Isend" bytes="3072" orank="85" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.4809e-05 4.7684e-06 5.0068e-06</hent>
<hent key="024001000000000000000C00000002D5" call="MPI_Isend" bytes="3072" orank="725" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.5974e-05 5.0068e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.4894e-04 4.4894e-04 4.4894e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.0252e-03 3.3903e-04 3.4499e-04</hent>
<hent key="0380010000000000000003800000000D" call="MPI_Irecv" bytes="896" orank="13" region="0" commid="0" count="313" tid="0" op="" dtype="" >1.0037e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000014" call="MPI_Irecv" bytes="896" orank="20" region="0" commid="0" count="2770" tid="0" op="" dtype="" >4.9496e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000016" call="MPI_Irecv" bytes="896" orank="22" region="0" commid="0" count="2793" tid="0" op="" dtype="" >5.0092e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000001D" call="MPI_Irecv" bytes="896" orank="29" region="0" commid="0" count="314" tid="0" op="" dtype="" >4.5538e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000055" call="MPI_Irecv" bytes="896" orank="85" region="0" commid="0" count="320" tid="0" op="" dtype="" >9.1314e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.6042e-05 6.6042e-05 6.6042e-05</hent>
<hent key="038001000000000000000380000002D5" call="MPI_Irecv" bytes="896" orank="725" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.4758e-04 0.0000e+00 8.1062e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000380000000055" call="MPI_Irecv" bytes="14336" orank="85" region="0" commid="0" count="182" tid="0" op="" dtype="" >2.7180e-05 0.0000e+00 2.8610e-06</hent>
<hent key="038001000000000000003800000002D5" call="MPI_Irecv" bytes="14336" orank="725" region="0" commid="0" count="81" tid="0" op="" dtype="" >1.8358e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000003800000000D" call="MPI_Isend" bytes="896" orank="13" region="0" commid="0" count="314" tid="0" op="" dtype="" >4.2653e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000038000000014" call="MPI_Isend" bytes="896" orank="20" region="0" commid="0" count="2714" tid="0" op="" dtype="" >2.2087e-03 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000038000000016" call="MPI_Isend" bytes="896" orank="22" region="0" commid="0" count="2659" tid="0" op="" dtype="" >2.7575e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000003800000001D" call="MPI_Isend" bytes="896" orank="29" region="0" commid="0" count="300" tid="0" op="" dtype="" >1.5273e-03 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000038000000055" call="MPI_Isend" bytes="896" orank="85" region="0" commid="0" count="320" tid="0" op="" dtype="" >1.3080e-03 2.8610e-06 9.0599e-06</hent>
<hent key="024001000000000000000380000002D5" call="MPI_Isend" bytes="896" orank="725" region="0" commid="0" count="347" tid="0" op="" dtype="" >1.4138e-03 2.8610e-06 8.8215e-06</hent>
<hent key="024001000000000000003800000002D5" call="MPI_Isend" bytes="14336" orank="725" region="0" commid="0" count="14" tid="0" op="" dtype="" >1.0109e-04 4.0531e-06 1.2159e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.3297e+00 1.5020e-05 1.3921e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.0902e-04 5.0902e-04 5.0902e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >3.5381e-04 3.5381e-04 3.5381e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >1.7550e-03 1.7550e-03 1.7550e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >7.0185e-02 3.4213e-04 6.4525e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.7697e-04 5.7697e-04 5.7697e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6096e+00 4.4799e-04 2.5596e-01</hent>
<hent key="0380010000000000000004000000000D" call="MPI_Irecv" bytes="1024" orank="13" region="0" commid="0" count="3392" tid="0" op="" dtype="" >5.8126e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000014" call="MPI_Irecv" bytes="1024" orank="20" region="0" commid="0" count="956" tid="0" op="" dtype="" >1.3828e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000016" call="MPI_Irecv" bytes="1024" orank="22" region="0" commid="0" count="930" tid="0" op="" dtype="" >1.6356e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000001D" call="MPI_Irecv" bytes="1024" orank="29" region="0" commid="0" count="3324" tid="0" op="" dtype="" >4.2129e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.0981e-05 0.0000e+00 1.0014e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.7166e-05 1.7166e-05 1.7166e-05</hent>
<hent key="0380010000000000000007000000000D" call="MPI_Irecv" bytes="1792" orank="13" region="0" commid="0" count="37" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000014" call="MPI_Irecv" bytes="1792" orank="20" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.1206e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000016" call="MPI_Irecv" bytes="1792" orank="22" region="0" commid="0" count="35" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000007000000001D" call="MPI_Irecv" bytes="1792" orank="29" region="0" commid="0" count="38" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000055" call="MPI_Irecv" bytes="1792" orank="85" region="0" commid="0" count="171" tid="0" op="" dtype="" >4.9829e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002D5" call="MPI_Irecv" bytes="1792" orank="725" region="0" commid="0" count="160" tid="0" op="" dtype="" >7.2718e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >1.0610e-04 9.5367e-07 5.0068e-05</hent>
<hent key="0240010000000000000004000000000D" call="MPI_Isend" bytes="1024" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.0800e-03 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000040000000014" call="MPI_Isend" bytes="1024" orank="20" region="0" commid="0" count="1014" tid="0" op="" dtype="" >7.3385e-04 0.0000e+00 4.2915e-05</hent>
<hent key="02400100000000000000040000000016" call="MPI_Isend" bytes="1024" orank="22" region="0" commid="0" count="1036" tid="0" op="" dtype="" >8.0371e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000004000000001D" call="MPI_Isend" bytes="1024" orank="29" region="0" commid="0" count="3348" tid="0" op="" dtype="" >8.9586e-03 9.5367e-07 6.1035e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >4.1008e-05 2.1458e-06 3.8862e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4332e+00 0.0000e+00 3.2495e+00</hent>
<hent key="038001000000000000000A000000000D" call="MPI_Irecv" bytes="2560" orank="13" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000014" call="MPI_Irecv" bytes="2560" orank="20" region="0" commid="0" count="5" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000016" call="MPI_Irecv" bytes="2560" orank="22" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000001D" call="MPI_Irecv" bytes="2560" orank="29" region="0" commid="0" count="8" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000055" call="MPI_Irecv" bytes="2560" orank="85" region="0" commid="0" count="52" tid="0" op="" dtype="" >1.3351e-05 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A00000002D5" call="MPI_Irecv" bytes="2560" orank="725" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.9312e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000007000000000D" call="MPI_Isend" bytes="1792" orank="13" region="0" commid="0" count="36" tid="0" op="" dtype="" >7.4148e-05 9.5367e-07 1.1921e-05</hent>
<hent key="02400100000000000000070000000014" call="MPI_Isend" bytes="1792" orank="20" region="0" commid="0" count="35" tid="0" op="" dtype="" >7.7963e-05 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000070000000016" call="MPI_Isend" bytes="1792" orank="22" region="0" commid="0" count="38" tid="0" op="" dtype="" >1.4210e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000007000000001D" call="MPI_Isend" bytes="1792" orank="29" region="0" commid="0" count="20" tid="0" op="" dtype="" >1.1206e-04 5.0068e-06 6.1989e-06</hent>
<hent key="02400100000000000000070000000055" call="MPI_Isend" bytes="1792" orank="85" region="0" commid="0" count="130" tid="0" op="" dtype="" >5.9104e-04 3.8147e-06 8.8215e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6128e-02 3.6128e-02 3.6128e-02</hent>
<hent key="024001000000000000000700000002D5" call="MPI_Isend" bytes="1792" orank="725" region="0" commid="0" count="132" tid="0" op="" dtype="" >5.7888e-04 1.9073e-06 5.9605e-06</hent>
<hent key="024001000000000000000A000000000D" call="MPI_Isend" bytes="2560" orank="13" region="0" commid="0" count="10" tid="0" op="" dtype="" >3.8862e-05 9.5367e-07 1.4067e-05</hent>
<hent key="024001000000000000000A0000000014" call="MPI_Isend" bytes="2560" orank="20" region="0" commid="0" count="9" tid="0" op="" dtype="" >2.1696e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000016" call="MPI_Isend" bytes="2560" orank="22" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.5272e-05 3.0994e-06 5.0068e-06</hent>
<hent key="024001000000000000000A000000001D" call="MPI_Isend" bytes="2560" orank="29" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.8849e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000055" call="MPI_Isend" bytes="2560" orank="85" region="0" commid="0" count="42" tid="0" op="" dtype="" >2.0194e-04 3.8147e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.8900e-04 5.6028e-05 2.2817e-04</hent>
<hent key="024001000000000000000A00000002D5" call="MPI_Isend" bytes="2560" orank="725" region="0" commid="0" count="38" tid="0" op="" dtype="" >1.7810e-04 3.8147e-06 6.1989e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.7161e-03 6.4111e-04 1.0750e-03</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6001e-05 3.6001e-05 3.6001e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.2452e-06 2.1458e-06 3.0994e-06</hent>
<hent key="0380010000000000000000040000000D" call="MPI_Irecv" bytes="4" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.5303e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000014" call="MPI_Irecv" bytes="4" orank="20" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.0810e-04 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000000400000016" call="MPI_Irecv" bytes="4" orank="22" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.7255e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000001D" call="MPI_Irecv" bytes="4" orank="29" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.7779e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000055" call="MPI_Irecv" bytes="4" orank="85" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.0323e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.3309e-03 4.2391e-04 1.1899e-03</hent>
<hent key="038001000000000000001C0000000014" call="MPI_Irecv" bytes="7168" orank="20" region="0" commid="0" count="9168" tid="0" op="" dtype="" >2.6772e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000001C0000000016" call="MPI_Irecv" bytes="7168" orank="22" region="0" commid="0" count="9259" tid="0" op="" dtype="" >1.2226e-03 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000004000002D5" call="MPI_Irecv" bytes="4" orank="725" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.1430e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000000040000000D" call="MPI_Isend" bytes="4" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1961e-03 0.0000e+00 3.0041e-05</hent>
<hent key="02400100000000000000000400000014" call="MPI_Isend" bytes="4" orank="20" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.5128e-03 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000400000016" call="MPI_Isend" bytes="4" orank="22" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.1751e-03 0.0000e+00 5.8889e-05</hent>
<hent key="0240010000000000000000040000001D" call="MPI_Isend" bytes="4" orank="29" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.8621e-02 3.8147e-06 1.8120e-05</hent>
<hent key="02400100000000000000000400000055" call="MPI_Isend" bytes="4" orank="85" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7094e-02 2.8610e-06 1.3113e-05</hent>
<hent key="024001000000000000001C0000000014" call="MPI_Isend" bytes="7168" orank="20" region="0" commid="0" count="8921" tid="0" op="" dtype="" >2.0604e-02 9.5367e-07 1.1921e-05</hent>
<hent key="024001000000000000001C0000000016" call="MPI_Isend" bytes="7168" orank="22" region="0" commid="0" count="8859" tid="0" op="" dtype="" >2.6807e-02 9.5367e-07 1.8120e-05</hent>
<hent key="024001000000000000000004000002D5" call="MPI_Isend" bytes="4" orank="725" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6641e-02 2.8610e-06 1.0991e-04</hent>
<hent key="0380010000000000000005000000000D" call="MPI_Irecv" bytes="1280" orank="13" region="0" commid="0" count="209" tid="0" op="" dtype="" >5.6028e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000014" call="MPI_Irecv" bytes="1280" orank="20" region="0" commid="0" count="196" tid="0" op="" dtype="" >5.5075e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000016" call="MPI_Irecv" bytes="1280" orank="22" region="0" commid="0" count="192" tid="0" op="" dtype="" >4.1723e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000001D" call="MPI_Irecv" bytes="1280" orank="29" region="0" commid="0" count="273" tid="0" op="" dtype="" >5.4121e-05 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000050000000055" call="MPI_Irecv" bytes="1280" orank="85" region="0" commid="0" count="313" tid="0" op="" dtype="" >1.0991e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.1459e+01 6.9141e-06 1.3911e-01</hent>
<hent key="0380010000000000000028000000000D" call="MPI_Irecv" bytes="10240" orank="13" region="0" commid="0" count="20" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000028000000001D" call="MPI_Irecv" bytes="10240" orank="29" region="0" commid="0" count="251" tid="0" op="" dtype="" >1.8120e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002D5" call="MPI_Irecv" bytes="1280" orank="725" region="0" commid="0" count="274" tid="0" op="" dtype="" >1.3304e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000D" call="MPI_Irecv" bytes="2048" orank="13" region="0" commid="0" count="18" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000014" call="MPI_Irecv" bytes="2048" orank="20" region="0" commid="0" count="14" tid="0" op="" dtype="" >6.4373e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000016" call="MPI_Irecv" bytes="2048" orank="22" region="0" commid="0" count="10" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000001D" call="MPI_Irecv" bytes="2048" orank="29" region="0" commid="0" count="11" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000055" call="MPI_Irecv" bytes="2048" orank="85" region="0" commid="0" count="3430" tid="0" op="" dtype="" >7.1979e-04 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000800000002D5" call="MPI_Irecv" bytes="2048" orank="725" region="0" commid="0" count="3434" tid="0" op="" dtype="" >5.3525e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000005000000000D" call="MPI_Isend" bytes="1280" orank="13" region="0" commid="0" count="207" tid="0" op="" dtype="" >3.2306e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000050000000014" call="MPI_Isend" bytes="1280" orank="20" region="0" commid="0" count="216" tid="0" op="" dtype="" >4.1223e-04 9.5367e-07 7.1526e-06</hent>
<hent key="02400100000000000000050000000016" call="MPI_Isend" bytes="1280" orank="22" region="0" commid="0" count="205" tid="0" op="" dtype="" >7.3767e-04 1.9073e-06 5.0068e-06</hent>
<hent key="0240010000000000000005000000001D" call="MPI_Isend" bytes="1280" orank="29" region="0" commid="0" count="232" tid="0" op="" dtype="" >1.0805e-03 9.5367e-07 9.7752e-06</hent>
<hent key="02400100000000000000050000000055" call="MPI_Isend" bytes="1280" orank="85" region="0" commid="0" count="325" tid="0" op="" dtype="" >1.3669e-03 2.8610e-06 8.8215e-06</hent>
<hent key="0240010000000000000028000000001D" call="MPI_Isend" bytes="10240" orank="29" region="0" commid="0" count="172" tid="0" op="" dtype="" >1.3254e-03 5.0068e-06 1.5020e-05</hent>
<hent key="024001000000000000000500000002D5" call="MPI_Isend" bytes="1280" orank="725" region="0" commid="0" count="303" tid="0" op="" dtype="" >1.2820e-03 2.8610e-06 8.1062e-06</hent>
</hash>
<internal rank="21" log_i="1723713849.503787" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="22" mpi_size="768" stamp_init="1723713791.136978" stamp_final="1723713849.501173" username="apac4" allocationname="unknown" flags="0" pid="1717085" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.83642e+01" utime="4.84409e+01" stime="7.54779e+00" mtime="3.19179e+01" gflop="0.00000e+00" gbyte="3.76911e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19179e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000211421147e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83050e+01" utime="4.84105e+01" stime="7.54017e+00" mtime="3.19179e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19179e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4849e+08" > 5.6558e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4934e+08" > 3.9661e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3463e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4298e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.9605e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5286e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6089e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.3390e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4881e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6175e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8884e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="197" >
<hent key="0240010000000000000008000000000E" call="MPI_Isend" bytes="2048" orank="14" region="0" commid="0" count="12" tid="0" op="" dtype="" >4.1723e-05 9.5367e-07 1.1921e-05</hent>
<hent key="02400100000000000000080000000015" call="MPI_Isend" bytes="2048" orank="21" region="0" commid="0" count="10" tid="0" op="" dtype="" >2.5511e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000017" call="MPI_Isend" bytes="2048" orank="23" region="0" commid="0" count="18" tid="0" op="" dtype="" >7.7009e-05 3.8147e-06 6.1989e-06</hent>
<hent key="0240010000000000000008000000001E" call="MPI_Isend" bytes="2048" orank="30" region="0" commid="0" count="12" tid="0" op="" dtype="" >7.0810e-05 5.0068e-06 9.7752e-06</hent>
<hent key="02400100000000000000080000000056" call="MPI_Isend" bytes="2048" orank="86" region="0" commid="0" count="3472" tid="0" op="" dtype="" >1.2016e-02 9.5367e-07 3.6955e-05</hent>
<hent key="024001000000000000000800000002D6" call="MPI_Isend" bytes="2048" orank="726" region="0" commid="0" count="3466" tid="0" op="" dtype="" >1.2614e-02 9.5367e-07 4.1008e-05</hent>
<hent key="038001000000000000000E00000002D6" call="MPI_Irecv" bytes="3584" orank="726" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000E" call="MPI_Irecv" bytes="640" orank="14" region="0" commid="0" count="386" tid="0" op="" dtype="" >1.3995e-04 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000028000000015" call="MPI_Irecv" bytes="640" orank="21" region="0" commid="0" count="435" tid="0" op="" dtype="" >1.8978e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000028000000017" call="MPI_Irecv" bytes="640" orank="23" region="0" commid="0" count="399" tid="0" op="" dtype="" >1.4329e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000002800000001E" call="MPI_Irecv" bytes="640" orank="30" region="0" commid="0" count="398" tid="0" op="" dtype="" >9.9182e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000028000000056" call="MPI_Irecv" bytes="640" orank="86" region="0" commid="0" count="268" tid="0" op="" dtype="" >9.0361e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="038001000000000000000280000002D6" call="MPI_Irecv" bytes="640" orank="726" region="0" commid="0" count="283" tid="0" op="" dtype="" >9.3460e-05 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000002800000000E" call="MPI_Isend" bytes="640" orank="14" region="0" commid="0" count="390" tid="0" op="" dtype="" >5.5552e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000028000000015" call="MPI_Isend" bytes="640" orank="21" region="0" commid="0" count="424" tid="0" op="" dtype="" >5.9724e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000028000000017" call="MPI_Isend" bytes="640" orank="23" region="0" commid="0" count="394" tid="0" op="" dtype="" >1.2765e-03 1.9073e-06 2.0981e-05</hent>
<hent key="0240010000000000000002800000001E" call="MPI_Isend" bytes="640" orank="30" region="0" commid="0" count="405" tid="0" op="" dtype="" >2.0037e-03 3.0994e-06 1.5020e-05</hent>
<hent key="02400100000000000000028000000056" call="MPI_Isend" bytes="640" orank="86" region="0" commid="0" count="307" tid="0" op="" dtype="" >1.3762e-03 2.8610e-06 1.2875e-05</hent>
<hent key="024001000000000000000280000002D6" call="MPI_Isend" bytes="640" orank="726" region="0" commid="0" count="286" tid="0" op="" dtype="" >1.2386e-03 2.8610e-06 1.4067e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="0380010000000000000001400000000E" call="MPI_Irecv" bytes="320" orank="14" region="0" commid="0" count="396" tid="0" op="" dtype="" >1.1754e-04 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000014000000015" call="MPI_Irecv" bytes="320" orank="21" region="0" commid="0" count="353" tid="0" op="" dtype="" >1.4424e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000017" call="MPI_Irecv" bytes="320" orank="23" region="0" commid="0" count="358" tid="0" op="" dtype="" >1.2970e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000001400000001E" call="MPI_Irecv" bytes="320" orank="30" region="0" commid="0" count="358" tid="0" op="" dtype="" >8.1301e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000014000000056" call="MPI_Irecv" bytes="320" orank="86" region="0" commid="0" count="185" tid="0" op="" dtype="" >7.3195e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002D6" call="MPI_Irecv" bytes="320" orank="726" region="0" commid="0" count="172" tid="0" op="" dtype="" >5.8413e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.3463e+00 0.0000e+00 1.1624e-01</hent>
<hent key="03800100000000000000400000000056" call="MPI_Irecv" bytes="16384" orank="86" region="0" commid="0" count="12534" tid="0" op="" dtype="" >3.9525e-03 0.0000e+00 2.9802e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-05 0.0000e+00 1.7881e-05</hent>
<hent key="038001000000000000004000000002D6" call="MPI_Irecv" bytes="16384" orank="726" region="0" commid="0" count="12691" tid="0" op="" dtype="" >8.2166e-03 0.0000e+00 5.3167e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000001400000000E" call="MPI_Isend" bytes="320" orank="14" region="0" commid="0" count="370" tid="0" op="" dtype="" >4.4107e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000014000000015" call="MPI_Isend" bytes="320" orank="21" region="0" commid="0" count="339" tid="0" op="" dtype="" >3.7503e-04 0.0000e+00 2.8610e-06</hent>
<hent key="02400100000000000000014000000017" call="MPI_Isend" bytes="320" orank="23" region="0" commid="0" count="394" tid="0" op="" dtype="" >1.1036e-03 1.1921e-06 8.8215e-06</hent>
<hent key="0240010000000000000001400000001E" call="MPI_Isend" bytes="320" orank="30" region="0" commid="0" count="346" tid="0" op="" dtype="" >1.6329e-03 2.8610e-06 1.5974e-05</hent>
<hent key="02400100000000000000014000000056" call="MPI_Isend" bytes="320" orank="86" region="0" commid="0" count="174" tid="0" op="" dtype="" >7.4410e-04 2.8610e-06 1.4067e-05</hent>
<hent key="0380010000000000000020000000000E" call="MPI_Irecv" bytes="8192" orank="14" region="0" commid="0" count="12668" tid="0" op="" dtype="" >5.0273e-03 0.0000e+00 1.6928e-05</hent>
<hent key="03800100000000000000200000000015" call="MPI_Irecv" bytes="8192" orank="21" region="0" commid="0" count="3840" tid="0" op="" dtype="" >1.4668e-03 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000200000000017" call="MPI_Irecv" bytes="8192" orank="23" region="0" commid="0" count="3750" tid="0" op="" dtype="" >6.5899e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000020000000001E" call="MPI_Irecv" bytes="8192" orank="30" region="0" commid="0" count="12639" tid="0" op="" dtype="" >2.9583e-03 0.0000e+00 3.7909e-05</hent>
<hent key="024001000000000000000140000002D6" call="MPI_Isend" bytes="320" orank="726" region="0" commid="0" count="152" tid="0" op="" dtype="" >6.3467e-04 2.8610e-06 1.2875e-05</hent>
<hent key="0380010000000000000000000000000E" call="MPI_Irecv" bytes="0" orank="14" region="0" commid="0" count="274" tid="0" op="" dtype="" >7.5817e-05 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000000000000015" call="MPI_Irecv" bytes="0" orank="21" region="0" commid="0" count="246" tid="0" op="" dtype="" >9.3937e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000000000000017" call="MPI_Irecv" bytes="0" orank="23" region="0" commid="0" count="263" tid="0" op="" dtype="" >7.4863e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000001E" call="MPI_Irecv" bytes="0" orank="30" region="0" commid="0" count="261" tid="0" op="" dtype="" >5.5790e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000000000000056" call="MPI_Irecv" bytes="0" orank="86" region="0" commid="0" count="146" tid="0" op="" dtype="" >3.6955e-05 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000400000000056" call="MPI_Isend" bytes="16384" orank="86" region="0" commid="0" count="12676" tid="0" op="" dtype="" >1.0851e-01 3.8147e-06 1.0705e-04</hent>
<hent key="038001000000000000000000000002D6" call="MPI_Irecv" bytes="0" orank="726" region="0" commid="0" count="147" tid="0" op="" dtype="" >5.3406e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002D6" call="MPI_Isend" bytes="16384" orank="726" region="0" commid="0" count="12691" tid="0" op="" dtype="" >1.2622e-01 2.8610e-06 1.3804e-04</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.4196e-04 0.0000e+00 6.0821e-04</hent>
<hent key="0240010000000000000020000000000E" call="MPI_Isend" bytes="8192" orank="14" region="0" commid="0" count="12611" tid="0" op="" dtype="" >9.1875e-03 0.0000e+00 4.1008e-05</hent>
<hent key="02400100000000000000200000000015" call="MPI_Isend" bytes="8192" orank="21" region="0" commid="0" count="3440" tid="0" op="" dtype="" >4.2424e-03 0.0000e+00 4.1962e-05</hent>
<hent key="02400100000000000000200000000017" call="MPI_Isend" bytes="8192" orank="23" region="0" commid="0" count="3009" tid="0" op="" dtype="" >5.3985e-03 0.0000e+00 1.2875e-05</hent>
<hent key="0240010000000000000020000000001E" call="MPI_Isend" bytes="8192" orank="30" region="0" commid="0" count="12667" tid="0" op="" dtype="" >1.1516e-01 3.8147e-06 5.1975e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0783e-05 5.0783e-05 5.0783e-05</hent>
<hent key="0240010000000000000000000000000E" call="MPI_Isend" bytes="0" orank="14" region="0" commid="0" count="280" tid="0" op="" dtype="" >2.7084e-04 0.0000e+00 8.8215e-06</hent>
<hent key="02400100000000000000000000000015" call="MPI_Isend" bytes="0" orank="21" region="0" commid="0" count="270" tid="0" op="" dtype="" >1.6975e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000017" call="MPI_Isend" bytes="0" orank="23" region="0" commid="0" count="235" tid="0" op="" dtype="" >5.0569e-04 9.5367e-07 7.0095e-05</hent>
<hent key="0240010000000000000000000000001E" call="MPI_Isend" bytes="0" orank="30" region="0" commid="0" count="274" tid="0" op="" dtype="" >1.1084e-03 9.5367e-07 1.5020e-05</hent>
<hent key="02400100000000000000000000000056" call="MPI_Isend" bytes="0" orank="86" region="0" commid="0" count="148" tid="0" op="" dtype="" >5.5647e-04 9.5367e-07 6.9141e-06</hent>
<hent key="024001000000000000000000000002D6" call="MPI_Isend" bytes="0" orank="726" region="0" commid="0" count="152" tid="0" op="" dtype="" >5.7268e-04 1.9073e-06 1.1206e-05</hent>
<hent key="0380010000000000000006000000000E" call="MPI_Irecv" bytes="1536" orank="14" region="0" commid="0" count="93" tid="0" op="" dtype="" >2.0504e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000015" call="MPI_Irecv" bytes="1536" orank="21" region="0" commid="0" count="110" tid="0" op="" dtype="" >4.4584e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000017" call="MPI_Irecv" bytes="1536" orank="23" region="0" commid="0" count="100" tid="0" op="" dtype="" >3.2663e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000006000000001E" call="MPI_Irecv" bytes="1536" orank="30" region="0" commid="0" count="80" tid="0" op="" dtype="" >2.1219e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000060000000056" call="MPI_Irecv" bytes="1536" orank="86" region="0" commid="0" count="243" tid="0" op="" dtype="" >1.0324e-04 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000600000002D6" call="MPI_Irecv" bytes="1536" orank="726" region="0" commid="0" count="224" tid="0" op="" dtype="" >9.3699e-05 0.0000e+00 7.1526e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0240010000000000000006000000000E" call="MPI_Isend" bytes="1536" orank="14" region="0" commid="0" count="98" tid="0" op="" dtype="" >2.3484e-04 9.5367e-07 1.4067e-05</hent>
<hent key="02400100000000000000060000000015" call="MPI_Isend" bytes="1536" orank="21" region="0" commid="0" count="100" tid="0" op="" dtype="" >2.2316e-04 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000060000000017" call="MPI_Isend" bytes="1536" orank="23" region="0" commid="0" count="97" tid="0" op="" dtype="" >3.6883e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000006000000001E" call="MPI_Isend" bytes="1536" orank="30" region="0" commid="0" count="84" tid="0" op="" dtype="" >4.5466e-04 4.0531e-06 1.2875e-05</hent>
<hent key="02400100000000000000060000000056" call="MPI_Isend" bytes="1536" orank="86" region="0" commid="0" count="189" tid="0" op="" dtype="" >9.3174e-04 3.8147e-06 1.3113e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002D6" call="MPI_Isend" bytes="1536" orank="726" region="0" commid="0" count="222" tid="0" op="" dtype="" >1.0974e-03 3.0994e-06 1.0967e-05</hent>
<hent key="038001000000000000000C0000000015" call="MPI_Irecv" bytes="3072" orank="21" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000C0000000056" call="MPI_Irecv" bytes="3072" orank="86" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002D6" call="MPI_Irecv" bytes="3072" orank="726" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000056" call="MPI_Isend" bytes="3072" orank="86" region="0" commid="0" count="9" tid="0" op="" dtype="" >4.6253e-05 4.0531e-06 6.1989e-06</hent>
<hent key="024001000000000000000C00000002D6" call="MPI_Isend" bytes="3072" orank="726" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.4067e-05 4.0531e-06 5.0068e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.6587e-04 4.6587e-04 4.6587e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.0750e-03 3.5191e-04 3.6716e-04</hent>
<hent key="0380010000000000000003800000000E" call="MPI_Irecv" bytes="896" orank="14" region="0" commid="0" count="308" tid="0" op="" dtype="" >9.2268e-05 0.0000e+00 3.8147e-06</hent>
<hent key="03800100000000000000038000000015" call="MPI_Irecv" bytes="896" orank="21" region="0" commid="0" count="2659" tid="0" op="" dtype="" >5.1308e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000038000000017" call="MPI_Irecv" bytes="896" orank="23" region="0" commid="0" count="2751" tid="0" op="" dtype="" >5.7435e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000003800000001E" call="MPI_Irecv" bytes="896" orank="30" region="0" commid="0" count="356" tid="0" op="" dtype="" >8.5115e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000056" call="MPI_Irecv" bytes="896" orank="86" region="0" commid="0" count="331" tid="0" op="" dtype="" >1.1897e-04 0.0000e+00 5.9605e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.1022e-05 5.1022e-05 5.1022e-05</hent>
<hent key="038001000000000000000380000002D6" call="MPI_Irecv" bytes="896" orank="726" region="0" commid="0" count="310" tid="0" op="" dtype="" >1.0467e-04 0.0000e+00 8.1062e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000380000000056" call="MPI_Irecv" bytes="14336" orank="86" region="0" commid="0" count="165" tid="0" op="" dtype="" >3.5763e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000003800000002D6" call="MPI_Irecv" bytes="14336" orank="726" region="0" commid="0" count="8" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0240010000000000000003800000000E" call="MPI_Isend" bytes="896" orank="14" region="0" commid="0" count="319" tid="0" op="" dtype="" >5.3692e-04 9.5367e-07 2.8133e-05</hent>
<hent key="02400100000000000000038000000015" call="MPI_Isend" bytes="896" orank="21" region="0" commid="0" count="2793" tid="0" op="" dtype="" >1.9653e-03 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000038000000017" call="MPI_Isend" bytes="896" orank="23" region="0" commid="0" count="2928" tid="0" op="" dtype="" >2.9559e-03 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000003800000001E" call="MPI_Isend" bytes="896" orank="30" region="0" commid="0" count="347" tid="0" op="" dtype="" >1.8413e-03 3.8147e-06 1.7166e-05</hent>
<hent key="02400100000000000000038000000056" call="MPI_Isend" bytes="896" orank="86" region="0" commid="0" count="323" tid="0" op="" dtype="" >1.5705e-03 2.8610e-06 2.0981e-05</hent>
<hent key="024001000000000000000380000002D6" call="MPI_Isend" bytes="896" orank="726" region="0" commid="0" count="315" tid="0" op="" dtype="" >1.4019e-03 2.8610e-06 1.2875e-05</hent>
<hent key="02400100000000000000380000000056" call="MPI_Isend" bytes="14336" orank="86" region="0" commid="0" count="23" tid="0" op="" dtype="" >2.9588e-04 5.9605e-06 3.8147e-05</hent>
<hent key="024001000000000000003800000002D6" call="MPI_Isend" bytes="14336" orank="726" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.4114e-04 7.1526e-06 4.5061e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.7500e+00 1.1206e-05 1.3920e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.0211e-04 5.0211e-04 5.0211e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >3.4881e-04 3.4881e-04 3.4881e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.2349e-03 2.2349e-03 2.2349e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >4.3988e-02 3.3116e-04 3.8456e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >1.3804e-04 1.3804e-04 1.3804e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6089e+00 4.8208e-04 2.5592e-01</hent>
<hent key="0380010000000000000004000000000E" call="MPI_Irecv" bytes="1024" orank="14" region="0" commid="0" count="3390" tid="0" op="" dtype="" >6.2203e-04 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000040000000015" call="MPI_Irecv" bytes="1024" orank="21" region="0" commid="0" count="1036" tid="0" op="" dtype="" >1.8311e-04 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000040000000017" call="MPI_Irecv" bytes="1024" orank="23" region="0" commid="0" count="990" tid="0" op="" dtype="" >1.8382e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000004000000001E" call="MPI_Irecv" bytes="1024" orank="30" region="0" commid="0" count="3378" tid="0" op="" dtype="" >5.7983e-04 0.0000e+00 2.0981e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >5.9605e-05 1.9073e-06 2.6941e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="0380010000000000000007000000000E" call="MPI_Irecv" bytes="1792" orank="14" region="0" commid="0" count="28" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000015" call="MPI_Irecv" bytes="1792" orank="21" region="0" commid="0" count="38" tid="0" op="" dtype="" >1.5736e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000017" call="MPI_Irecv" bytes="1792" orank="23" region="0" commid="0" count="46" tid="0" op="" dtype="" >2.7180e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000007000000001E" call="MPI_Irecv" bytes="1792" orank="30" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.1206e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000056" call="MPI_Irecv" bytes="1792" orank="86" region="0" commid="0" count="149" tid="0" op="" dtype="" >5.4836e-05 0.0000e+00 9.0599e-06</hent>
<hent key="038001000000000000000700000002D6" call="MPI_Irecv" bytes="1792" orank="726" region="0" commid="0" count="147" tid="0" op="" dtype="" >5.4836e-05 0.0000e+00 7.8678e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >9.4414e-05 9.5367e-07 5.1975e-05</hent>
<hent key="0240010000000000000004000000000E" call="MPI_Isend" bytes="1024" orank="14" region="0" commid="0" count="3376" tid="0" op="" dtype="" >2.5022e-03 0.0000e+00 2.1935e-05</hent>
<hent key="02400100000000000000040000000015" call="MPI_Isend" bytes="1024" orank="21" region="0" commid="0" count="930" tid="0" op="" dtype="" >5.6458e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000040000000017" call="MPI_Isend" bytes="1024" orank="23" region="0" commid="0" count="788" tid="0" op="" dtype="" >6.0868e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000004000000001E" call="MPI_Isend" bytes="1024" orank="30" region="0" commid="0" count="3390" tid="0" op="" dtype="" >9.2635e-03 9.5367e-07 2.4080e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4288e+00 0.0000e+00 3.2485e+00</hent>
<hent key="038001000000000000000A000000000E" call="MPI_Irecv" bytes="2560" orank="14" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000015" call="MPI_Irecv" bytes="2560" orank="21" region="0" commid="0" count="6" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000017" call="MPI_Irecv" bytes="2560" orank="23" region="0" commid="0" count="8" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000001E" call="MPI_Irecv" bytes="2560" orank="30" region="0" commid="0" count="5" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000056" call="MPI_Irecv" bytes="2560" orank="86" region="0" commid="0" count="43" tid="0" op="" dtype="" >1.3828e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002D6" call="MPI_Irecv" bytes="2560" orank="726" region="0" commid="0" count="48" tid="0" op="" dtype="" >2.0981e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000007000000000E" call="MPI_Isend" bytes="1792" orank="14" region="0" commid="0" count="29" tid="0" op="" dtype="" >6.3658e-05 9.5367e-07 1.1921e-05</hent>
<hent key="02400100000000000000070000000015" call="MPI_Isend" bytes="1792" orank="21" region="0" commid="0" count="35" tid="0" op="" dtype="" >7.7248e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000017" call="MPI_Isend" bytes="1792" orank="23" region="0" commid="0" count="33" tid="0" op="" dtype="" >1.3089e-04 2.8610e-06 5.9605e-06</hent>
<hent key="0240010000000000000007000000001E" call="MPI_Isend" bytes="1792" orank="30" region="0" commid="0" count="47" tid="0" op="" dtype="" >2.6107e-04 4.0531e-06 8.8215e-06</hent>
<hent key="02400100000000000000070000000056" call="MPI_Isend" bytes="1792" orank="86" region="0" commid="0" count="133" tid="0" op="" dtype="" >6.6161e-04 9.5367e-07 2.3842e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6175e-02 3.6175e-02 3.6175e-02</hent>
<hent key="024001000000000000000700000002D6" call="MPI_Isend" bytes="1792" orank="726" region="0" commid="0" count="130" tid="0" op="" dtype="" >6.2418e-04 9.5367e-07 1.8120e-05</hent>
<hent key="024001000000000000000A000000000E" call="MPI_Isend" bytes="2560" orank="14" region="0" commid="0" count="14" tid="0" op="" dtype="" >8.7976e-05 9.5367e-07 1.6212e-05</hent>
<hent key="024001000000000000000A0000000015" call="MPI_Isend" bytes="2560" orank="21" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.0252e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000017" call="MPI_Isend" bytes="2560" orank="23" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.6703e-05 2.8610e-06 4.0531e-06</hent>
<hent key="024001000000000000000A000000001E" call="MPI_Isend" bytes="2560" orank="30" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.6716e-05 5.0068e-06 8.8215e-06</hent>
<hent key="024001000000000000000A0000000056" call="MPI_Isend" bytes="2560" orank="86" region="0" commid="0" count="53" tid="0" op="" dtype="" >2.8706e-04 4.0531e-06 1.5974e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >5.1403e-04 5.9128e-05 2.4080e-04</hent>
<hent key="024001000000000000000A00000002D6" call="MPI_Isend" bytes="2560" orank="726" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.5511e-04 4.0531e-06 1.8120e-05</hent>
<hent key="03800100000000000000100000000056" call="MPI_Irecv" bytes="4096" orank="86" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.1921e-06 1.1921e-06 1.1921e-06</hent>
<hent key="038001000000000000001000000002D6" call="MPI_Irecv" bytes="4096" orank="726" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.8001e-03 6.6495e-04 1.1351e-03</hent>
<hent key="024001000000000000001000000002D6" call="MPI_Isend" bytes="4096" orank="726" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-05 3.0994e-05 3.0994e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 9.5367e-07 1.9073e-06</hent>
<hent key="0380010000000000000000040000000E" call="MPI_Irecv" bytes="4" orank="14" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.4840e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000000400000015" call="MPI_Irecv" bytes="4" orank="21" region="0" commid="0" count="3398" tid="0" op="" dtype="" >9.9778e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000017" call="MPI_Irecv" bytes="4" orank="23" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.4932e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000000040000001E" call="MPI_Irecv" bytes="4" orank="30" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.1144e-04 0.0000e+00 1.5020e-05</hent>
<hent key="03800100000000000000000400000056" call="MPI_Irecv" bytes="4" orank="86" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0889e-03 0.0000e+00 2.0981e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.4831e-03 4.3988e-04 1.2581e-03</hent>
<hent key="038001000000000000001C0000000015" call="MPI_Irecv" bytes="7168" orank="21" region="0" commid="0" count="8859" tid="0" op="" dtype="" >3.1996e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000001C0000000017" call="MPI_Irecv" bytes="7168" orank="23" region="0" commid="0" count="8949" tid="0" op="" dtype="" >1.6148e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000004000002D6" call="MPI_Irecv" bytes="4" orank="726" region="0" commid="0" count="3398" tid="0" op="" dtype="" >9.6107e-04 0.0000e+00 2.5034e-05</hent>
<hent key="0240010000000000000000040000000E" call="MPI_Isend" bytes="4" orank="14" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4584e-03 0.0000e+00 3.0041e-05</hent>
<hent key="02400100000000000000000400000015" call="MPI_Isend" bytes="4" orank="21" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2956e-03 0.0000e+00 4.9829e-05</hent>
<hent key="02400100000000000000000400000017" call="MPI_Isend" bytes="4" orank="23" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3267e-03 0.0000e+00 2.3842e-05</hent>
<hent key="0240010000000000000000040000001E" call="MPI_Isend" bytes="4" orank="30" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.9834e-02 3.8147e-06 2.4080e-05</hent>
<hent key="02400100000000000000000400000056" call="MPI_Isend" bytes="4" orank="86" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.9732e-02 2.8610e-06 3.2902e-05</hent>
<hent key="024001000000000000001C0000000015" call="MPI_Isend" bytes="7168" orank="21" region="0" commid="0" count="9259" tid="0" op="" dtype="" >2.0492e-02 9.5367e-07 1.5020e-05</hent>
<hent key="024001000000000000001C0000000017" call="MPI_Isend" bytes="7168" orank="23" region="0" commid="0" count="9690" tid="0" op="" dtype="" >2.7401e-02 9.5367e-07 3.8862e-05</hent>
<hent key="024001000000000000000004000002D6" call="MPI_Isend" bytes="4" orank="726" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.9032e-02 2.8610e-06 1.2302e-04</hent>
<hent key="0380010000000000000005000000000E" call="MPI_Irecv" bytes="1280" orank="14" region="0" commid="0" count="207" tid="0" op="" dtype="" >7.6056e-05 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000050000000015" call="MPI_Irecv" bytes="1280" orank="21" region="0" commid="0" count="205" tid="0" op="" dtype="" >7.9632e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000017" call="MPI_Irecv" bytes="1280" orank="23" region="0" commid="0" count="164" tid="0" op="" dtype="" >7.9632e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000005000000001E" call="MPI_Irecv" bytes="1280" orank="30" region="0" commid="0" count="209" tid="0" op="" dtype="" >3.5763e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000056" call="MPI_Irecv" bytes="1280" orank="86" region="0" commid="0" count="308" tid="0" op="" dtype="" >1.1468e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.1087e+01 7.8678e-06 1.3908e-01</hent>
<hent key="0380010000000000000028000000000E" call="MPI_Irecv" bytes="10240" orank="14" region="0" commid="0" count="31" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000028000000001E" call="MPI_Irecv" bytes="10240" orank="30" region="0" commid="0" count="60" tid="0" op="" dtype="" >3.5286e-05 0.0000e+00 2.5034e-05</hent>
<hent key="038001000000000000000500000002D6" call="MPI_Irecv" bytes="1280" orank="726" region="0" commid="0" count="294" tid="0" op="" dtype="" >1.2016e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000008000000000E" call="MPI_Irecv" bytes="2048" orank="14" region="0" commid="0" count="14" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000015" call="MPI_Irecv" bytes="2048" orank="21" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000017" call="MPI_Irecv" bytes="2048" orank="23" region="0" commid="0" count="18" tid="0" op="" dtype="" >6.4373e-06 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000008000000001E" call="MPI_Irecv" bytes="2048" orank="30" region="0" commid="0" count="15" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000056" call="MPI_Irecv" bytes="2048" orank="86" region="0" commid="0" count="3414" tid="0" op="" dtype="" >7.6127e-04 0.0000e+00 1.3113e-05</hent>
<hent key="038001000000000000000800000002D6" call="MPI_Irecv" bytes="2048" orank="726" region="0" commid="0" count="3462" tid="0" op="" dtype="" >8.0180e-04 0.0000e+00 2.0981e-05</hent>
<hent key="0240010000000000000005000000000E" call="MPI_Isend" bytes="1280" orank="14" region="0" commid="0" count="209" tid="0" op="" dtype="" >3.5763e-04 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000050000000015" call="MPI_Isend" bytes="1280" orank="21" region="0" commid="0" count="192" tid="0" op="" dtype="" >3.6073e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000017" call="MPI_Isend" bytes="1280" orank="23" region="0" commid="0" count="203" tid="0" op="" dtype="" >7.4863e-04 1.9073e-06 6.9141e-06</hent>
<hent key="0240010000000000000005000000001E" call="MPI_Isend" bytes="1280" orank="30" region="0" commid="0" count="186" tid="0" op="" dtype="" >9.5916e-04 9.5367e-07 1.5020e-05</hent>
<hent key="02400100000000000000050000000056" call="MPI_Isend" bytes="1280" orank="86" region="0" commid="0" count="289" tid="0" op="" dtype="" >1.4203e-03 3.8147e-06 1.5974e-05</hent>
<hent key="0240010000000000000028000000000E" call="MPI_Isend" bytes="10240" orank="14" region="0" commid="0" count="88" tid="0" op="" dtype="" >8.3208e-05 0.0000e+00 1.5020e-05</hent>
<hent key="0240010000000000000028000000001E" call="MPI_Isend" bytes="10240" orank="30" region="0" commid="0" count="32" tid="0" op="" dtype="" >2.8801e-04 5.0068e-06 1.8120e-05</hent>
<hent key="024001000000000000000500000002D6" call="MPI_Isend" bytes="1280" orank="726" region="0" commid="0" count="327" tid="0" op="" dtype="" >1.5190e-03 2.8610e-06 1.2875e-05</hent>
</hash>
<internal rank="22" log_i="1723713849.501173" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="23" mpi_size="768" stamp_init="1723713791.141642" stamp_final="1723713849.509754" username="apac4" allocationname="unknown" flags="0" pid="1717086" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >187554</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="5.83681e+01" utime="4.99585e+01" stime="7.03405e+00" mtime="3.25676e+01" gflop="0.00000e+00" gbyte="3.77853e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25676e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000191419145b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83107e+01" utime="4.99246e+01" stime="7.02955e+00" mtime="3.25676e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25676e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4972e+08" > 4.7839e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4832e+08" > 2.8675e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6605e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4389e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7643e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0054e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6090e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.2408e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0112e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6113e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9308e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="193" >
<hent key="0240010000000000000008000000000F" call="MPI_Isend" bytes="2048" orank="15" region="0" commid="0" count="12" tid="0" op="" dtype="" >3.7670e-05 9.5367e-07 1.1206e-05</hent>
<hent key="02400100000000000000080000000010" call="MPI_Isend" bytes="2048" orank="16" region="0" commid="0" count="13" tid="0" op="" dtype="" >5.4598e-05 2.8610e-06 6.1989e-06</hent>
<hent key="02400100000000000000080000000016" call="MPI_Isend" bytes="2048" orank="22" region="0" commid="0" count="18" tid="0" op="" dtype="" >4.4823e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000008000000001F" call="MPI_Isend" bytes="2048" orank="31" region="0" commid="0" count="12" tid="0" op="" dtype="" >6.6757e-05 5.0068e-06 5.9605e-06</hent>
<hent key="02400100000000000000080000000057" call="MPI_Isend" bytes="2048" orank="87" region="0" commid="0" count="3472" tid="0" op="" dtype="" >1.0207e-02 9.5367e-07 2.8849e-05</hent>
<hent key="024001000000000000000800000002D7" call="MPI_Isend" bytes="2048" orank="727" region="0" commid="0" count="3463" tid="0" op="" dtype="" >1.0336e-02 9.5367e-07 6.3896e-05</hent>
<hent key="024001000000000000000E0000000057" call="MPI_Isend" bytes="3584" orank="87" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0380010000000000000002800000000F" call="MPI_Irecv" bytes="640" orank="15" region="0" commid="0" count="386" tid="0" op="" dtype="" >9.7275e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000028000000010" call="MPI_Irecv" bytes="640" orank="16" region="0" commid="0" count="401" tid="0" op="" dtype="" >8.7261e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000016" call="MPI_Irecv" bytes="640" orank="22" region="0" commid="0" count="394" tid="0" op="" dtype="" >1.7834e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000002800000001F" call="MPI_Irecv" bytes="640" orank="31" region="0" commid="0" count="429" tid="0" op="" dtype="" >1.0109e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000057" call="MPI_Irecv" bytes="640" orank="87" region="0" commid="0" count="304" tid="0" op="" dtype="" >1.0824e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="038001000000000000000280000002D7" call="MPI_Irecv" bytes="640" orank="727" region="0" commid="0" count="296" tid="0" op="" dtype="" >8.1778e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000002800000000F" call="MPI_Isend" bytes="640" orank="15" region="0" commid="0" count="410" tid="0" op="" dtype="" >5.7793e-04 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000028000000010" call="MPI_Isend" bytes="640" orank="16" region="0" commid="0" count="383" tid="0" op="" dtype="" >1.2209e-03 1.9073e-06 7.1526e-06</hent>
<hent key="02400100000000000000028000000016" call="MPI_Isend" bytes="640" orank="22" region="0" commid="0" count="399" tid="0" op="" dtype="" >6.0177e-04 9.5367e-07 5.9605e-06</hent>
<hent key="0240010000000000000002800000001F" call="MPI_Isend" bytes="640" orank="31" region="0" commid="0" count="379" tid="0" op="" dtype="" >1.8427e-03 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000028000000057" call="MPI_Isend" bytes="640" orank="87" region="0" commid="0" count="261" tid="0" op="" dtype="" >1.1485e-03 2.8610e-06 1.0014e-05</hent>
<hent key="024001000000000000000280000002D7" call="MPI_Isend" bytes="640" orank="727" region="0" commid="0" count="273" tid="0" op="" dtype="" >1.1272e-03 2.8610e-06 8.1062e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1206e-05 1.1206e-05 1.1206e-05</hent>
<hent key="0380010000000000000001400000000F" call="MPI_Irecv" bytes="320" orank="15" region="0" commid="0" count="346" tid="0" op="" dtype="" >7.7724e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000010" call="MPI_Irecv" bytes="320" orank="16" region="0" commid="0" count="368" tid="0" op="" dtype="" >8.3447e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000016" call="MPI_Irecv" bytes="320" orank="22" region="0" commid="0" count="394" tid="0" op="" dtype="" >1.5044e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000001F" call="MPI_Irecv" bytes="320" orank="31" region="0" commid="0" count="331" tid="0" op="" dtype="" >8.0824e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000057" call="MPI_Irecv" bytes="320" orank="87" region="0" commid="0" count="165" tid="0" op="" dtype="" >4.6968e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002D7" call="MPI_Irecv" bytes="320" orank="727" region="0" commid="0" count="174" tid="0" op="" dtype="" >4.1246e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.6605e+00 0.0000e+00 1.3909e-01</hent>
<hent key="03800100000000000000400000000057" call="MPI_Irecv" bytes="16384" orank="87" region="0" commid="0" count="12659" tid="0" op="" dtype="" >3.3128e-03 0.0000e+00 7.1526e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.8120e-05 0.0000e+00 1.8120e-05</hent>
<hent key="038001000000000000004000000002D7" call="MPI_Irecv" bytes="16384" orank="727" region="0" commid="0" count="12690" tid="0" op="" dtype="" >4.2477e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 1.9073e-06</hent>
<hent key="0240010000000000000001400000000F" call="MPI_Isend" bytes="320" orank="15" region="0" commid="0" count="349" tid="0" op="" dtype="" >4.1509e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000014000000010" call="MPI_Isend" bytes="320" orank="16" region="0" commid="0" count="348" tid="0" op="" dtype="" >9.8491e-04 1.9073e-06 7.1526e-06</hent>
<hent key="02400100000000000000014000000016" call="MPI_Isend" bytes="320" orank="22" region="0" commid="0" count="358" tid="0" op="" dtype="" >4.3297e-04 9.5367e-07 5.9605e-06</hent>
<hent key="0240010000000000000001400000001F" call="MPI_Isend" bytes="320" orank="31" region="0" commid="0" count="389" tid="0" op="" dtype="" >1.8268e-03 3.8147e-06 1.8120e-05</hent>
<hent key="02400100000000000000014000000057" call="MPI_Isend" bytes="320" orank="87" region="0" commid="0" count="174" tid="0" op="" dtype="" >7.3314e-04 3.0994e-06 5.0068e-06</hent>
<hent key="0380010000000000000020000000000F" call="MPI_Irecv" bytes="8192" orank="15" region="0" commid="0" count="12667" tid="0" op="" dtype="" >3.5610e-03 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000200000000010" call="MPI_Irecv" bytes="8192" orank="16" region="0" commid="0" count="3421" tid="0" op="" dtype="" >5.2905e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000200000000016" call="MPI_Irecv" bytes="8192" orank="22" region="0" commid="0" count="3009" tid="0" op="" dtype="" >9.4509e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000020000000001F" call="MPI_Irecv" bytes="8192" orank="31" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.2987e-03 0.0000e+00 5.0068e-06</hent>
<hent key="024001000000000000000140000002D7" call="MPI_Isend" bytes="320" orank="727" region="0" commid="0" count="203" tid="0" op="" dtype="" >8.2207e-04 2.8610e-06 8.8215e-06</hent>
<hent key="0380010000000000000000000000000F" call="MPI_Irecv" bytes="0" orank="15" region="0" commid="0" count="245" tid="0" op="" dtype="" >5.9366e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000010" call="MPI_Irecv" bytes="0" orank="16" region="0" commid="0" count="250" tid="0" op="" dtype="" >5.2691e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000016" call="MPI_Irecv" bytes="0" orank="22" region="0" commid="0" count="235" tid="0" op="" dtype="" >9.3937e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000000000000001F" call="MPI_Irecv" bytes="0" orank="31" region="0" commid="0" count="252" tid="0" op="" dtype="" >4.9353e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000057" call="MPI_Irecv" bytes="0" orank="87" region="0" commid="0" count="145" tid="0" op="" dtype="" >3.4094e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000057" call="MPI_Isend" bytes="16384" orank="87" region="0" commid="0" count="12665" tid="0" op="" dtype="" >8.4026e-02 4.0531e-06 6.0797e-05</hent>
<hent key="038001000000000000000000000002D7" call="MPI_Irecv" bytes="0" orank="727" region="0" commid="0" count="140" tid="0" op="" dtype="" >2.2173e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002D7" call="MPI_Isend" bytes="16384" orank="727" region="0" commid="0" count="12683" tid="0" op="" dtype="" >9.0115e-02 2.8610e-06 1.5974e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >7.6175e-04 0.0000e+00 5.9390e-04</hent>
<hent key="0240010000000000000020000000000F" call="MPI_Isend" bytes="8192" orank="15" region="0" commid="0" count="12674" tid="0" op="" dtype="" >6.0949e-03 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000200000000010" call="MPI_Isend" bytes="8192" orank="16" region="0" commid="0" count="4033" tid="0" op="" dtype="" >7.0617e-03 0.0000e+00 2.1935e-05</hent>
<hent key="02400100000000000000200000000016" call="MPI_Isend" bytes="8192" orank="22" region="0" commid="0" count="3750" tid="0" op="" dtype="" >4.3273e-03 0.0000e+00 1.0586e-04</hent>
<hent key="0240010000000000000020000000001F" call="MPI_Isend" bytes="8192" orank="31" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.0659e-01 3.8147e-06 3.6955e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.6015e-05 4.6015e-05 4.6015e-05</hent>
<hent key="0240010000000000000000000000000F" call="MPI_Isend" bytes="0" orank="15" region="0" commid="0" count="252" tid="0" op="" dtype="" >1.9741e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000010" call="MPI_Isend" bytes="0" orank="16" region="0" commid="0" count="258" tid="0" op="" dtype="" >5.1260e-04 0.0000e+00 5.4121e-05</hent>
<hent key="02400100000000000000000000000016" call="MPI_Isend" bytes="0" orank="22" region="0" commid="0" count="263" tid="0" op="" dtype="" >2.1434e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000000000000001F" call="MPI_Isend" bytes="0" orank="31" region="0" commid="0" count="259" tid="0" op="" dtype="" >1.0531e-03 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000000000000057" call="MPI_Isend" bytes="0" orank="87" region="0" commid="0" count="154" tid="0" op="" dtype="" >5.7125e-04 9.5367e-07 5.0068e-06</hent>
<hent key="024001000000000000000000000002D7" call="MPI_Isend" bytes="0" orank="727" region="0" commid="0" count="137" tid="0" op="" dtype="" >5.0163e-04 9.5367e-07 6.9141e-06</hent>
<hent key="0380010000000000000006000000000F" call="MPI_Irecv" bytes="1536" orank="15" region="0" commid="0" count="93" tid="0" op="" dtype="" >1.8358e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000010" call="MPI_Irecv" bytes="1536" orank="16" region="0" commid="0" count="100" tid="0" op="" dtype="" >2.5749e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000016" call="MPI_Irecv" bytes="1536" orank="22" region="0" commid="0" count="97" tid="0" op="" dtype="" >3.9339e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000001F" call="MPI_Irecv" bytes="1536" orank="31" region="0" commid="0" count="95" tid="0" op="" dtype="" >2.3365e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000057" call="MPI_Irecv" bytes="1536" orank="87" region="0" commid="0" count="213" tid="0" op="" dtype="" >9.8944e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002D7" call="MPI_Irecv" bytes="1536" orank="727" region="0" commid="0" count="194" tid="0" op="" dtype="" >5.6982e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0240010000000000000006000000000F" call="MPI_Isend" bytes="1536" orank="15" region="0" commid="0" count="95" tid="0" op="" dtype="" >1.6975e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000060000000010" call="MPI_Isend" bytes="1536" orank="16" region="0" commid="0" count="109" tid="0" op="" dtype="" >4.1771e-04 2.8610e-06 5.9605e-06</hent>
<hent key="02400100000000000000060000000016" call="MPI_Isend" bytes="1536" orank="22" region="0" commid="0" count="100" tid="0" op="" dtype="" >2.1911e-04 9.5367e-07 1.1921e-05</hent>
<hent key="0240010000000000000006000000001F" call="MPI_Isend" bytes="1536" orank="31" region="0" commid="0" count="93" tid="0" op="" dtype="" >4.9496e-04 4.7684e-06 7.8678e-06</hent>
<hent key="02400100000000000000060000000057" call="MPI_Isend" bytes="1536" orank="87" region="0" commid="0" count="213" tid="0" op="" dtype="" >1.0145e-03 3.8147e-06 9.0599e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002D7" call="MPI_Isend" bytes="1536" orank="727" region="0" commid="0" count="201" tid="0" op="" dtype="" >9.1648e-04 3.8147e-06 9.0599e-06</hent>
<hent key="038001000000000000000C0000000057" call="MPI_Irecv" bytes="3072" orank="87" region="0" commid="0" count="10" tid="0" op="" dtype="" >6.6757e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002D7" call="MPI_Irecv" bytes="3072" orank="727" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000C000000000F" call="MPI_Isend" bytes="3072" orank="15" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000C000000001F" call="MPI_Isend" bytes="3072" orank="31" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="024001000000000000000C0000000057" call="MPI_Isend" bytes="3072" orank="87" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.0054e-05 5.0068e-06 6.1989e-06</hent>
<hent key="024001000000000000000C00000002D7" call="MPI_Isend" bytes="3072" orank="727" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.3855e-05 5.0068e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.7302e-04 4.7302e-04 4.7302e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.1239e-03 3.7003e-04 3.7694e-04</hent>
<hent key="0380010000000000000003800000000F" call="MPI_Irecv" bytes="896" orank="15" region="0" commid="0" count="360" tid="0" op="" dtype="" >9.5129e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000010" call="MPI_Irecv" bytes="896" orank="16" region="0" commid="0" count="2802" tid="0" op="" dtype="" >5.0354e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000016" call="MPI_Irecv" bytes="896" orank="22" region="0" commid="0" count="2928" tid="0" op="" dtype="" >5.2571e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000003800000001F" call="MPI_Irecv" bytes="896" orank="31" region="0" commid="0" count="328" tid="0" op="" dtype="" >9.3222e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000038000000057" call="MPI_Irecv" bytes="896" orank="87" region="0" commid="0" count="332" tid="0" op="" dtype="" >1.1683e-04 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.7949e-05 6.7949e-05 6.7949e-05</hent>
<hent key="038001000000000000000380000002D7" call="MPI_Irecv" bytes="896" orank="727" region="0" commid="0" count="324" tid="0" op="" dtype="" >8.2493e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="03800100000000000000380000000057" call="MPI_Irecv" bytes="14336" orank="87" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.0252e-05 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000003800000002D7" call="MPI_Irecv" bytes="14336" orank="727" region="0" commid="0" count="9" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.9073e-06</hent>
<hent key="0240010000000000000003800000000F" call="MPI_Isend" bytes="896" orank="15" region="0" commid="0" count="310" tid="0" op="" dtype="" >4.7684e-04 9.5367e-07 7.1526e-06</hent>
<hent key="02400100000000000000038000000010" call="MPI_Isend" bytes="896" orank="16" region="0" commid="0" count="2667" tid="0" op="" dtype="" >2.8241e-03 0.0000e+00 2.2173e-05</hent>
<hent key="02400100000000000000038000000016" call="MPI_Isend" bytes="896" orank="22" region="0" commid="0" count="2751" tid="0" op="" dtype="" >2.0778e-03 0.0000e+00 1.8120e-05</hent>
<hent key="0240010000000000000003800000001F" call="MPI_Isend" bytes="896" orank="31" region="0" commid="0" count="340" tid="0" op="" dtype="" >1.6830e-03 3.8147e-06 8.8215e-06</hent>
<hent key="02400100000000000000038000000057" call="MPI_Isend" bytes="896" orank="87" region="0" commid="0" count="321" tid="0" op="" dtype="" >1.4660e-03 3.8147e-06 1.0014e-05</hent>
<hent key="024001000000000000000380000002D7" call="MPI_Isend" bytes="896" orank="727" region="0" commid="0" count="342" tid="0" op="" dtype="" >1.4324e-03 2.8610e-06 9.0599e-06</hent>
<hent key="02400100000000000000380000000057" call="MPI_Isend" bytes="14336" orank="87" region="0" commid="0" count="34" tid="0" op="" dtype="" >2.1291e-04 5.0068e-06 8.1062e-06</hent>
<hent key="024001000000000000003800000002D7" call="MPI_Isend" bytes="14336" orank="727" region="0" commid="0" count="16" tid="0" op="" dtype="" >1.1969e-04 5.0068e-06 9.7752e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >8.0613e+00 7.8678e-06 1.3407e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >5.3096e-04 5.3096e-04 5.3096e-04</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >3.0112e-04 3.0112e-04 3.0112e-04</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >1.5030e-03 1.5030e-03 1.5030e-03</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >6.4510e-02 3.2592e-04 5.9345e-02</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >5.5003e-04 5.5003e-04 5.5003e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.6090e+00 4.4322e-04 2.5589e-01</hent>
<hent key="0380010000000000000004000000000F" call="MPI_Irecv" bytes="1024" orank="15" region="0" commid="0" count="3390" tid="0" op="" dtype="" >5.5814e-04 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000040000000010" call="MPI_Irecv" bytes="1024" orank="16" region="0" commid="0" count="932" tid="0" op="" dtype="" >1.3304e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000016" call="MPI_Irecv" bytes="1024" orank="22" region="0" commid="0" count="788" tid="0" op="" dtype="" >1.0562e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000004000000001F" call="MPI_Irecv" bytes="1024" orank="31" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.7612e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.7643e-05 0.0000e+00 7.8678e-06</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0380010000000000000007000000000F" call="MPI_Irecv" bytes="1792" orank="15" region="0" commid="0" count="39" tid="0" op="" dtype="" >8.5831e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000010" call="MPI_Irecv" bytes="1792" orank="16" region="0" commid="0" count="41" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000016" call="MPI_Irecv" bytes="1792" orank="22" region="0" commid="0" count="33" tid="0" op="" dtype="" >1.4544e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000001F" call="MPI_Irecv" bytes="1792" orank="31" region="0" commid="0" count="32" tid="0" op="" dtype="" >8.8215e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000057" call="MPI_Irecv" bytes="1792" orank="87" region="0" commid="0" count="162" tid="0" op="" dtype="" >7.2002e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002D7" call="MPI_Irecv" bytes="1792" orank="727" region="0" commid="0" count="136" tid="0" op="" dtype="" >3.5763e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >7.5579e-05 9.5367e-07 5.0068e-05</hent>
<hent key="0240010000000000000004000000000F" call="MPI_Isend" bytes="1024" orank="15" region="0" commid="0" count="3392" tid="0" op="" dtype="" >2.2421e-03 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000040000000010" call="MPI_Isend" bytes="1024" orank="16" region="0" commid="0" count="1074" tid="0" op="" dtype="" >7.8225e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000040000000016" call="MPI_Isend" bytes="1024" orank="22" region="0" commid="0" count="990" tid="0" op="" dtype="" >6.2013e-04 0.0000e+00 1.1921e-05</hent>
<hent key="0240010000000000000004000000001F" call="MPI_Isend" bytes="1024" orank="31" region="0" commid="0" count="3398" tid="0" op="" dtype="" >9.2523e-03 9.5367e-07 9.0599e-06</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >5.2452e-06 2.1458e-06 3.0994e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >4.4380e+00 0.0000e+00 3.2516e+00</hent>
<hent key="038001000000000000000A000000000F" call="MPI_Irecv" bytes="2560" orank="15" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000010" call="MPI_Irecv" bytes="2560" orank="16" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000016" call="MPI_Irecv" bytes="2560" orank="22" region="0" commid="0" count="7" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000001F" call="MPI_Irecv" bytes="2560" orank="31" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000057" call="MPI_Irecv" bytes="2560" orank="87" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.0504e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002D7" call="MPI_Irecv" bytes="2560" orank="727" region="0" commid="0" count="40" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0240010000000000000007000000000F" call="MPI_Isend" bytes="1792" orank="15" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.2374e-04 9.5367e-07 1.4067e-05</hent>
<hent key="02400100000000000000070000000010" call="MPI_Isend" bytes="1792" orank="16" region="0" commid="0" count="44" tid="0" op="" dtype="" >1.7762e-04 2.8610e-06 7.8678e-06</hent>
<hent key="02400100000000000000070000000016" call="MPI_Isend" bytes="1792" orank="22" region="0" commid="0" count="46" tid="0" op="" dtype="" >1.0777e-04 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000007000000001F" call="MPI_Isend" bytes="1792" orank="31" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.7571e-04 4.7684e-06 6.1989e-06</hent>
<hent key="02400100000000000000070000000057" call="MPI_Isend" bytes="1792" orank="87" region="0" commid="0" count="145" tid="0" op="" dtype="" >6.9118e-04 9.5367e-07 7.1526e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6113e-02 3.6113e-02 3.6113e-02</hent>
<hent key="024001000000000000000700000002D7" call="MPI_Isend" bytes="1792" orank="727" region="0" commid="0" count="119" tid="0" op="" dtype="" >5.3382e-04 2.1458e-06 6.1989e-06</hent>
<hent key="024001000000000000000A000000000F" call="MPI_Isend" bytes="2560" orank="15" region="0" commid="0" count="3" tid="0" op="" dtype="" >5.7220e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000A0000000010" call="MPI_Isend" bytes="2560" orank="16" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.8120e-05 4.0531e-06 5.0068e-06</hent>
<hent key="024001000000000000000A0000000016" call="MPI_Isend" bytes="2560" orank="22" region="0" commid="0" count="8" tid="0" op="" dtype="" >2.0266e-05 1.9073e-06 4.0531e-06</hent>
<hent key="024001000000000000000A000000001F" call="MPI_Isend" bytes="2560" orank="31" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.8849e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000057" call="MPI_Isend" bytes="2560" orank="87" region="0" commid="0" count="45" tid="0" op="" dtype="" >2.3341e-04 3.8147e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >5.2810e-04 6.0081e-05 2.4605e-04</hent>
<hent key="024001000000000000000A00000002D7" call="MPI_Isend" bytes="2560" orank="727" region="0" commid="0" count="54" tid="0" op="" dtype="" >2.6512e-04 3.8147e-06 6.1989e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.4980e-03 6.9594e-04 8.0204e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6001e-05 3.6001e-05 3.6001e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.2452e-06 2.1458e-06 3.0994e-06</hent>
<hent key="0380010000000000000000040000000F" call="MPI_Irecv" bytes="4" orank="15" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.7639e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000010" call="MPI_Irecv" bytes="4" orank="16" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.3167e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000016" call="MPI_Irecv" bytes="4" orank="22" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.3709e-04 0.0000e+00 9.5129e-05</hent>
<hent key="0380010000000000000000040000001F" call="MPI_Irecv" bytes="4" orank="31" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.3620e-04 0.0000e+00 1.3113e-05</hent>
<hent key="03800100000000000000000400000057" call="MPI_Irecv" bytes="4" orank="87" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.2445e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.6168e-03 4.5800e-04 1.2929e-03</hent>
<hent key="038001000000000000001C0000000010" call="MPI_Irecv" bytes="7168" orank="16" region="0" commid="0" count="9278" tid="0" op="" dtype="" >1.4222e-03 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000001C0000000016" call="MPI_Irecv" bytes="7168" orank="22" region="0" commid="0" count="9690" tid="0" op="" dtype="" >2.9879e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000004000002D7" call="MPI_Irecv" bytes="4" orank="727" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.4564e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000000040000000F" call="MPI_Isend" bytes="4" orank="15" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3480e-03 0.0000e+00 2.7895e-05</hent>
<hent key="02400100000000000000000400000010" call="MPI_Isend" bytes="4" orank="16" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.4016e-03 0.0000e+00 5.6982e-05</hent>
<hent key="02400100000000000000000400000016" call="MPI_Isend" bytes="4" orank="22" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4827e-03 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000040000001F" call="MPI_Isend" bytes="4" orank="31" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.4500e-02 3.8147e-06 2.1935e-05</hent>
<hent key="02400100000000000000000400000057" call="MPI_Isend" bytes="4" orank="87" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.8212e-02 2.8610e-06 1.2875e-05</hent>
<hent key="024001000000000000001C0000000010" call="MPI_Isend" bytes="7168" orank="16" region="0" commid="0" count="8666" tid="0" op="" dtype="" >2.5705e-02 9.5367e-07 1.5020e-05</hent>
<hent key="024001000000000000001C0000000016" call="MPI_Isend" bytes="7168" orank="22" region="0" commid="0" count="8949" tid="0" op="" dtype="" >1.9506e-02 9.5367e-07 1.0014e-05</hent>
<hent key="024001000000000000000004000002D7" call="MPI_Isend" bytes="4" orank="727" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7470e-02 2.8610e-06 1.1492e-04</hent>
<hent key="0380010000000000000005000000000F" call="MPI_Irecv" bytes="1280" orank="15" region="0" commid="0" count="219" tid="0" op="" dtype="" >5.9366e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000050000000010" call="MPI_Irecv" bytes="1280" orank="16" region="0" commid="0" count="188" tid="0" op="" dtype="" >4.2439e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000016" call="MPI_Irecv" bytes="1280" orank="22" region="0" commid="0" count="203" tid="0" op="" dtype="" >8.9169e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000001F" call="MPI_Irecv" bytes="1280" orank="31" region="0" commid="0" count="211" tid="0" op="" dtype="" >4.4584e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000057" call="MPI_Irecv" bytes="1280" orank="87" region="0" commid="0" count="276" tid="0" op="" dtype="" >1.0920e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.1180e+01 6.9141e-06 1.3907e-01</hent>
<hent key="0380010000000000000028000000000F" call="MPI_Irecv" bytes="10240" orank="15" region="0" commid="0" count="32" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002D7" call="MPI_Irecv" bytes="1280" orank="727" region="0" commid="0" count="313" tid="0" op="" dtype="" >8.6069e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000F" call="MPI_Irecv" bytes="2048" orank="15" region="0" commid="0" count="15" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000010" call="MPI_Irecv" bytes="2048" orank="16" region="0" commid="0" count="13" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000016" call="MPI_Irecv" bytes="2048" orank="22" region="0" commid="0" count="18" tid="0" op="" dtype="" >8.8215e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000001F" call="MPI_Irecv" bytes="2048" orank="31" region="0" commid="0" count="18" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000057" call="MPI_Irecv" bytes="2048" orank="87" region="0" commid="0" count="3450" tid="0" op="" dtype="" >7.3695e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002D7" call="MPI_Irecv" bytes="2048" orank="727" region="0" commid="0" count="3478" tid="0" op="" dtype="" >5.2953e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000005000000000F" call="MPI_Isend" bytes="1280" orank="15" region="0" commid="0" count="224" tid="0" op="" dtype="" >3.8028e-04 0.0000e+00 1.2875e-05</hent>
<hent key="02400100000000000000050000000010" call="MPI_Isend" bytes="1280" orank="16" region="0" commid="0" count="197" tid="0" op="" dtype="" >7.2455e-04 2.8610e-06 9.0599e-06</hent>
<hent key="02400100000000000000050000000016" call="MPI_Isend" bytes="1280" orank="22" region="0" commid="0" count="164" tid="0" op="" dtype="" >3.3665e-04 9.5367e-07 9.0599e-06</hent>
<hent key="0240010000000000000005000000001F" call="MPI_Isend" bytes="1280" orank="31" region="0" commid="0" count="189" tid="0" op="" dtype="" >9.6750e-04 3.8147e-06 7.1526e-06</hent>
<hent key="02400100000000000000050000000057" call="MPI_Isend" bytes="1280" orank="87" region="0" commid="0" count="304" tid="0" op="" dtype="" >1.4250e-03 3.8147e-06 9.0599e-06</hent>
<hent key="0240010000000000000028000000000F" call="MPI_Isend" bytes="10240" orank="15" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.7405e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000500000002D7" call="MPI_Isend" bytes="1280" orank="727" region="0" commid="0" count="299" tid="0" op="" dtype="" >1.2763e-03 2.8610e-06 9.0599e-06</hent>
</hash>
<internal rank="23" log_i="1723713849.509754" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="24" mpi_size="768" stamp_init="1723713791.118935" stamp_final="1723713849.511531" username="apac4" allocationname="unknown" flags="0" pid="619364" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83926e+01" utime="4.29571e+01" stime="1.28719e+01" mtime="3.22181e+01" gflop="0.00000e+00" gbyte="3.83911e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22181e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000e614d555e614e614b0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83073e+01" utime="4.29255e+01" stime="1.28624e+01" mtime="3.22181e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22181e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5041e+08" > 6.3760e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4763e+08" > 3.9987e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7849e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.5312e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.4809e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6075e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.8668e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9612e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6491e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8571e+01 </func>
</region>
</regions>
<internal rank="24" log_i="1723713849.511531" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="25" mpi_size="768" stamp_init="1723713791.119716" stamp_final="1723713849.504904" username="apac4" allocationname="unknown" flags="0" pid="619365" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83852e+01" utime="5.01444e+01" stime="6.93454e+00" mtime="3.25130e+01" gflop="0.00000e+00" gbyte="3.78391e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25130e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001a14dd551a141a14fd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82935e+01" utime="5.01110e+01" stime="6.92595e+00" mtime="3.25130e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25130e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4729e+08" > 4.5865e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4984e+08" > 3.0940e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4079e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4200e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2650e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0934e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6065e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0013e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5392e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6518e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9433e+01 </func>
</region>
</regions>
<internal rank="25" log_i="1723713849.504904" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="26" mpi_size="768" stamp_init="1723713791.118927" stamp_final="1723713849.498360" username="apac4" allocationname="unknown" flags="0" pid="619366" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83794e+01" utime="4.76241e+01" stime="7.53858e+00" mtime="3.15256e+01" gflop="0.00000e+00" gbyte="3.77522e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15256e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82941e+01" utime="4.75972e+01" stime="7.52346e+00" mtime="3.15256e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15256e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4816e+08" > 8.0172e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4835e+08" > 5.4596e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8430e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4384e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.2983e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0962e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6068e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.3260e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7394e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6106e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8626e+01 </func>
</region>
</regions>
<internal rank="26" log_i="1723713849.498360" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="27" mpi_size="768" stamp_init="1723713791.118931" stamp_final="1723713849.511258" username="apac4" allocationname="unknown" flags="0" pid="619367" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83923e+01" utime="5.07892e+01" stime="6.30668e+00" mtime="3.23204e+01" gflop="0.00000e+00" gbyte="3.74466e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23204e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ce15ce150d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83068e+01" utime="5.07546e+01" stime="6.30032e+00" mtime="3.23204e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23204e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4904e+08" > 4.7698e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4872e+08" > 2.9841e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0519e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4273e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0965e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6070e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.5577e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9492e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6529e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9571e+01 </func>
</region>
</regions>
<internal rank="27" log_i="1723713849.511258" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="28" mpi_size="768" stamp_init="1723713791.118938" stamp_final="1723713849.511074" username="apac4" allocationname="unknown" flags="0" pid="619368" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83921e+01" utime="4.73439e+01" stime="7.70292e+00" mtime="3.19088e+01" gflop="0.00000e+00" gbyte="3.76896e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19088e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83054e+01" utime="4.73143e+01" stime="7.69145e+00" mtime="3.19088e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19088e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4861e+08" > 7.8534e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4800e+08" > 4.8045e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3090e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4356e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.6601e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0745e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6061e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0109e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9492e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6499e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8570e+01 </func>
</region>
</regions>
<internal rank="28" log_i="1723713849.511074" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="29" mpi_size="768" stamp_init="1723713791.119915" stamp_final="1723713849.511510" username="apac4" allocationname="unknown" flags="0" pid="619369" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83916e+01" utime="5.06917e+01" stime="6.45189e+00" mtime="3.30525e+01" gflop="0.00000e+00" gbyte="3.77434e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.30525e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b2146955b214b214a0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83081e+01" utime="5.06573e+01" stime="6.44543e+00" mtime="3.30525e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.30525e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.0599e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4698e+08" > 4.5207e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4722e+08" > 2.7131e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5089e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4062e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0958e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6062e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0393e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3784e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6545e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9895e+01 </func>
</region>
</regions>
<internal rank="29" log_i="1723713849.511510" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="30" mpi_size="768" stamp_init="1723713791.128586" stamp_final="1723713849.517636" username="apac4" allocationname="unknown" flags="0" pid="619370" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83891e+01" utime="4.80857e+01" stime="7.52117e+00" mtime="3.18844e+01" gflop="0.00000e+00" gbyte="3.77922e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18844e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4251426142714f55627142714a1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82784e+01" utime="4.80546e+01" stime="7.51050e+00" mtime="3.18844e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18844e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4795e+08" > 6.4868e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4837e+08" > 4.5331e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0726e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4191e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1301e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0745e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6058e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0572e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7299e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6496e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8938e+01 </func>
</region>
</regions>
<internal rank="30" log_i="1723713849.517636" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="31" mpi_size="768" stamp_init="1723713791.124743" stamp_final="1723713849.504800" username="apac4" allocationname="unknown" flags="0" pid="619371" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83801e+01" utime="5.05094e+01" stime="6.54454e+00" mtime="3.25137e+01" gflop="0.00000e+00" gbyte="3.77422e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25137e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf439153a153b158f563b153b1528" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82889e+01" utime="5.04762e+01" stime="6.53649e+00" mtime="3.25137e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25137e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4841e+08" > 4.4258e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4896e+08" > 3.1102e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4386e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4182e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0827e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6041e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2064e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8896e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6549e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9422e+01 </func>
</region>
</regions>
<internal rank="31" log_i="1723713849.504800" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="32" mpi_size="768" stamp_init="1723713791.126262" stamp_final="1723713849.508355" username="apac4" allocationname="unknown" flags="0" pid="619372" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83821e+01" utime="4.79306e+01" stime="7.39223e+00" mtime="3.21761e+01" gflop="0.00000e+00" gbyte="3.76057e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21761e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44c144d144f14c3554f144e14e0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82722e+01" utime="4.78979e+01" stime="7.38233e+00" mtime="3.21761e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21761e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4838e+08" > 5.6356e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4887e+08" > 4.2951e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5690e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4935e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.5808e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.1696e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6047e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1291e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7800e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6489e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8809e+01 </func>
</region>
</regions>
<internal rank="32" log_i="1723713849.508355" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="33" mpi_size="768" stamp_init="1723713791.129247" stamp_final="1723713849.511092" username="apac4" allocationname="unknown" flags="0" pid="619373" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83818e+01" utime="5.04285e+01" stime="6.67797e+00" mtime="3.30093e+01" gflop="0.00000e+00" gbyte="3.76740e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.30093e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000089148414f1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82746e+01" utime="5.03902e+01" stime="6.67470e+00" mtime="3.30093e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.30093e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4850e+08" > 3.7522e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4943e+08" > 2.7517e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9807e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4000e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4080e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1091e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6037e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2842e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1781e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6515e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9462e+01 </func>
</region>
</regions>
<internal rank="33" log_i="1723713849.511092" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="34" mpi_size="768" stamp_init="1723713791.131685" stamp_final="1723713849.498263" username="apac4" allocationname="unknown" flags="0" pid="619374" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83666e+01" utime="4.81548e+01" stime="7.48834e+00" mtime="3.20032e+01" gflop="0.00000e+00" gbyte="3.76282e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20032e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a215a415a515ab55a515a51525" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82573e+01" utime="4.81227e+01" stime="7.47930e+00" mtime="3.20032e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20032e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4913e+08" > 5.8067e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4842e+08" > 4.4078e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5625e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4087e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.0991e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0545e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6041e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1877e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0088e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6511e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8644e+01 </func>
</region>
</regions>
<internal rank="34" log_i="1723713849.498263" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="35" mpi_size="768" stamp_init="1723713791.139037" stamp_final="1723713849.514622" username="apac4" allocationname="unknown" flags="0" pid="619375" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83756e+01" utime="5.07713e+01" stime="6.35480e+00" mtime="3.26668e+01" gflop="0.00000e+00" gbyte="3.74676e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26668e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4201421142214845622142214f7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82826e+01" utime="5.07443e+01" stime="6.34095e+00" mtime="3.26668e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26668e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4980e+08" > 3.5471e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4906e+08" > 2.9330e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4685e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4157e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0548e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6042e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2360e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1304e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6506e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9640e+01 </func>
</region>
</regions>
<internal rank="35" log_i="1723713849.514622" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="36" mpi_size="768" stamp_init="1723713791.136788" stamp_final="1723713849.499027" username="apac4" allocationname="unknown" flags="0" pid="619376" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83622e+01" utime="4.87562e+01" stime="7.15766e+00" mtime="3.23966e+01" gflop="0.00000e+00" gbyte="3.74638e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23966e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82656e+01" utime="4.87289e+01" stime="7.14396e+00" mtime="3.23966e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23966e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4869e+08" > 4.7243e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4785e+08" > 4.1752e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7046e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4145e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.4301e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0838e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6029e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3813e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8205e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6531e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8998e+01 </func>
</region>
</regions>
<internal rank="36" log_i="1723713849.499027" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="37" mpi_size="768" stamp_init="1723713791.139426" stamp_final="1723713849.511193" username="apac4" allocationname="unknown" flags="0" pid="619377" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83718e+01" utime="5.05081e+01" stime="6.62170e+00" mtime="3.31396e+01" gflop="0.00000e+00" gbyte="3.76911e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.31396e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003715371528" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82613e+01" utime="5.04748e+01" stime="6.61459e+00" mtime="3.31396e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.31396e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4829e+08" > 3.5546e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4784e+08" > 2.9083e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6413e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4373e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9312e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0841e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6031e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3458e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7680e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6512e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9915e+01 </func>
</region>
</regions>
<internal rank="37" log_i="1723713849.511193" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="38" mpi_size="768" stamp_init="1723713791.141787" stamp_final="1723713849.507690" username="apac4" allocationname="unknown" flags="0" pid="619378" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83659e+01" utime="4.85131e+01" stime="7.22070e+00" mtime="3.22611e+01" gflop="0.00000e+00" gbyte="3.76774e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22611e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000e4146d55e414e414dd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82573e+01" utime="4.84837e+01" stime="7.20906e+00" mtime="3.22611e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22611e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4773e+08" > 5.7869e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4644e+08" > 3.4060e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6954e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4280e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4186e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1090e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6035e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3174e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1185e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6542e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8760e+01 </func>
</region>
</regions>
<internal rank="38" log_i="1723713849.507690" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="39" mpi_size="768" stamp_init="1723713791.143989" stamp_final="1723713849.511155" username="apac4" allocationname="unknown" flags="0" pid="619379" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83672e+01" utime="5.05668e+01" stime="6.58815e+00" mtime="3.26085e+01" gflop="0.00000e+00" gbyte="3.76808e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26085e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a314a514a6144155a614a614bf" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82624e+01" utime="5.05331e+01" stime="6.58130e+00" mtime="3.26085e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26085e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4683e+08" > 3.6980e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4726e+08" > 2.4858e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5337e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4202e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1090e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6027e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3529e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2401e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6546e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9496e+01 </func>
</region>
</regions>
<internal rank="39" log_i="1723713849.511155" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="40" mpi_size="768" stamp_init="1723713791.147995" stamp_final="1723713849.505175" username="apac4" allocationname="unknown" flags="0" pid="619380" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83572e+01" utime="4.87290e+01" stime="7.14738e+00" mtime="3.15566e+01" gflop="0.00000e+00" gbyte="3.77079e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15566e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006614ad56661466145b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82540e+01" utime="4.87022e+01" stime="7.13364e+00" mtime="3.15566e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15566e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4905e+08" > 6.3840e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4922e+08" > 4.2735e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1172e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4249e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2092e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0514e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6017e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4724e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6608e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6110e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8573e+01 </func>
</region>
</regions>
<internal rank="40" log_i="1723713849.505175" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="41" mpi_size="768" stamp_init="1723713791.150945" stamp_final="1723713849.498708" username="apac4" allocationname="unknown" flags="0" pid="619381" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83478e+01" utime="5.03431e+01" stime="6.76427e+00" mtime="3.23850e+01" gflop="0.00000e+00" gbyte="3.76003e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23850e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000391478553914381463" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82494e+01" utime="5.03121e+01" stime="6.75478e+00" mtime="3.23850e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23850e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4879e+08" > 4.9019e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4969e+08" > 3.1883e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6274e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4149e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1005e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6013e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5381e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4499e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6549e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9057e+01 </func>
</region>
</regions>
<internal rank="41" log_i="1723713849.498708" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="42" mpi_size="768" stamp_init="1723713791.153645" stamp_final="1723713849.504668" username="apac4" allocationname="unknown" flags="0" pid="619382" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83510e+01" utime="4.71332e+01" stime="7.76886e+00" mtime="3.22629e+01" gflop="0.00000e+00" gbyte="3.77346e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22629e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c314c514c614d556c614c614f2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82522e+01" utime="4.71043e+01" stime="7.75810e+00" mtime="3.22629e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22629e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4936e+08" > 6.4304e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4802e+08" > 4.4492e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9958e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5095e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2970e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0170e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6021e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4557e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0184e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6508e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8398e+01 </func>
</region>
</regions>
<internal rank="42" log_i="1723713849.504668" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="43" mpi_size="768" stamp_init="1723713791.159301" stamp_final="1723713849.497547" username="apac4" allocationname="unknown" flags="0" pid="619383" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83382e+01" utime="4.89327e+01" stime="6.96599e+00" mtime="3.26630e+01" gflop="0.00000e+00" gbyte="3.77834e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26630e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82425e+01" utime="4.89065e+01" stime="6.95230e+00" mtime="3.26630e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26630e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4849e+08" > 4.8011e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4930e+08" > 2.8704e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7793e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5133e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4080e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0170e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6011e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5212e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7204e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6542e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9191e+01 </func>
</region>
</regions>
<internal rank="43" log_i="1723713849.497547" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="44" mpi_size="768" stamp_init="1723713791.157261" stamp_final="1723713849.505289" username="apac4" allocationname="unknown" flags="0" pid="619384" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83480e+01" utime="4.78011e+01" stime="7.99599e+00" mtime="3.20038e+01" gflop="0.00000e+00" gbyte="3.78056e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20038e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4db15dd15de155355de15de1527" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82527e+01" utime="4.77704e+01" stime="7.98686e+00" mtime="3.20038e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20038e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4965e+08" > 6.7665e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4917e+08" > 5.0511e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7606e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4263e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5250e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0513e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6018e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4882e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0494e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6540e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8328e+01 </func>
</region>
</regions>
<internal rank="44" log_i="1723713849.505289" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="45" mpi_size="768" stamp_init="1723713791.166454" stamp_final="1723713849.517525" username="apac4" allocationname="unknown" flags="0" pid="619385" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83511e+01" utime="5.01338e+01" stime="6.94146e+00" mtime="3.23928e+01" gflop="0.00000e+00" gbyte="3.77121e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23928e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009e159d1504" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82613e+01" utime="5.01012e+01" stime="6.93518e+00" mtime="3.23928e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23928e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4839e+08" > 4.7231e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4926e+08" > 2.9039e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1464e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4155e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0518e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6013e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5350e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3498e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6498e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9571e+01 </func>
</region>
</regions>
<internal rank="45" log_i="1723713849.517525" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="46" mpi_size="768" stamp_init="1723713791.161680" stamp_final="1723713849.510535" username="apac4" allocationname="unknown" flags="0" pid="619386" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83489e+01" utime="4.73099e+01" stime="7.70151e+00" mtime="3.19649e+01" gflop="0.00000e+00" gbyte="3.75565e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19649e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004514441456" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82596e+01" utime="4.72767e+01" stime="7.69546e+00" mtime="3.19649e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19649e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4762e+08" > 6.0672e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4892e+08" > 4.8391e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3338e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5236e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8862e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0125e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6004e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5903e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7895e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6541e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8787e+01 </func>
</region>
</regions>
<internal rank="46" log_i="1723713849.510535" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="47" mpi_size="768" stamp_init="1723713791.166122" stamp_final="1723713849.504691" username="apac4" allocationname="unknown" flags="0" pid="619387" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="5.83386e+01" utime="4.98192e+01" stime="7.02227e+00" mtime="3.25908e+01" gflop="0.00000e+00" gbyte="3.77117e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25908e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005c1427555c145c14a4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82495e+01" utime="4.97891e+01" stime="7.01345e+00" mtime="3.25908e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25908e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4967e+08" > 4.8994e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4845e+08" > 2.9343e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6837e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4199e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0769e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6006e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6045e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7204e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6492e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9207e+01 </func>
</region>
</regions>
<internal rank="47" log_i="1723713849.504691" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="48" mpi_size="768" stamp_init="1723713791.123549" stamp_final="1723713849.512980" username="apac4" allocationname="unknown" flags="0" pid="3031306" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83894e+01" utime="4.24129e+01" stime="1.26133e+01" mtime="3.15167e+01" gflop="0.00000e+00" gbyte="3.87032e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15167e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82988e+01" utime="4.23829e+01" stime="1.26018e+01" mtime="3.15167e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15167e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4838e+08" > 8.0401e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4885e+08" > 4.3564e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3296e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4615e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1938e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5995e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6621e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0112e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6147e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8224e+01 </func>
</region>
</regions>
<internal rank="48" log_i="1723713849.512980" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="49" mpi_size="768" stamp_init="1723713791.123943" stamp_final="1723713849.506601" username="apac4" allocationname="unknown" flags="0" pid="3031307" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83827e+01" utime="4.93608e+01" stime="6.59031e+00" mtime="3.23918e+01" gflop="0.00000e+00" gbyte="3.76823e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23918e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82875e+01" utime="4.93255e+01" stime="6.58429e+00" mtime="3.23918e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23918e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4962e+08" > 4.5857e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4861e+08" > 3.4471e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4720e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4317e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0601e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.6000e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6770e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9206e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6464e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9281e+01 </func>
</region>
</regions>
<internal rank="49" log_i="1723713849.506601" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="50" mpi_size="768" stamp_init="1723713791.123584" stamp_final="1723713849.512175" username="apac4" allocationname="unknown" flags="0" pid="3031308" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83886e+01" utime="4.69914e+01" stime="7.44197e+00" mtime="3.17291e+01" gflop="0.00000e+00" gbyte="3.76389e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17291e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82985e+01" utime="4.69564e+01" stime="7.43628e+00" mtime="3.17291e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17291e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4893e+08" > 6.3408e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4921e+08" > 4.9200e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2835e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4254e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0252e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0615e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5988e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7564e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1209e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6140e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8622e+01 </func>
</region>
</regions>
<internal rank="50" log_i="1723713849.512175" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="51" mpi_size="768" stamp_init="1723713791.123479" stamp_final="1723713849.507714" username="apac4" allocationname="unknown" flags="0" pid="3031309" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83842e+01" utime="4.94469e+01" stime="6.62486e+00" mtime="3.27914e+01" gflop="0.00000e+00" gbyte="3.75477e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27914e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002614335526142614a6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82889e+01" utime="4.94157e+01" stime="6.61464e+00" mtime="3.27914e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27914e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127164" bytes="8.5003e+08" > 4.4270e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4976e+08" > 3.1381e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6516e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4210e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7895e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0636e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5994e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7384e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5119e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6454e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9527e+01 </func>
</region>
</regions>
<internal rank="51" log_i="1723713849.507714" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="52" mpi_size="768" stamp_init="1723713791.123487" stamp_final="1723713849.513120" username="apac4" allocationname="unknown" flags="0" pid="3031310" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83896e+01" utime="4.68010e+01" stime="7.43842e+00" mtime="3.16255e+01" gflop="0.00000e+00" gbyte="3.77132e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16255e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82982e+01" utime="4.67687e+01" stime="7.42937e+00" mtime="3.16255e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16255e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4954e+08" > 6.1511e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4980e+08" > 4.3277e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3275e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4224e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.1475e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0616e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5995e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7287e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2902e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6547e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8501e+01 </func>
</region>
</regions>
<internal rank="52" log_i="1723713849.513120" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="53" mpi_size="768" stamp_init="1723713791.123474" stamp_final="1723713849.511590" username="apac4" allocationname="unknown" flags="0" pid="3031311" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83881e+01" utime="4.97049e+01" stime="6.30598e+00" mtime="3.23849e+01" gflop="0.00000e+00" gbyte="3.75172e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23849e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82969e+01" utime="4.96701e+01" stime="6.29966e+00" mtime="3.23849e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23849e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4785e+08" > 4.6229e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5043e+08" > 3.0159e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1332e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4298e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5259e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0620e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5989e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7875e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0088e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6469e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9614e+01 </func>
</region>
</regions>
<internal rank="53" log_i="1723713849.511590" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="54" mpi_size="768" stamp_init="1723713791.123696" stamp_final="1723713849.505808" username="apac4" allocationname="unknown" flags="0" pid="3031312" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83821e+01" utime="4.69244e+01" stime="7.40738e+00" mtime="3.18375e+01" gflop="0.00000e+00" gbyte="3.78075e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18375e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4251427142814705528142714b7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82872e+01" utime="4.68906e+01" stime="7.39987e+00" mtime="3.18375e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18375e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4967e+08" > 6.6937e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4812e+08" > 4.5097e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5168e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4267e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0157e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0617e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5981e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7975e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2687e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6538e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8465e+01 </func>
</region>
</regions>
<internal rank="54" log_i="1723713849.505808" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="55" mpi_size="768" stamp_init="1723713791.125288" stamp_final="1723713849.512131" username="apac4" allocationname="unknown" flags="0" pid="3031313" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83868e+01" utime="4.96496e+01" stime="6.39836e+00" mtime="3.26136e+01" gflop="0.00000e+00" gbyte="3.77319e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26136e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005a146c555a145a14a8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83004e+01" utime="4.96179e+01" stime="6.38957e+00" mtime="3.26136e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26136e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4894e+08" > 4.6718e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4848e+08" > 3.6638e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7435e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4248e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0622e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5985e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8263e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9707e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6467e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9226e+01 </func>
</region>
</regions>
<internal rank="55" log_i="1723713849.512131" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="56" mpi_size="768" stamp_init="1723713791.127501" stamp_final="1723713849.516746" username="apac4" allocationname="unknown" flags="0" pid="3031314" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83892e+01" utime="4.63874e+01" stime="7.73767e+00" mtime="3.20903e+01" gflop="0.00000e+00" gbyte="3.76362e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20903e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82965e+01" utime="4.63552e+01" stime="7.72954e+00" mtime="3.20903e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20903e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4805e+08" > 7.3186e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4913e+08" > 4.1515e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1944e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4289e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.4182e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0620e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5982e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8610e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2306e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6512e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7977e+01 </func>
</region>
</regions>
<internal rank="56" log_i="1723713849.516746" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="57" mpi_size="768" stamp_init="1723713791.131001" stamp_final="1723713849.507575" username="apac4" allocationname="unknown" flags="0" pid="3031315" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83766e+01" utime="4.92849e+01" stime="6.75566e+00" mtime="3.30707e+01" gflop="0.00000e+00" gbyte="3.77808e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.30707e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009214455692148d14c4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82811e+01" utime="4.92508e+01" stime="6.74810e+00" mtime="3.30707e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.30707e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4856e+08" > 4.6807e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4832e+08" > 2.8314e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.4412e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4270e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1696e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0635e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5978e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8978e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3498e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6478e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8989e+01 </func>
</region>
</regions>
<internal rank="57" log_i="1723713849.507575" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="58" mpi_size="768" stamp_init="1723713791.133053" stamp_final="1723713849.507758" username="apac4" allocationname="unknown" flags="0" pid="3031316" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83747e+01" utime="4.71041e+01" stime="7.38027e+00" mtime="3.21348e+01" gflop="0.00000e+00" gbyte="3.77316e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21348e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a714a714f4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82761e+01" utime="4.70784e+01" stime="7.36388e+00" mtime="3.21348e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21348e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4951e+08" > 6.5409e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4915e+08" > 5.9637e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7956e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4208e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.4346e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0626e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5972e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9198e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2997e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6478e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8489e+01 </func>
</region>
</regions>
<internal rank="58" log_i="1723713849.507758" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="59" mpi_size="768" stamp_init="1723713791.135048" stamp_final="1723713849.506869" username="apac4" allocationname="unknown" flags="0" pid="3031317" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83718e+01" utime="4.95359e+01" stime="6.50276e+00" mtime="3.28168e+01" gflop="0.00000e+00" gbyte="3.75565e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28168e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82758e+01" utime="4.95014e+01" stime="6.49590e+00" mtime="3.28168e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28168e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4902e+08" > 4.6308e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4837e+08" > 2.4443e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8334e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4390e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0634e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5977e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9084e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4595e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6451e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9340e+01 </func>
</region>
</regions>
<internal rank="59" log_i="1723713849.506869" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="60" mpi_size="768" stamp_init="1723713791.137628" stamp_final="1723713849.500973" username="apac4" allocationname="unknown" flags="0" pid="3031318" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83633e+01" utime="4.70588e+01" stime="7.41876e+00" mtime="3.21456e+01" gflop="0.00000e+00" gbyte="3.76312e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21456e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000012152f561215121522" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82659e+01" utime="4.70280e+01" stime="7.40841e+00" mtime="3.21456e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21456e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4785e+08" > 6.1918e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4934e+08" > 4.6433e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0974e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4380e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6308e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0620e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5968e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0008e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9898e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6502e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8229e+01 </func>
</region>
</regions>
<internal rank="60" log_i="1723713849.500973" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="61" mpi_size="768" stamp_init="1723713791.140242" stamp_final="1723713849.500912" username="apac4" allocationname="unknown" flags="0" pid="3031319" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83607e+01" utime="4.96403e+01" stime="6.37691e+00" mtime="3.25167e+01" gflop="0.00000e+00" gbyte="3.78147e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25167e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf45b155c155d15d1565d155d1547" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82515e+01" utime="4.96114e+01" stime="6.36475e+00" mtime="3.25167e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25167e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4827e+08" > 4.5265e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4859e+08" > 2.4828e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2600e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4419e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0635e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5965e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0312e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6798e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6461e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9620e+01 </func>
</region>
</regions>
<internal rank="61" log_i="1723713849.500912" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="62" mpi_size="768" stamp_init="1723713791.142677" stamp_final="1723713849.512601" username="apac4" allocationname="unknown" flags="0" pid="3031320" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83699e+01" utime="4.74437e+01" stime="7.24054e+00" mtime="3.19915e+01" gflop="0.00000e+00" gbyte="3.77605e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19915e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf41a1532154415c85544153f1513" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82734e+01" utime="4.74161e+01" stime="7.22719e+00" mtime="3.19915e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19915e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4941e+08" > 5.9051e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4909e+08" > 4.4268e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5857e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4321e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.8532e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0630e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5952e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1593e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7490e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6485e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8622e+01 </func>
</region>
</regions>
<internal rank="62" log_i="1723713849.512601" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="63" mpi_size="768" stamp_init="1723713791.145709" stamp_final="1723713849.501042" username="apac4" allocationname="unknown" flags="0" pid="3031321" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83553e+01" utime="4.95038e+01" stime="6.56729e+00" mtime="3.27345e+01" gflop="0.00000e+00" gbyte="3.76869e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27345e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f815f8152a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82639e+01" utime="4.94724e+01" stime="6.55790e+00" mtime="3.27345e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27345e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4977e+08" > 4.5333e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4951e+08" > 2.7603e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0670e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4243e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0630e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5959e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0921e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6512e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6447e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9046e+01 </func>
</region>
</regions>
<internal rank="63" log_i="1723713849.501042" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="64" mpi_size="768" stamp_init="1723713791.148361" stamp_final="1723713849.501075" username="apac4" allocationname="unknown" flags="0" pid="3031322" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83527e+01" utime="4.61929e+01" stime="8.31168e+00" mtime="3.12700e+01" gflop="0.00000e+00" gbyte="3.74527e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.12700e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44414451447148a554714461471" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82604e+01" utime="4.61575e+01" stime="8.30675e+00" mtime="3.12700e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.12700e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4796e+08" > 1.0553e+00 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4848e+08" > 6.9149e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2594e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4814e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7548e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5949e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1702e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9588e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6504e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7750e+01 </func>
</region>
</regions>
<internal rank="64" log_i="1723713849.501075" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="65" mpi_size="768" stamp_init="1723713791.151155" stamp_final="1723713849.507719" username="apac4" allocationname="unknown" flags="0" pid="3031323" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83566e+01" utime="4.93675e+01" stime="6.66812e+00" mtime="3.23244e+01" gflop="0.00000e+00" gbyte="3.77064e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23244e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000044153f1519" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82657e+01" utime="4.93347e+01" stime="6.66049e+00" mtime="3.23244e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23244e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.5043e+08" > 5.6358e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4915e+08" > 3.2035e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3842e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4164e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0616e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5948e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1599e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6703e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6446e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9214e+01 </func>
</region>
</regions>
<internal rank="65" log_i="1723713849.507719" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="66" mpi_size="768" stamp_init="1723713791.153823" stamp_final="1723713849.512743" username="apac4" allocationname="unknown" flags="0" pid="3031324" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83589e+01" utime="4.73494e+01" stime="7.48545e+00" mtime="3.19761e+01" gflop="0.00000e+00" gbyte="3.74622e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19761e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b814b914ba147755ba14ba146e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82714e+01" utime="4.73152e+01" stime="7.47992e+00" mtime="3.19761e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19761e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4944e+08" > 6.8214e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4957e+08" > 3.8086e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2637e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4338e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.2493e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0616e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5949e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1882e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8300e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6476e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8843e+01 </func>
</region>
</regions>
<internal rank="66" log_i="1723713849.512743" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="67" mpi_size="768" stamp_init="1723713791.156566" stamp_final="1723713849.507069" username="apac4" allocationname="unknown" flags="0" pid="3031325" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83505e+01" utime="4.90471e+01" stime="6.91507e+00" mtime="3.23912e+01" gflop="0.00000e+00" gbyte="3.77007e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23912e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d114d114e4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82643e+01" utime="4.90132e+01" stime="6.90937e+00" mtime="3.23912e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23912e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4841e+08" > 5.5401e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4850e+08" > 3.1649e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2085e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4222e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0635e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5940e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2341e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6584e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6017e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9459e+01 </func>
</region>
</regions>
<internal rank="67" log_i="1723713849.507069" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="68" mpi_size="768" stamp_init="1723713791.159571" stamp_final="1723713849.507555" username="apac4" allocationname="unknown" flags="0" pid="3031326" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83480e+01" utime="4.72232e+01" stime="7.44114e+00" mtime="3.20226e+01" gflop="0.00000e+00" gbyte="3.77048e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20226e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000096149514b0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82624e+01" utime="4.71887e+01" stime="7.43596e+00" mtime="3.20226e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20226e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4854e+08" > 6.9279e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4912e+08" > 4.0982e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5673e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4212e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.1195e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0618e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5937e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2568e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7990e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6476e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8585e+01 </func>
</region>
</regions>
<internal rank="68" log_i="1723713849.507555" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="69" mpi_size="768" stamp_init="1723713791.161343" stamp_final="1723713849.506549" username="apac4" allocationname="unknown" flags="0" pid="3031327" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83452e+01" utime="4.90504e+01" stime="7.00292e+00" mtime="3.26324e+01" gflop="0.00000e+00" gbyte="3.77251e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26324e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4841485148714395687148614cc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82630e+01" utime="4.90196e+01" stime="6.99450e+00" mtime="3.26324e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26324e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4895e+08" > 5.5332e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4991e+08" > 2.9800e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6149e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4366e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1696e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0622e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5932e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.3185e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7299e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6014e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9283e+01 </func>
</region>
</regions>
<internal rank="69" log_i="1723713849.506549" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="70" mpi_size="768" stamp_init="1723713791.163693" stamp_final="1723713849.512600" username="apac4" allocationname="unknown" flags="0" pid="3031328" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83489e+01" utime="4.61250e+01" stime="8.04168e+00" mtime="3.19120e+01" gflop="0.00000e+00" gbyte="3.77277e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19120e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82661e+01" utime="4.60960e+01" stime="8.03172e+00" mtime="3.19120e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19120e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4978e+08" > 8.2168e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4886e+08" > 7.8724e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5680e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4239e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1209e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0619e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5937e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.3067e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1900e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6483e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8304e+01 </func>
</region>
</regions>
<internal rank="70" log_i="1723713849.512600" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="71" mpi_size="768" stamp_init="1723713791.164563" stamp_final="1723713849.515908" username="apac4" allocationname="unknown" flags="0" pid="3031329" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="5.83513e+01" utime="4.89214e+01" stime="6.68153e+00" mtime="3.20571e+01" gflop="0.00000e+00" gbyte="3.74458e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20571e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e814e914eb14e155eb14ea1458" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82704e+01" utime="4.88882e+01" stime="6.67574e+00" mtime="3.20571e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20571e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.0599e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4987e+08" > 5.5012e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4890e+08" > 2.7746e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3193e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4296e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0619e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5933e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.3474e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0208e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6430e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9015e+01 </func>
</region>
</regions>
<internal rank="71" log_i="1723713849.515908" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="72" mpi_size="768" stamp_init="1723713791.106014" stamp_final="1723713849.507379" username="apac4" allocationname="unknown" flags="0" pid="299699" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.84014e+01" utime="4.26297e+01" stime="1.21765e+01" mtime="3.18252e+01" gflop="0.00000e+00" gbyte="3.86829e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18252e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83577e+01" utime="4.25974e+01" stime="1.21688e+01" mtime="3.18252e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18252e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4953e+08" > 6.0379e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5068e+08" > 5.0941e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9569e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4356e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7042e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0650e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5931e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.3382e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8491e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6447e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8043e+01 </func>
</region>
</regions>
<internal rank="72" log_i="1723713849.507379" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="73" mpi_size="768" stamp_init="1723713791.106660" stamp_final="1723713849.503837" username="apac4" allocationname="unknown" flags="0" pid="299700" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83972e+01" utime="4.96832e+01" stime="6.41892e+00" mtime="3.24234e+01" gflop="0.00000e+00" gbyte="3.76934e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24234e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d114d114a0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83560e+01" utime="4.96516e+01" stime="6.41201e+00" mtime="3.24234e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24234e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5066e+08" > 4.6231e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4925e+08" > 3.7464e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4155e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4998e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4796e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0122e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5932e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.3620e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.4986e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6413e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9343e+01 </func>
</region>
</regions>
<internal rank="73" log_i="1723713849.503837" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="74" mpi_size="768" stamp_init="1723713791.143852" stamp_final="1723713849.519970" username="apac4" allocationname="unknown" flags="0" pid="299701" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83761e+01" utime="4.76611e+01" stime="7.02719e+00" mtime="3.16534e+01" gflop="0.00000e+00" gbyte="3.78216e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16534e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82941e+01" utime="4.76296e+01" stime="7.01693e+00" mtime="3.16534e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16534e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4955e+08" > 7.0288e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4951e+08" > 4.8543e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9391e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4234e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1706e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0651e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5929e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.3712e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3116e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6441e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8821e+01 </func>
</region>
</regions>
<internal rank="74" log_i="1723713849.519970" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="75" mpi_size="768" stamp_init="1723713791.148971" stamp_final="1723713849.506809" username="apac4" allocationname="unknown" flags="0" pid="299702" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83578e+01" utime="4.96003e+01" stime="6.46580e+00" mtime="3.24884e+01" gflop="0.00000e+00" gbyte="3.75927e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24884e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000075147414c1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82758e+01" utime="4.95684e+01" stime="6.45677e+00" mtime="3.24884e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24884e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4909e+08" > 4.7892e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4910e+08" > 3.0731e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3181e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4312e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7895e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0670e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5923e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.4135e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.3913e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6010e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9510e+01 </func>
</region>
</regions>
<internal rank="75" log_i="1723713849.506809" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="76" mpi_size="768" stamp_init="1723713791.112769" stamp_final="1723713849.520325" username="apac4" allocationname="unknown" flags="0" pid="299703" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.84076e+01" utime="4.76226e+01" stime="7.21989e+00" mtime="3.23101e+01" gflop="0.00000e+00" gbyte="3.74908e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23101e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83535e+01" utime="4.75938e+01" stime="7.20846e+00" mtime="3.23101e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23101e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4765e+08" > 5.9808e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4997e+08" > 3.5926e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6835e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4980e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3484e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0172e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5922e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.4383e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5797e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6441e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8821e+01 </func>
</region>
</regions>
<internal rank="76" log_i="1723713849.520325" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="77" mpi_size="768" stamp_init="1723713791.116657" stamp_final="1723713849.508464" username="apac4" allocationname="unknown" flags="0" pid="299704" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83918e+01" utime="5.04607e+01" stime="6.62355e+00" mtime="3.26416e+01" gflop="0.00000e+00" gbyte="3.77106e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26416e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83364e+01" utime="5.04259e+01" stime="6.61859e+00" mtime="3.26416e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26416e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4835e+08" > 4.5905e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4803e+08" > 3.1752e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5618e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4226e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7180e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7679e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5917e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.4757e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7609e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6018e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9416e+01 </func>
</region>
</regions>
<internal rank="77" log_i="1723713849.508464" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="78" mpi_size="768" stamp_init="1723713791.119883" stamp_final="1723713849.506147" username="apac4" allocationname="unknown" flags="0" pid="299705" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83863e+01" utime="4.64372e+01" stime="7.42688e+00" mtime="3.15818e+01" gflop="0.00000e+00" gbyte="3.76827e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15818e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf45e1477148914e35689148414b1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83224e+01" utime="4.64083e+01" stime="7.41493e+00" mtime="3.15818e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15818e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4789e+08" > 7.5920e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4758e+08" > 6.8536e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1923e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4852e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5378e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0172e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5917e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.4825e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7394e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6433e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8406e+01 </func>
</region>
</regions>
<internal rank="78" log_i="1723713849.506147" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="79" mpi_size="768" stamp_init="1723713791.122732" stamp_final="1723713849.510641" username="apac4" allocationname="unknown" flags="0" pid="299706" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83879e+01" utime="5.06390e+01" stime="6.48476e+00" mtime="3.24380e+01" gflop="0.00000e+00" gbyte="3.77129e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24380e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83128e+01" utime="5.06093e+01" stime="6.47339e+00" mtime="3.24380e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24380e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4882e+08" > 4.5201e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5003e+08" > 3.0993e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7611e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4042e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2411e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7013e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5909e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.5571e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.4295e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6391e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9039e+01 </func>
</region>
</regions>
<internal rank="79" log_i="1723713849.510641" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="80" mpi_size="768" stamp_init="1723713791.124487" stamp_final="1723713849.510211" username="apac4" allocationname="unknown" flags="0" pid="299707" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83857e+01" utime="4.77484e+01" stime="7.00606e+00" mtime="3.22385e+01" gflop="0.00000e+00" gbyte="3.77831e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22385e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a315a3150b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83113e+01" utime="4.77173e+01" stime="6.99596e+00" mtime="3.22385e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22385e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5001e+08" > 5.0913e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4727e+08" > 4.5643e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8591e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4423e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4241e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0649e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5903e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.6597e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0708e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6431e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8649e+01 </func>
</region>
</regions>
<internal rank="80" log_i="1723713849.510211" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="81" mpi_size="768" stamp_init="1723713791.127558" stamp_final="1723713849.507362" username="apac4" allocationname="unknown" flags="0" pid="299708" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83798e+01" utime="5.08787e+01" stime="6.27627e+00" mtime="3.28206e+01" gflop="0.00000e+00" gbyte="3.76148e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28206e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4af14b014b1144455b114b1147f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83003e+01" utime="5.08488e+01" stime="6.26552e+00" mtime="3.28206e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28206e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4847e+08" > 3.7256e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5103e+08" > 2.8645e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5751e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4240e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9789e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7207e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5908e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.6137e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6798e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6398e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9669e+01 </func>
</region>
</regions>
<internal rank="81" log_i="1723713849.507362" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="82" mpi_size="768" stamp_init="1723713791.129958" stamp_final="1723713849.512654" username="apac4" allocationname="unknown" flags="0" pid="299709" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83827e+01" utime="4.89084e+01" stime="7.06191e+00" mtime="3.21417e+01" gflop="0.00000e+00" gbyte="3.78174e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21417e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d015d0154e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82977e+01" utime="4.88750e+01" stime="7.05266e+00" mtime="3.21417e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21417e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5105e+08" > 5.0496e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4889e+08" > 3.8637e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3647e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4230e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0133e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8858e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5898e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.6515e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0899e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6420e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9058e+01 </func>
</region>
</regions>
<internal rank="82" log_i="1723713849.512654" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="83" mpi_size="768" stamp_init="1723713791.132651" stamp_final="1723713849.514866" username="apac4" allocationname="unknown" flags="0" pid="299710" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83822e+01" utime="5.06196e+01" stime="6.50293e+00" mtime="3.25702e+01" gflop="0.00000e+00" gbyte="3.78212e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25702e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b215b415b515c355b515b51527" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83021e+01" utime="5.05877e+01" stime="6.49370e+00" mtime="3.25702e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25702e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4880e+08" > 3.5148e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5031e+08" > 2.9487e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3352e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4293e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8893e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5904e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.6551e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9612e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6403e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9672e+01 </func>
</region>
</regions>
<internal rank="83" log_i="1723713849.514866" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="84" mpi_size="768" stamp_init="1723713791.135458" stamp_final="1723713849.512484" username="apac4" allocationname="unknown" flags="0" pid="299711" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83770e+01" utime="4.78326e+01" stime="6.91810e+00" mtime="3.22646e+01" gflop="0.00000e+00" gbyte="3.77571e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22646e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c314c514c614e756c614c614c8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82912e+01" utime="4.78049e+01" stime="6.90369e+00" mtime="3.22646e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22646e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4904e+08" > 4.7928e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4915e+08" > 3.8870e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5347e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5098e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.6210e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0159e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5895e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.6956e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6107e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6428e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9027e+01 </func>
</region>
</regions>
<internal rank="84" log_i="1723713849.512484" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="85" mpi_size="768" stamp_init="1723713791.138102" stamp_final="1723713849.503661" username="apac4" allocationname="unknown" flags="0" pid="299712" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83656e+01" utime="5.06064e+01" stime="6.49628e+00" mtime="3.28292e+01" gflop="0.00000e+00" gbyte="3.75450e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28292e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82788e+01" utime="5.05725e+01" stime="6.48889e+00" mtime="3.28292e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28292e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4905e+08" > 3.6672e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4946e+08" > 3.1412e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1258e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4328e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5272e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7695e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5890e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.7879e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1996e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6390e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.0121e+01 </func>
</region>
</regions>
<internal rank="85" log_i="1723713849.503661" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="86" mpi_size="768" stamp_init="1723713791.140122" stamp_final="1723713849.501996" username="apac4" allocationname="unknown" flags="0" pid="299713" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83619e+01" utime="4.75125e+01" stime="7.22687e+00" mtime="3.23512e+01" gflop="0.00000e+00" gbyte="3.74271e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23512e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000024144f55241424145f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82769e+01" utime="4.74801e+01" stime="7.21861e+00" mtime="3.23512e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23512e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4988e+08" > 5.0682e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4909e+08" > 4.2353e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0535e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5069e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.3034e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0159e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5894e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.7068e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6798e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6034e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8568e+01 </func>
</region>
</regions>
<internal rank="86" log_i="1723713849.501996" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="87" mpi_size="768" stamp_init="1723713791.142150" stamp_final="1723713849.503311" username="apac4" allocationname="unknown" flags="0" pid="299714" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83612e+01" utime="4.98835e+01" stime="6.26500e+00" mtime="3.26683e+01" gflop="0.00000e+00" gbyte="3.76694e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26683e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82788e+01" utime="4.98503e+01" stime="6.25712e+00" mtime="3.26683e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26683e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4848e+08" > 3.5673e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4985e+08" > 2.8707e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5135e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5109e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0159e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5887e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.7819e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0494e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6390e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9589e+01 </func>
</region>
</regions>
<internal rank="87" log_i="1723713849.503311" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="88" mpi_size="768" stamp_init="1723713791.162768" stamp_final="1723713849.509119" username="apac4" allocationname="unknown" flags="0" pid="299715" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83464e+01" utime="4.61138e+01" stime="7.64405e+00" mtime="3.16238e+01" gflop="0.00000e+00" gbyte="3.76778e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16238e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b314ee56b314b314d7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82757e+01" utime="4.60832e+01" stime="7.63540e+00" mtime="3.16238e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16238e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4848e+08" > 8.5694e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4961e+08" > 8.9626e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3838e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4393e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1090e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0650e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5886e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.7956e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9302e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6444e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8136e+01 </func>
</region>
</regions>
<internal rank="88" log_i="1723713849.509119" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="89" mpi_size="768" stamp_init="1723713791.147808" stamp_final="1723713849.514432" username="apac4" allocationname="unknown" flags="0" pid="299716" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83666e+01" utime="4.96367e+01" stime="6.46792e+00" mtime="3.27451e+01" gflop="0.00000e+00" gbyte="3.77804e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27451e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000bb144f55bb14bb1475" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82835e+01" utime="4.96096e+01" stime="6.45393e+00" mtime="3.27451e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27451e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5019e+08" > 4.7949e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4849e+08" > 3.2068e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8383e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4931e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7405e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0136e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5884e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.8510e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.3007e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6402e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9235e+01 </func>
</region>
</regions>
<internal rank="89" log_i="1723713849.514432" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="90" mpi_size="768" stamp_init="1723713791.151842" stamp_final="1723713849.501985" username="apac4" allocationname="unknown" flags="0" pid="299717" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83501e+01" utime="4.84075e+01" stime="7.53778e+00" mtime="3.17549e+01" gflop="0.00000e+00" gbyte="3.75175e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17549e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000fb15fa1550" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82698e+01" utime="4.83747e+01" stime="7.52942e+00" mtime="3.17549e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17549e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4854e+08" > 5.8960e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5020e+08" > 4.2166e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4746e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4004e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6642e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6948e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5880e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.8327e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.3603e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6433e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8498e+01 </func>
</region>
</regions>
<internal rank="90" log_i="1723713849.501985" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="91" mpi_size="768" stamp_init="1723713791.155364" stamp_final="1723713849.514432" username="apac4" allocationname="unknown" flags="0" pid="299718" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83591e+01" utime="5.03602e+01" stime="6.78639e+00" mtime="3.23810e+01" gflop="0.00000e+00" gbyte="3.77247e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23810e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009615961534" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82781e+01" utime="5.03268e+01" stime="6.78105e+00" mtime="3.23810e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23810e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4904e+08" > 4.6442e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4938e+08" > 3.1682e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4677e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4194e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9045e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5877e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.9243e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.4199e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6384e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9245e+01 </func>
</region>
</regions>
<internal rank="91" log_i="1723713849.514432" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="92" mpi_size="768" stamp_init="1723713791.156961" stamp_final="1723713849.506018" username="apac4" allocationname="unknown" flags="0" pid="299719" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83491e+01" utime="4.63511e+01" stime="7.79325e+00" mtime="3.18929e+01" gflop="0.00000e+00" gbyte="3.77613e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18929e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf461156215631588556315631505" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82725e+01" utime="4.63198e+01" stime="7.78425e+00" mtime="3.18929e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18929e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4966e+08" > 7.0751e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4835e+08" > 4.5233e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6499e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5025e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7421e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0170e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5876e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.9251e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6703e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6435e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8314e+01 </func>
</region>
</regions>
<internal rank="92" log_i="1723713849.506018" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="93" mpi_size="768" stamp_init="1723713791.158510" stamp_final="1723713849.503813" username="apac4" allocationname="unknown" flags="0" pid="299720" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83453e+01" utime="4.94888e+01" stime="6.61181e+00" mtime="3.22873e+01" gflop="0.00000e+00" gbyte="3.77014e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22873e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001514151457" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82699e+01" utime="4.94584e+01" stime="6.60262e+00" mtime="3.22873e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22873e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4710e+08" > 4.8049e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4796e+08" > 2.9533e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9576e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4987e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0170e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5870e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.9952e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0017e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6375e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9650e+01 </func>
</region>
</regions>
<internal rank="93" log_i="1723713849.503813" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="94" mpi_size="768" stamp_init="1723713791.160602" stamp_final="1723713849.502033" username="apac4" allocationname="unknown" flags="0" pid="299721" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83414e+01" utime="4.74289e+01" stime="7.32419e+00" mtime="3.17241e+01" gflop="0.00000e+00" gbyte="3.76141e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17241e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008f1587568f158f152e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82668e+01" utime="4.73932e+01" stime="7.32077e+00" mtime="3.17241e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17241e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4958e+08" > 6.1458e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4798e+08" > 4.8101e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2356e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4792e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.7532e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0178e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5873e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.9620e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5392e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6420e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8671e+01 </func>
</region>
</regions>
<internal rank="94" log_i="1723713849.502033" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="95" mpi_size="768" stamp_init="1723713791.164491" stamp_final="1723713849.515405" username="apac4" allocationname="unknown" flags="0" pid="299722" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="5.83509e+01" utime="4.92966e+01" stime="6.79408e+00" mtime="3.28882e+01" gflop="0.00000e+00" gbyte="3.77926e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28882e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44a154b154c15a7554c154c151f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82788e+01" utime="4.92666e+01" stime="6.78470e+00" mtime="3.28882e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28882e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4861e+08" > 5.0211e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4886e+08" > 3.0679e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0348e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4927e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8133e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0177e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5871e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.9848e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8300e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6331e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9157e+01 </func>
</region>
</regions>
<internal rank="95" log_i="1723713849.515405" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="96" mpi_size="768" stamp_init="1723713791.126129" stamp_final="1723713849.517119" username="apac4" allocationname="unknown" flags="0" pid="1714196" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83910e+01" utime="4.28775e+01" stime="1.25635e+01" mtime="3.09527e+01" gflop="0.00000e+00" gbyte="3.83297e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.09527e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a315d256a315a31539" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82976e+01" utime="4.28483e+01" stime="1.25518e+01" mtime="3.09527e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.09527e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4921e+08" > 6.8570e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5003e+08" > 5.4561e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.5286e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4887e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6750e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8597e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5859e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.1237e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2115e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6404e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8524e+01 </func>
</region>
</regions>
<internal rank="96" log_i="1723713849.517119" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="97" mpi_size="768" stamp_init="1723713791.134310" stamp_final="1723713849.500598" username="apac4" allocationname="unknown" flags="0" pid="1714197" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83663e+01" utime="4.94255e+01" stime="6.65281e+00" mtime="3.27249e+01" gflop="0.00000e+00" gbyte="3.77144e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27249e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43115331534157e553415341528" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82619e+01" utime="4.93930e+01" stime="6.64284e+00" mtime="3.27249e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27249e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4981e+08" > 4.4262e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4979e+08" > 2.8277e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7548e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4231e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0558e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5862e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.0475e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7704e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5941e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9367e+01 </func>
</region>
</regions>
<internal rank="97" log_i="1723713849.500598" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="98" mpi_size="768" stamp_init="1723713791.126118" stamp_final="1723713849.501438" username="apac4" allocationname="unknown" flags="0" pid="1714198" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83753e+01" utime="4.73454e+01" stime="7.19590e+00" mtime="3.16469e+01" gflop="0.00000e+00" gbyte="3.78223e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16469e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001e1456561e141e14e2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82824e+01" utime="4.73162e+01" stime="7.18363e+00" mtime="3.16469e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16469e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4914e+08" > 5.7193e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4981e+08" > 4.0192e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2212e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4516e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0185e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0668e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5852e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.1069e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4809e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6399e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8638e+01 </func>
</region>
</regions>
<internal rank="98" log_i="1723713849.501438" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="99" mpi_size="768" stamp_init="1723713791.125946" stamp_final="1723713849.510602" username="apac4" allocationname="unknown" flags="0" pid="1714199" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83847e+01" utime="4.92896e+01" stime="6.73186e+00" mtime="3.24990e+01" gflop="0.00000e+00" gbyte="3.77033e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24990e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f314f414f5141e56f514f5149f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82925e+01" utime="4.92568e+01" stime="6.72311e+00" mtime="3.24990e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24990e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5030e+08" > 4.5227e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4991e+08" > 2.7811e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4799e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4469e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7643e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0666e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5861e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.0978e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1304e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6383e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9372e+01 </func>
</region>
</regions>
<internal rank="99" log_i="1723713849.510602" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="100" mpi_size="768" stamp_init="1723713791.126124" stamp_final="1723713849.517499" username="apac4" allocationname="unknown" flags="0" pid="1714200" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83914e+01" utime="4.73571e+01" stime="7.40251e+00" mtime="3.20300e+01" gflop="0.00000e+00" gbyte="3.78063e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20300e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82964e+01" utime="4.73251e+01" stime="7.39326e+00" mtime="3.20300e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20300e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5024e+08" > 5.9932e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4945e+08" > 4.4526e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6107e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4412e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5148e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0572e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5857e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.0789e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0208e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6403e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8609e+01 </func>
</region>
</regions>
<internal rank="100" log_i="1723713849.517499" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="101" mpi_size="768" stamp_init="1723713791.130133" stamp_final="1723713849.511141" username="apac4" allocationname="unknown" flags="0" pid="1714201" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83810e+01" utime="4.94145e+01" stime="6.65468e+00" mtime="3.30705e+01" gflop="0.00000e+00" gbyte="3.76999e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.30705e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d914db14dc143955dc14dc14c8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82847e+01" utime="4.93795e+01" stime="6.64873e+00" mtime="3.30705e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.30705e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4748e+08" > 4.3217e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5086e+08" > 2.7566e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3842e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4437e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6941e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0646e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5858e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.1287e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0208e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6384e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.0065e+01 </func>
</region>
</regions>
<internal rank="101" log_i="1723713849.511141" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="102" mpi_size="768" stamp_init="1723713791.137731" stamp_final="1723713849.501419" username="apac4" allocationname="unknown" flags="0" pid="1714202" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83637e+01" utime="4.65594e+01" stime="7.53514e+00" mtime="3.12498e+01" gflop="0.00000e+00" gbyte="3.77712e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.12498e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000241497552414241489" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82606e+01" utime="4.65302e+01" stime="7.52219e+00" mtime="3.12498e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.12498e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4872e+08" > 6.8124e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4702e+08" > 4.3082e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7127e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4419e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0514e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0572e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5849e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.2041e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1304e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6398e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8660e+01 </func>
</region>
</regions>
<internal rank="102" log_i="1723713849.501419" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="103" mpi_size="768" stamp_init="1723713791.136027" stamp_final="1723713849.509510" username="apac4" allocationname="unknown" flags="0" pid="1714203" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83735e+01" utime="4.93721e+01" stime="6.69348e+00" mtime="3.26913e+01" gflop="0.00000e+00" gbyte="3.77533e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26913e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b8142455b814b81489" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82737e+01" utime="4.93433e+01" stime="6.68047e+00" mtime="3.26913e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26913e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4874e+08" > 4.6500e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4791e+08" > 2.9563e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9516e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4314e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0660e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5844e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.2380e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9898e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6382e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9094e+01 </func>
</region>
</regions>
<internal rank="103" log_i="1723713849.509510" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="104" mpi_size="768" stamp_init="1723713791.126114" stamp_final="1723713849.496810" username="apac4" allocationname="unknown" flags="0" pid="1714204" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83707e+01" utime="4.78548e+01" stime="7.14300e+00" mtime="3.24202e+01" gflop="0.00000e+00" gbyte="3.76404e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24202e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000eb14ea55eb14e614c9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82773e+01" utime="4.78297e+01" stime="7.12715e+00" mtime="3.24202e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24202e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5046e+08" > 4.6469e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4876e+08" > 4.0591e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9359e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5030e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0529e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0101e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5844e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.2316e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5715e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6411e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8811e+01 </func>
</region>
</regions>
<internal rank="104" log_i="1723713849.496810" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="105" mpi_size="768" stamp_init="1723713791.126882" stamp_final="1723713849.518100" username="apac4" allocationname="unknown" flags="0" pid="1714205" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83912e+01" utime="4.97828e+01" stime="6.36317e+00" mtime="3.25708e+01" gflop="0.00000e+00" gbyte="3.76377e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25708e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44215441545153c554515441509" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83001e+01" utime="4.97498e+01" stime="6.35578e+00" mtime="3.25708e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25708e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4964e+08" > 3.4843e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4957e+08" > 2.7310e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5044e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4955e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0108e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5847e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.2413e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7895e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6371e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9531e+01 </func>
</region>
</regions>
<internal rank="105" log_i="1723713849.518100" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="106" mpi_size="768" stamp_init="1723713791.130648" stamp_final="1723713849.505492" username="apac4" allocationname="unknown" flags="0" pid="1714206" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83748e+01" utime="4.76429e+01" stime="7.00054e+00" mtime="3.15722e+01" gflop="0.00000e+00" gbyte="3.74607e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15722e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000671552556715671543" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82780e+01" utime="4.76121e+01" stime="6.99029e+00" mtime="3.15722e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15722e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4824e+08" > 4.9205e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5037e+08" > 4.5982e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0602e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4958e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.0810e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0101e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5837e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.3021e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2806e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5984e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8815e+01 </func>
</region>
</regions>
<internal rank="106" log_i="1723713849.505492" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="107" mpi_size="768" stamp_init="1723713791.132446" stamp_final="1723713849.511519" username="apac4" allocationname="unknown" flags="0" pid="1714207" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83791e+01" utime="4.97841e+01" stime="6.31040e+00" mtime="3.31255e+01" gflop="0.00000e+00" gbyte="3.76595e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.31255e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82829e+01" utime="4.97591e+01" stime="6.29438e+00" mtime="3.31255e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.31255e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4880e+08" > 3.4536e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4907e+08" > 2.8809e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7353e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4975e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4080e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0112e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5837e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.3543e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3116e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6356e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9853e+01 </func>
</region>
</regions>
<internal rank="107" log_i="1723713849.511519" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="108" mpi_size="768" stamp_init="1723713791.135873" stamp_final="1723713849.510312" username="apac4" allocationname="unknown" flags="0" pid="1714208" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83744e+01" utime="4.62082e+01" stime="7.57793e+00" mtime="3.18179e+01" gflop="0.00000e+00" gbyte="3.76518e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18179e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82722e+01" utime="4.61788e+01" stime="7.56594e+00" mtime="3.18179e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18179e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4929e+08" > 7.1501e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4762e+08" > 5.4512e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3661e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4962e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.5020e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0110e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5836e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.3455e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2401e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6403e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8520e+01 </func>
</region>
</regions>
<internal rank="108" log_i="1723713849.510312" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="109" mpi_size="768" stamp_init="1723713791.137295" stamp_final="1723713849.499764" username="apac4" allocationname="unknown" flags="0" pid="1714209" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83625e+01" utime="4.97951e+01" stime="6.31010e+00" mtime="3.28357e+01" gflop="0.00000e+00" gbyte="3.77377e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28357e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000391538150d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82643e+01" utime="4.97665e+01" stime="6.29760e+00" mtime="3.28357e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28357e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4794e+08" > 3.4027e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4875e+08" > 2.4779e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2453e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5065e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8849e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0109e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5829e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.4231e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1710e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6324e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.0054e+01 </func>
</region>
</regions>
<internal rank="109" log_i="1723713849.499764" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="110" mpi_size="768" stamp_init="1723713791.140854" stamp_final="1723713849.501450" username="apac4" allocationname="unknown" flags="0" pid="1714210" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83606e+01" utime="4.80565e+01" stime="6.81027e+00" mtime="3.21054e+01" gflop="0.00000e+00" gbyte="3.76514e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21054e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4df14e014e1143156e114e11468" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82587e+01" utime="4.80228e+01" stime="6.80330e+00" mtime="3.21054e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21054e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4780e+08" > 4.6661e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4856e+08" > 3.8385e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4795e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4843e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.3195e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0114e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5827e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.4316e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5095e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6400e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8971e+01 </func>
</region>
</regions>
<internal rank="110" log_i="1723713849.501450" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="111" mpi_size="768" stamp_init="1723713791.141924" stamp_final="1723713849.514991" username="apac4" allocationname="unknown" flags="0" pid="1714211" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83731e+01" utime="4.96618e+01" stime="6.50021e+00" mtime="3.27683e+01" gflop="0.00000e+00" gbyte="3.74889e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27683e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4421443144414895544144414c3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82738e+01" utime="4.96312e+01" stime="6.48998e+00" mtime="3.27683e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27683e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4909e+08" > 3.4231e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4912e+08" > 3.1616e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7206e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4960e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4080e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0118e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5813e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.5504e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3593e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6276e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9513e+01 </func>
</region>
</regions>
<internal rank="111" log_i="1723713849.514991" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="112" mpi_size="768" stamp_init="1723713791.161799" stamp_final="1723713849.511667" username="apac4" allocationname="unknown" flags="0" pid="1714212" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83499e+01" utime="4.79647e+01" stime="7.11619e+00" mtime="3.21277e+01" gflop="0.00000e+00" gbyte="3.78017e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21277e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ae14b014b114a056b114b114fe" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82602e+01" utime="4.79315e+01" stime="7.10868e+00" mtime="3.21277e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21277e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4884e+08" > 5.1951e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4984e+08" > 3.5866e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9073e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4293e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6989e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0597e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5823e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.4847e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1590e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6392e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8522e+01 </func>
</region>
</regions>
<internal rank="112" log_i="1723713849.511667" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="113" mpi_size="768" stamp_init="1723713791.149425" stamp_final="1723713849.505556" username="apac4" allocationname="unknown" flags="0" pid="1714213" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83561e+01" utime="4.92498e+01" stime="6.88317e+00" mtime="3.25045e+01" gflop="0.00000e+00" gbyte="3.76522e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25045e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007b147b14c8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82612e+01" utime="4.92195e+01" stime="6.87302e+00" mtime="3.25045e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25045e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5021e+08" > 4.8284e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4933e+08" > 3.3819e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7271e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4797e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9312e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0108e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5820e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.5101e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7800e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6240e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9117e+01 </func>
</region>
</regions>
<internal rank="113" log_i="1723713849.505556" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="114" mpi_size="768" stamp_init="1723713791.152400" stamp_final="1723713849.512339" username="apac4" allocationname="unknown" flags="0" pid="1714214" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83599e+01" utime="4.74758e+01" stime="7.32078e+00" mtime="3.21028e+01" gflop="0.00000e+00" gbyte="3.77834e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21028e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82636e+01" utime="4.74450e+01" stime="7.31033e+00" mtime="3.21028e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21028e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5031e+08" > 6.3244e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4955e+08" > 4.0082e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6795e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5037e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6703e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0114e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5813e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.5727e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4094e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6379e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8582e+01 </func>
</region>
</regions>
<internal rank="114" log_i="1723713849.512339" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="115" mpi_size="768" stamp_init="1723713791.154376" stamp_final="1723713849.500094" username="apac4" allocationname="unknown" flags="0" pid="1714215" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83457e+01" utime="4.93338e+01" stime="6.80078e+00" mtime="3.24401e+01" gflop="0.00000e+00" gbyte="3.77022e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24401e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d115d01513" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82526e+01" utime="4.93022e+01" stime="6.79212e+00" mtime="3.24401e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24401e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4985e+08" > 4.7329e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4852e+08" > 3.1800e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5762e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4864e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0115e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5809e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.5926e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7585e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6237e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9208e+01 </func>
</region>
</regions>
<internal rank="115" log_i="1723713849.500094" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="116" mpi_size="768" stamp_init="1723713791.157358" stamp_final="1723713849.501123" username="apac4" allocationname="unknown" flags="0" pid="1714216" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83438e+01" utime="4.68988e+01" stime="7.57968e+00" mtime="3.18150e+01" gflop="0.00000e+00" gbyte="3.76266e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18150e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82525e+01" utime="4.68689e+01" stime="7.57018e+00" mtime="3.18150e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18150e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4714e+08" > 6.7440e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4920e+08" > 4.2509e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6981e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4399e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.3409e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0678e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5807e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.6059e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3903e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5959e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8238e+01 </func>
</region>
</regions>
<internal rank="116" log_i="1723713849.501123" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="117" mpi_size="768" stamp_init="1723713791.159164" stamp_final="1723713849.501223" username="apac4" allocationname="unknown" flags="0" pid="1714217" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83421e+01" utime="4.94948e+01" stime="6.62137e+00" mtime="3.24163e+01" gflop="0.00000e+00" gbyte="3.77102e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24163e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82531e+01" utime="4.94598e+01" stime="6.61633e+00" mtime="3.24163e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24163e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4918e+08" > 4.6010e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4855e+08" > 2.8735e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3962e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4967e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4319e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0115e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5806e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.6630e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1114e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6210e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9369e+01 </func>
</region>
</regions>
<internal rank="117" log_i="1723713849.501223" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="118" mpi_size="768" stamp_init="1723713791.161823" stamp_final="1723713849.503787" username="apac4" allocationname="unknown" flags="0" pid="1714218" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83420e+01" utime="4.74679e+01" stime="7.07163e+00" mtime="3.15134e+01" gflop="0.00000e+00" gbyte="3.76694e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15134e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001e141e1470" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82543e+01" utime="4.74363e+01" stime="7.06386e+00" mtime="3.15134e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15134e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4987e+08" > 6.3733e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4926e+08" > 4.4871e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3654e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4394e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.4740e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0679e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5805e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.6732e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7919e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6385e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8301e+01 </func>
</region>
</regions>
<internal rank="118" log_i="1723713849.503787" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="119" mpi_size="768" stamp_init="1723713791.163926" stamp_final="1723713849.513410" username="apac4" allocationname="unknown" flags="0" pid="1714219" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="5.83495e+01" utime="4.93708e+01" stime="6.70653e+00" mtime="3.24861e+01" gflop="0.00000e+00" gbyte="3.77083e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24861e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf449144a144b143a564b144b146e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82630e+01" utime="4.93379e+01" stime="6.70002e+00" mtime="3.24861e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24861e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127164" bytes="8.4967e+08" > 4.9655e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4922e+08" > 3.1811e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6412e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4362e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0680e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5799e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.7305e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3092e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6220e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9159e+01 </func>
</region>
</regions>
<internal rank="119" log_i="1723713849.513410" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="120" mpi_size="768" stamp_init="1723713791.104146" stamp_final="1723713849.507339" username="apac4" allocationname="unknown" flags="0" pid="711618" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.84032e+01" utime="4.15147e+01" stime="1.31826e+01" mtime="3.23075e+01" gflop="0.00000e+00" gbyte="3.86467e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23075e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4651467146814c05668146714dc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83379e+01" utime="4.14868e+01" stime="1.31706e+01" mtime="3.23075e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23075e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4772e+08" > 9.2122e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4802e+08" > 5.6278e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.4869e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4178e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6343e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0214e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5794e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.7353e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9111e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5936e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7654e+01 </func>
</region>
</regions>
<internal rank="120" log_i="1723713849.507339" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="121" mpi_size="768" stamp_init="1723713791.108731" stamp_final="1723713849.508350" username="apac4" allocationname="unknown" flags="0" pid="711619" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83996e+01" utime="5.02593e+01" stime="6.83125e+00" mtime="3.21239e+01" gflop="0.00000e+00" gbyte="3.77567e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21239e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005815571501" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83187e+01" utime="5.02283e+01" stime="6.82146e+00" mtime="3.21239e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21239e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4928e+08" > 5.6070e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4670e+08" > 3.5306e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4098e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4182e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0219e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5793e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.7848e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0088e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6394e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8944e+01 </func>
</region>
</regions>
<internal rank="121" log_i="1723713849.508350" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="122" mpi_size="768" stamp_init="1723713791.104147" stamp_final="1723713849.502281" username="apac4" allocationname="unknown" flags="0" pid="711620" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83981e+01" utime="4.79537e+01" stime="7.74424e+00" mtime="3.22309e+01" gflop="0.00000e+00" gbyte="3.76186e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22309e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ed14ee14ef141456ef14ef14a1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83326e+01" utime="4.79207e+01" stime="7.73844e+00" mtime="3.22309e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22309e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4793e+08" > 7.2055e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4903e+08" > 4.2752e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6143e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4097e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3049e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0961e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5788e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.8396e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0494e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6419e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8667e+01 </func>
</region>
</regions>
<internal rank="122" log_i="1723713849.502281" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="123" mpi_size="768" stamp_init="1723713791.104154" stamp_final="1723713849.513886" username="apac4" allocationname="unknown" flags="0" pid="711621" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.84097e+01" utime="5.01592e+01" stime="6.90682e+00" mtime="3.20317e+01" gflop="0.00000e+00" gbyte="3.76621e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20317e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83454e+01" utime="5.01259e+01" stime="6.90093e+00" mtime="3.20317e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20317e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4843e+08" > 5.6147e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4790e+08" > 2.6949e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9956e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4062e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0951e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5788e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.8612e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0899e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6389e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9278e+01 </func>
</region>
</regions>
<internal rank="123" log_i="1723713849.513886" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="124" mpi_size="768" stamp_init="1723713791.105485" stamp_final="1723713849.500659" username="apac4" allocationname="unknown" flags="0" pid="711622" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83952e+01" utime="4.82063e+01" stime="7.70731e+00" mtime="3.21829e+01" gflop="0.00000e+00" gbyte="3.76205e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21829e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002e1418552e142d14f8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83308e+01" utime="4.81701e+01" stime="7.70571e+00" mtime="3.21829e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21829e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127164" bytes="8.4816e+08" > 6.8066e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4784e+08" > 5.4410e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6257e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4350e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5318e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0844e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5788e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.8429e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0899e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6428e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8623e+01 </func>
</region>
</regions>
<internal rank="124" log_i="1723713849.500659" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="125" mpi_size="768" stamp_init="1723713791.108986" stamp_final="1723713849.508120" username="apac4" allocationname="unknown" flags="0" pid="711623" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83991e+01" utime="4.98974e+01" stime="7.12392e+00" mtime="3.27028e+01" gflop="0.00000e+00" gbyte="3.77029e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27028e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003015d855301530150a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83226e+01" utime="4.98685e+01" stime="7.11341e+00" mtime="3.27028e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27028e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 5.5021e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4810e+08" > 3.0266e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6745e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4205e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0970e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5787e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.8170e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4189e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6398e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9264e+01 </func>
</region>
</regions>
<internal rank="125" log_i="1723713849.508120" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="126" mpi_size="768" stamp_init="1723713791.112264" stamp_final="1723713849.505902" username="apac4" allocationname="unknown" flags="0" pid="711624" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83936e+01" utime="4.73377e+01" stime="7.84091e+00" mtime="3.17372e+01" gflop="0.00000e+00" gbyte="3.76839e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17372e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000811481147d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83144e+01" utime="4.73068e+01" stime="7.83134e+00" mtime="3.17372e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17372e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4989e+08" > 9.8798e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5019e+08" > 5.8271e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1147e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4160e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.6197e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0845e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5778e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.9263e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2687e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6425e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8388e+01 </func>
</region>
</regions>
<internal rank="126" log_i="1723713849.505902" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="127" mpi_size="768" stamp_init="1723713791.112822" stamp_final="1723713849.505180" username="apac4" allocationname="unknown" flags="0" pid="711625" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83924e+01" utime="5.00397e+01" stime="7.06736e+00" mtime="3.25009e+01" gflop="0.00000e+00" gbyte="3.76137e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25009e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83156e+01" utime="5.00092e+01" stime="7.05861e+00" mtime="3.25009e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25009e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4901e+08" > 5.6424e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5061e+08" > 3.2039e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2183e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4171e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0853e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5778e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.9532e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8801e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6409e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8507e+01 </func>
</region>
</regions>
<internal rank="127" log_i="1723713849.505180" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="128" mpi_size="768" stamp_init="1723713791.118492" stamp_final="1723713849.504413" username="apac4" allocationname="unknown" flags="0" pid="711626" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83859e+01" utime="4.70618e+01" stime="8.13767e+00" mtime="3.22102e+01" gflop="0.00000e+00" gbyte="3.76881e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22102e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83040e+01" utime="4.70313e+01" stime="8.12807e+00" mtime="3.22102e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22102e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4992e+08" > 7.5171e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4887e+08" > 5.4128e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0816e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.5047e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.7268e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5764e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.0969e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8801e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6439e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8127e+01 </func>
</region>
</regions>
<internal rank="128" log_i="1723713849.504413" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="129" mpi_size="768" stamp_init="1723713791.117257" stamp_final="1723713849.511009" username="apac4" allocationname="unknown" flags="0" pid="711627" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83938e+01" utime="5.03943e+01" stime="6.74767e+00" mtime="3.27533e+01" gflop="0.00000e+00" gbyte="3.77316e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27533e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4951496149714cc5597149714d1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83003e+01" utime="5.03665e+01" stime="6.73550e+00" mtime="3.27533e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27533e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4892e+08" > 4.2363e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5018e+08" > 3.3444e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7848e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4235e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0955e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5760e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.1283e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1304e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6407e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9324e+01 </func>
</region>
</regions>
<internal rank="129" log_i="1723713849.511009" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="130" mpi_size="768" stamp_init="1723713791.118874" stamp_final="1723713849.505989" username="apac4" allocationname="unknown" flags="0" pid="711628" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83871e+01" utime="4.79883e+01" stime="7.60262e+00" mtime="3.21622e+01" gflop="0.00000e+00" gbyte="3.77853e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21622e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83051e+01" utime="4.79565e+01" stime="7.59385e+00" mtime="3.21622e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21622e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4896e+08" > 5.9358e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4874e+08" > 4.9779e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4878e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4301e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.0118e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0960e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5754e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.1374e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3903e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6438e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8830e+01 </func>
</region>
</regions>
<internal rank="130" log_i="1723713849.505989" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="131" mpi_size="768" stamp_init="1723713791.121029" stamp_final="1723713849.511884" username="apac4" allocationname="unknown" flags="0" pid="711629" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83909e+01" utime="5.02399e+01" stime="6.89768e+00" mtime="3.32239e+01" gflop="0.00000e+00" gbyte="3.74798e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.32239e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83105e+01" utime="5.02108e+01" stime="6.88692e+00" mtime="3.32239e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.32239e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4838e+08" > 4.3262e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4904e+08" > 3.1472e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9545e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4304e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7643e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0959e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5752e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.1949e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3998e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6428e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9611e+01 </func>
</region>
</regions>
<internal rank="131" log_i="1723713849.511884" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="132" mpi_size="768" stamp_init="1723713791.122904" stamp_final="1723713849.508835" username="apac4" allocationname="unknown" flags="0" pid="711630" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83859e+01" utime="4.71697e+01" stime="7.66048e+00" mtime="3.19235e+01" gflop="0.00000e+00" gbyte="3.77380e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19235e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83043e+01" utime="4.71408e+01" stime="7.64958e+00" mtime="3.19235e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19235e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4854e+08" > 7.5608e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4791e+08" > 6.6791e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4566e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4194e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6742e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0455e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5756e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.1331e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3092e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6444e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8450e+01 </func>
</region>
</regions>
<internal rank="132" log_i="1723713849.508835" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="133" mpi_size="768" stamp_init="1723713791.130609" stamp_final="1723713849.500162" username="apac4" allocationname="unknown" flags="0" pid="711631" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83696e+01" utime="5.01834e+01" stime="6.93336e+00" mtime="3.25278e+01" gflop="0.00000e+00" gbyte="3.77350e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25278e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82877e+01" utime="5.01568e+01" stime="6.91969e+00" mtime="3.25278e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25278e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4860e+08" > 4.3315e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4817e+08" > 3.7614e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1806e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4296e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0456e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5756e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.1678e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0589e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6409e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9688e+01 </func>
</region>
</regions>
<internal rank="133" log_i="1723713849.500162" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="134" mpi_size="768" stamp_init="1723713791.128421" stamp_final="1723713849.506572" username="apac4" allocationname="unknown" flags="0" pid="711632" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83782e+01" utime="4.71778e+01" stime="7.56298e+00" mtime="3.26484e+01" gflop="0.00000e+00" gbyte="3.75206e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26484e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f415b756f415f41515" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82944e+01" utime="4.71450e+01" stime="7.55568e+00" mtime="3.26484e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26484e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4816e+08" > 5.7178e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5076e+08" > 4.8542e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1539e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4830e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7083e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0468e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5745e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.2728e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1996e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6440e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8673e+01 </func>
</region>
</regions>
<internal rank="134" log_i="1723713849.506572" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="135" mpi_size="768" stamp_init="1723713791.130606" stamp_final="1723713849.499908" username="apac4" allocationname="unknown" flags="0" pid="711633" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83693e+01" utime="4.90805e+01" stime="6.71889e+00" mtime="3.25789e+01" gflop="0.00000e+00" gbyte="3.76823e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25789e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82894e+01" utime="4.90523e+01" stime="6.70777e+00" mtime="3.25789e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25789e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5081e+08" > 4.1180e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4891e+08" > 3.3605e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7428e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4789e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6226e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0468e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5744e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.2888e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3808e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6398e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9211e+01 </func>
</region>
</regions>
<internal rank="135" log_i="1723713849.499908" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="136" mpi_size="768" stamp_init="1723713791.133809" stamp_final="1723713849.501734" username="apac4" allocationname="unknown" flags="0" pid="711634" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83679e+01" utime="4.74019e+01" stime="7.95872e+00" mtime="3.19903e+01" gflop="0.00000e+00" gbyte="3.77407e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19903e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43a143c143d1449563d143d1477" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82885e+01" utime="4.73696e+01" stime="7.95155e+00" mtime="3.19903e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19903e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4782e+08" > 7.1582e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5054e+08" > 5.3010e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.3299e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4087e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2627e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0469e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5742e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.2962e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3998e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6417e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7712e+01 </func>
</region>
</regions>
<internal rank="136" log_i="1723713849.501734" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="137" mpi_size="768" stamp_init="1723713791.134748" stamp_final="1723713849.506805" username="apac4" allocationname="unknown" flags="0" pid="711635" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83721e+01" utime="5.01046e+01" stime="7.01980e+00" mtime="3.25493e+01" gflop="0.00000e+00" gbyte="3.76507e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25493e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82929e+01" utime="5.00763e+01" stime="7.00909e+00" mtime="3.25493e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25493e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4997e+08" > 4.6656e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4881e+08" > 3.0336e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9338e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4196e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0833e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5737e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.3675e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2210e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6395e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8936e+01 </func>
</region>
</regions>
<internal rank="137" log_i="1723713849.506805" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="138" mpi_size="768" stamp_init="1723713791.142262" stamp_final="1723713849.512091" username="apac4" allocationname="unknown" flags="0" pid="711636" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83698e+01" utime="4.74182e+01" stime="7.94783e+00" mtime="3.18649e+01" gflop="0.00000e+00" gbyte="3.77380e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18649e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006b1476566b146b1488" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82970e+01" utime="4.73844e+01" stime="7.94277e+00" mtime="3.18649e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18649e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4889e+08" > 6.4278e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4894e+08" > 5.1669e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8295e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4200e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.8467e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0470e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5741e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.3038e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2091e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6409e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8162e+01 </func>
</region>
</regions>
<internal rank="138" log_i="1723713849.512091" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="139" mpi_size="768" stamp_init="1723713791.139222" stamp_final="1723713849.504101" username="apac4" allocationname="unknown" flags="0" pid="711637" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83649e+01" utime="5.00651e+01" stime="6.88783e+00" mtime="3.19836e+01" gflop="0.00000e+00" gbyte="3.76850e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19836e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f114f314f4141156f414f314b2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82906e+01" utime="5.00321e+01" stime="6.88216e+00" mtime="3.19836e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19836e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4876e+08" > 4.7770e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4944e+08" > 3.2382e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2290e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4136e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5259e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0774e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5729e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.4098e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1090e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6380e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9069e+01 </func>
</region>
</regions>
<internal rank="139" log_i="1723713849.504101" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="140" mpi_size="768" stamp_init="1723713791.142042" stamp_final="1723713849.511571" username="apac4" allocationname="unknown" flags="0" pid="711638" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83695e+01" utime="4.76718e+01" stime="8.00569e+00" mtime="3.21740e+01" gflop="0.00000e+00" gbyte="3.77251e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21740e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4fe15ff15b2151e55b2155f150c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82957e+01" utime="4.76402e+01" stime="7.99850e+00" mtime="3.21740e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21740e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4981e+08" > 6.3891e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4738e+08" > 5.1212e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2544e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4244e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3370e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0955e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5726e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.4451e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1304e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6422e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8028e+01 </func>
</region>
</regions>
<internal rank="140" log_i="1723713849.511571" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="141" mpi_size="768" stamp_init="1723713791.143820" stamp_final="1723713849.515124" username="apac4" allocationname="unknown" flags="0" pid="711639" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83713e+01" utime="5.03003e+01" stime="6.85220e+00" mtime="3.21826e+01" gflop="0.00000e+00" gbyte="3.76987e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21826e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002d143a552d142c14f0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82967e+01" utime="5.02651e+01" stime="6.84902e+00" mtime="3.21826e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21826e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 4.6605e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4993e+08" > 2.8663e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2122e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4000e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2398e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0955e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5730e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.4351e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9898e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6347e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9312e+01 </func>
</region>
</regions>
<internal rank="141" log_i="1723713849.515124" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="142" mpi_size="768" stamp_init="1723713791.145589" stamp_final="1723713849.510882" username="apac4" allocationname="unknown" flags="0" pid="711640" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83653e+01" utime="4.78000e+01" stime="7.88588e+00" mtime="3.21615e+01" gflop="0.00000e+00" gbyte="3.74737e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21615e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4821483148414ff5584148414cd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82929e+01" utime="4.77715e+01" stime="7.87622e+00" mtime="3.21615e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21615e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5016e+08" > 6.7963e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4780e+08" > 5.6527e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0494e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4205e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.6083e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0958e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5724e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.4951e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1281e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6411e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8192e+01 </func>
</region>
</regions>
<internal rank="142" log_i="1723713849.510882" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="143" mpi_size="768" stamp_init="1723713791.148069" stamp_final="1723713849.507019" username="apac4" allocationname="unknown" flags="0" pid="711641" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="5.83590e+01" utime="5.00666e+01" stime="7.08337e+00" mtime="3.26915e+01" gflop="0.00000e+00" gbyte="3.76583e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26915e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4301532153315a8563315331536" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82890e+01" utime="5.00337e+01" stime="7.07792e+00" mtime="3.26915e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26915e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4884e+08" > 4.8322e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4873e+08" > 2.8740e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8477e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4145e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6226e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0968e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5728e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.4539e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3021e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6347e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9153e+01 </func>
</region>
</regions>
<internal rank="143" log_i="1723713849.507019" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="144" mpi_size="768" stamp_init="1723713791.145971" stamp_final="1723713849.513324" username="apac4" allocationname="unknown" flags="0" pid="580295" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83674e+01" utime="4.25148e+01" stime="1.19491e+01" mtime="3.15772e+01" gflop="0.00000e+00" gbyte="3.85632e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15772e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82543e+01" utime="4.24846e+01" stime="1.19379e+01" mtime="3.15772e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15772e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5056e+08" > 6.3275e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4907e+08" > 4.5781e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2666e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4222e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2544e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0403e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5710e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.6045e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1710e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5895e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8516e+01 </func>
</region>
</regions>
<internal rank="144" log_i="1723713849.513324" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="145" mpi_size="768" stamp_init="1723713791.145990" stamp_final="1723713849.508515" username="apac4" allocationname="unknown" flags="0" pid="580296" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83625e+01" utime="4.93887e+01" stime="6.67290e+00" mtime="3.27846e+01" gflop="0.00000e+00" gbyte="3.76976e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27846e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82481e+01" utime="4.93518e+01" stime="6.66740e+00" mtime="3.27846e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27846e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4928e+08" > 4.5146e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4953e+08" > 2.9238e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7943e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4434e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0405e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5724e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.4941e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5606e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6135e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9372e+01 </func>
</region>
</regions>
<internal rank="145" log_i="1723713849.508515" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="146" mpi_size="768" stamp_init="1723713791.145974" stamp_final="1723713849.507873" username="apac4" allocationname="unknown" flags="0" pid="580297" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83619e+01" utime="4.73208e+01" stime="7.24968e+00" mtime="3.17100e+01" gflop="0.00000e+00" gbyte="3.76072e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17100e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006c156c1527" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82537e+01" utime="4.72838e+01" stime="7.24503e+00" mtime="3.17100e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17100e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4895e+08" > 6.0420e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4927e+08" > 4.3735e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2642e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4402e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.4400e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0404e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5712e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.5957e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3402e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6309e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8664e+01 </func>
</region>
</regions>
<internal rank="146" log_i="1723713849.507873" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="147" mpi_size="768" stamp_init="1723713791.146268" stamp_final="1723713849.511584" username="apac4" allocationname="unknown" flags="0" pid="580298" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83653e+01" utime="4.95548e+01" stime="6.51765e+00" mtime="3.25523e+01" gflop="0.00000e+00" gbyte="3.77460e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25523e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f314f414f5148555f514f514de" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82524e+01" utime="4.95313e+01" stime="6.49873e+00" mtime="3.25523e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25523e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4833e+08" > 4.5473e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4974e+08" > 2.8168e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3265e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4495e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0441e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5717e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.5655e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4308e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6097e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9595e+01 </func>
</region>
</regions>
<internal rank="147" log_i="1723713849.511584" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="148" mpi_size="768" stamp_init="1723713791.145976" stamp_final="1723713849.508342" username="apac4" allocationname="unknown" flags="0" pid="580299" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83624e+01" utime="4.64910e+01" stime="7.60785e+00" mtime="3.18996e+01" gflop="0.00000e+00" gbyte="3.75416e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18996e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4631464146514925565146514a7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82545e+01" utime="4.64640e+01" stime="7.59329e+00" mtime="3.18996e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18996e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5051e+08" > 7.1400e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4906e+08" > 5.1069e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3505e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4475e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2368e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0405e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5707e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.6007e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5821e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6313e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8640e+01 </func>
</region>
</regions>
<internal rank="148" log_i="1723713849.508342" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="149" mpi_size="768" stamp_init="1723713791.146164" stamp_final="1723713849.505299" username="apac4" allocationname="unknown" flags="0" pid="580300" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83591e+01" utime="4.96136e+01" stime="6.36762e+00" mtime="3.21844e+01" gflop="0.00000e+00" gbyte="3.77319e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21844e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000012156f551215121506" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82453e+01" utime="4.95859e+01" stime="6.35335e+00" mtime="3.21844e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21844e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4969e+08" > 4.5873e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4983e+08" > 3.0174e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7135e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4469e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0466e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5710e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.6007e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6107e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6103e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9835e+01 </func>
</region>
</regions>
<internal rank="149" log_i="1723713849.505299" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="150" mpi_size="768" stamp_init="1723713791.146156" stamp_final="1723713849.513451" username="apac4" allocationname="unknown" flags="0" pid="580301" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83673e+01" utime="4.59926e+01" stime="8.01741e+00" mtime="3.20690e+01" gflop="0.00000e+00" gbyte="3.77571e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20690e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a714a814aa147255aa14a914a5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82548e+01" utime="4.59633e+01" stime="8.00533e+00" mtime="3.20690e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20690e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4952e+08" > 7.7918e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4943e+08" > 6.5120e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6048e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4350e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6377e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0405e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5699e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.7087e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7204e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6143e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8489e+01 </func>
</region>
</regions>
<internal rank="150" log_i="1723713849.513451" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="151" mpi_size="768" stamp_init="1723713791.146279" stamp_final="1723713849.520691" username="apac4" allocationname="unknown" flags="0" pid="580302" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83744e+01" utime="4.93607e+01" stime="6.47681e+00" mtime="3.23292e+01" gflop="0.00000e+00" gbyte="3.75919e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23292e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf438143a143b1469553b143a14d0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82614e+01" utime="4.93243e+01" stime="6.47080e+00" mtime="3.23292e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23292e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4974e+08" > 4.6101e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5061e+08" > 3.0717e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4358e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4429e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0407e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5708e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.6623e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9993e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6101e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9264e+01 </func>
</region>
</regions>
<internal rank="151" log_i="1723713849.520691" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="152" mpi_size="768" stamp_init="1723713791.146203" stamp_final="1723713849.513923" username="apac4" allocationname="unknown" flags="0" pid="580303" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83677e+01" utime="4.64845e+01" stime="7.75517e+00" mtime="3.18492e+01" gflop="0.00000e+00" gbyte="3.78033e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18492e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf45f1460146114ab56611461147b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82577e+01" utime="4.64560e+01" stime="7.74200e+00" mtime="3.18492e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18492e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4946e+08" > 6.0858e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4876e+08" > 4.7476e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8082e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4490e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.0859e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0405e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5700e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.7297e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0208e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6144e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8237e+01 </func>
</region>
</regions>
<internal rank="152" log_i="1723713849.513923" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="153" mpi_size="768" stamp_init="1723713791.145982" stamp_final="1723713849.511731" username="apac4" allocationname="unknown" flags="0" pid="580304" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83657e+01" utime="4.92491e+01" stime="6.76259e+00" mtime="3.31381e+01" gflop="0.00000e+00" gbyte="3.78208e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.31381e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004915491547" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82520e+01" utime="4.92142e+01" stime="6.75521e+00" mtime="3.31381e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.31381e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4865e+08" > 3.6376e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4954e+08" > 2.9678e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1406e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4565e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7180e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0468e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5697e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.7369e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9206e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6099e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9447e+01 </func>
</region>
</regions>
<internal rank="153" log_i="1723713849.511731" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="154" mpi_size="768" stamp_init="1723713791.149312" stamp_final="1723713849.501539" username="apac4" allocationname="unknown" flags="0" pid="580305" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83522e+01" utime="4.76203e+01" stime="7.24570e+00" mtime="3.21634e+01" gflop="0.00000e+00" gbyte="3.76392e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21634e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4fd14ff143d1435563d14ff1486" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82399e+01" utime="4.75871e+01" stime="7.23778e+00" mtime="3.21634e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21634e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4828e+08" > 5.0837e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4852e+08" > 4.0752e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8770e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4298e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0824e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0414e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5689e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.7733e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0684e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6144e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8613e+01 </func>
</region>
</regions>
<internal rank="154" log_i="1723713849.501539" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="155" mpi_size="768" stamp_init="1723713791.154084" stamp_final="1723713849.504342" username="apac4" allocationname="unknown" flags="0" pid="580306" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83503e+01" utime="4.90660e+01" stime="6.72088e+00" mtime="3.26647e+01" gflop="0.00000e+00" gbyte="3.77766e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26647e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d7141755d714d614d7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82368e+01" utime="4.90355e+01" stime="6.70922e+00" mtime="3.26647e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26647e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4894e+08" > 3.5384e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4803e+08" > 2.7883e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6076e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4541e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0440e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5699e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.7495e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9492e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6077e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9523e+01 </func>
</region>
</regions>
<internal rank="155" log_i="1723713849.504342" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="156" mpi_size="768" stamp_init="1723713791.156647" stamp_final="1723713849.513080" username="apac4" allocationname="unknown" flags="0" pid="580307" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83564e+01" utime="4.68774e+01" stime="7.56801e+00" mtime="3.19502e+01" gflop="0.00000e+00" gbyte="3.77003e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19502e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82439e+01" utime="4.68375e+01" stime="7.56678e+00" mtime="3.19502e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19502e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4911e+08" > 5.7258e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4833e+08" > 4.1134e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9547e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4319e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.4618e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0405e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5676e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.9782e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9111e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6114e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8255e+01 </func>
</region>
</regions>
<internal rank="156" log_i="1723713849.513080" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="157" mpi_size="768" stamp_init="1723713791.154088" stamp_final="1723713849.504313" username="apac4" allocationname="unknown" flags="0" pid="580308" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83502e+01" utime="4.94269e+01" stime="6.62781e+00" mtime="3.29108e+01" gflop="0.00000e+00" gbyte="3.77731e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.29108e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82369e+01" utime="4.93949e+01" stime="6.61838e+00" mtime="3.29108e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.29108e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4736e+08" > 3.7606e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4894e+08" > 2.8843e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4125e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4529e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0406e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5689e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.8508e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3021e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6092e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9946e+01 </func>
</region>
</regions>
<internal rank="157" log_i="1723713849.504313" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="158" mpi_size="768" stamp_init="1723713791.153389" stamp_final="1723713849.501720" username="apac4" allocationname="unknown" flags="0" pid="580309" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83483e+01" utime="4.78567e+01" stime="7.13115e+00" mtime="3.20473e+01" gflop="0.00000e+00" gbyte="3.76904e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20473e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003914381468" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82332e+01" utime="4.78250e+01" stime="7.12148e+00" mtime="3.20473e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20473e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4938e+08" > 4.6444e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4926e+08" > 4.0603e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7430e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4410e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1231e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0460e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5679e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.9475e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8110e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6116e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8656e+01 </func>
</region>
</regions>
<internal rank="158" log_i="1723713849.501720" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="159" mpi_size="768" stamp_init="1723713791.154201" stamp_final="1723713849.501413" username="apac4" allocationname="unknown" flags="0" pid="580310" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83472e+01" utime="4.94933e+01" stime="6.53984e+00" mtime="3.25622e+01" gflop="0.00000e+00" gbyte="3.76495e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25622e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf45b155d155e1523555e155e150e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82339e+01" utime="4.94601e+01" stime="6.53204e+00" mtime="3.25622e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25622e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4921e+08" > 3.5738e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4913e+08" > 2.7099e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8309e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4318e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0460e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5684e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.9062e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3116e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6086e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9215e+01 </func>
</region>
</regions>
<internal rank="159" log_i="1723713849.501413" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="160" mpi_size="768" stamp_init="1723713791.156244" stamp_final="1723713849.513418" username="apac4" allocationname="unknown" flags="0" pid="580311" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83572e+01" utime="4.70845e+01" stime="7.61386e+00" mtime="3.18047e+01" gflop="0.00000e+00" gbyte="3.76812e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18047e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82458e+01" utime="4.70500e+01" stime="7.60670e+00" mtime="3.18047e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18047e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4825e+08" > 6.8971e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4909e+08" > 4.7902e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8646e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4423e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.9700e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0403e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5674e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.9596e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3188e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6140e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8060e+01 </func>
</region>
</regions>
<internal rank="160" log_i="1723713849.513418" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="161" mpi_size="768" stamp_init="1723713791.158715" stamp_final="1723713849.508015" username="apac4" allocationname="unknown" flags="0" pid="580312" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83493e+01" utime="4.91776e+01" stime="6.87546e+00" mtime="3.24315e+01" gflop="0.00000e+00" gbyte="3.77270e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24315e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82370e+01" utime="4.91478e+01" stime="6.86404e+00" mtime="3.24315e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24315e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4815e+08" > 4.9804e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4948e+08" > 3.1344e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0717e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4395e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0406e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5671e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.0374e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8896e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6092e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8696e+01 </func>
</region>
</regions>
<internal rank="161" log_i="1723713849.508015" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="162" mpi_size="768" stamp_init="1723713791.162320" stamp_final="1723713849.513676" username="apac4" allocationname="unknown" flags="0" pid="580313" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83514e+01" utime="4.72830e+01" stime="7.48742e+00" mtime="3.17882e+01" gflop="0.00000e+00" gbyte="3.78235e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17882e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000fd141d56fd14fd1459" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82415e+01" utime="4.72479e+01" stime="7.48128e+00" mtime="3.17882e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17882e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4936e+08" > 6.2887e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4815e+08" > 4.5743e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7574e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4433e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.8678e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0406e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5674e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.9983e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8396e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6129e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8218e+01 </func>
</region>
</regions>
<internal rank="162" log_i="1723713849.513676" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="163" mpi_size="768" stamp_init="1723713791.166774" stamp_final="1723713849.504241" username="apac4" allocationname="unknown" flags="0" pid="580314" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83375e+01" utime="4.92185e+01" stime="6.77456e+00" mtime="3.24164e+01" gflop="0.00000e+00" gbyte="3.76244e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24164e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c714f155c714c61478" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82310e+01" utime="4.91871e+01" stime="6.76581e+00" mtime="3.24164e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24164e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5038e+08" > 4.8131e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4841e+08" > 3.0078e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5076e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4488e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6226e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0410e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5668e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.0766e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6989e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6077e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9254e+01 </func>
</region>
</regions>
<internal rank="163" log_i="1723713849.504241" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="164" mpi_size="768" stamp_init="1723713791.171584" stamp_final="1723713849.513045" username="apac4" allocationname="unknown" flags="0" pid="580315" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83415e+01" utime="4.74490e+01" stime="7.40388e+00" mtime="3.18870e+01" gflop="0.00000e+00" gbyte="3.76404e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18870e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a014f655a014a01486" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82388e+01" utime="4.74205e+01" stime="7.39230e+00" mtime="3.18870e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18870e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4808e+08" > 6.0451e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5021e+08" > 4.2430e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5655e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4419e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.5844e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0404e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5665e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.0880e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2902e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6106e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8538e+01 </func>
</region>
</regions>
<internal rank="164" log_i="1723713849.513045" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="165" mpi_size="768" stamp_init="1723713791.169141" stamp_final="1723713849.505823" username="apac4" allocationname="unknown" flags="0" pid="580316" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83367e+01" utime="4.94982e+01" stime="6.55290e+00" mtime="3.25711e+01" gflop="0.00000e+00" gbyte="3.77293e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25711e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82327e+01" utime="4.94685e+01" stime="6.54181e+00" mtime="3.25711e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25711e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5021e+08" > 4.8586e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4865e+08" > 2.9999e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2975e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4268e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2650e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0444e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5667e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.0798e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7990e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6097e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9633e+01 </func>
</region>
</regions>
<internal rank="165" log_i="1723713849.505823" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="166" mpi_size="768" stamp_init="1723713791.170305" stamp_final="1723713849.508236" username="apac4" allocationname="unknown" flags="0" pid="580317" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83379e+01" utime="4.74785e+01" stime="7.46706e+00" mtime="3.17730e+01" gflop="0.00000e+00" gbyte="3.77174e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17730e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a214a314a4144056a414a4147b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82366e+01" utime="4.74512e+01" stime="7.45399e+00" mtime="3.17730e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17730e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4926e+08" > 6.0271e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4989e+08" > 4.5525e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8341e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4032e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1563e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0408e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5653e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.1623e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8181e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6112e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8191e+01 </func>
</region>
</regions>
<internal rank="166" log_i="1723713849.508236" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="167" mpi_size="768" stamp_init="1723713791.177560" stamp_final="1723713849.511559" username="apac4" allocationname="unknown" flags="0" pid="580318" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="5.83340e+01" utime="4.90978e+01" stime="6.93409e+00" mtime="3.22860e+01" gflop="0.00000e+00" gbyte="3.76568e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22860e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82364e+01" utime="4.90666e+01" stime="6.92554e+00" mtime="3.22860e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22860e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4925e+08" > 4.8423e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4835e+08" > 3.0702e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0902e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4378e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0408e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5659e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.1562e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9683e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6077e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8548e+01 </func>
</region>
</regions>
<internal rank="167" log_i="1723713849.511559" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="168" mpi_size="768" stamp_init="1723713791.105716" stamp_final="1723713849.520781" username="apac4" allocationname="unknown" flags="0" pid="691292" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.84151e+01" utime="4.28447e+01" stime="1.20531e+01" mtime="3.22002e+01" gflop="0.00000e+00" gbyte="3.86814e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22002e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b6149256b614b61476" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83602e+01" utime="4.28114e+01" stime="1.20466e+01" mtime="3.22002e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22002e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4935e+08" > 6.2084e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4920e+08" > 4.6605e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2497e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4351e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3210e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0603e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5660e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.1325e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5988e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6089e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8132e+01 </func>
</region>
</regions>
<internal rank="168" log_i="1723713849.520781" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="169" mpi_size="768" stamp_init="1723713791.105842" stamp_final="1723713849.510303" username="apac4" allocationname="unknown" flags="0" pid="691293" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.84045e+01" utime="4.94779e+01" stime="6.57623e+00" mtime="3.22333e+01" gflop="0.00000e+00" gbyte="3.77739e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22333e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007a157b557a157a1526" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83439e+01" utime="4.94502e+01" stime="6.56279e+00" mtime="3.22333e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22333e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4761e+08" > 4.5090e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5003e+08" > 3.0665e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3330e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4007e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7180e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0661e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5666e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.0871e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7585e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6034e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9298e+01 </func>
</region>
</regions>
<internal rank="169" log_i="1723713849.510303" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="170" mpi_size="768" stamp_init="1723713791.106209" stamp_final="1723713849.513232" username="apac4" allocationname="unknown" flags="0" pid="691294" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.84070e+01" utime="4.72513e+01" stime="7.47254e+00" mtime="3.22914e+01" gflop="0.00000e+00" gbyte="3.78071e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22914e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d615d6151f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83561e+01" utime="4.72139e+01" stime="7.47030e+00" mtime="3.22914e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22914e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4869e+08" > 5.9686e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4798e+08" > 6.2839e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7567e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4121e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.4652e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0648e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5658e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.1069e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8586e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6077e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8745e+01 </func>
</region>
</regions>
<internal rank="170" log_i="1723713849.513232" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="171" mpi_size="768" stamp_init="1723713791.108545" stamp_final="1723713849.506060" username="apac4" allocationname="unknown" flags="0" pid="691295" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83975e+01" utime="4.94134e+01" stime="6.65209e+00" mtime="3.26596e+01" gflop="0.00000e+00" gbyte="3.77899e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26596e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001f15f8551f151f151a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83396e+01" utime="4.93802e+01" stime="6.64541e+00" mtime="3.26596e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26596e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4950e+08" > 4.4827e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4868e+08" > 2.9140e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6754e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4385e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1696e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0682e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5657e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.1445e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0804e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5552e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9347e+01 </func>
</region>
</regions>
<internal rank="171" log_i="1723713849.506060" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="172" mpi_size="768" stamp_init="1723713791.117775" stamp_final="1723713849.520061" username="apac4" allocationname="unknown" flags="0" pid="691296" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.84023e+01" utime="4.67184e+01" stime="7.74043e+00" mtime="3.20988e+01" gflop="0.00000e+00" gbyte="3.77277e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20988e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83193e+01" utime="4.66830e+01" stime="7.73455e+00" mtime="3.20988e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20988e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4933e+08" > 6.2918e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4882e+08" > 4.3783e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6331e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4414e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4687e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0605e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5652e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.1827e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9683e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6059e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8637e+01 </func>
</region>
</regions>
<internal rank="172" log_i="1723713849.520061" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="173" mpi_size="768" stamp_init="1723713791.114825" stamp_final="1723713849.501521" username="apac4" allocationname="unknown" flags="0" pid="691297" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83867e+01" utime="4.96347e+01" stime="6.48149e+00" mtime="3.20836e+01" gflop="0.00000e+00" gbyte="3.75729e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20836e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f614f61499" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83013e+01" utime="4.95996e+01" stime="6.47416e+00" mtime="3.20836e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20836e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4849e+08" > 4.4140e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4873e+08" > 2.6929e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7280e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5106e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0094e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5655e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.2002e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3307e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6042e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9713e+01 </func>
</region>
</regions>
<internal rank="173" log_i="1723713849.501521" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="174" mpi_size="768" stamp_init="1723713791.117785" stamp_final="1723713849.514021" username="apac4" allocationname="unknown" flags="0" pid="691298" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83962e+01" utime="4.71934e+01" stime="7.48697e+00" mtime="3.18109e+01" gflop="0.00000e+00" gbyte="3.78025e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18109e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83143e+01" utime="4.71604e+01" stime="7.47844e+00" mtime="3.18109e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18109e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4877e+08" > 5.8822e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4932e+08" > 3.8260e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6617e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4303e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.0797e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0639e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5644e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.2300e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7204e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6069e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8375e+01 </func>
</region>
</regions>
<internal rank="174" log_i="1723713849.514021" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="175" mpi_size="768" stamp_init="1723713791.120186" stamp_final="1723713849.508029" username="apac4" allocationname="unknown" flags="0" pid="691299" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83878e+01" utime="4.93895e+01" stime="6.66547e+00" mtime="3.22055e+01" gflop="0.00000e+00" gbyte="3.75702e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22055e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007b147a14fb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82977e+01" utime="4.93583e+01" stime="6.65410e+00" mtime="3.22055e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22055e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4990e+08" > 4.4697e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4900e+08" > 3.0790e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6315e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4372e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0678e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5638e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.3406e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1185e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6038e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8938e+01 </func>
</region>
</regions>
<internal rank="175" log_i="1723713849.508029" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="176" mpi_size="768" stamp_init="1723713791.122352" stamp_final="1723713849.501154" username="apac4" allocationname="unknown" flags="0" pid="691300" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83788e+01" utime="4.68126e+01" stime="7.62979e+00" mtime="3.22453e+01" gflop="0.00000e+00" gbyte="3.76480e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22453e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf45e1460146114c85561146114ee" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82939e+01" utime="4.67811e+01" stime="7.62070e+00" mtime="3.22453e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22453e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4904e+08" > 5.3086e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4939e+08" > 4.2032e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.4287e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4359e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2792e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0598e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5633e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.3860e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4881e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6056e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8082e+01 </func>
</region>
</regions>
<internal rank="176" log_i="1723713849.501154" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="177" mpi_size="768" stamp_init="1723713791.131075" stamp_final="1723713849.500829" username="apac4" allocationname="unknown" flags="0" pid="691301" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83698e+01" utime="4.92775e+01" stime="6.79140e+00" mtime="3.26954e+01" gflop="0.00000e+00" gbyte="3.76904e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26954e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82807e+01" utime="4.92502e+01" stime="6.77674e+00" mtime="3.26954e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26954e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5082e+08" > 3.5015e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4729e+08" > 2.7425e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9596e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4365e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0626e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5636e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.3952e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8110e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6030e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9205e+01 </func>
</region>
</regions>
<internal rank="177" log_i="1723713849.500829" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="178" mpi_size="768" stamp_init="1723713791.131408" stamp_final="1723713849.511939" username="apac4" allocationname="unknown" flags="0" pid="691302" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83805e+01" utime="4.79579e+01" stime="7.11528e+00" mtime="3.21276e+01" gflop="0.00000e+00" gbyte="3.78201e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21276e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008c14861494" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82893e+01" utime="4.79239e+01" stime="7.10621e+00" mtime="3.21276e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21276e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4915e+08" > 4.4130e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5075e+08" > 3.3762e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8189e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4377e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.7909e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0622e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5624e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.4615e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2616e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5573e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8681e+01 </func>
</region>
</regions>
<internal rank="178" log_i="1723713849.511939" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="179" mpi_size="768" stamp_init="1723713791.131072" stamp_final="1723713849.515948" username="apac4" allocationname="unknown" flags="0" pid="691303" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83849e+01" utime="4.94782e+01" stime="6.51645e+00" mtime="3.25590e+01" gflop="0.00000e+00" gbyte="3.78033e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25590e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003e1561563e153e1552" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82960e+01" utime="4.94466e+01" stime="6.50513e+00" mtime="3.25590e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25590e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4998e+08" > 3.4134e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5016e+08" > 2.7774e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6216e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4130e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0657e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5626e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.5049e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0398e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6007e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9436e+01 </func>
</region>
</regions>
<internal rank="179" log_i="1723713849.515948" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="180" mpi_size="768" stamp_init="1723713791.132134" stamp_final="1723713849.503538" username="apac4" allocationname="unknown" flags="0" pid="691304" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83714e+01" utime="4.60691e+01" stime="7.72786e+00" mtime="3.20613e+01" gflop="0.00000e+00" gbyte="3.76560e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20613e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf42615281529157b562915291537" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82802e+01" utime="4.60371e+01" stime="7.71882e+00" mtime="3.20613e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20613e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.0967e-05 </func>
<func name="MPI_Isend" count="127164" bytes="8.4891e+08" > 5.9540e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4953e+08" > 5.1268e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7832e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4218e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.8181e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0636e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5625e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.4688e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7895e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6047e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8484e+01 </func>
</region>
</regions>
<internal rank="180" log_i="1723713849.503538" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="181" mpi_size="768" stamp_init="1723713791.135407" stamp_final="1723713849.499381" username="apac4" allocationname="unknown" flags="0" pid="691305" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83640e+01" utime="4.96828e+01" stime="6.43341e+00" mtime="3.25053e+01" gflop="0.00000e+00" gbyte="3.76900e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25053e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000065145c55651465146a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82755e+01" utime="4.96458e+01" stime="6.42931e+00" mtime="3.25053e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25053e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4832e+08" > 3.4037e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4850e+08" > 2.7142e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3952e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5044e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0118e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5627e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.4917e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0804e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5993e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9572e+01 </func>
</region>
</regions>
<internal rank="181" log_i="1723713849.499381" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="182" mpi_size="768" stamp_init="1723713791.142520" stamp_final="1723713849.512286" username="apac4" allocationname="unknown" flags="0" pid="691306" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83698e+01" utime="4.77804e+01" stime="7.33979e+00" mtime="3.27098e+01" gflop="0.00000e+00" gbyte="3.76965e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27098e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006215f456621562150d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82827e+01" utime="4.77473e+01" stime="7.33172e+00" mtime="3.27098e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27098e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5092e+08" > 4.2222e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4841e+08" > 4.0805e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.5708e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4413e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.6294e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0635e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5619e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.5739e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2997e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6057e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8517e+01 </func>
</region>
</regions>
<internal rank="182" log_i="1723713849.512286" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="183" mpi_size="768" stamp_init="1723713791.140477" stamp_final="1723713849.503163" username="apac4" allocationname="unknown" flags="0" pid="691307" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83627e+01" utime="4.92140e+01" stime="6.86431e+00" mtime="3.25302e+01" gflop="0.00000e+00" gbyte="3.78246e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25302e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a814a81480" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82738e+01" utime="4.91864e+01" stime="6.84941e+00" mtime="3.25302e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25302e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4851e+08" > 3.4549e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5049e+08" > 2.7033e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9008e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5148e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0109e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5617e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.5822e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1710e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5989e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9077e+01 </func>
</region>
</regions>
<internal rank="183" log_i="1723713849.503163" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="184" mpi_size="768" stamp_init="1723713791.143368" stamp_final="1723713849.515503" username="apac4" allocationname="unknown" flags="0" pid="691308" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83721e+01" utime="4.62655e+01" stime="7.79831e+00" mtime="3.19996e+01" gflop="0.00000e+00" gbyte="3.76942e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19996e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a414a656a414a314db" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82858e+01" utime="4.62378e+01" stime="7.78505e+00" mtime="3.19996e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19996e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4744e+08" > 6.0198e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4870e+08" > 4.5805e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.5443e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4216e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1743e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0600e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5604e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.7164e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7490e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6055e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7660e+01 </func>
</region>
</regions>
<internal rank="184" log_i="1723713849.515503" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="185" mpi_size="768" stamp_init="1723713791.146797" stamp_final="1723713849.507220" username="apac4" allocationname="unknown" flags="0" pid="691309" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83604e+01" utime="4.90595e+01" stime="7.00651e+00" mtime="3.24811e+01" gflop="0.00000e+00" gbyte="3.76812e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24811e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d4159455d415d4154f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82737e+01" utime="4.90298e+01" stime="6.99498e+00" mtime="3.24811e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24811e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4903e+08" > 4.4882e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4886e+08" > 2.9302e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1024e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4398e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0626e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5608e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.6800e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6703e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5967e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8744e+01 </func>
</region>
</regions>
<internal rank="185" log_i="1723713849.507220" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="186" mpi_size="768" stamp_init="1723713791.147794" stamp_final="1723713849.512733" username="apac4" allocationname="unknown" flags="0" pid="691310" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83649e+01" utime="4.79279e+01" stime="7.29407e+00" mtime="3.21401e+01" gflop="0.00000e+00" gbyte="3.77041e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21401e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000034143314f9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82813e+01" utime="4.78955e+01" stime="7.28610e+00" mtime="3.21401e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21401e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4846e+08" > 5.1613e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4989e+08" > 3.5575e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9741e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4413e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.5088e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0636e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5605e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.7101e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1495e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6049e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8455e+01 </func>
</region>
</regions>
<internal rank="186" log_i="1723713849.512733" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="187" mpi_size="768" stamp_init="1723713791.156435" stamp_final="1723713849.515792" username="apac4" allocationname="unknown" flags="0" pid="691311" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83594e+01" utime="4.91024e+01" stime="6.96831e+00" mtime="3.27482e+01" gflop="0.00000e+00" gbyte="3.76583e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27482e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c515ef55c515bf1511" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82818e+01" utime="4.90743e+01" stime="6.95632e+00" mtime="3.27482e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27482e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4904e+08" > 4.6262e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4911e+08" > 3.0421e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7336e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4474e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4319e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0634e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5596e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.7609e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2806e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5962e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9357e+01 </func>
</region>
</regions>
<internal rank="187" log_i="1723713849.515792" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="188" mpi_size="768" stamp_init="1723713791.159287" stamp_final="1723713849.512485" username="apac4" allocationname="unknown" flags="0" pid="691312" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83532e+01" utime="4.72550e+01" stime="7.46505e+00" mtime="3.21280e+01" gflop="0.00000e+00" gbyte="3.78330e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21280e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4af14c814da14e155da14d414d4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82774e+01" utime="4.72214e+01" stime="7.45842e+00" mtime="3.21280e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21280e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4779e+08" > 5.7225e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4802e+08" > 4.0534e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9002e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4423e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.6757e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0599e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5598e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.7765e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9206e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6046e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8459e+01 </func>
</region>
</regions>
<internal rank="188" log_i="1723713849.512485" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="189" mpi_size="768" stamp_init="1723713791.156425" stamp_final="1723713849.501653" username="apac4" allocationname="unknown" flags="0" pid="691313" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83452e+01" utime="4.92290e+01" stime="6.89301e+00" mtime="3.26623e+01" gflop="0.00000e+00" gbyte="3.76846e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26623e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ba14bc14bd141b55bd14bd14bb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82679e+01" utime="4.91999e+01" stime="6.88213e+00" mtime="3.26623e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26623e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4787e+08" > 4.6165e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4765e+08" > 2.8189e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6388e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5002e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0091e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5605e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.7089e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6608e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5964e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9371e+01 </func>
</region>
</regions>
<internal rank="189" log_i="1723713849.501653" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="190" mpi_size="768" stamp_init="1723713791.159027" stamp_final="1723713849.514869" username="apac4" allocationname="unknown" flags="0" pid="691314" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83558e+01" utime="4.68540e+01" stime="7.62010e+00" mtime="3.13806e+01" gflop="0.00000e+00" gbyte="3.77457e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.13806e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4951497149814e55698149814f2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82810e+01" utime="4.68201e+01" stime="7.61432e+00" mtime="3.13806e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.13806e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4749e+08" > 6.4122e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4847e+08" > 4.8067e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5230e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4351e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.4836e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0636e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5598e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.7815e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0303e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6054e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8016e+01 </func>
</region>
</regions>
<internal rank="190" log_i="1723713849.514869" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="191" mpi_size="768" stamp_init="1723713791.160365" stamp_final="1723713849.512244" username="apac4" allocationname="unknown" flags="0" pid="691315" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="5.83519e+01" utime="4.86769e+01" stime="6.90003e+00" mtime="3.22604e+01" gflop="0.00000e+00" gbyte="3.75336e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22604e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f414f514f614e155f614f614e8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82783e+01" utime="4.86442e+01" stime="6.89282e+00" mtime="3.22604e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22604e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4896e+08" > 4.4803e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4701e+08" > 2.7983e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2550e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4484e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0668e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5597e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.7900e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3188e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5955e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8360e+01 </func>
</region>
</regions>
<internal rank="191" log_i="1723713849.512244" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="192" mpi_size="768" stamp_init="1723713791.125279" stamp_final="1723713849.512676" username="apac4" allocationname="unknown" flags="0" pid="847283" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83874e+01" utime="4.13150e+01" stime="1.31782e+01" mtime="3.15888e+01" gflop="0.00000e+00" gbyte="3.85975e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15888e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b114b314b4141755b414b414d2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82848e+01" utime="4.12829e+01" stime="1.31685e+01" mtime="3.15888e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15888e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4839e+08" > 8.5628e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4759e+08" > 5.9399e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4005e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4965e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3635e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.5272e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5569e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.0371e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0208e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5978e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8109e+01 </func>
</region>
</regions>
<internal rank="192" log_i="1723713849.512676" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="193" mpi_size="768" stamp_init="1723713791.125289" stamp_final="1723713849.502348" username="apac4" allocationname="unknown" flags="0" pid="847284" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83771e+01" utime="5.02961e+01" stime="6.86949e+00" mtime="3.25850e+01" gflop="0.00000e+00" gbyte="3.77480e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25850e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a9146f55a914a914f4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82695e+01" utime="5.02627e+01" stime="6.86015e+00" mtime="3.25850e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25850e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4849e+08" > 4.4994e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4924e+08" > 3.4002e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4301e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4277e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6226e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8228e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5600e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.7500e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8205e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5897e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9491e+01 </func>
</region>
</regions>
<internal rank="193" log_i="1723713849.502348" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="194" mpi_size="768" stamp_init="1723713791.125696" stamp_final="1723713849.512622" username="apac4" allocationname="unknown" flags="0" pid="847285" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83869e+01" utime="4.70114e+01" stime="7.55475e+00" mtime="3.17974e+01" gflop="0.00000e+00" gbyte="3.76842e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17974e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82860e+01" utime="4.69785e+01" stime="7.54632e+00" mtime="3.17974e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17974e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4893e+08" > 6.2344e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4773e+08" > 4.4460e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5299e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4487e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2831e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0663e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5592e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.8029e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3593e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5978e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8429e+01 </func>
</region>
</regions>
<internal rank="194" log_i="1723713849.512622" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="195" mpi_size="768" stamp_init="1723713791.124540" stamp_final="1723713849.512081" username="apac4" allocationname="unknown" flags="0" pid="847286" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83875e+01" utime="4.92666e+01" stime="6.45744e+00" mtime="3.20977e+01" gflop="0.00000e+00" gbyte="3.77136e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20977e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009f1438559f149f14e4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82814e+01" utime="4.92344e+01" stime="6.44738e+00" mtime="3.20977e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20977e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4721e+08" > 4.4170e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4753e+08" > 3.4333e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7518e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4407e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0663e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5589e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.8772e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1590e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5869e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9709e+01 </func>
</region>
</regions>
<internal rank="195" log_i="1723713849.512081" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="196" mpi_size="768" stamp_init="1723713791.125168" stamp_final="1723713849.504861" username="apac4" allocationname="unknown" flags="0" pid="847287" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83797e+01" utime="4.81686e+01" stime="7.53712e+00" mtime="3.18552e+01" gflop="0.00000e+00" gbyte="3.77903e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18552e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000078147e557814781483" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82769e+01" utime="4.81395e+01" stime="7.52405e+00" mtime="3.18552e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18552e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4826e+08" > 6.1270e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4778e+08" > 4.0221e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2499e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4195e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2374e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7557e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5597e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.7982e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7895e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5966e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8781e+01 </func>
</region>
</regions>
<internal rank="196" log_i="1723713849.504861" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="197" mpi_size="768" stamp_init="1723713791.124696" stamp_final="1723713849.516528" username="apac4" allocationname="unknown" flags="0" pid="847288" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83918e+01" utime="5.02607e+01" stime="6.86568e+00" mtime="3.28436e+01" gflop="0.00000e+00" gbyte="3.76865e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28436e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82869e+01" utime="5.02321e+01" stime="6.85230e+00" mtime="3.28436e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28436e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4945e+08" > 4.4072e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4926e+08" > 2.8289e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5130e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4243e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9228e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5580e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.9256e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8300e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5315e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9685e+01 </func>
</region>
</regions>
<internal rank="197" log_i="1723713849.516528" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="198" mpi_size="768" stamp_init="1723713791.125173" stamp_final="1723713849.503603" username="apac4" allocationname="unknown" flags="0" pid="847289" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83784e+01" utime="4.85141e+01" stime="7.36546e+00" mtime="3.17768e+01" gflop="0.00000e+00" gbyte="3.74825e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17768e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82765e+01" utime="4.84837e+01" stime="7.35301e+00" mtime="3.17768e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17768e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4991e+08" > 5.2485e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4980e+08" > 3.9138e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1329e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4059e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0118e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8528e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5585e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.9142e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9397e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5955e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8912e+01 </func>
</region>
</regions>
<internal rank="198" log_i="1723713849.503603" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="199" mpi_size="768" stamp_init="1723713791.125236" stamp_final="1723713849.516145" username="apac4" allocationname="unknown" flags="0" pid="847290" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83909e+01" utime="5.02297e+01" stime="6.93007e+00" mtime="3.26442e+01" gflop="0.00000e+00" gbyte="3.77445e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26442e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ae14ae14ea" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82841e+01" utime="5.01976e+01" stime="6.92031e+00" mtime="3.26442e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26442e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4926e+08" > 4.5917e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4941e+08" > 3.5028e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8048e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4129e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5749e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8516e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5579e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.9381e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1090e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5834e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9180e+01 </func>
</region>
</regions>
<internal rank="199" log_i="1723713849.516145" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="200" mpi_size="768" stamp_init="1723713791.126251" stamp_final="1723713849.500987" username="apac4" allocationname="unknown" flags="0" pid="847291" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83747e+01" utime="4.77029e+01" stime="7.27322e+00" mtime="3.24791e+01" gflop="0.00000e+00" gbyte="3.74935e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24791e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82696e+01" utime="4.76673e+01" stime="7.26734e+00" mtime="3.24791e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24791e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4900e+08" > 4.2834e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4898e+08" > 3.7708e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.6294e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4496e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.3870e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0670e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5566e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.0961e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1900e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5958e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8207e+01 </func>
</region>
</regions>
<internal rank="200" log_i="1723713849.500987" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="201" mpi_size="768" stamp_init="1723713791.125424" stamp_final="1723713849.497451" username="apac4" allocationname="unknown" flags="0" pid="847292" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83720e+01" utime="5.00296e+01" stime="7.00037e+00" mtime="3.28551e+01" gflop="0.00000e+00" gbyte="3.76896e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28551e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4631465146614dd5566146614d8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82658e+01" utime="5.00048e+01" stime="6.98350e+00" mtime="3.28551e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28551e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4850e+08" > 3.5658e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4838e+08" > 2.6753e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8944e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4313e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7643e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0016e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5580e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.9675e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.7384e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5841e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9392e+01 </func>
</region>
</regions>
<internal rank="201" log_i="1723713849.497451" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="202" mpi_size="768" stamp_init="1723713791.129536" stamp_final="1723713849.501086" username="apac4" allocationname="unknown" flags="0" pid="847293" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83716e+01" utime="4.74263e+01" stime="7.47167e+00" mtime="3.28164e+01" gflop="0.00000e+00" gbyte="3.75011e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28164e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000cb15ca1522" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82652e+01" utime="4.73953e+01" stime="7.46093e+00" mtime="3.28164e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28164e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1206e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4920e+08" > 4.5652e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4807e+08" > 3.9756e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.3523e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4608e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.2254e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0670e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5571e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.0509e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1686e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5917e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8786e+01 </func>
</region>
</regions>
<internal rank="202" log_i="1723713849.501086" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="203" mpi_size="768" stamp_init="1723713791.129900" stamp_final="1723713849.512937" username="apac4" allocationname="unknown" flags="0" pid="847294" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83830e+01" utime="5.02516e+01" stime="6.73118e+00" mtime="3.26112e+01" gflop="0.00000e+00" gbyte="3.77968e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26112e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4cb14cc14ce146f56ce14cd1467" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82774e+01" utime="5.02196e+01" stime="6.72161e+00" mtime="3.26112e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26112e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4945e+08" > 3.4402e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4874e+08" > 2.6397e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4294e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4284e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0015e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5573e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.0440e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3998e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5834e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9629e+01 </func>
</region>
</regions>
<internal rank="203" log_i="1723713849.512937" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="204" mpi_size="768" stamp_init="1723713791.132528" stamp_final="1723713849.501301" username="apac4" allocationname="unknown" flags="0" pid="847295" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83688e+01" utime="4.73462e+01" stime="7.62688e+00" mtime="3.15328e+01" gflop="0.00000e+00" gbyte="3.77857e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15328e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b215b2150e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82633e+01" utime="4.73145e+01" stime="7.61689e+00" mtime="3.15328e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15328e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.0599e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4769e+08" > 6.0179e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4860e+08" > 6.3879e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0564e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4029e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8013e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7013e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5556e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.1433e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0899e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5920e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8656e+01 </func>
</region>
</regions>
<internal rank="204" log_i="1723713849.501301" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="205" mpi_size="768" stamp_init="1723713791.134935" stamp_final="1723713849.515722" username="apac4" allocationname="unknown" flags="0" pid="847296" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83808e+01" utime="5.04867e+01" stime="6.70336e+00" mtime="3.28419e+01" gflop="0.00000e+00" gbyte="3.76514e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28419e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48f14a714b9145855b914b414b6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82753e+01" utime="5.04538e+01" stime="6.69495e+00" mtime="3.28419e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28419e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4974e+08" > 3.4041e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4879e+08" > 3.1774e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4993e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4078e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0111e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5568e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.0957e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0684e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5835e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9808e+01 </func>
</region>
</regions>
<internal rank="205" log_i="1723713849.515722" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="206" mpi_size="768" stamp_init="1723713791.138667" stamp_final="1723713849.512636" username="apac4" allocationname="unknown" flags="0" pid="847297" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83740e+01" utime="4.88284e+01" stime="7.35304e+00" mtime="3.17864e+01" gflop="0.00000e+00" gbyte="3.77071e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17864e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82564e+01" utime="4.88003e+01" stime="7.33977e+00" mtime="3.17864e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17864e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127164" bytes="8.4949e+08" > 4.5630e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5027e+08" > 4.4137e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5125e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4285e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6332e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6884e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5555e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.2332e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0112e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5909e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8594e+01 </func>
</region>
</regions>
<internal rank="206" log_i="1723713849.512636" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="207" mpi_size="768" stamp_init="1723713791.140127" stamp_final="1723713849.503053" username="apac4" allocationname="unknown" flags="0" pid="847298" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83629e+01" utime="5.01251e+01" stime="7.02114e+00" mtime="3.30736e+01" gflop="0.00000e+00" gbyte="3.77663e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.30736e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000311431148f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82485e+01" utime="5.00948e+01" stime="7.00986e+00" mtime="3.30736e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.30736e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4915e+08" > 3.5743e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5057e+08" > 2.4914e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2031e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4157e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0078e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5542e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.3268e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4714e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5299e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9319e+01 </func>
</region>
</regions>
<internal rank="207" log_i="1723713849.503053" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="208" mpi_size="768" stamp_init="1723713791.143693" stamp_final="1723713849.512787" username="apac4" allocationname="unknown" flags="0" pid="847299" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83691e+01" utime="4.72565e+01" stime="7.44800e+00" mtime="3.20533e+01" gflop="0.00000e+00" gbyte="3.75217e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20533e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82573e+01" utime="4.72252e+01" stime="7.43863e+00" mtime="3.20533e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20533e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4783e+08" > 6.3276e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4920e+08" > 4.1979e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9287e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4540e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8201e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0563e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5552e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.2537e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7704e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5906e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8267e+01 </func>
</region>
</regions>
<internal rank="208" log_i="1723713849.512787" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="209" mpi_size="768" stamp_init="1723713791.147582" stamp_final="1723713849.516600" username="apac4" allocationname="unknown" flags="0" pid="847300" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83690e+01" utime="5.02447e+01" stime="6.92123e+00" mtime="3.26614e+01" gflop="0.00000e+00" gbyte="3.77975e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26614e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82582e+01" utime="5.02119e+01" stime="6.91288e+00" mtime="3.26614e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26614e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4965e+08" > 4.8057e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4779e+08" > 3.0447e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5628e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4215e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0737e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5559e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.1871e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8896e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5829e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9405e+01 </func>
</region>
</regions>
<internal rank="209" log_i="1723713849.516600" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="210" mpi_size="768" stamp_init="1723713791.148747" stamp_final="1723713849.510782" username="apac4" allocationname="unknown" flags="0" pid="847301" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83620e+01" utime="4.85920e+01" stime="7.38700e+00" mtime="3.23566e+01" gflop="0.00000e+00" gbyte="3.76926e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23566e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4161418141914d35519141914b4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82641e+01" utime="4.85577e+01" stime="7.38105e+00" mtime="3.23566e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23566e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4888e+08" > 6.5653e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4911e+08" > 4.0801e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6661e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3987e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.0810e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7172e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5550e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.2376e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4404e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5894e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8844e+01 </func>
</region>
</regions>
<internal rank="210" log_i="1723713849.510782" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="211" mpi_size="768" stamp_init="1723713791.152478" stamp_final="1723713849.505879" username="apac4" allocationname="unknown" flags="0" pid="847302" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83534e+01" utime="5.02473e+01" stime="6.91744e+00" mtime="3.26809e+01" gflop="0.00000e+00" gbyte="3.76640e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26809e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82476e+01" utime="5.02155e+01" stime="6.90929e+00" mtime="3.26809e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26809e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4885e+08" > 4.9363e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4999e+08" > 2.9821e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7492e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4307e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0506e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5548e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.2960e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4094e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5855e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9219e+01 </func>
</region>
</regions>
<internal rank="211" log_i="1723713849.505879" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="212" mpi_size="768" stamp_init="1723713791.154760" stamp_final="1723713849.511392" username="apac4" allocationname="unknown" flags="0" pid="847303" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83566e+01" utime="4.80783e+01" stime="7.69573e+00" mtime="3.20299e+01" gflop="0.00000e+00" gbyte="3.76671e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20299e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004d1485554d144d1499" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82563e+01" utime="4.80427e+01" stime="7.69166e+00" mtime="3.20299e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20299e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4862e+08" > 6.1039e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5002e+08" > 3.9415e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6066e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3889e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6131e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6941e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5543e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.3432e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8515e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5904e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8632e+01 </func>
</region>
</regions>
<internal rank="212" log_i="1723713849.511392" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="213" mpi_size="768" stamp_init="1723713791.157484" stamp_final="1723713849.516758" username="apac4" allocationname="unknown" flags="0" pid="847304" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83593e+01" utime="5.02430e+01" stime="6.94221e+00" mtime="3.27467e+01" gflop="0.00000e+00" gbyte="3.78014e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27467e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c815a655c815c81533" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82528e+01" utime="5.02109e+01" stime="6.93429e+00" mtime="3.27467e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27467e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4933e+08" > 4.7910e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4935e+08" > 3.0393e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4925e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4234e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0723e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5548e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.2939e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8014e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5318e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9561e+01 </func>
</region>
</regions>
<internal rank="213" log_i="1723713849.516758" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="214" mpi_size="768" stamp_init="1723713791.161209" stamp_final="1723713849.512445" username="apac4" allocationname="unknown" flags="0" pid="847305" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83512e+01" utime="4.89684e+01" stime="7.21864e+00" mtime="3.20196e+01" gflop="0.00000e+00" gbyte="3.76457e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20196e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4331535153615ee56361536154a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82634e+01" utime="4.89349e+01" stime="7.21321e+00" mtime="3.20196e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20196e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4972e+08" > 5.3409e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4924e+08" > 3.4374e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6508e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4275e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.1008e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6890e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5539e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.3884e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9016e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5900e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8622e+01 </func>
</region>
</regions>
<internal rank="214" log_i="1723713849.512445" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="215" mpi_size="768" stamp_init="1723713791.163155" stamp_final="1723713849.508970" username="apac4" allocationname="unknown" flags="0" pid="847306" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="5.83458e+01" utime="5.01643e+01" stime="6.97500e+00" mtime="3.27660e+01" gflop="0.00000e+00" gbyte="3.77644e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27660e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000aa14de55aa14a514bc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82458e+01" utime="5.01372e+01" stime="6.96229e+00" mtime="3.27660e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27660e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4933e+08" > 4.7247e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4982e+08" > 2.7304e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9880e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3995e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6856e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5543e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.3487e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9302e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5856e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9128e+01 </func>
</region>
</regions>
<internal rank="215" log_i="1723713849.508970" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="216" mpi_size="768" stamp_init="1723713791.099836" stamp_final="1723713849.500166" username="apac4" allocationname="unknown" flags="0" pid="596874" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.84003e+01" utime="4.26548e+01" stime="1.22235e+01" mtime="3.20923e+01" gflop="0.00000e+00" gbyte="3.86894e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20923e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83499e+01" utime="4.26185e+01" stime="1.22200e+01" mtime="3.20923e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20923e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4809e+08" > 6.0184e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4891e+08" > 3.3516e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1100e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4760e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.5541e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0242e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5538e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.4275e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7514e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5863e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8183e+01 </func>
</region>
</regions>
<internal rank="216" log_i="1723713849.500166" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="217" mpi_size="768" stamp_init="1723713791.105638" stamp_final="1723713849.509703" username="apac4" allocationname="unknown" flags="0" pid="596875" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.84041e+01" utime="4.94879e+01" stime="6.61754e+00" mtime="3.25805e+01" gflop="0.00000e+00" gbyte="3.78098e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25805e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83346e+01" utime="4.94568e+01" stime="6.60813e+00" mtime="3.25805e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25805e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4858e+08" > 4.5293e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4905e+08" > 2.8319e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3936e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4879e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0268e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5535e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.4395e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8801e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5741e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9537e+01 </func>
</region>
</regions>
<internal rank="217" log_i="1723713849.509703" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="218" mpi_size="768" stamp_init="1723713791.108099" stamp_final="1723713849.510532" username="apac4" allocationname="unknown" flags="0" pid="596876" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.84024e+01" utime="4.80180e+01" stime="6.94994e+00" mtime="3.19134e+01" gflop="0.00000e+00" gbyte="3.76801e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19134e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000039143914c1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83343e+01" utime="4.79857e+01" stime="6.94352e+00" mtime="3.19134e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19134e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4804e+08" > 5.2785e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4941e+08" > 4.0988e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2464e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4736e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8589e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0269e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5529e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.5057e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2997e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5855e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8941e+01 </func>
</region>
</regions>
<internal rank="218" log_i="1723713849.510532" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="219" mpi_size="768" stamp_init="1723713791.104740" stamp_final="1723713849.506271" username="apac4" allocationname="unknown" flags="0" pid="596877" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.84015e+01" utime="4.94438e+01" stime="6.53927e+00" mtime="3.25930e+01" gflop="0.00000e+00" gbyte="3.74538e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25930e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4fc14fd14fe14da56fe14fe14f8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83442e+01" utime="4.94122e+01" stime="6.53186e+00" mtime="3.25930e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25930e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4964e+08" > 4.4808e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4796e+08" > 2.7127e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3928e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4811e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0270e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5530e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.4976e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7990e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5740e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9563e+01 </func>
</region>
</regions>
<internal rank="219" log_i="1723713849.506271" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="220" mpi_size="768" stamp_init="1723713791.107204" stamp_final="1723713849.500158" username="apac4" allocationname="unknown" flags="0" pid="596878" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83930e+01" utime="4.67381e+01" stime="7.76288e+00" mtime="3.21991e+01" gflop="0.00000e+00" gbyte="3.77964e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21991e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ab14aa1464" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83254e+01" utime="4.67072e+01" stime="7.75325e+00" mtime="3.21991e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21991e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127164" bytes="8.4958e+08" > 6.1591e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4849e+08" > 4.2295e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7901e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4752e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.0702e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0242e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5524e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.5439e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0708e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5854e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8588e+01 </func>
</region>
</regions>
<internal rank="220" log_i="1723713849.500158" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="221" mpi_size="768" stamp_init="1723713791.109195" stamp_final="1723713849.509365" username="apac4" allocationname="unknown" flags="0" pid="596879" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.84002e+01" utime="4.93341e+01" stime="6.74919e+00" mtime="3.24921e+01" gflop="0.00000e+00" gbyte="3.77243e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24921e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4671468146a141c566a146914ea" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83189e+01" utime="4.93010e+01" stime="6.74335e+00" mtime="3.24921e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24921e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4944e+08" > 4.5082e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4913e+08" > 2.5905e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3349e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4654e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4305e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0322e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5527e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.5299e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8086e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5746e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9529e+01 </func>
</region>
</regions>
<internal rank="221" log_i="1723713849.509365" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="222" mpi_size="768" stamp_init="1723713791.113135" stamp_final="1723713849.512125" username="apac4" allocationname="unknown" flags="0" pid="596880" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83990e+01" utime="4.71840e+01" stime="7.50437e+00" mtime="3.21279e+01" gflop="0.00000e+00" gbyte="3.76728e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21279e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83252e+01" utime="4.71544e+01" stime="7.49360e+00" mtime="3.21279e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21279e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4996e+08" > 5.9746e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5145e+08" > 4.0103e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8351e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4818e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.0366e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0242e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5526e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.5305e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0279e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5862e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8487e+01 </func>
</region>
</regions>
<internal rank="222" log_i="1723713849.512125" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="223" mpi_size="768" stamp_init="1723713791.114731" stamp_final="1723713849.512568" username="apac4" allocationname="unknown" flags="0" pid="596881" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83978e+01" utime="4.94198e+01" stime="6.74173e+00" mtime="3.26155e+01" gflop="0.00000e+00" gbyte="3.76816e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26155e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000037149d5537143614d2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83162e+01" utime="4.93911e+01" stime="6.72939e+00" mtime="3.26155e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26155e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5010e+08" > 4.5358e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4944e+08" > 2.6380e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0252e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4758e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0041e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0242e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5515e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.6142e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7990e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5747e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8957e+01 </func>
</region>
</regions>
<internal rank="223" log_i="1723713849.512568" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="224" mpi_size="768" stamp_init="1723713791.116448" stamp_final="1723713849.500096" username="apac4" allocationname="unknown" flags="0" pid="596882" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83836e+01" utime="4.66300e+01" stime="7.80449e+00" mtime="3.21749e+01" gflop="0.00000e+00" gbyte="3.77640e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21749e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000093149314e2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83030e+01" utime="4.66003e+01" stime="7.79370e+00" mtime="3.21749e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21749e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4987e+08" > 5.6711e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4881e+08" > 4.5160e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2562e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4895e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.7880e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0245e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5506e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.6485e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2711e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5851e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8129e+01 </func>
</region>
</regions>
<internal rank="224" log_i="1723713849.500096" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="225" mpi_size="768" stamp_init="1723713791.118063" stamp_final="1723713849.506673" username="apac4" allocationname="unknown" flags="0" pid="596883" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83886e+01" utime="4.96791e+01" stime="6.46590e+00" mtime="3.23715e+01" gflop="0.00000e+00" gbyte="3.74649e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23715e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006e1447556e146e14ad" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83059e+01" utime="4.96505e+01" stime="6.45413e+00" mtime="3.23715e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23715e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4919e+08" > 3.5896e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4931e+08" > 3.2824e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4880e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4834e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2650e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0292e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5516e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.6432e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4595e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5758e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9325e+01 </func>
</region>
</regions>
<internal rank="225" log_i="1723713849.506673" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="226" mpi_size="768" stamp_init="1723713791.128419" stamp_final="1723713849.509315" username="apac4" allocationname="unknown" flags="0" pid="596884" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83809e+01" utime="4.64030e+01" stime="7.74187e+00" mtime="3.14815e+01" gflop="0.00000e+00" gbyte="3.77716e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.14815e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4971499149a1410559a14991482" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82969e+01" utime="4.63699e+01" stime="7.73438e+00" mtime="3.14815e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.14815e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4836e+08" > 6.3444e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4913e+08" > 5.4493e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2293e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4806e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.9578e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0254e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5498e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.7966e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1304e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5847e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8397e+01 </func>
</region>
</regions>
<internal rank="226" log_i="1723713849.509315" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="227" mpi_size="768" stamp_init="1723713791.127113" stamp_final="1723713849.506001" username="apac4" allocationname="unknown" flags="0" pid="596885" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83789e+01" utime="4.95829e+01" stime="6.55886e+00" mtime="3.27729e+01" gflop="0.00000e+00" gbyte="3.77720e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27729e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf467156815691577556915691505" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82948e+01" utime="4.95477e+01" stime="6.55252e+00" mtime="3.27729e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27729e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4949e+08" > 3.6095e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4885e+08" > 2.8386e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5398e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4706e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0253e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5505e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.7208e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8205e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5729e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9694e+01 </func>
</region>
</regions>
<internal rank="227" log_i="1723713849.506001" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="228" mpi_size="768" stamp_init="1723713791.127131" stamp_final="1723713849.513923" username="apac4" allocationname="unknown" flags="0" pid="596886" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83868e+01" utime="4.64094e+01" stime="7.88279e+00" mtime="3.25447e+01" gflop="0.00000e+00" gbyte="3.77831e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25447e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83019e+01" utime="4.63741e+01" stime="7.87685e+00" mtime="3.25447e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25447e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4931e+08" > 5.5863e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4989e+08" > 5.3115e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0556e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4829e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.7991e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0262e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5506e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.7377e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2687e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5834e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8706e+01 </func>
</region>
</regions>
<internal rank="228" log_i="1723713849.513923" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="229" mpi_size="768" stamp_init="1723713791.128413" stamp_final="1723713849.512793" username="apac4" allocationname="unknown" flags="0" pid="596887" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83844e+01" utime="4.96444e+01" stime="6.54124e+00" mtime="3.27605e+01" gflop="0.00000e+00" gbyte="3.77850e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27605e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f4146255f414f414ed" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83015e+01" utime="4.96135e+01" stime="6.53141e+00" mtime="3.27605e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27605e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.3447e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4907e+08" > 3.4697e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4935e+08" > 2.3777e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3898e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4837e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0324e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5500e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.8007e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6894e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5728e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9830e+01 </func>
</region>
</regions>
<internal rank="229" log_i="1723713849.512793" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="230" mpi_size="768" stamp_init="1723713791.130630" stamp_final="1723713849.504312" username="apac4" allocationname="unknown" flags="0" pid="596888" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83737e+01" utime="4.74719e+01" stime="7.36106e+00" mtime="3.15999e+01" gflop="0.00000e+00" gbyte="3.76835e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15999e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000054145314b2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82926e+01" utime="4.74411e+01" stime="7.35221e+00" mtime="3.15999e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15999e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.7881e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4949e+08" > 5.9078e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4869e+08" > 4.2592e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3312e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4867e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.7410e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0264e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5493e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.8512e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0398e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5824e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8465e+01 </func>
</region>
</regions>
<internal rank="230" log_i="1723713849.504312" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="231" mpi_size="768" stamp_init="1723713791.140448" stamp_final="1723713849.506607" username="apac4" allocationname="unknown" flags="0" pid="596889" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83662e+01" utime="4.93866e+01" stime="6.78025e+00" mtime="3.25162e+01" gflop="0.00000e+00" gbyte="3.76934e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25162e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c315c515c615ae55c615c6154e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82878e+01" utime="4.93541e+01" stime="6.77288e+00" mtime="3.25162e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25162e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4905e+08" > 3.5895e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4926e+08" > 2.8929e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0517e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4842e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2636e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0264e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5493e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.8744e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2401e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5728e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8912e+01 </func>
</region>
</regions>
<internal rank="231" log_i="1723713849.506607" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="232" mpi_size="768" stamp_init="1723713791.141504" stamp_final="1723713849.509660" username="apac4" allocationname="unknown" flags="0" pid="596890" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83682e+01" utime="4.62379e+01" stime="8.02344e+00" mtime="3.13721e+01" gflop="0.00000e+00" gbyte="3.77106e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.13721e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82912e+01" utime="4.62039e+01" stime="8.01726e+00" mtime="3.13721e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.13721e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4888e+08" > 7.1765e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4938e+08" > 6.1370e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5361e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4763e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2316e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0247e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5487e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.9110e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2091e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5780e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7902e+01 </func>
</region>
</regions>
<internal rank="232" log_i="1723713849.509660" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="233" mpi_size="768" stamp_init="1723713791.140451" stamp_final="1723713849.512678" username="apac4" allocationname="unknown" flags="0" pid="596891" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83722e+01" utime="4.93187e+01" stime="6.59357e+00" mtime="3.21505e+01" gflop="0.00000e+00" gbyte="3.76823e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21505e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000bb142456bb14bb14c9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82941e+01" utime="4.92887e+01" stime="6.58425e+00" mtime="3.21505e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21505e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4894e+08" > 4.6985e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4901e+08" > 3.0622e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3028e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4818e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0272e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5494e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.8754e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9087e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5715e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9184e+01 </func>
</region>
</regions>
<internal rank="233" log_i="1723713849.512678" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="234" mpi_size="768" stamp_init="1723713791.141433" stamp_final="1723713849.509195" username="apac4" allocationname="unknown" flags="0" pid="596892" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83678e+01" utime="4.73822e+01" stime="7.25292e+00" mtime="3.14361e+01" gflop="0.00000e+00" gbyte="3.78567e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.14361e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f514f714f8142655f814f814b2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82906e+01" utime="4.73514e+01" stime="7.24409e+00" mtime="3.14361e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.14361e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5066e+08" > 6.6857e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4783e+08" > 5.3020e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1658e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4455e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.5159e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0293e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5482e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.9374e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1495e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5775e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8416e+01 </func>
</region>
</regions>
<internal rank="234" log_i="1723713849.509195" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="235" mpi_size="768" stamp_init="1723713791.143214" stamp_final="1723713849.501644" username="apac4" allocationname="unknown" flags="0" pid="596893" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83584e+01" utime="4.93252e+01" stime="6.81070e+00" mtime="3.24214e+01" gflop="0.00000e+00" gbyte="3.74924e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24214e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf41f14201421145a5621142114e1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82819e+01" utime="4.92940e+01" stime="6.80296e+00" mtime="3.24214e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24214e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4892e+08" > 4.7386e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5079e+08" > 3.2092e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3979e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4774e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8133e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0295e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5485e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.9165e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8205e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5215e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9358e+01 </func>
</region>
</regions>
<internal rank="235" log_i="1723713849.501644" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="236" mpi_size="768" stamp_init="1723713791.146095" stamp_final="1723713849.508742" username="apac4" allocationname="unknown" flags="0" pid="596894" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83626e+01" utime="4.73406e+01" stime="7.50152e+00" mtime="3.21829e+01" gflop="0.00000e+00" gbyte="3.77216e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21829e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82884e+01" utime="4.73121e+01" stime="7.49096e+00" mtime="3.21829e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21829e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4950e+08" > 6.1890e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4947e+08" > 4.6231e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7382e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4796e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.2029e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0278e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5485e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.9509e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0804e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5745e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8612e+01 </func>
</region>
</regions>
<internal rank="236" log_i="1723713849.508742" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="237" mpi_size="768" stamp_init="1723713791.147394" stamp_final="1723713849.501507" username="apac4" allocationname="unknown" flags="0" pid="596895" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83541e+01" utime="4.92197e+01" stime="6.88873e+00" mtime="3.25743e+01" gflop="0.00000e+00" gbyte="3.74378e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25743e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf468146a146b1425556b146a14d0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82816e+01" utime="4.91902e+01" stime="6.87992e+00" mtime="3.25743e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25743e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.5763e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4924e+08" > 4.7887e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4975e+08" > 2.9998e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5093e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4741e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0295e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5468e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.0920e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3784e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5726e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9399e+01 </func>
</region>
</regions>
<internal rank="237" log_i="1723713849.501507" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="238" mpi_size="768" stamp_init="1723713791.150047" stamp_final="1723713849.501764" username="apac4" allocationname="unknown" flags="0" pid="596896" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83517e+01" utime="4.70524e+01" stime="7.63120e+00" mtime="3.21344e+01" gflop="0.00000e+00" gbyte="3.77609e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21344e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000be14bd14cb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82807e+01" utime="4.70228e+01" stime="7.62186e+00" mtime="3.21344e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21344e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4904e+08" > 6.7526e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4885e+08" > 4.8298e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8982e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4710e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.0792e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0278e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5472e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.0549e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8682e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5744e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8354e+01 </func>
</region>
</regions>
<internal rank="238" log_i="1723713849.501764" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="239" mpi_size="768" stamp_init="1723713791.151597" stamp_final="1723713849.516234" username="apac4" allocationname="unknown" flags="0" pid="596897" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="5.83646e+01" utime="4.86963e+01" stime="7.19745e+00" mtime="3.24902e+01" gflop="0.00000e+00" gbyte="3.77579e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24902e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006714c15667146614ba" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82948e+01" utime="4.86638e+01" stime="7.19157e+00" mtime="3.24902e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24902e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4866e+08" > 4.8225e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4875e+08" > 3.0951e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0782e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4606e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6703e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0276e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5470e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.0690e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1900e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5717e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8757e+01 </func>
</region>
</regions>
<internal rank="239" log_i="1723713849.516234" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="240" mpi_size="768" stamp_init="1723713791.101678" stamp_final="1723713849.516888" username="apac4" allocationname="unknown" flags="0" pid="2779573" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.84152e+01" utime="4.10824e+01" stime="1.34561e+01" mtime="3.17541e+01" gflop="0.00000e+00" gbyte="3.85586e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17541e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83498e+01" utime="4.10527e+01" stime="1.34455e+01" mtime="3.17541e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17541e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5013e+08" > 8.3791e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4986e+08" > 6.0516e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0989e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4164e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.0581e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0519e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5464e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.1539e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.4390e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5924e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7573e+01 </func>
</region>
</regions>
<internal rank="240" log_i="1723713849.516888" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="241" mpi_size="768" stamp_init="1723713791.101646" stamp_final="1723713849.499736" username="apac4" allocationname="unknown" flags="0" pid="2779574" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83981e+01" utime="5.03808e+01" stime="6.76279e+00" mtime="3.27440e+01" gflop="0.00000e+00" gbyte="3.78014e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27440e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83258e+01" utime="5.03475e+01" stime="6.75405e+00" mtime="3.27440e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27440e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5006e+08" > 4.6738e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4980e+08" > 3.9349e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7695e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4244e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5749e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1015e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5469e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.1121e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.4796e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5952e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9279e+01 </func>
</region>
</regions>
<internal rank="241" log_i="1723713849.499736" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="242" mpi_size="768" stamp_init="1723713791.101683" stamp_final="1723713849.506141" username="apac4" allocationname="unknown" flags="0" pid="2779575" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.84045e+01" utime="4.66050e+01" stime="8.18651e+00" mtime="3.16649e+01" gflop="0.00000e+00" gbyte="3.76514e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16649e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009f142b559f149f14a4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83402e+01" utime="4.65751e+01" stime="8.17640e+00" mtime="3.16649e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16649e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4851e+08" > 8.1391e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4901e+08" > 6.8274e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4416e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4188e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1281e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0524e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5459e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.1590e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6107e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5925e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8160e+01 </func>
</region>
</regions>
<internal rank="242" log_i="1723713849.506141" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="243" mpi_size="768" stamp_init="1723713791.103850" stamp_final="1723713849.508403" username="apac4" allocationname="unknown" flags="0" pid="2779576" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.84046e+01" utime="5.04113e+01" stime="6.68898e+00" mtime="3.25804e+01" gflop="0.00000e+00" gbyte="3.77663e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25804e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf47a147c147d144a567d147c14b9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83269e+01" utime="5.03774e+01" stime="6.68171e+00" mtime="3.25804e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25804e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4876e+08" > 4.3394e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4808e+08" > 3.0632e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2693e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4106e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0510e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5467e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.1308e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0708e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5923e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9676e+01 </func>
</region>
</regions>
<internal rank="243" log_i="1723713849.508403" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="244" mpi_size="768" stamp_init="1723713791.107072" stamp_final="1723713849.510282" username="apac4" allocationname="unknown" flags="0" pid="2779577" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.84032e+01" utime="4.72263e+01" stime="7.87587e+00" mtime="3.22749e+01" gflop="0.00000e+00" gbyte="3.77476e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22749e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005614875556145514ff" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83255e+01" utime="4.71972e+01" stime="7.86446e+00" mtime="3.22749e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22749e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4928e+08" > 6.4720e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4907e+08" > 5.0799e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6125e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4207e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0553e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0795e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5464e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.1695e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5105e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5941e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8778e+01 </func>
</region>
</regions>
<internal rank="244" log_i="1723713849.510282" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="245" mpi_size="768" stamp_init="1723713791.109728" stamp_final="1723713849.512241" username="apac4" allocationname="unknown" flags="0" pid="2779578" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.84025e+01" utime="5.03668e+01" stime="6.71652e+00" mtime="3.26435e+01" gflop="0.00000e+00" gbyte="3.77930e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26435e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a814a814c4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83110e+01" utime="5.03352e+01" stime="6.70536e+00" mtime="3.26435e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26435e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4800e+08" > 4.3056e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4925e+08" > 3.0528e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2431e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4046e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0796e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5452e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.2559e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5988e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5396e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9773e+01 </func>
</region>
</regions>
<internal rank="245" log_i="1723713849.512241" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="246" mpi_size="768" stamp_init="1723713791.112369" stamp_final="1723713849.509727" username="apac4" allocationname="unknown" flags="0" pid="2779579" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83974e+01" utime="4.83918e+01" stime="7.59418e+00" mtime="3.20830e+01" gflop="0.00000e+00" gbyte="3.76278e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20830e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005e14bb555e145e145d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83112e+01" utime="4.83666e+01" stime="7.57796e+00" mtime="3.20830e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20830e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5005e+08" > 5.5496e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5038e+08" > 3.3259e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7705e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4205e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.3028e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0994e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5455e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.2704e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0112e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5870e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8539e+01 </func>
</region>
</regions>
<internal rank="246" log_i="1723713849.509727" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="247" mpi_size="768" stamp_init="1723713791.114977" stamp_final="1723713849.514478" username="apac4" allocationname="unknown" flags="0" pid="2779580" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83995e+01" utime="4.99574e+01" stime="7.13878e+00" mtime="3.26667e+01" gflop="0.00000e+00" gbyte="3.77708e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26667e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002b142a14cc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83032e+01" utime="4.99212e+01" stime="7.13186e+00" mtime="3.26667e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26667e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4940e+08" > 4.4765e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4996e+08" > 3.0684e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8384e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4220e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0997e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5439e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.3915e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5487e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5926e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9164e+01 </func>
</region>
</regions>
<internal rank="247" log_i="1723713849.514478" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="248" mpi_size="768" stamp_init="1723713791.117519" stamp_final="1723713849.505139" username="apac4" allocationname="unknown" flags="0" pid="2779581" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83876e+01" utime="4.61072e+01" stime="7.99597e+00" mtime="3.20057e+01" gflop="0.00000e+00" gbyte="3.76797e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20057e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43014321433142f5533143314ac" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82952e+01" utime="4.60766e+01" stime="7.98447e+00" mtime="3.20057e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20057e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4883e+08" > 6.2093e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4713e+08" > 5.0937e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.4017e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4809e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.6339e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0303e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5449e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.2811e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6608e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5889e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7762e+01 </func>
</region>
</regions>
<internal rank="248" log_i="1723713849.505139" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="249" mpi_size="768" stamp_init="1723713791.120110" stamp_final="1723713849.518239" username="apac4" allocationname="unknown" flags="0" pid="2779582" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83981e+01" utime="5.03073e+01" stime="6.83800e+00" mtime="3.26011e+01" gflop="0.00000e+00" gbyte="3.76534e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26011e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000020141f146b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83032e+01" utime="5.02770e+01" stime="6.82594e+00" mtime="3.26011e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26011e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4953e+08" > 4.5448e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4866e+08" > 2.6967e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9199e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4231e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1015e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5443e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.3931e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7108e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5922e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9012e+01 </func>
</region>
</regions>
<internal rank="249" log_i="1723713849.518239" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="250" mpi_size="768" stamp_init="1723713791.122167" stamp_final="1723713849.500239" username="apac4" allocationname="unknown" flags="0" pid="2779583" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83781e+01" utime="4.80900e+01" stime="7.63328e+00" mtime="3.22045e+01" gflop="0.00000e+00" gbyte="3.75759e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22045e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82819e+01" utime="4.80628e+01" stime="7.61764e+00" mtime="3.22045e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22045e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4873e+08" > 5.7798e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4860e+08" > 4.5889e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1044e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4118e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6280e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0866e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5435e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.4700e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7800e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5878e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8298e+01 </func>
</region>
</regions>
<internal rank="250" log_i="1723713849.500239" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="251" mpi_size="768" stamp_init="1723713791.124542" stamp_final="1723713849.511767" username="apac4" allocationname="unknown" flags="0" pid="2779584" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83872e+01" utime="5.02028e+01" stime="6.86662e+00" mtime="3.28224e+01" gflop="0.00000e+00" gbyte="3.75694e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28224e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4da14db14dd147655dd14dc14d6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82919e+01" utime="5.01682e+01" stime="6.85875e+00" mtime="3.28224e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28224e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4842e+08" > 4.5930e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4894e+08" > 2.7510e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7110e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4052e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6941e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0868e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5441e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.3736e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0804e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5895e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9457e+01 </func>
</region>
</regions>
<internal rank="251" log_i="1723713849.511767" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="252" mpi_size="768" stamp_init="1723713791.127114" stamp_final="1723713849.510688" username="apac4" allocationname="unknown" flags="0" pid="2779585" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83836e+01" utime="4.69966e+01" stime="7.55451e+00" mtime="3.24253e+01" gflop="0.00000e+00" gbyte="3.77338e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24253e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006d1557556d156d1532" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82910e+01" utime="4.69621e+01" stime="7.54668e+00" mtime="3.24253e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24253e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4917e+08" > 5.6541e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4860e+08" > 3.7451e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0565e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5036e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.7994e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0303e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5427e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.5394e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.4605e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5413e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8573e+01 </func>
</region>
</regions>
<internal rank="252" log_i="1723713849.510688" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="253" mpi_size="768" stamp_init="1723713791.135585" stamp_final="1723713849.500060" username="apac4" allocationname="unknown" flags="0" pid="2779586" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83645e+01" utime="4.94259e+01" stime="6.64070e+00" mtime="3.27291e+01" gflop="0.00000e+00" gbyte="3.75977e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27291e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82738e+01" utime="4.93977e+01" stime="6.62781e+00" mtime="3.27291e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27291e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4809e+08" > 4.4582e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5101e+08" > 2.6794e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6489e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4809e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3842e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0303e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5438e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.4465e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6584e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5899e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9442e+01 </func>
</region>
</regions>
<internal rank="253" log_i="1723713849.500060" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="254" mpi_size="768" stamp_init="1723713791.132063" stamp_final="1723713849.509707" username="apac4" allocationname="unknown" flags="0" pid="2779587" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83776e+01" utime="4.76958e+01" stime="7.73587e+00" mtime="3.20800e+01" gflop="0.00000e+00" gbyte="3.77434e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20800e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009f149e145e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82861e+01" utime="4.76640e+01" stime="7.72627e+00" mtime="3.20800e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20800e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4923e+08" > 6.8223e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4892e+08" > 4.8323e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9943e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4253e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.6791e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0997e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5434e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.4868e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1304e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5893e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8165e+01 </func>
</region>
</regions>
<internal rank="254" log_i="1723713849.509707" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="255" mpi_size="768" stamp_init="1723713791.135585" stamp_final="1723713849.502743" username="apac4" allocationname="unknown" flags="0" pid="2779588" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83672e+01" utime="4.99574e+01" stime="7.12988e+00" mtime="3.27940e+01" gflop="0.00000e+00" gbyte="3.74725e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27940e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82771e+01" utime="4.99253e+01" stime="7.12038e+00" mtime="3.27940e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27940e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4843e+08" > 4.3980e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4923e+08" > 2.6378e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.4809e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4206e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0994e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5430e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.5186e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5201e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5943e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8662e+01 </func>
</region>
</regions>
<internal rank="255" log_i="1723713849.502743" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="256" mpi_size="768" stamp_init="1723713791.138741" stamp_final="1723713849.509881" username="apac4" allocationname="unknown" flags="0" pid="2779589" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83711e+01" utime="4.72879e+01" stime="8.30323e+00" mtime="3.20938e+01" gflop="0.00000e+00" gbyte="3.76221e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20938e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002e152e150a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82842e+01" utime="4.72569e+01" stime="8.29384e+00" mtime="3.20938e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20938e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4868e+08" > 7.7761e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4841e+08" > 4.7181e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5421e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2020e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3333e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5407e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.7554e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9707e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5961e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8537e+01 </func>
</region>
</regions>
<internal rank="256" log_i="1723713849.509881" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="257" mpi_size="768" stamp_init="1723713791.140537" stamp_final="1723713849.504316" username="apac4" allocationname="unknown" flags="0" pid="2779590" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83638e+01" utime="5.00952e+01" stime="7.02752e+00" mtime="3.28389e+01" gflop="0.00000e+00" gbyte="3.73829e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28389e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4881489148a14e5558a148a14a3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82766e+01" utime="5.00623e+01" stime="7.02075e+00" mtime="3.28389e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28389e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4899e+08" > 5.6596e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4845e+08" > 3.2214e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4882e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4226e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6226e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0805e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5398e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.8365e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5702e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5925e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9567e+01 </func>
</region>
</regions>
<internal rank="257" log_i="1723713849.504316" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="258" mpi_size="768" stamp_init="1723713791.142899" stamp_final="1723713849.507495" username="apac4" allocationname="unknown" flags="0" pid="2779591" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83646e+01" utime="4.68463e+01" stime="8.13089e+00" mtime="3.21238e+01" gflop="0.00000e+00" gbyte="3.77235e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21238e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001314131497" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82800e+01" utime="4.68116e+01" stime="8.12495e+00" mtime="3.21238e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21238e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4749e+08" > 9.2201e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4827e+08" > 6.6317e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5348e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4290e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1683e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0870e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5390e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.8967e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8014e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5387e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8409e+01 </func>
</region>
</regions>
<internal rank="258" log_i="1723713849.507495" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="259" mpi_size="768" stamp_init="1723713791.145071" stamp_final="1723713849.507208" username="apac4" allocationname="unknown" flags="0" pid="2779592" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83621e+01" utime="5.01685e+01" stime="6.94841e+00" mtime="3.27783e+01" gflop="0.00000e+00" gbyte="3.76923e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27783e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f914f914d5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82785e+01" utime="5.01386e+01" stime="6.93846e+00" mtime="3.27783e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27783e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4719e+08" > 5.4325e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4809e+08" > 3.1238e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3471e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4265e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1046e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5394e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.8473e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6488e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5911e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9666e+01 </func>
</region>
</regions>
<internal rank="259" log_i="1723713849.507208" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="260" mpi_size="768" stamp_init="1723713791.153219" stamp_final="1723713849.516663" username="apac4" allocationname="unknown" flags="0" pid="2779593" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83634e+01" utime="4.74188e+01" stime="7.82608e+00" mtime="3.13743e+01" gflop="0.00000e+00" gbyte="3.77735e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.13743e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c414d355c414c314b4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82856e+01" utime="4.73899e+01" stime="7.81519e+00" mtime="3.13743e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.13743e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4809e+08" > 8.5702e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4823e+08" > 5.2786e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.3188e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4249e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.7036e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0521e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5390e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.8831e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9397e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5937e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8961e+01 </func>
</region>
</regions>
<internal rank="260" log_i="1723713849.516663" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="261" mpi_size="768" stamp_init="1723713791.151143" stamp_final="1723713849.501717" username="apac4" allocationname="unknown" flags="0" pid="2779594" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83506e+01" utime="4.94147e+01" stime="6.64100e+00" mtime="3.26582e+01" gflop="0.00000e+00" gbyte="3.74424e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26582e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009a159a1546" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82712e+01" utime="4.93856e+01" stime="6.63066e+00" mtime="3.26582e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26582e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5000e+08" > 5.6300e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4914e+08" > 2.9587e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1467e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4896e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6941e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0302e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5384e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.9437e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.4199e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5929e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9745e+01 </func>
</region>
</regions>
<internal rank="261" log_i="1723713849.501717" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="262" mpi_size="768" stamp_init="1723713791.152698" stamp_final="1723713849.515562" username="apac4" allocationname="unknown" flags="0" pid="2779595" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83629e+01" utime="4.88153e+01" stime="7.29368e+00" mtime="3.20644e+01" gflop="0.00000e+00" gbyte="3.77743e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20644e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ec15ec153b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82845e+01" utime="4.87813e+01" stime="7.28757e+00" mtime="3.20644e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20644e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 7.0583e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5038e+08" > 4.3162e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1471e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4090e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.5088e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0771e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5382e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.0142e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8801e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5946e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8997e+01 </func>
</region>
</regions>
<internal rank="262" log_i="1723713849.515562" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="263" mpi_size="768" stamp_init="1723713791.154404" stamp_final="1723713849.509548" username="apac4" allocationname="unknown" flags="0" pid="2779596" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="5.83551e+01" utime="4.99212e+01" stime="7.20565e+00" mtime="3.26300e+01" gflop="0.00000e+00" gbyte="3.75610e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26300e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000014141314ef" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82798e+01" utime="4.98907e+01" stime="7.19726e+00" mtime="3.26300e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26300e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4965e+08" > 5.3731e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4984e+08" > 2.7618e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5894e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4240e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0819e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5383e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.0018e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0088e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5958e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9289e+01 </func>
</region>
</regions>
<internal rank="263" log_i="1723713849.509548" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="264" mpi_size="768" stamp_init="1723713791.630666" stamp_final="1723713849.506783" username="apac4" allocationname="unknown" flags="0" pid="687709" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78761e+01" utime="4.00169e+01" stime="1.18832e+01" mtime="2.82271e+01" gflop="0.00000e+00" gbyte="3.85956e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82271e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000311531154d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77472e+01" utime="3.99899e+01" stime="1.18676e+01" mtime="2.82271e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82271e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4899e+08" > 8.0037e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4849e+08" > 5.7234e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5088e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1808e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1091e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.7528e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5384e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.9674e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.8597e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5979e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8016e+01 </func>
</region>
</regions>
<internal rank="264" log_i="1723713849.506783" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="265" mpi_size="768" stamp_init="1723713791.631017" stamp_final="1723713849.505460" username="apac4" allocationname="unknown" flags="0" pid="687710" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78744e+01" utime="4.72860e+01" stime="6.26790e+00" mtime="2.91093e+01" gflop="0.00000e+00" gbyte="3.76076e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91093e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4811583158415fd55841584150b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77429e+01" utime="4.72520e+01" stime="6.25845e+00" mtime="2.91093e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91093e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4765e+08" > 4.5315e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4811e+08" > 3.1363e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3501e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1954e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7643e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0039e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5387e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.9486e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.4901e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5957e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9415e+01 </func>
</region>
</regions>
<internal rank="265" log_i="1723713849.505460" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="266" mpi_size="768" stamp_init="1723713791.632452" stamp_final="1723713849.501684" username="apac4" allocationname="unknown" flags="0" pid="687711" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78692e+01" utime="4.48127e+01" stime="7.42335e+00" mtime="2.86345e+01" gflop="0.00000e+00" gbyte="3.76553e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86345e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007f147f14a8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77431e+01" utime="4.47813e+01" stime="7.41297e+00" mtime="2.86345e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86345e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4714e+08" > 6.4239e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4681e+08" > 5.0333e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5395e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1866e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.3280e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.2738e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5379e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.9850e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.8406e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5969e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8548e+01 </func>
</region>
</regions>
<internal rank="266" log_i="1723713849.501684" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="267" mpi_size="768" stamp_init="1723713791.630798" stamp_final="1723713849.511266" username="apac4" allocationname="unknown" flags="0" pid="687712" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78805e+01" utime="4.73651e+01" stime="6.53558e+00" mtime="2.92364e+01" gflop="0.00000e+00" gbyte="3.77193e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92364e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000471546153d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77566e+01" utime="4.73344e+01" stime="6.52397e+00" mtime="2.92364e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92364e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4946e+08" > 4.5464e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4862e+08" > 2.9951e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3260e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1595e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.1963e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5381e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.0148e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.8001e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5909e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9603e+01 </func>
</region>
</regions>
<internal rank="267" log_i="1723713849.511266" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="268" mpi_size="768" stamp_init="1723713791.630727" stamp_final="1723713849.511615" username="apac4" allocationname="unknown" flags="0" pid="687713" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78809e+01" utime="4.53160e+01" stime="7.22008e+00" mtime="2.88881e+01" gflop="0.00000e+00" gbyte="3.77056e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88881e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000491449146a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77527e+01" utime="4.52866e+01" stime="7.20754e+00" mtime="2.88881e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88881e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4847e+08" > 6.0200e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4998e+08" > 4.3154e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3663e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1847e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.9779e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.8062e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5379e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.0291e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.6713e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5979e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9024e+01 </func>
</region>
</regions>
<internal rank="268" log_i="1723713849.511615" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="269" mpi_size="768" stamp_init="1723713791.632157" stamp_final="1723713849.502751" username="apac4" allocationname="unknown" flags="0" pid="687714" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78706e+01" utime="4.73630e+01" stime="6.44122e+00" mtime="2.90173e+01" gflop="0.00000e+00" gbyte="3.78063e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90173e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44314451446142d5646144614e0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77435e+01" utime="4.73276e+01" stime="6.43411e+00" mtime="2.90173e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90173e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5026e+08" > 4.7100e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4987e+08" > 2.9803e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9026e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1868e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.5139e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5372e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.0629e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.7905e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5913e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9764e+01 </func>
</region>
</regions>
<internal rank="269" log_i="1723713849.502751" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="270" mpi_size="768" stamp_init="1723713791.631401" stamp_final="1723713849.510277" username="apac4" allocationname="unknown" flags="0" pid="687715" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78789e+01" utime="4.48317e+01" stime="7.27501e+00" mtime="2.83064e+01" gflop="0.00000e+00" gbyte="3.74237e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83064e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ab14aa14a9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77513e+01" utime="4.48010e+01" stime="7.26395e+00" mtime="2.83064e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83064e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5047e+08" > 7.5114e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5049e+08" > 4.9767e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8712e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1836e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.0333e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.7948e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5366e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.1023e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.9503e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5979e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8788e+01 </func>
</region>
</regions>
<internal rank="270" log_i="1723713849.510277" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="271" mpi_size="768" stamp_init="1723713791.630652" stamp_final="1723713849.511574" username="apac4" allocationname="unknown" flags="0" pid="687716" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78809e+01" utime="4.72602e+01" stime="6.41094e+00" mtime="2.92046e+01" gflop="0.00000e+00" gbyte="3.76686e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92046e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a514a614a714a555a714a71484" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77534e+01" utime="4.72287e+01" stime="6.40049e+00" mtime="2.92046e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92046e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4948e+08" > 4.5801e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4949e+08" > 3.1008e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3412e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1933e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9308e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5361e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.1770e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.9312e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5915e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9517e+01 </func>
</region>
</regions>
<internal rank="271" log_i="1723713849.511574" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="272" mpi_size="768" stamp_init="1723713791.630654" stamp_final="1723713849.516742" username="apac4" allocationname="unknown" flags="0" pid="687717" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78861e+01" utime="4.39770e+01" stime="7.66783e+00" mtime="2.88335e+01" gflop="0.00000e+00" gbyte="3.76842e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88335e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77577e+01" utime="4.39453e+01" stime="7.65747e+00" mtime="2.88335e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88335e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5101e+08" > 5.8219e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4949e+08" > 4.7703e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0275e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1833e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2646e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.8286e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5360e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.2110e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.4615e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5957e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8329e+01 </func>
</region>
</regions>
<internal rank="272" log_i="1723713849.516742" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="273" mpi_size="768" stamp_init="1723713791.630644" stamp_final="1723713849.514846" username="apac4" allocationname="unknown" flags="0" pid="687718" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78842e+01" utime="4.74466e+01" stime="6.46074e+00" mtime="2.92523e+01" gflop="0.00000e+00" gbyte="3.77750e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92523e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005114c555511451147d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77571e+01" utime="4.74110e+01" stime="6.45376e+00" mtime="2.92523e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92523e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4887e+08" > 3.6693e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5044e+08" > 2.6068e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1751e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1714e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0330e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5354e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.2912e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.6594e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5938e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9848e+01 </func>
</region>
</regions>
<internal rank="273" log_i="1723713849.514846" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="274" mpi_size="768" stamp_init="1723713791.630984" stamp_final="1723713849.506558" username="apac4" allocationname="unknown" flags="0" pid="687719" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78756e+01" utime="4.56754e+01" stime="7.17871e+00" mtime="2.94405e+01" gflop="0.00000e+00" gbyte="3.75191e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.94405e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ff14d856ff14fe1483" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77456e+01" utime="4.56493e+01" stime="7.16313e+00" mtime="2.94405e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.94405e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4974e+08" > 4.8912e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4849e+08" > 4.0939e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1070e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1873e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.9366e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.4581e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5348e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.3518e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.6999e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5981e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8952e+01 </func>
</region>
</regions>
<internal rank="274" log_i="1723713849.506558" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="275" mpi_size="768" stamp_init="1723713791.630660" stamp_final="1723713849.501484" username="apac4" allocationname="unknown" flags="0" pid="687720" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78708e+01" utime="4.73173e+01" stime="6.52649e+00" mtime="2.92928e+01" gflop="0.00000e+00" gbyte="3.77964e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92928e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b114b314b414fa55b414b414be" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77433e+01" utime="4.72840e+01" stime="6.51706e+00" mtime="2.92928e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92928e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4894e+08" > 3.6292e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4866e+08" > 2.6271e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5274e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1845e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0705e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5351e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.3157e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.5092e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5922e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9527e+01 </func>
</region>
</regions>
<internal rank="275" log_i="1723713849.501484" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="276" mpi_size="768" stamp_init="1723713791.632215" stamp_final="1723713849.511778" username="apac4" allocationname="unknown" flags="0" pid="687721" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78796e+01" utime="4.57227e+01" stime="6.94965e+00" mtime="2.89227e+01" gflop="0.00000e+00" gbyte="3.77335e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89227e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77528e+01" utime="4.56937e+01" stime="6.93666e+00" mtime="2.89227e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89227e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4852e+08" > 5.1235e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4865e+08" > 3.7059e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5176e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1951e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3069e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.8444e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5344e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.3574e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.9002e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5982e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8994e+01 </func>
</region>
</regions>
<internal rank="276" log_i="1723713849.511778" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="277" mpi_size="768" stamp_init="1723713791.630908" stamp_final="1723713849.501107" username="apac4" allocationname="unknown" flags="0" pid="687722" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78702e+01" utime="4.72955e+01" stime="6.57132e+00" mtime="2.94361e+01" gflop="0.00000e+00" gbyte="3.77232e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.94361e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000cb14c5146d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77453e+01" utime="4.72690e+01" stime="6.55592e+00" mtime="2.94361e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.94361e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4915e+08" > 3.5166e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4934e+08" > 2.4721e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3836e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2011e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1499e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5343e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.4017e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.7190e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5911e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9819e+01 </func>
</region>
</regions>
<internal rank="277" log_i="1723713849.501107" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="278" mpi_size="768" stamp_init="1723713791.630844" stamp_final="1723713849.503520" username="apac4" allocationname="unknown" flags="0" pid="687723" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78727e+01" utime="4.55572e+01" stime="7.01813e+00" mtime="2.87530e+01" gflop="0.00000e+00" gbyte="3.77895e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87530e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000062146214c1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77429e+01" utime="4.55257e+01" stime="7.00746e+00" mtime="2.87530e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87530e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4837e+08" > 5.2724e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4928e+08" > 4.0219e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3709e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1857e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.8903e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.8542e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5333e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.4186e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.8597e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5953e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8966e+01 </func>
</region>
</regions>
<internal rank="278" log_i="1723713849.503520" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="279" mpi_size="768" stamp_init="1723713791.632343" stamp_final="1723713849.505577" username="apac4" allocationname="unknown" flags="0" pid="687724" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78732e+01" utime="4.74334e+01" stime="6.39193e+00" mtime="2.93947e+01" gflop="0.00000e+00" gbyte="3.78014e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.93947e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006a15f4566a15691551" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77468e+01" utime="4.73982e+01" stime="6.38511e+00" mtime="2.93947e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.93947e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4991e+08" > 3.5459e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4968e+08" > 2.6115e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5432e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1972e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0014e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1170e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5339e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.4321e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.6809e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5928e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9608e+01 </func>
</region>
</regions>
<internal rank="279" log_i="1723713849.505577" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="280" mpi_size="768" stamp_init="1723713791.631038" stamp_final="1723713849.505390" username="apac4" allocationname="unknown" flags="0" pid="687725" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78744e+01" utime="4.47434e+01" stime="7.45563e+00" mtime="2.84632e+01" gflop="0.00000e+00" gbyte="3.76682e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84632e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000035142156351435147e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77464e+01" utime="4.47150e+01" stime="7.44175e+00" mtime="2.84632e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84632e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4780e+08" > 7.4327e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4748e+08" > 5.3100e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6788e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1293e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5105e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.9037e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5336e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.4383e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.5998e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5958e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8196e+01 </func>
</region>
</regions>
<internal rank="280" log_i="1723713849.505390" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="281" mpi_size="768" stamp_init="1723713791.630674" stamp_final="1723713849.506037" username="apac4" allocationname="unknown" flags="0" pid="687726" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78754e+01" utime="4.73767e+01" stime="6.52183e+00" mtime="2.92184e+01" gflop="0.00000e+00" gbyte="3.76312e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92184e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a815a915aa15bc55aa15aa1511" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77503e+01" utime="4.73502e+01" stime="6.50573e+00" mtime="2.92184e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92184e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4784e+08" > 5.0616e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4912e+08" > 3.0018e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4543e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1713e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.9812e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5336e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.4676e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.6713e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5909e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9393e+01 </func>
</region>
</regions>
<internal rank="281" log_i="1723713849.506037" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="282" mpi_size="768" stamp_init="1723713791.630658" stamp_final="1723713849.504899" username="apac4" allocationname="unknown" flags="0" pid="687727" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78742e+01" utime="4.54381e+01" stime="7.35781e+00" mtime="2.92131e+01" gflop="0.00000e+00" gbyte="3.76968e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92131e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43d143e143f1428563f143f146e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77466e+01" utime="4.54056e+01" stime="7.34782e+00" mtime="2.92131e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92131e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4962e+08" > 6.0111e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4875e+08" > 4.2090e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2171e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1907e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.1049e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.8570e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5334e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.4951e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.6499e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5957e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8499e+01 </func>
</region>
</regions>
<internal rank="282" log_i="1723713849.504899" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="283" mpi_size="768" stamp_init="1723713791.630659" stamp_final="1723713849.508046" username="apac4" allocationname="unknown" flags="0" pid="687728" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78774e+01" utime="4.74300e+01" stime="6.48543e+00" mtime="2.86799e+01" gflop="0.00000e+00" gbyte="3.75862e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86799e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f7143655f714f21469" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77525e+01" utime="4.74005e+01" stime="6.47268e+00" mtime="2.86799e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86799e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4853e+08" > 4.9125e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4984e+08" > 2.9689e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1180e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1939e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0014e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.0878e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5331e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.5220e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.8907e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5970e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9183e+01 </func>
</region>
</regions>
<internal rank="283" log_i="1723713849.508046" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="284" mpi_size="768" stamp_init="1723713791.632175" stamp_final="1723713849.505339" username="apac4" allocationname="unknown" flags="0" pid="687729" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78732e+01" utime="4.47127e+01" stime="7.55473e+00" mtime="2.84469e+01" gflop="0.00000e+00" gbyte="3.76431e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84469e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77468e+01" utime="4.46825e+01" stime="7.54271e+00" mtime="2.84469e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84469e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4900e+08" > 7.0999e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4848e+08" > 5.1878e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0326e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1849e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5620e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.1431e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5329e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.5375e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.5497e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5977e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8804e+01 </func>
</region>
</regions>
<internal rank="284" log_i="1723713849.505339" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="285" mpi_size="768" stamp_init="1723713791.632318" stamp_final="1723713849.501295" username="apac4" allocationname="unknown" flags="0" pid="687730" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78690e+01" utime="4.75375e+01" stime="6.30209e+00" mtime="2.92319e+01" gflop="0.00000e+00" gbyte="3.77441e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92319e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000591559151d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77429e+01" utime="4.75048e+01" stime="6.29279e+00" mtime="2.92319e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92319e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4818e+08" > 5.0291e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5029e+08" > 2.8835e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0878e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1847e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9370e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5323e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.5869e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.6809e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5934e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9763e+01 </func>
</region>
</regions>
<internal rank="285" log_i="1723713849.501295" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="286" mpi_size="768" stamp_init="1723713791.630662" stamp_final="1723713849.511581" username="apac4" allocationname="unknown" flags="0" pid="687731" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78809e+01" utime="4.60003e+01" stime="6.82881e+00" mtime="2.84486e+01" gflop="0.00000e+00" gbyte="3.77815e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84486e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77513e+01" utime="4.59610e+01" stime="6.82540e+00" mtime="2.84486e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84486e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5006e+08" > 5.5697e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4950e+08" > 3.9023e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2274e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1967e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0054e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.1169e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5324e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.5895e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.5807e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5956e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8765e+01 </func>
</region>
</regions>
<internal rank="286" log_i="1723713849.511581" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="287" mpi_size="768" stamp_init="1723713791.630668" stamp_final="1723713849.506996" username="apac4" allocationname="unknown" flags="0" pid="687732" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="5.78763e+01" utime="4.72758e+01" stime="6.51578e+00" mtime="2.91621e+01" gflop="0.00000e+00" gbyte="3.77750e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91621e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001c141b14c5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.77485e+01" utime="4.72451e+01" stime="6.50348e+00" mtime="2.91621e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91621e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4972e+08" > 5.2573e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4825e+08" > 3.1955e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5809e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.1850e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5497e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.2289e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5317e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.6809e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.8191e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5916e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9175e+01 </func>
</region>
</regions>
<internal rank="287" log_i="1723713849.506996" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="288" mpi_size="768" stamp_init="1723713791.106750" stamp_final="1723713849.507387" username="apac4" allocationname="unknown" flags="0" pid="557917" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.84006e+01" utime="4.27833e+01" stime="1.28506e+01" mtime="3.18457e+01" gflop="0.00000e+00" gbyte="3.86257e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18457e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000db15db1535" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83465e+01" utime="4.27530e+01" stime="1.28405e+01" mtime="3.18457e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18457e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4899e+08" > 6.3619e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4815e+08" > 4.9860e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6787e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4054e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4416e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3288e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5313e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.7002e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7919e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5918e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8343e+01 </func>
</region>
</regions>
<internal rank="288" log_i="1723713849.507387" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="289" mpi_size="768" stamp_init="1723713791.106758" stamp_final="1723713849.515693" username="apac4" allocationname="unknown" flags="0" pid="557918" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.84089e+01" utime="4.95057e+01" stime="6.63074e+00" mtime="3.22858e+01" gflop="0.00000e+00" gbyte="3.78330e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22858e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48d158e158f15a8558f158f1530" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83488e+01" utime="4.94748e+01" stime="6.62073e+00" mtime="3.22858e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22858e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4846e+08" > 4.7217e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4885e+08" > 2.8816e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4017e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4796e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0145e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5309e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.7287e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8205e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5890e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9235e+01 </func>
</region>
</regions>
<internal rank="289" log_i="1723713849.515693" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="290" mpi_size="768" stamp_init="1723713791.113424" stamp_final="1723713849.501354" username="apac4" allocationname="unknown" flags="0" pid="557919" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83879e+01" utime="4.80399e+01" stime="7.35577e+00" mtime="3.17698e+01" gflop="0.00000e+00" gbyte="3.77705e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17698e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49214941495142355951495145d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83156e+01" utime="4.80081e+01" stime="7.34646e+00" mtime="3.17698e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17698e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4893e+08" > 5.6886e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4887e+08" > 4.2565e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5007e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4289e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1492e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3336e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5303e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.7386e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1996e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5925e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8521e+01 </func>
</region>
</regions>
<internal rank="290" log_i="1723713849.501354" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="291" mpi_size="768" stamp_init="1723713791.110439" stamp_final="1723713849.499765" username="apac4" allocationname="unknown" flags="0" pid="557920" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83893e+01" utime="4.93410e+01" stime="6.56703e+00" mtime="3.27210e+01" gflop="0.00000e+00" gbyte="3.76408e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27210e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000151415147f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83228e+01" utime="4.93092e+01" stime="6.55915e+00" mtime="3.27210e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27210e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 4.6752e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4851e+08" > 2.7914e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4765e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4837e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1696e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0180e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5316e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.6942e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5501e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5879e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9592e+01 </func>
</region>
</regions>
<internal rank="291" log_i="1723713849.499765" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="292" mpi_size="768" stamp_init="1723713791.113865" stamp_final="1723713849.506935" username="apac4" allocationname="unknown" flags="0" pid="557921" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83931e+01" utime="4.76000e+01" stime="7.00244e+00" mtime="3.19339e+01" gflop="0.00000e+00" gbyte="3.77323e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19339e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83209e+01" utime="4.75684e+01" stime="6.99327e+00" mtime="3.19339e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19339e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4823e+08" > 7.0105e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4906e+08" > 4.4375e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1745e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4539e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8565e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0208e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5310e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.7360e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0708e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5923e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8856e+01 </func>
</region>
</regions>
<internal rank="292" log_i="1723713849.506935" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="293" mpi_size="768" stamp_init="1723713791.116508" stamp_final="1723713849.506874" username="apac4" allocationname="unknown" flags="0" pid="557922" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83904e+01" utime="5.01618e+01" stime="5.92652e+00" mtime="3.20271e+01" gflop="0.00000e+00" gbyte="3.74763e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20271e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48414851487144e5587148614ec" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83111e+01" utime="5.01277e+01" stime="5.91929e+00" mtime="3.20271e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20271e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4943e+08" > 4.4446e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4945e+08" > 2.8069e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.4848e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4468e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0259e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5308e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.7370e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5405e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5862e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9943e+01 </func>
</region>
</regions>
<internal rank="293" log_i="1723713849.506874" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="294" mpi_size="768" stamp_init="1723713791.120598" stamp_final="1723713849.507379" username="apac4" allocationname="unknown" flags="0" pid="557923" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83868e+01" utime="4.75078e+01" stime="7.21864e+00" mtime="3.18072e+01" gflop="0.00000e+00" gbyte="3.74790e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18072e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82982e+01" utime="4.74802e+01" stime="7.20432e+00" mtime="3.18072e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18072e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4880e+08" > 6.1743e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5008e+08" > 3.5463e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5283e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4790e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8156e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0207e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5304e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.8037e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3593e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5922e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8443e+01 </func>
</region>
</regions>
<internal rank="294" log_i="1723713849.507379" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="295" mpi_size="768" stamp_init="1723713791.122311" stamp_final="1723713849.504098" username="apac4" allocationname="unknown" flags="0" pid="557924" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83818e+01" utime="4.97321e+01" stime="6.36587e+00" mtime="3.24402e+01" gflop="0.00000e+00" gbyte="3.78071e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24402e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf42014221423141e5523142314f3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82926e+01" utime="4.97026e+01" stime="6.35275e+00" mtime="3.24402e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24402e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4976e+08" > 4.6022e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4767e+08" > 2.9943e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5897e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4854e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9789e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0207e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5302e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.8373e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4094e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5855e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9200e+01 </func>
</region>
</regions>
<internal rank="295" log_i="1723713849.504098" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="296" mpi_size="768" stamp_init="1723713791.129175" stamp_final="1723713849.511222" username="apac4" allocationname="unknown" flags="0" pid="557925" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83820e+01" utime="4.81765e+01" stime="7.32017e+00" mtime="3.17841e+01" gflop="0.00000e+00" gbyte="3.76331e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17841e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008f158f151c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82870e+01" utime="4.81444e+01" stime="7.31003e+00" mtime="3.17841e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17841e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5098e+08" > 5.0103e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4819e+08" > 5.3225e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7313e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3949e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4416e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5295e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.8494e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9492e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5910e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8395e+01 </func>
</region>
</regions>
<internal rank="296" log_i="1723713849.511222" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="297" mpi_size="768" stamp_init="1723713791.127191" stamp_final="1723713849.515505" username="apac4" allocationname="unknown" flags="0" pid="557926" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83883e+01" utime="4.98958e+01" stime="6.21543e+00" mtime="3.23386e+01" gflop="0.00000e+00" gbyte="3.76781e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23386e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82900e+01" utime="4.98590e+01" stime="6.20930e+00" mtime="3.23386e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23386e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4871e+08" > 3.6737e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4940e+08" > 2.9430e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1957e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4844e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0227e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5289e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.9729e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3498e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5837e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9584e+01 </func>
</region>
</regions>
<internal rank="297" log_i="1723713849.515505" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="298" mpi_size="768" stamp_init="1723713791.129179" stamp_final="1723713849.511230" username="apac4" allocationname="unknown" flags="0" pid="557927" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83821e+01" utime="4.65309e+01" stime="7.52542e+00" mtime="3.15299e+01" gflop="0.00000e+00" gbyte="3.77415e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15299e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82849e+01" utime="4.65006e+01" stime="7.51327e+00" mtime="3.15299e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15299e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4892e+08" > 6.3286e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4963e+08" > 5.0970e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9778e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4850e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.8161e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0203e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5285e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.0008e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9397e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5932e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8708e+01 </func>
</region>
</regions>
<internal rank="298" log_i="1723713849.511230" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="299" mpi_size="768" stamp_init="1723713791.131375" stamp_final="1723713849.508254" username="apac4" allocationname="unknown" flags="0" pid="557928" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83769e+01" utime="4.97818e+01" stime="6.30215e+00" mtime="3.28435e+01" gflop="0.00000e+00" gbyte="3.77724e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28435e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000034157a553415331551" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82775e+01" utime="4.97518e+01" stime="6.28893e+00" mtime="3.28435e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28435e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4956e+08" > 3.6645e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4838e+08" > 2.9391e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5992e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4953e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3842e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0261e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5272e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.1201e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5286e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5842e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9672e+01 </func>
</region>
</regions>
<internal rank="299" log_i="1723713849.508254" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="300" mpi_size="768" stamp_init="1723713791.135331" stamp_final="1723713849.511166" username="apac4" allocationname="unknown" flags="0" pid="557929" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83758e+01" utime="4.84583e+01" stime="6.46468e+00" mtime="3.18135e+01" gflop="0.00000e+00" gbyte="3.74901e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18135e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f214f214e5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82804e+01" utime="4.84314e+01" stime="6.44898e+00" mtime="3.18135e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18135e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5055e+08" > 4.8233e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4886e+08" > 4.6649e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7867e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4935e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2288e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0130e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5287e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.9973e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0398e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5924e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9324e+01 </func>
</region>
</regions>
<internal rank="300" log_i="1723713849.511166" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="301" mpi_size="768" stamp_init="1723713791.136929" stamp_final="1723713849.499534" username="apac4" allocationname="unknown" flags="0" pid="557930" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83626e+01" utime="4.99806e+01" stime="6.07507e+00" mtime="3.24162e+01" gflop="0.00000e+00" gbyte="3.77441e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24162e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82655e+01" utime="4.99500e+01" stime="6.06343e+00" mtime="3.24162e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24162e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4843e+08" > 3.5992e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4972e+08" > 2.8859e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9420e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4813e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0227e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5289e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.9770e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3498e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5840e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9927e+01 </func>
</region>
</regions>
<internal rank="301" log_i="1723713849.499534" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="302" mpi_size="768" stamp_init="1723713791.144855" stamp_final="1723713849.499815" username="apac4" allocationname="unknown" flags="0" pid="557931" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83550e+01" utime="4.78495e+01" stime="6.70206e+00" mtime="3.20537e+01" gflop="0.00000e+00" gbyte="3.77224e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20537e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f415f615f7158155f715f61523" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82649e+01" utime="4.78167e+01" stime="6.69340e+00" mtime="3.20537e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20537e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4913e+08" > 5.5515e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4961e+08" > 4.0262e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6658e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4744e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1134e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0129e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5285e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.0143e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9302e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5903e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8650e+01 </func>
</region>
</regions>
<internal rank="302" log_i="1723713849.499815" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="303" mpi_size="768" stamp_init="1723713791.141834" stamp_final="1723713849.501801" username="apac4" allocationname="unknown" flags="0" pid="557932" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83600e+01" utime="4.97141e+01" stime="6.38608e+00" mtime="3.27200e+01" gflop="0.00000e+00" gbyte="3.77666e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27200e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002c142714a3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82627e+01" utime="4.96810e+01" stime="6.37851e+00" mtime="3.27200e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27200e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4928e+08" > 3.7471e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5019e+08" > 2.9623e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8118e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4881e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0259e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5284e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.0261e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1686e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5831e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9335e+01 </func>
</region>
</regions>
<internal rank="303" log_i="1723713849.501801" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="304" mpi_size="768" stamp_init="1723713791.145771" stamp_final="1723713849.512755" username="apac4" allocationname="unknown" flags="0" pid="557933" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83670e+01" utime="4.64258e+01" stime="7.84865e+00" mtime="3.18643e+01" gflop="0.00000e+00" gbyte="3.78117e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18643e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000e415d556e415e3151f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82773e+01" utime="4.63999e+01" stime="7.83277e+00" mtime="3.18643e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18643e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4879e+08" > 6.2415e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5014e+08" > 5.1144e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2884e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4702e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0393e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0091e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5277e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.0980e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2115e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5900e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7726e+01 </func>
</region>
</regions>
<internal rank="304" log_i="1723713849.512755" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="305" mpi_size="768" stamp_init="1723713791.147844" stamp_final="1723713849.509559" username="apac4" allocationname="unknown" flags="0" pid="557934" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83617e+01" utime="4.98215e+01" stime="6.25497e+00" mtime="3.23574e+01" gflop="0.00000e+00" gbyte="3.76610e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23574e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ca14c9149d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82724e+01" utime="4.97899e+01" stime="6.24522e+00" mtime="3.23574e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23574e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.2159e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5011e+08" > 4.8842e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4882e+08" > 3.2710e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5059e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4909e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0175e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5266e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.1766e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0208e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5318e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9168e+01 </func>
</region>
</regions>
<internal rank="305" log_i="1723713849.509559" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="306" mpi_size="768" stamp_init="1723713791.149984" stamp_final="1723713849.500425" username="apac4" allocationname="unknown" flags="0" pid="557935" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83504e+01" utime="4.75087e+01" stime="7.50217e+00" mtime="3.24132e+01" gflop="0.00000e+00" gbyte="3.76808e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24132e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82645e+01" utime="4.74801e+01" stime="7.49003e+00" mtime="3.24132e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24132e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4911e+08" > 5.8157e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4784e+08" > 5.4368e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.5360e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4869e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2708e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0242e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5272e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.1234e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1805e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5876e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8075e+01 </func>
</region>
</regions>
<internal rank="306" log_i="1723713849.500425" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="307" mpi_size="768" stamp_init="1723713791.152126" stamp_final="1723713849.511938" username="apac4" allocationname="unknown" flags="0" pid="557936" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83598e+01" utime="4.91439e+01" stime="6.92119e+00" mtime="3.25678e+01" gflop="0.00000e+00" gbyte="3.77316e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25678e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c9143b55c914c914f1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82745e+01" utime="4.91128e+01" stime="6.91186e+00" mtime="3.25678e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25678e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4700e+08" > 4.8921e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4851e+08" > 3.0446e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4473e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4614e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0246e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5273e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.1444e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2783e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5834e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9460e+01 </func>
</region>
</regions>
<internal rank="307" log_i="1723713849.511938" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="308" mpi_size="768" stamp_init="1723713791.158999" stamp_final="1723713849.501840" username="apac4" allocationname="unknown" flags="0" pid="557937" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83428e+01" utime="4.67129e+01" stime="7.72833e+00" mtime="3.17285e+01" gflop="0.00000e+00" gbyte="3.76858e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17285e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82631e+01" utime="4.66829e+01" stime="7.71801e+00" mtime="3.17285e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17285e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4885e+08" > 6.9189e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4831e+08" > 4.6553e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8682e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4720e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.1258e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0090e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5263e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.1836e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4308e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5429e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8983e+01 </func>
</region>
</regions>
<internal rank="308" log_i="1723713849.501840" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="309" mpi_size="768" stamp_init="1723713791.184535" stamp_final="1723713849.511057" username="apac4" allocationname="unknown" flags="0" pid="557938" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83265e+01" utime="4.92772e+01" stime="6.85241e+00" mtime="3.22075e+01" gflop="0.00000e+00" gbyte="3.78277e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22075e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008b14b5568b148b1471" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82628e+01" utime="4.92416e+01" stime="6.84921e+00" mtime="3.22075e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22075e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4916e+08" > 4.9105e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4682e+08" > 3.8184e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0082e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4864e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0090e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5267e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.2021e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2592e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5855e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9520e+01 </func>
</region>
</regions>
<internal rank="309" log_i="1723713849.511057" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="310" mpi_size="768" stamp_init="1723713791.185278" stamp_final="1723713849.519161" username="apac4" allocationname="unknown" flags="0" pid="557939" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83339e+01" utime="4.74202e+01" stime="7.48316e+00" mtime="3.16654e+01" gflop="0.00000e+00" gbyte="3.76331e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16654e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a714a214e1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82703e+01" utime="4.73901e+01" stime="7.47353e+00" mtime="3.16654e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16654e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4839e+08" > 6.5250e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5015e+08" > 4.8669e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5461e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4823e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0112e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5263e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.2101e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9802e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5370e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8270e+01 </func>
</region>
</regions>
<internal rank="310" log_i="1723713849.519161" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="311" mpi_size="768" stamp_init="1723713791.160854" stamp_final="1723713849.506812" username="apac4" allocationname="unknown" flags="0" pid="557940" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="5.83460e+01" utime="4.92591e+01" stime="6.82337e+00" mtime="3.26450e+01" gflop="0.00000e+00" gbyte="3.76793e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26450e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000541455555414531464" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82657e+01" utime="4.92264e+01" stime="6.81611e+00" mtime="3.26450e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26450e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4948e+08" > 4.8823e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4894e+08" > 3.0362e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8487e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4848e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9789e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0194e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5259e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.2540e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5810e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5850e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9119e+01 </func>
</region>
</regions>
<internal rank="311" log_i="1723713849.506812" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="312" mpi_size="768" stamp_init="1723713791.098952" stamp_final="1723713849.516608" username="apac4" allocationname="unknown" flags="0" pid="545823" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.84177e+01" utime="4.19045e+01" stime="1.26225e+01" mtime="3.14718e+01" gflop="0.00000e+00" gbyte="3.85769e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.14718e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000bf142255bf14bf1456" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83647e+01" utime="4.18703e+01" stime="1.26180e+01" mtime="3.14718e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.14718e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4617e+08" > 7.8816e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4799e+08" > 4.5219e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4564e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4917e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8983e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0160e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5259e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.2921e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6298e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6086e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8017e+01 </func>
</region>
</regions>
<internal rank="312" log_i="1723713849.516608" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="313" mpi_size="768" stamp_init="1723713791.098940" stamp_final="1723713849.510433" username="apac4" allocationname="unknown" flags="0" pid="545824" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.84115e+01" utime="4.94813e+01" stime="6.65809e+00" mtime="3.28141e+01" gflop="0.00000e+00" gbyte="3.75397e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28141e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf434153515371591563715361544" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83496e+01" utime="4.94439e+01" stime="6.65530e+00" mtime="3.28141e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28141e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4718e+08" > 5.5209e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4818e+08" > 3.1662e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9683e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5044e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0172e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5259e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.2456e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5201e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6080e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9086e+01 </func>
</region>
</regions>
<internal rank="313" log_i="1723713849.510433" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="314" mpi_size="768" stamp_init="1723713791.100910" stamp_final="1723713849.507814" username="apac4" allocationname="unknown" flags="0" pid="545825" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.84069e+01" utime="4.54860e+01" stime="7.96158e+00" mtime="3.19264e+01" gflop="0.00000e+00" gbyte="3.77560e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19264e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83521e+01" utime="4.54527e+01" stime="7.95675e+00" mtime="3.19264e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19264e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4814e+08" > 1.1421e+00 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4681e+08" > 6.8921e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5371e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4986e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.5327e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0159e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5259e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.2882e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5797e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6086e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8008e+01 </func>
</region>
</regions>
<internal rank="314" log_i="1723713849.507814" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="315" mpi_size="768" stamp_init="1723713791.102283" stamp_final="1723713849.510505" username="apac4" allocationname="unknown" flags="0" pid="545826" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.84082e+01" utime="4.92965e+01" stime="6.67353e+00" mtime="3.26067e+01" gflop="0.00000e+00" gbyte="3.77338e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26067e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e114e114de" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83475e+01" utime="4.92654e+01" stime="6.66558e+00" mtime="3.26067e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26067e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4838e+08" > 5.5417e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4805e+08" > 2.8004e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7359e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4823e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0169e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5251e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.3406e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0398e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6066e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9135e+01 </func>
</region>
</regions>
<internal rank="315" log_i="1723713849.510505" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="316" mpi_size="768" stamp_init="1723713791.105021" stamp_final="1723713849.513780" username="apac4" allocationname="unknown" flags="0" pid="545827" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.84088e+01" utime="4.73005e+01" stime="7.29282e+00" mtime="3.19091e+01" gflop="0.00000e+00" gbyte="3.76957e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19091e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000eb15eb1523" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83426e+01" utime="4.72671e+01" stime="7.28703e+00" mtime="3.19091e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19091e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4820e+08" > 8.1079e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4936e+08" > 4.2734e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2923e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4705e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2531e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0166e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5244e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.4035e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7108e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6091e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8619e+01 </func>
</region>
</regions>
<internal rank="316" log_i="1723713849.513780" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="317" mpi_size="768" stamp_init="1723713791.107554" stamp_final="1723713849.510534" username="apac4" allocationname="unknown" flags="0" pid="545828" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.84030e+01" utime="4.96115e+01" stime="6.52690e+00" mtime="3.25011e+01" gflop="0.00000e+00" gbyte="3.77491e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25011e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001b15e7551b151b153f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83140e+01" utime="4.95778e+01" stime="6.51995e+00" mtime="3.25011e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25011e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.5831e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4868e+08" > 5.5033e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4924e+08" > 2.7656e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3591e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4864e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0169e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5250e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.3858e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2091e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6058e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9406e+01 </func>
</region>
</regions>
<internal rank="317" log_i="1723713849.510534" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="318" mpi_size="768" stamp_init="1723713791.110413" stamp_final="1723713849.506260" username="apac4" allocationname="unknown" flags="0" pid="545829" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83958e+01" utime="4.67447e+01" stime="7.40224e+00" mtime="3.14807e+01" gflop="0.00000e+00" gbyte="3.76282e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.14807e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c815ff55c815c71503" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83230e+01" utime="4.67094e+01" stime="7.39760e+00" mtime="3.14807e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.14807e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4997e+08" > 8.1974e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4822e+08" > 4.8783e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0999e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5065e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1587e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0166e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5234e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.4707e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9087e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5602e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8335e+01 </func>
</region>
</regions>
<internal rank="318" log_i="1723713849.506260" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="319" mpi_size="768" stamp_init="1723713791.113388" stamp_final="1723713849.503738" username="apac4" allocationname="unknown" flags="0" pid="545830" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83903e+01" utime="4.95318e+01" stime="6.60584e+00" mtime="3.27013e+01" gflop="0.00000e+00" gbyte="3.76984e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27013e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a515a615a815bf56a815a71536" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83121e+01" utime="4.94968e+01" stime="6.59984e+00" mtime="3.27013e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27013e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4896e+08" > 5.3275e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4953e+08" > 2.9643e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0314e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5028e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0175e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5238e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.5002e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7108e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6069e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8932e+01 </func>
</region>
</regions>
<internal rank="319" log_i="1723713849.503738" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="320" mpi_size="768" stamp_init="1723713791.114915" stamp_final="1723713849.506190" username="apac4" allocationname="unknown" flags="0" pid="545831" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83913e+01" utime="4.75009e+01" stime="7.63044e+00" mtime="3.24950e+01" gflop="0.00000e+00" gbyte="3.76209e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24950e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003c1598553c153b150f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83106e+01" utime="4.74702e+01" stime="7.62099e+00" mtime="3.24950e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24950e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4786e+08" > 7.1906e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4725e+08" > 7.7785e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7175e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4170e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.4976e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.5811e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5220e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.6376e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5797e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6094e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8808e+01 </func>
</region>
</regions>
<internal rank="320" log_i="1723713849.506190" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="321" mpi_size="768" stamp_init="1723713791.116310" stamp_final="1723713849.509988" username="apac4" allocationname="unknown" flags="0" pid="545832" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83937e+01" utime="5.06693e+01" stime="6.45839e+00" mtime="3.30852e+01" gflop="0.00000e+00" gbyte="3.76652e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.30852e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c9148b55c914c914f4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83115e+01" utime="5.06387e+01" stime="6.44857e+00" mtime="3.30852e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.30852e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4820e+08" > 4.2563e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4971e+08" > 3.4529e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5599e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4347e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7491e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5225e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.6015e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7299e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6039e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9878e+01 </func>
</region>
</regions>
<internal rank="321" log_i="1723713849.509988" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="322" mpi_size="768" stamp_init="1723713791.123359" stamp_final="1723713849.516295" username="apac4" allocationname="unknown" flags="0" pid="545833" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83929e+01" utime="4.70893e+01" stime="7.13181e+00" mtime="3.18521e+01" gflop="0.00000e+00" gbyte="3.74931e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18521e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000084143c5584148414c4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83125e+01" utime="4.70558e+01" stime="7.12504e+00" mtime="3.18521e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18521e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4806e+08" > 6.6500e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4729e+08" > 4.9971e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1526e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4935e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1587e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0143e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5224e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.6371e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8396e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6099e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8821e+01 </func>
</region>
</regions>
<internal rank="322" log_i="1723713849.516295" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="323" mpi_size="768" stamp_init="1723713791.121152" stamp_final="1723713849.499934" username="apac4" allocationname="unknown" flags="0" pid="545834" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83788e+01" utime="4.98188e+01" stime="6.31814e+00" mtime="3.30757e+01" gflop="0.00000e+00" gbyte="3.77586e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.30757e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4571459145a1473555a145a14af" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82959e+01" utime="4.97837e+01" stime="6.31178e+00" mtime="3.30757e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.30757e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4907e+08" > 4.2721e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4853e+08" > 3.3069e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5807e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4996e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3842e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0175e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5217e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.7117e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9516e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5533e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9863e+01 </func>
</region>
</regions>
<internal rank="323" log_i="1723713849.499934" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="324" mpi_size="768" stamp_init="1723713791.123727" stamp_final="1723713849.500268" username="apac4" allocationname="unknown" flags="0" pid="545835" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83765e+01" utime="4.75978e+01" stime="6.92539e+00" mtime="3.17918e+01" gflop="0.00000e+00" gbyte="3.77567e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17918e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82960e+01" utime="4.75659e+01" stime="6.91668e+00" mtime="3.17918e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17918e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4921e+08" > 6.0324e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4914e+08" > 4.0085e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8487e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4996e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.7417e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0169e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5208e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.7644e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5606e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6083e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9125e+01 </func>
</region>
</regions>
<internal rank="324" log_i="1723713849.500268" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="325" mpi_size="768" stamp_init="1723713791.125968" stamp_final="1723713849.503455" username="apac4" allocationname="unknown" flags="0" pid="545836" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83775e+01" utime="4.96375e+01" stime="6.46762e+00" mtime="3.30179e+01" gflop="0.00000e+00" gbyte="3.77686e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.30179e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003115301505" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82953e+01" utime="4.96078e+01" stime="6.45636e+00" mtime="3.30179e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.30179e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4866e+08" > 4.2915e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5037e+08" > 3.3398e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4483e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4961e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0171e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5213e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.7596e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0708e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6044e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9939e+01 </func>
</region>
</regions>
<internal rank="325" log_i="1723713849.503455" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="326" mpi_size="768" stamp_init="1723713791.128466" stamp_final="1723713849.506680" username="apac4" allocationname="unknown" flags="0" pid="545837" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83782e+01" utime="4.77253e+01" stime="7.00564e+00" mtime="3.18572e+01" gflop="0.00000e+00" gbyte="3.76968e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18572e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82992e+01" utime="4.76943e+01" stime="6.99677e+00" mtime="3.18572e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18572e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4923e+08" > 6.0485e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4762e+08" > 4.4485e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0620e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5022e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3379e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0169e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5209e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.7979e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3402e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6077e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8969e+01 </func>
</region>
</regions>
<internal rank="326" log_i="1723713849.506680" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="327" mpi_size="768" stamp_init="1723713791.130743" stamp_final="1723713849.514482" username="apac4" allocationname="unknown" flags="0" pid="545838" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83837e+01" utime="4.96973e+01" stime="6.39304e+00" mtime="3.27473e+01" gflop="0.00000e+00" gbyte="3.76438e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27473e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000063159d56631563153c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83047e+01" utime="4.96633e+01" stime="6.38738e+00" mtime="3.27473e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27473e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4880e+08" > 4.3195e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4859e+08" > 3.0876e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5201e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4910e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0169e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5211e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.7830e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7299e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6044e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9601e+01 </func>
</region>
</regions>
<internal rank="327" log_i="1723713849.514482" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="328" mpi_size="768" stamp_init="1723713791.133732" stamp_final="1723713849.511976" username="apac4" allocationname="unknown" flags="0" pid="545839" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83782e+01" utime="4.68471e+01" stime="7.38477e+00" mtime="3.20239e+01" gflop="0.00000e+00" gbyte="3.76534e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20239e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83019e+01" utime="4.68179e+01" stime="7.37471e+00" mtime="3.20239e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20239e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4769e+08" > 6.5951e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4884e+08" > 5.1101e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9599e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5023e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.1900e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0133e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5208e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.8058e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8396e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6098e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8178e+01 </func>
</region>
</regions>
<internal rank="328" log_i="1723713849.511976" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="329" mpi_size="768" stamp_init="1723713791.137435" stamp_final="1723713849.504152" username="apac4" allocationname="unknown" flags="0" pid="545840" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83667e+01" utime="5.01861e+01" stime="6.93598e+00" mtime="3.30190e+01" gflop="0.00000e+00" gbyte="3.77296e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.30190e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82915e+01" utime="5.01564e+01" stime="6.92588e+00" mtime="3.30190e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.30190e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4828e+08" > 4.8117e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4910e+08" > 3.1248e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2372e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4215e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0041e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7481e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5205e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.8427e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5702e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6038e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9095e+01 </func>
</region>
</regions>
<internal rank="329" log_i="1723713849.504152" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="330" mpi_size="768" stamp_init="1723713791.138679" stamp_final="1723713849.507695" username="apac4" allocationname="unknown" flags="0" pid="545841" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83690e+01" utime="4.68997e+01" stime="7.63870e+00" mtime="3.21757e+01" gflop="0.00000e+00" gbyte="3.76530e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21757e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000101588551015101533" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82957e+01" utime="4.68666e+01" stime="7.63296e+00" mtime="3.21757e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21757e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4752e+08" > 7.0293e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4596e+08" > 6.7122e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6497e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4883e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0998e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0134e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5196e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.9437e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9993e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6084e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8595e+01 </func>
</region>
</regions>
<internal rank="330" log_i="1723713849.507695" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="331" mpi_size="768" stamp_init="1723713791.143622" stamp_final="1723713849.505912" username="apac4" allocationname="unknown" flags="0" pid="545842" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83623e+01" utime="4.91087e+01" stime="6.99868e+00" mtime="3.30643e+01" gflop="0.00000e+00" gbyte="3.76675e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.30643e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82928e+01" utime="4.90777e+01" stime="6.99081e+00" mtime="3.30643e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.30643e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4910e+08" > 4.6580e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4778e+08" > 2.7102e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7467e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5030e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0135e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5195e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.9003e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1805e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6031e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9653e+01 </func>
</region>
</regions>
<internal rank="331" log_i="1723713849.505912" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="332" mpi_size="768" stamp_init="1723713791.142636" stamp_final="1723713849.504475" username="apac4" allocationname="unknown" flags="0" pid="545843" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83618e+01" utime="4.73864e+01" stime="7.43574e+00" mtime="3.22747e+01" gflop="0.00000e+00" gbyte="3.76591e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22747e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000018149d5518141814c1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82912e+01" utime="4.73551e+01" stime="7.42854e+00" mtime="3.22747e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22747e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4831e+08" > 6.0111e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4985e+08" > 5.7559e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5536e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4973e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7428e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0166e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5193e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.9575e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3903e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6102e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8893e+01 </func>
</region>
</regions>
<internal rank="332" log_i="1723713849.504475" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="333" mpi_size="768" stamp_init="1723713791.144789" stamp_final="1723713849.511310" username="apac4" allocationname="unknown" flags="0" pid="545844" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83665e+01" utime="5.02229e+01" stime="6.91093e+00" mtime="3.22111e+01" gflop="0.00000e+00" gbyte="3.76522e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22111e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82976e+01" utime="5.01949e+01" stime="6.90049e+00" mtime="3.22111e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22111e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4934e+08" > 4.7295e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5001e+08" > 2.9289e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0633e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4150e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7368e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5187e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.9827e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8801e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6054e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9478e+01 </func>
</region>
</regions>
<internal rank="333" log_i="1723713849.511310" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="334" mpi_size="768" stamp_init="1723713791.147196" stamp_final="1723713849.506488" username="apac4" allocationname="unknown" flags="0" pid="545845" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83593e+01" utime="4.76695e+01" stime="7.25747e+00" mtime="3.20293e+01" gflop="0.00000e+00" gbyte="3.76827e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20293e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b214b414b5142355b514b514cb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82936e+01" utime="4.76398e+01" stime="7.24863e+00" mtime="3.20293e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20293e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5018e+08" > 5.8430e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4838e+08" > 4.2829e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4904e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4919e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2487e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0165e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5189e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.9978e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8706e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6091e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8745e+01 </func>
</region>
</regions>
<internal rank="334" log_i="1723713849.506488" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="335" mpi_size="768" stamp_init="1723713791.148212" stamp_final="1723713849.509896" username="apac4" allocationname="unknown" flags="0" pid="545846" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="5.83617e+01" utime="4.95238e+01" stime="6.59854e+00" mtime="3.24703e+01" gflop="0.00000e+00" gbyte="3.76862e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24703e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e115e215e315c256e315e31507" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82937e+01" utime="4.94937e+01" stime="6.59011e+00" mtime="3.24703e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24703e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4950e+08" > 4.7959e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4820e+08" > 3.0262e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3919e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4916e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0165e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5189e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0011e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.4796e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6027e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9405e+01 </func>
</region>
</regions>
<internal rank="335" log_i="1723713849.509896" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="336" mpi_size="768" stamp_init="1723713791.102639" stamp_final="1723713849.501254" username="apac4" allocationname="unknown" flags="0" pid="822457" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83986e+01" utime="4.09616e+01" stime="1.29271e+01" mtime="3.19946e+01" gflop="0.00000e+00" gbyte="3.86723e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19946e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a815aa15ab15ab55ab15aa152e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83436e+01" utime="4.09268e+01" stime="1.29218e+01" mtime="3.19946e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19946e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4943e+08" > 8.4213e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4914e+08" > 4.8494e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5993e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5014e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3829e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0014e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5178e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0068e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6202e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6121e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8344e+01 </func>
</region>
</regions>
<internal rank="336" log_i="1723713849.501254" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="337" mpi_size="768" stamp_init="1723713791.103762" stamp_final="1723713849.496522" username="apac4" allocationname="unknown" flags="0" pid="822458" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83928e+01" utime="4.95498e+01" stime="6.54776e+00" mtime="3.22941e+01" gflop="0.00000e+00" gbyte="3.76190e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22941e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4de15df15e1157855e115e01530" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83285e+01" utime="4.95173e+01" stime="6.53896e+00" mtime="3.22941e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22941e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4946e+08" > 4.7143e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4909e+08" > 3.2145e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9608e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5132e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4319e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0043e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5178e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0115e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5296e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6670e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9656e+01 </func>
</region>
</regions>
<internal rank="337" log_i="1723713849.496522" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="338" mpi_size="768" stamp_init="1723713791.105736" stamp_final="1723713849.513103" username="apac4" allocationname="unknown" flags="0" pid="822459" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.84074e+01" utime="4.73509e+01" stime="7.37525e+00" mtime="3.23462e+01" gflop="0.00000e+00" gbyte="3.77918e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23462e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009c1486569c1497145a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83394e+01" utime="4.73245e+01" stime="7.36069e+00" mtime="3.23462e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23462e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4811e+08" > 6.2182e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4855e+08" > 3.8228e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5268e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4966e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.3658e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0031e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5183e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0072e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6703e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6659e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9004e+01 </func>
</region>
</regions>
<internal rank="338" log_i="1723713849.513103" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="339" mpi_size="768" stamp_init="1723713791.107546" stamp_final="1723713849.510226" username="apac4" allocationname="unknown" flags="0" pid="822460" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.84027e+01" utime="4.94158e+01" stime="6.48664e+00" mtime="3.25283e+01" gflop="0.00000e+00" gbyte="3.75343e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25283e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000eb14ea1455" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83359e+01" utime="4.93831e+01" stime="6.47925e+00" mtime="3.25283e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25283e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4779e+08" > 4.4270e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4846e+08" > 2.7914e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2070e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4883e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5272e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0031e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5181e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0087e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0398e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6050e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9704e+01 </func>
</region>
</regions>
<internal rank="339" log_i="1723713849.510226" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="340" mpi_size="768" stamp_init="1723713791.111634" stamp_final="1723713849.501462" username="apac4" allocationname="unknown" flags="0" pid="822461" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83898e+01" utime="4.66782e+01" stime="7.59310e+00" mtime="3.17174e+01" gflop="0.00000e+00" gbyte="3.76175e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17174e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83085e+01" utime="4.66509e+01" stime="7.57955e+00" mtime="3.17174e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17174e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4999e+08" > 6.6784e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4934e+08" > 3.7588e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1505e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5064e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.4271e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0016e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5168e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0215e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1400e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6635e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8688e+01 </func>
</region>
</regions>
<internal rank="340" log_i="1723713849.501462" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="341" mpi_size="768" stamp_init="1723713791.114585" stamp_final="1723713849.505850" username="apac4" allocationname="unknown" flags="0" pid="822462" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83913e+01" utime="5.04076e+01" stime="6.68659e+00" mtime="3.22587e+01" gflop="0.00000e+00" gbyte="3.78098e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22587e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003a141c553a143a145d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83100e+01" utime="5.03748e+01" stime="6.67674e+00" mtime="3.22587e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22587e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5086e+08" > 4.3936e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4968e+08" > 2.9260e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9996e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3950e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6717e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5162e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0286e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8181e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6661e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9643e+01 </func>
</region>
</regions>
<internal rank="341" log_i="1723713849.505850" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="342" mpi_size="768" stamp_init="1723713791.121629" stamp_final="1723713849.510664" username="apac4" allocationname="unknown" flags="0" pid="822463" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83890e+01" utime="4.76188e+01" stime="7.24396e+00" mtime="3.18953e+01" gflop="0.00000e+00" gbyte="3.75355e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18953e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000221522151d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82966e+01" utime="4.75905e+01" stime="7.23007e+00" mtime="3.18953e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18953e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4953e+08" > 5.6973e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4870e+08" > 3.4379e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5541e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4854e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.5061e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0015e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5172e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0181e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5606e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6618e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8594e+01 </func>
</region>
</regions>
<internal rank="342" log_i="1723713849.510664" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="343" mpi_size="768" stamp_init="1723713791.118774" stamp_final="1723713849.496852" username="apac4" allocationname="unknown" flags="0" pid="822464" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83781e+01" utime="4.96965e+01" stime="6.39144e+00" mtime="3.22368e+01" gflop="0.00000e+00" gbyte="3.76263e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22368e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000dd15dd151d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82796e+01" utime="4.96672e+01" stime="6.37791e+00" mtime="3.22368e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22368e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4924e+08" > 4.4994e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4908e+08" > 2.8715e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8801e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5002e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3365e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0041e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5158e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0319e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9588e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6652e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9718e+01 </func>
</region>
</regions>
<internal rank="343" log_i="1723713849.496852" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="344" mpi_size="768" stamp_init="1723713791.121104" stamp_final="1723713849.506324" username="apac4" allocationname="unknown" flags="0" pid="822465" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83852e+01" utime="4.77891e+01" stime="6.92835e+00" mtime="3.19864e+01" gflop="0.00000e+00" gbyte="3.78658e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19864e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003b143b14e4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82894e+01" utime="4.77554e+01" stime="6.91930e+00" mtime="3.19864e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19864e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4940e+08" > 4.6030e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4679e+08" > 3.9574e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6191e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4866e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.8738e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0049e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5158e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0255e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0899e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6601e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8714e+01 </func>
</region>
</regions>
<internal rank="344" log_i="1723713849.506324" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="345" mpi_size="768" stamp_init="1723713791.123452" stamp_final="1723713849.504004" username="apac4" allocationname="unknown" flags="0" pid="822466" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83806e+01" utime="5.04719e+01" stime="6.52626e+00" mtime="3.24623e+01" gflop="0.00000e+00" gbyte="3.75164e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24623e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001b142f551b141a14c4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82856e+01" utime="5.04402e+01" stime="6.51581e+00" mtime="3.24623e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24623e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4743e+08" > 3.6450e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4802e+08" > 2.6578e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6282e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4101e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6976e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5160e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0301e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1090e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6663e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9280e+01 </func>
</region>
</regions>
<internal rank="345" log_i="1723713849.504004" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="346" mpi_size="768" stamp_init="1723713791.130822" stamp_final="1723713849.500374" username="apac4" allocationname="unknown" flags="0" pid="822467" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83696e+01" utime="4.65042e+01" stime="7.36950e+00" mtime="3.17827e+01" gflop="0.00000e+00" gbyte="3.75641e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17827e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000df14de14df" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82733e+01" utime="4.64709e+01" stime="7.35999e+00" mtime="3.17827e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17827e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127164" bytes="8.4825e+08" > 6.2832e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4829e+08" > 5.9889e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5519e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4931e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.3222e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0059e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5148e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0389e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9016e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6655e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8388e+01 </func>
</region>
</regions>
<internal rank="346" log_i="1723713849.500374" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="347" mpi_size="768" stamp_init="1723713791.127382" stamp_final="1723713849.508769" username="apac4" allocationname="unknown" flags="0" pid="822468" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83814e+01" utime="5.07409e+01" stime="6.34963e+00" mtime="3.26124e+01" gflop="0.00000e+00" gbyte="3.76694e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26124e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001f14e7551f141f1478" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82834e+01" utime="5.07101e+01" stime="6.33767e+00" mtime="3.26124e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26124e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4906e+08" > 3.6591e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4818e+08" > 2.6713e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6147e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3986e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0080e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5157e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0332e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1495e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6615e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9450e+01 </func>
</region>
</regions>
<internal rank="347" log_i="1723713849.508769" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="348" mpi_size="768" stamp_init="1723713791.130817" stamp_final="1723713849.503134" username="apac4" allocationname="unknown" flags="0" pid="822469" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83723e+01" utime="4.80491e+01" stime="6.93267e+00" mtime="3.25266e+01" gflop="0.00000e+00" gbyte="3.76450e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25266e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4721474147514d7557514741474" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82767e+01" utime="4.80125e+01" stime="6.92764e+00" mtime="3.25266e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25266e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4945e+08" > 4.8925e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5025e+08" > 4.3093e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8505e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5087e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.3332e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0048e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5151e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0361e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8801e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6640e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8969e+01 </func>
</region>
</regions>
<internal rank="348" log_i="1723713849.503134" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="349" mpi_size="768" stamp_init="1723713791.133238" stamp_final="1723713849.505276" username="apac4" allocationname="unknown" flags="0" pid="822470" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83720e+01" utime="5.06934e+01" stime="6.26148e+00" mtime="3.27016e+01" gflop="0.00000e+00" gbyte="3.77029e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27016e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf41c151e151f151b551f151e1534" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82778e+01" utime="5.06619e+01" stime="6.25106e+00" mtime="3.27016e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27016e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4890e+08" > 3.5976e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4872e+08" > 2.5464e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3319e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4228e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7294e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5151e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0389e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3593e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6605e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9808e+01 </func>
</region>
</regions>
<internal rank="349" log_i="1723713849.505276" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="350" mpi_size="768" stamp_init="1723713791.135643" stamp_final="1723713849.508095" username="apac4" allocationname="unknown" flags="0" pid="822471" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83725e+01" utime="4.77279e+01" stime="7.04403e+00" mtime="3.22881e+01" gflop="0.00000e+00" gbyte="3.76534e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22881e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000401440147a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82780e+01" utime="4.76984e+01" stime="7.03235e+00" mtime="3.22881e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22881e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4825e+08" > 5.1420e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5005e+08" > 3.9254e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6108e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5122e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.8917e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0049e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5151e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0398e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2902e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6636e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8951e+01 </func>
</region>
</regions>
<internal rank="350" log_i="1723713849.508095" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="351" mpi_size="768" stamp_init="1723713791.137583" stamp_final="1723713849.508720" username="apac4" allocationname="unknown" flags="0" pid="822472" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83711e+01" utime="5.02541e+01" stime="6.83814e+00" mtime="3.25313e+01" gflop="0.00000e+00" gbyte="3.76522e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25313e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf469146b146c147f566c146c14c2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82779e+01" utime="5.02242e+01" stime="6.82660e+00" mtime="3.25313e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25313e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4908e+08" > 3.4512e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4898e+08" > 3.1240e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3728e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4091e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0090e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5136e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0511e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4022e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6621e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9617e+01 </func>
</region>
</regions>
<internal rank="351" log_i="1723713849.508720" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="352" mpi_size="768" stamp_init="1723713791.146856" stamp_final="1723713849.498228" username="apac4" allocationname="unknown" flags="0" pid="822473" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83514e+01" utime="4.70920e+01" stime="8.03449e+00" mtime="3.19430e+01" gflop="0.00000e+00" gbyte="3.75832e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19430e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000df14de14f7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82655e+01" utime="4.70614e+01" stime="8.02412e+00" mtime="3.19430e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19430e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5009e+08" > 7.6461e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5000e+08" > 5.5479e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4642e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4252e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.8726e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.5853e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5139e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0500e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5988e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6657e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8483e+01 </func>
</region>
</regions>
<internal rank="352" log_i="1723713849.498228" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="353" mpi_size="768" stamp_init="1723713791.143860" stamp_final="1723713849.510468" username="apac4" allocationname="unknown" flags="0" pid="822474" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83666e+01" utime="5.00374e+01" stime="6.98043e+00" mtime="3.26476e+01" gflop="0.00000e+00" gbyte="3.77872e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26476e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c314c514c6144f56c614c514c4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82778e+01" utime="5.00034e+01" stime="6.97303e+00" mtime="3.26476e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26476e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4845e+08" > 4.8544e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4856e+08" > 3.0176e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9619e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4247e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5259e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6940e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5123e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0691e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0684e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6593e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8992e+01 </func>
</region>
</regions>
<internal rank="353" log_i="1723713849.510468" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="354" mpi_size="768" stamp_init="1723713791.146861" stamp_final="1723713849.510138" username="apac4" allocationname="unknown" flags="0" pid="822475" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83633e+01" utime="4.87302e+01" stime="7.29751e+00" mtime="3.21405e+01" gflop="0.00000e+00" gbyte="3.76434e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21405e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82768e+01" utime="4.86962e+01" stime="7.29128e+00" mtime="3.21405e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21405e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4921e+08" > 6.0231e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4928e+08" > 4.2739e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6150e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4105e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.4012e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7249e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5133e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0582e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7490e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6629e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8716e+01 </func>
</region>
</regions>
<internal rank="354" log_i="1723713849.510138" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="355" mpi_size="768" stamp_init="1723713791.148748" stamp_final="1723713849.508737" username="apac4" allocationname="unknown" flags="0" pid="822476" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83600e+01" utime="5.04134e+01" stime="6.71234e+00" mtime="3.24082e+01" gflop="0.00000e+00" gbyte="3.76968e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24082e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82741e+01" utime="5.03813e+01" stime="6.70397e+00" mtime="3.24082e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24082e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4936e+08" > 4.7488e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4927e+08" > 3.2102e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2693e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4231e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0266e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7170e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5134e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0577e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.4915e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6616e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9456e+01 </func>
</region>
</regions>
<internal rank="355" log_i="1723713849.508737" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="356" mpi_size="768" stamp_init="1723713791.151560" stamp_final="1723713849.497169" username="apac4" allocationname="unknown" flags="0" pid="822477" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83456e+01" utime="4.75110e+01" stime="7.33204e+00" mtime="3.17949e+01" gflop="0.00000e+00" gbyte="3.76434e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17949e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4cc15ce15cf155356cf15cf1530" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82619e+01" utime="4.74863e+01" stime="7.31669e+00" mtime="3.17949e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17949e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4899e+08" > 6.3645e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4970e+08" > 4.4253e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0387e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5103e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.1049e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0022e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5125e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0648e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6393e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6660e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8907e+01 </func>
</region>
</regions>
<internal rank="356" log_i="1723713849.497169" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="357" mpi_size="768" stamp_init="1723713791.156068" stamp_final="1723713849.508857" username="apac4" allocationname="unknown" flags="0" pid="822478" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83528e+01" utime="4.94176e+01" stime="6.64469e+00" mtime="3.23341e+01" gflop="0.00000e+00" gbyte="3.77026e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23341e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82715e+01" utime="4.93879e+01" stime="6.63435e+00" mtime="3.23341e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23341e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4990e+08" > 4.9711e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4937e+08" > 3.5244e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1904e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5086e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0032e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5116e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0727e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3712e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6585e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9444e+01 </func>
</region>
</regions>
<internal rank="357" log_i="1723713849.508857" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="358" mpi_size="768" stamp_init="1723713791.159888" stamp_final="1723713849.515848" username="apac4" allocationname="unknown" flags="0" pid="822479" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83560e+01" utime="4.69754e+01" stime="7.50267e+00" mtime="3.22980e+01" gflop="0.00000e+00" gbyte="3.76503e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22980e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82798e+01" utime="4.69403e+01" stime="7.49819e+00" mtime="3.22980e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22980e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127164" bytes="8.4974e+08" > 6.9902e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4885e+08" > 5.4645e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6500e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4937e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.9299e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0046e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5124e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0667e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6011e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6623e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8736e+01 </func>
</region>
</regions>
<internal rank="358" log_i="1723713849.515848" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="359" mpi_size="768" stamp_init="1723713791.159988" stamp_final="1723713849.515667" username="apac4" allocationname="unknown" flags="0" pid="822480" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="5.83557e+01" utime="4.96167e+01" stime="6.49685e+00" mtime="3.23665e+01" gflop="0.00000e+00" gbyte="3.78098e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23665e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf47a147c147d14c6567d147d14b3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82777e+01" utime="4.95852e+01" stime="6.48850e+00" mtime="3.23665e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23665e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4887e+08" > 4.9210e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4973e+08" > 3.3892e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3001e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5113e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5272e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0045e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5121e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0703e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8300e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6617e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9368e+01 </func>
</region>
</regions>
<internal rank="359" log_i="1723713849.515667" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="360" mpi_size="768" stamp_init="1723713791.120266" stamp_final="1723713849.508406" username="apac4" allocationname="unknown" flags="0" pid="694541" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83881e+01" utime="4.27789e+01" stime="1.26036e+01" mtime="3.17429e+01" gflop="0.00000e+00" gbyte="3.85273e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17429e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83381e+01" utime="4.27486e+01" stime="1.25947e+01" mtime="3.17429e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17429e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4931e+08" > 5.9877e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4778e+08" > 4.1903e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9147e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3992e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3031e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.5858e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5118e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0752e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6107e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6730e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8093e+01 </func>
</region>
</regions>
<internal rank="360" log_i="1723713849.508406" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="361" mpi_size="768" stamp_init="1723713791.120587" stamp_final="1723713849.503942" username="apac4" allocationname="unknown" flags="0" pid="694542" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83834e+01" utime="5.04911e+01" stime="6.51269e+00" mtime="3.19725e+01" gflop="0.00000e+00" gbyte="3.77918e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19725e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83290e+01" utime="5.04578e+01" stime="6.50545e+00" mtime="3.19725e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19725e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4965e+08" > 4.7719e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4880e+08" > 3.3069e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8204e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4142e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5749e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1350e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5120e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0713e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5010e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6780e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9530e+01 </func>
</region>
</regions>
<internal rank="361" log_i="1723713849.503942" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="362" mpi_size="768" stamp_init="1723713791.120418" stamp_final="1723713849.508438" username="apac4" allocationname="unknown" flags="0" pid="694543" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83880e+01" utime="4.83539e+01" stime="7.31581e+00" mtime="3.16179e+01" gflop="0.00000e+00" gbyte="3.77926e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16179e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007e147d1478" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83367e+01" utime="4.83285e+01" stime="7.30153e+00" mtime="3.16179e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16179e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4784e+08" > 5.9238e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5085e+08" > 4.0530e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8953e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4159e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4558e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.7971e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5119e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0722e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8706e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6729e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8978e+01 </func>
</region>
</regions>
<internal rank="362" log_i="1723713849.508438" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="363" mpi_size="768" stamp_init="1723713791.122752" stamp_final="1723713849.507838" username="apac4" allocationname="unknown" flags="0" pid="694544" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83851e+01" utime="5.03674e+01" stime="6.70555e+00" mtime="3.23849e+01" gflop="0.00000e+00" gbyte="3.77647e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23849e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83312e+01" utime="5.03325e+01" stime="6.70151e+00" mtime="3.23849e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23849e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.5067e+08" > 4.4401e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4717e+08" > 3.1946e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2927e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4076e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8000e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5118e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0736e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7895e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6750e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9514e+01 </func>
</region>
</regions>
<internal rank="363" log_i="1723713849.507838" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="364" mpi_size="768" stamp_init="1723713791.147819" stamp_final="1723713849.513859" username="apac4" allocationname="unknown" flags="0" pid="694545" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83660e+01" utime="4.85857e+01" stime="6.99652e+00" mtime="3.18012e+01" gflop="0.00000e+00" gbyte="3.77731e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18012e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf498149a149b14ab559b149b1460" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82739e+01" utime="4.85514e+01" stime="6.98934e+00" mtime="3.18012e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18012e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4953e+08" > 5.8069e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4829e+08" > 3.8479e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.5151e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4088e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.8228e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.5895e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5112e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0757e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6989e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6194e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9587e+01 </func>
</region>
</regions>
<internal rank="364" log_i="1723713849.513859" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="365" mpi_size="768" stamp_init="1723713791.151781" stamp_final="1723713849.511155" username="apac4" allocationname="unknown" flags="0" pid="694546" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83594e+01" utime="5.04834e+01" stime="6.60190e+00" mtime="3.24981e+01" gflop="0.00000e+00" gbyte="3.78098e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24981e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000015149b551514151487" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82694e+01" utime="5.04520e+01" stime="6.59219e+00" mtime="3.24981e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24981e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127164" bytes="8.4735e+08" > 4.5111e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5112e+08" > 2.9517e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0643e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4069e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6271e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5113e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0787e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7299e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6759e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9864e+01 </func>
</region>
</regions>
<internal rank="365" log_i="1723713849.511155" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="366" mpi_size="768" stamp_init="1723713791.134819" stamp_final="1723713849.502394" username="apac4" allocationname="unknown" flags="0" pid="694547" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83676e+01" utime="4.75501e+01" stime="7.62784e+00" mtime="3.17012e+01" gflop="0.00000e+00" gbyte="3.74619e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17012e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a214a314a4141755a414a41479" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82843e+01" utime="4.75177e+01" stime="7.61862e+00" mtime="3.17012e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17012e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127164" bytes="8.5163e+08" > 6.9867e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4845e+08" > 6.7984e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1905e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4264e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.0572e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.2977e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5105e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0859e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0994e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6737e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8628e+01 </func>
</region>
</regions>
<internal rank="366" log_i="1723713849.502394" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="367" mpi_size="768" stamp_init="1723713791.134624" stamp_final="1723713849.510464" username="apac4" allocationname="unknown" flags="0" pid="694548" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83758e+01" utime="5.02298e+01" stime="6.80224e+00" mtime="3.24801e+01" gflop="0.00000e+00" gbyte="3.77651e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24801e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000eb14eb14ad" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82928e+01" utime="5.01995e+01" stime="6.79008e+00" mtime="3.24801e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24801e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4856e+08" > 4.5721e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4918e+08" > 3.5146e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4214e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4198e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.7366e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5107e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0851e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0494e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6725e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9453e+01 </func>
</region>
</regions>
<internal rank="367" log_i="1723713849.510464" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="368" mpi_size="768" stamp_init="1723713791.136249" stamp_final="1723713849.512286" username="apac4" allocationname="unknown" flags="0" pid="694549" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83760e+01" utime="4.78566e+01" stime="7.48011e+00" mtime="3.21812e+01" gflop="0.00000e+00" gbyte="3.78105e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21812e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e914eb14ec14e155ec14eb14f2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82937e+01" utime="4.78227e+01" stime="7.47267e+00" mtime="3.21812e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21812e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4856e+08" > 5.2057e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4861e+08" > 4.0564e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2426e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4159e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.6228e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.2250e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5091e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1018e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0303e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6782e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8263e+01 </func>
</region>
</regions>
<internal rank="368" log_i="1723713849.512286" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="369" mpi_size="768" stamp_init="1723713791.139404" stamp_final="1723713849.506947" username="apac4" allocationname="unknown" flags="0" pid="694550" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83675e+01" utime="5.04529e+01" stime="6.60084e+00" mtime="3.24308e+01" gflop="0.00000e+00" gbyte="3.76720e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24308e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000aa153f55aa15aa152c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82822e+01" utime="5.04251e+01" stime="6.58694e+00" mtime="3.24308e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24308e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4809e+08" > 3.5155e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4889e+08" > 2.5746e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4184e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3978e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6451e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9921e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5084e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1084e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6512e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6702e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9541e+01 </func>
</region>
</regions>
<internal rank="369" log_i="1723713849.506947" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="370" mpi_size="768" stamp_init="1723713791.144933" stamp_final="1723713849.510936" username="apac4" allocationname="unknown" flags="0" pid="694551" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83660e+01" utime="4.71835e+01" stime="7.87278e+00" mtime="3.22177e+01" gflop="0.00000e+00" gbyte="3.75736e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22177e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ee14ee14dc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82742e+01" utime="4.71546e+01" stime="7.85905e+00" mtime="3.22177e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22177e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.3828e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4954e+08" > 5.8643e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4988e+08" > 4.9578e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9877e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4093e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.4625e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5250e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5083e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1036e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0994e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6741e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8494e+01 </func>
</region>
</regions>
<internal rank="370" log_i="1723713849.510936" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="371" mpi_size="768" stamp_init="1723713791.149021" stamp_final="1723713849.500880" username="apac4" allocationname="unknown" flags="0" pid="694552" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83519e+01" utime="5.07651e+01" stime="6.32460e+00" mtime="3.25488e+01" gflop="0.00000e+00" gbyte="3.78002e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25488e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b7159756b715b71538" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82604e+01" utime="5.07346e+01" stime="6.31425e+00" mtime="3.25488e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25488e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4944e+08" > 3.6233e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4963e+08" > 2.8613e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3891e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4229e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.7127e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5086e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1041e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1090e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6710e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9653e+01 </func>
</region>
</regions>
<internal rank="371" log_i="1723713849.500880" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="372" mpi_size="768" stamp_init="1723713791.147453" stamp_final="1723713849.507310" username="apac4" allocationname="unknown" flags="0" pid="694553" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83599e+01" utime="4.86221e+01" stime="7.06401e+00" mtime="3.20955e+01" gflop="0.00000e+00" gbyte="3.77293e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20955e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf45515561557155b555715571517" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82707e+01" utime="4.85889e+01" stime="7.05611e+00" mtime="3.20955e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20955e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4911e+08" > 5.1060e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4848e+08" > 3.8579e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3896e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4316e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.3937e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0404e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5077e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1128e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6798e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6774e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9029e+01 </func>
</region>
</regions>
<internal rank="372" log_i="1723713849.507310" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="373" mpi_size="768" stamp_init="1723713791.149305" stamp_final="1723713849.510683" username="apac4" allocationname="unknown" flags="0" pid="694554" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83614e+01" utime="5.04452e+01" stime="6.58804e+00" mtime="3.26615e+01" gflop="0.00000e+00" gbyte="3.77571e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26615e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4bd14be14bf147f55bf14bf14eb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82701e+01" utime="5.04162e+01" stime="6.57435e+00" mtime="3.26615e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26615e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4901e+08" > 3.9145e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4944e+08" > 2.9259e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4422e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4231e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.2237e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5083e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1093e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8801e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6725e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9677e+01 </func>
</region>
</regions>
<internal rank="373" log_i="1723713849.510683" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="374" mpi_size="768" stamp_init="1723713791.153438" stamp_final="1723713849.512123" username="apac4" allocationname="unknown" flags="0" pid="694555" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83587e+01" utime="4.80147e+01" stime="7.63794e+00" mtime="3.20367e+01" gflop="0.00000e+00" gbyte="3.77480e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20367e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4751476147814bf5678147714df" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82699e+01" utime="4.79794e+01" stime="7.63245e+00" mtime="3.20367e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20367e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4925e+08" > 4.9859e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4889e+08" > 4.1668e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9547e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4284e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8934e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1398e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5070e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1219e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2711e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6775e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8415e+01 </func>
</region>
</regions>
<internal rank="374" log_i="1723713849.512123" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="375" mpi_size="768" stamp_init="1723713791.155723" stamp_final="1723713849.500978" username="apac4" allocationname="unknown" flags="0" pid="694556" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83453e+01" utime="5.02285e+01" stime="6.80503e+00" mtime="3.25883e+01" gflop="0.00000e+00" gbyte="3.74619e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25883e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002e1497562e142e14ba" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82576e+01" utime="5.01981e+01" stime="6.79495e+00" mtime="3.25883e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25883e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4880e+08" > 3.5498e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4908e+08" > 2.8421e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8244e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4081e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1412e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5074e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1194e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6393e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6719e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9275e+01 </func>
</region>
</regions>
<internal rank="375" log_i="1723713849.500978" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="376" mpi_size="768" stamp_init="1723713791.158914" stamp_final="1723713849.504186" username="apac4" allocationname="unknown" flags="0" pid="694557" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83453e+01" utime="4.70497e+01" stime="8.09021e+00" mtime="3.20714e+01" gflop="0.00000e+00" gbyte="3.76888e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20714e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000cc147955cc14cb1455" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82596e+01" utime="4.70174e+01" stime="8.08193e+00" mtime="3.20714e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20714e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4696e+08" > 7.3627e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4523e+08" > 4.9815e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1995e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4102e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.1839e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.2222e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5065e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1257e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5415e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6777e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7981e+01 </func>
</region>
</regions>
<internal rank="376" log_i="1723713849.504186" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="377" mpi_size="768" stamp_init="1723713791.161508" stamp_final="1723713849.515807" username="apac4" allocationname="unknown" flags="0" pid="694558" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83543e+01" utime="5.02542e+01" stime="6.81860e+00" mtime="3.21847e+01" gflop="0.00000e+00" gbyte="3.77209e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21847e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4711573157415dd55741574151c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82647e+01" utime="5.02246e+01" stime="6.80881e+00" mtime="3.21847e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21847e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4780e+08" > 4.6912e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4808e+08" > 3.0765e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8149e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4162e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9222e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5067e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1263e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4189e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6722e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8758e+01 </func>
</region>
</regions>
<internal rank="377" log_i="1723713849.515807" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="378" mpi_size="768" stamp_init="1723713791.162854" stamp_final="1723713849.508774" username="apac4" allocationname="unknown" flags="0" pid="694559" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83459e+01" utime="4.78220e+01" stime="7.73883e+00" mtime="3.17890e+01" gflop="0.00000e+00" gbyte="3.77434e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17890e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4fb15fc15fe158a56fe15fd1505" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82613e+01" utime="4.77860e+01" stime="7.73467e+00" mtime="3.17890e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17890e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4847e+08" > 6.2225e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4929e+08" > 4.4825e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0793e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4213e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7759e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1680e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5068e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1232e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5797e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6750e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7921e+01 </func>
</region>
</regions>
<internal rank="378" log_i="1723713849.508774" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="379" mpi_size="768" stamp_init="1723713791.165163" stamp_final="1723713849.506733" username="apac4" allocationname="unknown" flags="0" pid="694560" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83416e+01" utime="5.01444e+01" stime="6.90503e+00" mtime="3.24225e+01" gflop="0.00000e+00" gbyte="3.75420e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24225e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82585e+01" utime="5.01083e+01" stime="6.90066e+00" mtime="3.24225e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24225e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4922e+08" > 4.8108e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4951e+08" > 3.1197e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7793e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4137e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9087e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1686e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5057e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1321e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9802e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6175e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9020e+01 </func>
</region>
</regions>
<internal rank="379" log_i="1723713849.506733" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="380" mpi_size="768" stamp_init="1723713791.167796" stamp_final="1723713849.502398" username="apac4" allocationname="unknown" flags="0" pid="694561" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83346e+01" utime="4.83622e+01" stime="7.47683e+00" mtime="3.20923e+01" gflop="0.00000e+00" gbyte="3.77075e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20923e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b514b414fb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82539e+01" utime="4.83332e+01" stime="7.46599e+00" mtime="3.20923e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20923e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4965e+08" > 6.0158e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4777e+08" > 3.9601e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8110e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4101e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.4741e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.2292e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5061e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1305e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8610e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6776e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8533e+01 </func>
</region>
</regions>
<internal rank="380" log_i="1723713849.502398" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="381" mpi_size="768" stamp_init="1723713791.169287" stamp_final="1723713849.512532" username="apac4" allocationname="unknown" flags="0" pid="694562" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83432e+01" utime="5.04965e+01" stime="6.52391e+00" mtime="3.23484e+01" gflop="0.00000e+00" gbyte="3.77407e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23484e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c715aa55c715c71519" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82640e+01" utime="5.04636e+01" stime="6.51715e+00" mtime="3.23484e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23484e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4913e+08" > 4.7641e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4937e+08" > 2.9429e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3528e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3732e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8916e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5054e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1393e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0494e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6716e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9421e+01 </func>
</region>
</regions>
<internal rank="381" log_i="1723713849.512532" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="382" mpi_size="768" stamp_init="1723713791.175911" stamp_final="1723713849.514936" username="apac4" allocationname="unknown" flags="0" pid="694563" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83390e+01" utime="4.84670e+01" stime="7.32576e+00" mtime="3.18900e+01" gflop="0.00000e+00" gbyte="3.77235e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18900e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82632e+01" utime="4.84351e+01" stime="7.31797e+00" mtime="3.18900e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18900e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.0490e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4885e+08" > 6.0756e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4892e+08" > 3.9432e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5584e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4041e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.4094e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.2181e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5057e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1363e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6894e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6736e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8592e+01 </func>
</region>
</regions>
<internal rank="382" log_i="1723713849.514936" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="383" mpi_size="768" stamp_init="1723713791.173332" stamp_final="1723713849.507671" username="apac4" allocationname="unknown" flags="0" pid="694564" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="5.83343e+01" utime="5.03038e+01" stime="6.71656e+00" mtime="3.20312e+01" gflop="0.00000e+00" gbyte="3.77552e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20312e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c2159655c215bd1500" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82581e+01" utime="5.02721e+01" stime="6.70874e+00" mtime="3.20312e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20312e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4836e+08" > 4.8077e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4889e+08" > 2.5269e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3358e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4138e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.7458e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5044e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1489e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8896e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6710e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9082e+01 </func>
</region>
</regions>
<internal rank="383" log_i="1723713849.507671" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="384" mpi_size="768" stamp_init="1723713791.110600" stamp_final="1723713849.513512" username="apac4" allocationname="unknown" flags="0" pid="721537" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.84029e+01" utime="4.25219e+01" stime="1.29532e+01" mtime="3.22025e+01" gflop="0.00000e+00" gbyte="3.86105e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22025e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000064144f56641464148e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83309e+01" utime="4.24910e+01" stime="1.29445e+01" mtime="3.22025e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22025e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4776e+08" > 7.0776e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4811e+08" > 4.9880e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6535e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4658e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0818e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5032e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1584e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0684e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7146e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8658e+01 </func>
</region>
</regions>
<internal rank="384" log_i="1723713849.513512" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="385" mpi_size="768" stamp_init="1723713791.110586" stamp_final="1723713849.508605" username="apac4" allocationname="unknown" flags="0" pid="721538" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83980e+01" utime="5.03016e+01" stime="6.74888e+00" mtime="3.22469e+01" gflop="0.00000e+00" gbyte="3.78475e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22469e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ba148b55ba14b91498" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83266e+01" utime="5.02681e+01" stime="6.74219e+00" mtime="3.22469e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22469e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4957e+08" > 4.6469e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4856e+08" > 3.5604e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1586e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4293e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.0990e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5055e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1378e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2687e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7231e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9451e+01 </func>
</region>
</regions>
<internal rank="385" log_i="1723713849.508605" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="386" mpi_size="768" stamp_init="1723713791.113210" stamp_final="1723713849.504664" username="apac4" allocationname="unknown" flags="0" pid="721539" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83915e+01" utime="4.84019e+01" stime="7.61229e+00" mtime="3.20500e+01" gflop="0.00000e+00" gbyte="3.77102e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20500e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a5158a55a515a4150e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83171e+01" utime="4.83734e+01" stime="7.60178e+00" mtime="3.20500e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20500e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4802e+08" > 5.5213e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4757e+08" > 4.6025e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5421e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4083e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.2943e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.5804e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5041e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1523e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9778e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7148e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8799e+01 </func>
</region>
</regions>
<internal rank="386" log_i="1723713849.504664" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="387" mpi_size="768" stamp_init="1723713791.110585" stamp_final="1723713849.500536" username="apac4" allocationname="unknown" flags="0" pid="721540" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83900e+01" utime="5.02496e+01" stime="6.72126e+00" mtime="3.22769e+01" gflop="0.00000e+00" gbyte="3.76625e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22769e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f614f61479" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83191e+01" utime="5.02215e+01" stime="6.70964e+00" mtime="3.22769e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22769e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4863e+08" > 4.5763e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4856e+08" > 3.4805e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9619e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4151e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.2407e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5029e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1616e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2282e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7204e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9698e+01 </func>
</region>
</regions>
<internal rank="387" log_i="1723713849.500536" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="388" mpi_size="768" stamp_init="1723713791.110595" stamp_final="1723713849.508407" username="apac4" allocationname="unknown" flags="0" pid="721541" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83978e+01" utime="4.70046e+01" stime="8.13512e+00" mtime="3.22014e+01" gflop="0.00000e+00" gbyte="3.76411e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22014e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006d15d9556d156d1511" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83293e+01" utime="4.69804e+01" stime="8.12014e+00" mtime="3.22014e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22014e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4939e+08" > 7.8829e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4832e+08" > 5.3709e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6374e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4227e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5233e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.0306e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5044e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1479e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9016e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7164e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8589e+01 </func>
</region>
</regions>
<internal rank="388" log_i="1723713849.508407" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="389" mpi_size="768" stamp_init="1723713791.132966" stamp_final="1723713849.513978" username="apac4" allocationname="unknown" flags="0" pid="721542" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83810e+01" utime="5.04757e+01" stime="6.60154e+00" mtime="3.25355e+01" gflop="0.00000e+00" gbyte="3.77449e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25355e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4401442144314e65543144314ab" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83026e+01" utime="5.04479e+01" stime="6.58952e+00" mtime="3.25355e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25355e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4811e+08" > 4.4988e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4994e+08" > 2.9464e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0881e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4194e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4772e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5039e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1544e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5501e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7242e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9837e+01 </func>
</region>
</regions>
<internal rank="389" log_i="1723713849.513978" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="390" mpi_size="768" stamp_init="1723713791.136296" stamp_final="1723713849.509369" username="apac4" allocationname="unknown" flags="0" pid="721543" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83731e+01" utime="4.87936e+01" stime="7.35876e+00" mtime="3.21625e+01" gflop="0.00000e+00" gbyte="3.78063e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21625e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d814d81470" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82943e+01" utime="4.87616e+01" stime="7.35124e+00" mtime="3.21625e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21625e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4981e+08" > 5.6934e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4843e+08" > 4.8181e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6599e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4216e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.3883e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.0273e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5042e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1517e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1710e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7171e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8756e+01 </func>
</region>
</regions>
<internal rank="390" log_i="1723713849.509369" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="391" mpi_size="768" stamp_init="1723713791.110604" stamp_final="1723713849.504593" username="apac4" allocationname="unknown" flags="0" pid="721544" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83940e+01" utime="5.01552e+01" stime="6.81246e+00" mtime="3.24660e+01" gflop="0.00000e+00" gbyte="3.77846e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24660e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d515d715d815a856d815d7154c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83242e+01" utime="5.01204e+01" stime="6.80758e+00" mtime="3.24660e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24660e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4947e+08" > 4.4845e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4942e+08" > 3.2178e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2571e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3973e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4086e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5039e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1537e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3092e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7232e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9620e+01 </func>
</region>
</regions>
<internal rank="391" log_i="1723713849.504593" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="392" mpi_size="768" stamp_init="1723713791.112970" stamp_final="1723713849.505112" username="apac4" allocationname="unknown" flags="0" pid="721545" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83921e+01" utime="4.70669e+01" stime="8.07634e+00" mtime="3.20426e+01" gflop="0.00000e+00" gbyte="3.78048e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20426e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83217e+01" utime="4.70334e+01" stime="8.07029e+00" mtime="3.20426e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20426e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4784e+08" > 5.8579e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4761e+08" > 4.1745e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8546e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4203e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.5430e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.9429e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5036e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1560e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0184e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7212e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8428e+01 </func>
</region>
</regions>
<internal rank="392" log_i="1723713849.505112" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="393" mpi_size="768" stamp_init="1723713791.115788" stamp_final="1723713849.504020" username="apac4" allocationname="unknown" flags="0" pid="721546" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83882e+01" utime="5.04646e+01" stime="6.65672e+00" mtime="3.26677e+01" gflop="0.00000e+00" gbyte="3.75126e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26677e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf470147214731435557314721457" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83124e+01" utime="5.04327e+01" stime="6.64876e+00" mtime="3.26677e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26677e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4765e+08" > 3.6222e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4787e+08" > 2.7335e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7438e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4237e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8233e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5032e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1625e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9397e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7177e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9396e+01 </func>
</region>
</regions>
<internal rank="393" log_i="1723713849.504020" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="394" mpi_size="768" stamp_init="1723713791.118644" stamp_final="1723713849.504880" username="apac4" allocationname="unknown" flags="0" pid="721547" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83862e+01" utime="4.85219e+01" stime="7.43805e+00" mtime="3.28389e+01" gflop="0.00000e+00" gbyte="3.77934e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28389e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000076141f5576147614c6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83100e+01" utime="4.84912e+01" stime="7.42892e+00" mtime="3.28389e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28389e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4748e+08" > 4.4320e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4767e+08" > 3.5712e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.4516e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4077e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.5061e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4846e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5027e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1658e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1114e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7193e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8789e+01 </func>
</region>
</regions>
<internal rank="394" log_i="1723713849.504880" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="395" mpi_size="768" stamp_init="1723713791.119571" stamp_final="1723713849.511398" username="apac4" allocationname="unknown" flags="0" pid="721548" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83918e+01" utime="5.04681e+01" stime="6.58503e+00" mtime="3.25955e+01" gflop="0.00000e+00" gbyte="3.76343e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25955e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf47c147e147f149d557f147f148b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83138e+01" utime="5.04363e+01" stime="6.57738e+00" mtime="3.25955e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25955e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4914e+08" > 3.5718e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4961e+08" > 2.5089e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4171e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4155e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6689e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6596e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5028e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1659e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4595e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7175e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9667e+01 </func>
</region>
</regions>
<internal rank="395" log_i="1723713849.511398" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="396" mpi_size="768" stamp_init="1723713791.147221" stamp_final="1723713849.509207" username="apac4" allocationname="unknown" flags="0" pid="721549" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83620e+01" utime="4.76726e+01" stime="7.92705e+00" mtime="3.26351e+01" gflop="0.00000e+00" gbyte="3.75664e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26351e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ea15ea150c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82912e+01" utime="4.76427e+01" stime="7.91822e+00" mtime="3.26351e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26351e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4929e+08" > 5.8740e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4894e+08" > 4.0264e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1462e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4158e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1373e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.9516e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5023e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1698e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0899e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7208e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8739e+01 </func>
</region>
</regions>
<internal rank="396" log_i="1723713849.509207" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="397" mpi_size="768" stamp_init="1723713791.149446" stamp_final="1723713849.513838" username="apac4" allocationname="unknown" flags="0" pid="721550" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83644e+01" utime="5.01497e+01" stime="6.94009e+00" mtime="3.28581e+01" gflop="0.00000e+00" gbyte="3.76511e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28581e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008914891476" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82959e+01" utime="5.01164e+01" stime="6.93529e+00" mtime="3.28581e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28581e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4884e+08" > 3.7930e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4886e+08" > 2.6418e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6982e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4172e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8610e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.2607e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5016e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1742e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5310e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7196e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9628e+01 </func>
</region>
</regions>
<internal rank="397" log_i="1723713849.513838" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="398" mpi_size="768" stamp_init="1723713791.127473" stamp_final="1723713849.517747" username="apac4" allocationname="unknown" flags="0" pid="721551" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83903e+01" utime="4.83123e+01" stime="7.77686e+00" mtime="3.25646e+01" gflop="0.00000e+00" gbyte="3.75660e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25646e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83085e+01" utime="4.82829e+01" stime="7.76583e+00" mtime="3.25646e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25646e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 4.7307e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4878e+08" > 3.4761e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0282e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4245e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.5249e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3916e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5014e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1798e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5501e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7211e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8889e+01 </func>
</region>
</regions>
<internal rank="398" log_i="1723713849.517747" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="399" mpi_size="768" stamp_init="1723713791.129836" stamp_final="1723713849.510567" username="apac4" allocationname="unknown" flags="0" pid="721552" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83807e+01" utime="5.00732e+01" stime="6.99297e+00" mtime="3.25166e+01" gflop="0.00000e+00" gbyte="3.76118e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25166e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000027142614b7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82993e+01" utime="5.00426e+01" stime="6.98231e+00" mtime="3.25166e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25166e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4876e+08" > 3.7812e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4932e+08" > 2.5587e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4571e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4192e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6689e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3910e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5002e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1913e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.7003e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7180e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9526e+01 </func>
</region>
</regions>
<internal rank="399" log_i="1723713849.510567" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="400" mpi_size="768" stamp_init="1723713791.133388" stamp_final="1723713849.504045" username="apac4" allocationname="unknown" flags="0" pid="721553" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83707e+01" utime="4.72696e+01" stime="7.93795e+00" mtime="3.19619e+01" gflop="0.00000e+00" gbyte="3.77319e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19619e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b1144c55b114b0147f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82937e+01" utime="4.72418e+01" stime="7.92587e+00" mtime="3.19619e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19619e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4957e+08" > 6.9088e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4908e+08" > 5.5105e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5373e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4075e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.2617e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.6668e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5008e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1822e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3808e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7234e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8560e+01 </func>
</region>
</regions>
<internal rank="400" log_i="1723713849.504045" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="401" mpi_size="768" stamp_init="1723713791.135380" stamp_final="1723713849.505479" username="apac4" allocationname="unknown" flags="0" pid="721554" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83701e+01" utime="5.03194e+01" stime="6.74603e+00" mtime="3.23642e+01" gflop="0.00000e+00" gbyte="3.76980e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23642e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001914925519141914d6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82948e+01" utime="5.02923e+01" stime="6.73353e+00" mtime="3.23642e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23642e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5003e+08" > 4.9682e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5027e+08" > 3.4775e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2109e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4066e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5308e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5007e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1839e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2496e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7200e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9503e+01 </func>
</region>
</regions>
<internal rank="401" log_i="1723713849.505479" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="402" mpi_size="768" stamp_init="1723713791.138254" stamp_final="1723713849.511271" username="apac4" allocationname="unknown" flags="0" pid="721555" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83730e+01" utime="4.84237e+01" stime="7.55964e+00" mtime="3.19905e+01" gflop="0.00000e+00" gbyte="3.77121e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19905e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ed14ec14e0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82974e+01" utime="4.83889e+01" stime="7.55516e+00" mtime="3.19905e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19905e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4886e+08" > 5.7947e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4931e+08" > 3.8058e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4436e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4136e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.6419e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1013e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5007e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1876e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9612e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7237e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8801e+01 </func>
</region>
</regions>
<internal rank="402" log_i="1723713849.511271" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="403" mpi_size="768" stamp_init="1723713791.140622" stamp_final="1723713849.509311" username="apac4" allocationname="unknown" flags="0" pid="721556" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83687e+01" utime="5.01507e+01" stime="6.79902e+00" mtime="3.24341e+01" gflop="0.00000e+00" gbyte="3.77083e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24341e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82944e+01" utime="5.01214e+01" stime="6.78904e+00" mtime="3.24341e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24341e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4929e+08" > 4.9089e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4876e+08" > 2.8918e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3991e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4093e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1036e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5002e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1917e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1805e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7159e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9398e+01 </func>
</region>
</regions>
<internal rank="403" log_i="1723713849.509311" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="404" mpi_size="768" stamp_init="1723713791.142767" stamp_final="1723713849.504277" username="apac4" allocationname="unknown" flags="0" pid="721557" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83615e+01" utime="4.72487e+01" stime="7.83374e+00" mtime="3.18272e+01" gflop="0.00000e+00" gbyte="3.77144e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18272e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82882e+01" utime="4.72228e+01" stime="7.82079e+00" mtime="3.18272e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18272e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4911e+08" > 7.0554e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4988e+08" > 4.5683e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2204e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4007e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.9228e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.6696e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4994e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1980e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9993e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7210e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8745e+01 </func>
</region>
</regions>
<internal rank="404" log_i="1723713849.504277" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="405" mpi_size="768" stamp_init="1723713791.149451" stamp_final="1723713849.508610" username="apac4" allocationname="unknown" flags="0" pid="721558" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83592e+01" utime="5.04079e+01" stime="6.69688e+00" mtime="3.25410e+01" gflop="0.00000e+00" gbyte="3.78147e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25410e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ec14eb14ea" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82904e+01" utime="5.03777e+01" stime="6.68792e+00" mtime="3.25410e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25410e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4955e+08" > 4.8847e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4881e+08" > 2.8673e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2295e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4228e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5259e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.6684e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4994e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1972e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3808e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7158e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9668e+01 </func>
</region>
</regions>
<internal rank="405" log_i="1723713849.508610" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="406" mpi_size="768" stamp_init="1723713791.146745" stamp_final="1723713849.508468" username="apac4" allocationname="unknown" flags="0" pid="721559" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83617e+01" utime="4.76023e+01" stime="7.78345e+00" mtime="3.18116e+01" gflop="0.00000e+00" gbyte="3.76625e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18116e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf45f14601462141a556214611496" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82914e+01" utime="4.75704e+01" stime="7.77723e+00" mtime="3.18116e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18116e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4862e+08" > 6.6392e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4788e+08" > 4.9189e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2918e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4174e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.7036e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3971e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4996e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1980e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4285e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6719e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8679e+01 </func>
</region>
</regions>
<internal rank="406" log_i="1723713849.508468" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="407" mpi_size="768" stamp_init="1723713791.149492" stamp_final="1723713849.501121" username="apac4" allocationname="unknown" flags="0" pid="721560" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="5.83516e+01" utime="4.99900e+01" stime="6.76608e+00" mtime="3.20765e+01" gflop="0.00000e+00" gbyte="3.78365e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20765e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82827e+01" utime="4.99575e+01" stime="6.75964e+00" mtime="3.20765e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20765e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4763e+08" > 4.9466e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4975e+08" > 3.1091e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0242e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4121e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4635e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4991e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2032e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0398e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7151e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9403e+01 </func>
</region>
</regions>
<internal rank="407" log_i="1723713849.501121" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="408" mpi_size="768" stamp_init="1723713791.104751" stamp_final="1723713849.506505" username="apac4" allocationname="unknown" flags="0" pid="3090881" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.84018e+01" utime="4.32341e+01" stime="1.18086e+01" mtime="3.21895e+01" gflop="0.00000e+00" gbyte="3.85555e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21895e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a2146c55a214a11474" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83529e+01" utime="4.32050e+01" stime="1.17982e+01" mtime="3.21895e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21895e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4859e+08" > 5.3809e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4857e+08" > 3.9841e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7869e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4883e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0634e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0266e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4985e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2083e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8610e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7241e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8650e+01 </func>
</region>
</regions>
<internal rank="408" log_i="1723713849.506505" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="409" mpi_size="768" stamp_init="1723713791.106439" stamp_final="1723713849.506899" username="apac4" allocationname="unknown" flags="0" pid="3090882" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.84005e+01" utime="5.00456e+01" stime="6.74428e+00" mtime="3.28391e+01" gflop="0.00000e+00" gbyte="3.78693e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28391e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf41e14201421146f562114211469" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83420e+01" utime="5.00097e+01" stime="6.73834e+00" mtime="3.28391e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28391e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4837e+08" > 4.4935e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4794e+08" > 3.2299e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6929e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4126e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6985e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4987e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2081e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9206e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7304e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9498e+01 </func>
</region>
</regions>
<internal rank="409" log_i="1723713849.506899" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="410" mpi_size="768" stamp_init="1723713791.107418" stamp_final="1723713849.507030" username="apac4" allocationname="unknown" flags="0" pid="3090883" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83996e+01" utime="4.68574e+01" stime="8.06366e+00" mtime="3.19569e+01" gflop="0.00000e+00" gbyte="3.77079e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19569e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000891583150b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83465e+01" utime="4.68217e+01" stime="8.05999e+00" mtime="3.19569e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19569e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4850e+08" > 6.6060e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4922e+08" > 5.3137e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6189e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4026e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.6045e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7262e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4981e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2103e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0398e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7247e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8467e+01 </func>
</region>
</regions>
<internal rank="410" log_i="1723713849.507030" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="411" mpi_size="768" stamp_init="1723713791.114297" stamp_final="1723713849.509584" username="apac4" allocationname="unknown" flags="0" pid="3090884" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83953e+01" utime="5.01993e+01" stime="6.89909e+00" mtime="3.26639e+01" gflop="0.00000e+00" gbyte="3.77815e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26639e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ce15c81532" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83005e+01" utime="5.01707e+01" stime="6.88495e+00" mtime="3.26639e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26639e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4906e+08" > 4.5675e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4768e+08" > 3.0551e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7952e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4161e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7220e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4983e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2120e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1900e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7228e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9211e+01 </func>
</region>
</regions>
<internal rank="411" log_i="1723713849.509584" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="412" mpi_size="768" stamp_init="1723713791.113890" stamp_final="1723713849.517713" username="apac4" allocationname="unknown" flags="0" pid="3090885" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.84038e+01" utime="4.79346e+01" stime="7.14046e+00" mtime="3.19049e+01" gflop="0.00000e+00" gbyte="3.77850e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19049e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83348e+01" utime="4.79035e+01" stime="7.13030e+00" mtime="3.19049e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19049e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4768e+08" > 6.1040e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4897e+08" > 4.5430e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4490e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4898e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.9268e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0267e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4984e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2122e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9016e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7211e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8623e+01 </func>
</region>
</regions>
<internal rank="412" log_i="1723713849.517713" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="413" mpi_size="768" stamp_init="1723713791.116408" stamp_final="1723713849.499683" username="apac4" allocationname="unknown" flags="0" pid="3090886" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83833e+01" utime="5.02922e+01" stime="6.82988e+00" mtime="3.29106e+01" gflop="0.00000e+00" gbyte="3.77346e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.29106e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4fb15fd15fe157555fe15fe1548" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83103e+01" utime="5.02644e+01" stime="6.81713e+00" mtime="3.29106e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.29106e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4797e+08" > 4.3327e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4677e+08" > 2.7018e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2628e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4013e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0212e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4973e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2233e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3188e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7202e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.0027e+01 </func>
</region>
</regions>
<internal rank="413" log_i="1723713849.499683" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="414" mpi_size="768" stamp_init="1723713791.118485" stamp_final="1723713849.501808" username="apac4" allocationname="unknown" flags="0" pid="3090887" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83833e+01" utime="4.75116e+01" stime="7.21031e+00" mtime="3.17051e+01" gflop="0.00000e+00" gbyte="3.77884e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17051e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf41e15201521151f55211521153f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83018e+01" utime="4.74840e+01" stime="7.19629e+00" mtime="3.17051e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17051e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4855e+08" > 5.8990e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4806e+08" > 4.8146e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7415e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4841e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.4520e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0267e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4974e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2191e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2210e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7232e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9151e+01 </func>
</region>
</regions>
<internal rank="414" log_i="1723713849.501808" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="415" mpi_size="768" stamp_init="1723713791.142196" stamp_final="1723713849.512413" username="apac4" allocationname="unknown" flags="0" pid="3090888" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83702e+01" utime="4.93644e+01" stime="6.75010e+00" mtime="3.26132e+01" gflop="0.00000e+00" gbyte="3.77338e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26132e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000cc147255cc14c714c2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82701e+01" utime="4.93350e+01" stime="6.73716e+00" mtime="3.26132e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26132e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4809e+08" > 4.4090e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4982e+08" > 2.4305e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4896e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4817e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0267e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4959e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2360e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3283e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7194e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9493e+01 </func>
</region>
</regions>
<internal rank="415" log_i="1723713849.512413" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="416" mpi_size="768" stamp_init="1723713791.144563" stamp_final="1723713849.513904" username="apac4" allocationname="unknown" flags="0" pid="3090889" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83693e+01" utime="4.76246e+01" stime="7.42631e+00" mtime="3.27470e+01" gflop="0.00000e+00" gbyte="3.77148e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27470e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e414e414b2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82738e+01" utime="4.75957e+01" stime="7.41372e+00" mtime="3.27470e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27470e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4867e+08" > 4.5982e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4932e+08" > 4.7464e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.5729e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4917e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7390e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0271e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4958e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2346e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0899e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7224e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8489e+01 </func>
</region>
</regions>
<internal rank="416" log_i="1723713849.513904" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="417" mpi_size="768" stamp_init="1723713791.125394" stamp_final="1723713849.518052" username="apac4" allocationname="unknown" flags="0" pid="3090890" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83927e+01" utime="5.03329e+01" stime="6.80318e+00" mtime="3.27402e+01" gflop="0.00000e+00" gbyte="3.77804e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27402e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003515351536" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82964e+01" utime="5.02982e+01" stime="6.79406e+00" mtime="3.27402e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27402e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4822e+08" > 3.8215e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4905e+08" > 2.9813e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6943e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4236e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0384e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4954e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2391e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0088e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6703e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9450e+01 </func>
</region>
</regions>
<internal rank="417" log_i="1723713849.518052" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="418" mpi_size="768" stamp_init="1723713791.129328" stamp_final="1723713849.507977" username="apac4" allocationname="unknown" flags="0" pid="3090891" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83786e+01" utime="4.73503e+01" stime="7.83920e+00" mtime="3.22277e+01" gflop="0.00000e+00" gbyte="3.77449e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22277e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48c158d158e1581558e158e153f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82753e+01" utime="4.73142e+01" stime="7.83185e+00" mtime="3.22277e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22277e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4976e+08" > 6.0367e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5017e+08" > 4.9946e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7234e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4106e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9569e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9632e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4954e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2417e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4499e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7223e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8681e+01 </func>
</region>
</regions>
<internal rank="418" log_i="1723713849.507977" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="419" mpi_size="768" stamp_init="1723713791.131461" stamp_final="1723713849.509214" username="apac4" allocationname="unknown" flags="0" pid="3090892" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83778e+01" utime="4.99152e+01" stime="7.16568e+00" mtime="3.27994e+01" gflop="0.00000e+00" gbyte="3.77163e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27994e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82767e+01" utime="4.98875e+01" stime="7.14936e+00" mtime="3.27994e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27994e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.0599e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5028e+08" > 3.6624e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5047e+08" > 2.6403e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8717e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4059e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9608e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4951e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2419e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2592e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6701e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9373e+01 </func>
</region>
</regions>
<internal rank="419" log_i="1723713849.509214" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="420" mpi_size="768" stamp_init="1723713791.134348" stamp_final="1723713849.509641" username="apac4" allocationname="unknown" flags="0" pid="3090893" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83753e+01" utime="4.85481e+01" stime="7.36912e+00" mtime="3.20186e+01" gflop="0.00000e+00" gbyte="3.74992e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20186e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d615b855d615d61508" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82721e+01" utime="4.85169e+01" stime="7.35720e+00" mtime="3.20186e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20186e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4982e+08" > 4.9957e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4877e+08" > 3.8679e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1983e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4212e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1778e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9080e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4945e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2460e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0899e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7223e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9105e+01 </func>
</region>
</regions>
<internal rank="420" log_i="1723713849.509641" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="421" mpi_size="768" stamp_init="1723713791.136594" stamp_final="1723713849.499255" username="apac4" allocationname="unknown" flags="0" pid="3090894" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83627e+01" utime="5.03465e+01" stime="6.72442e+00" mtime="3.29012e+01" gflop="0.00000e+00" gbyte="3.77518e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.29012e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000093145f5693149314b2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82607e+01" utime="5.03166e+01" stime="6.71073e+00" mtime="3.29012e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.29012e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4832e+08" > 3.4189e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4838e+08" > 2.9111e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6009e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4045e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7180e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0029e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4943e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2504e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3188e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7201e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9768e+01 </func>
</region>
</regions>
<internal rank="421" log_i="1723713849.499255" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="422" mpi_size="768" stamp_init="1723713791.142304" stamp_final="1723713849.506370" username="apac4" allocationname="unknown" flags="0" pid="3090895" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83641e+01" utime="4.83758e+01" stime="7.69528e+00" mtime="3.25436e+01" gflop="0.00000e+00" gbyte="3.77975e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25436e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49214941495143f5595149414da" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82661e+01" utime="4.83450e+01" stime="7.68327e+00" mtime="3.25436e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25436e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.7752e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4883e+08" > 4.6179e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4828e+08" > 4.0509e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9451e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4133e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4186e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9769e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4944e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2493e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2711e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7211e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8926e+01 </func>
</region>
</regions>
<internal rank="422" log_i="1723713849.506370" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="423" mpi_size="768" stamp_init="1723713791.142183" stamp_final="1723713849.499397" username="apac4" allocationname="unknown" flags="0" pid="3090896" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83572e+01" utime="5.01869e+01" stime="6.89782e+00" mtime="3.32999e+01" gflop="0.00000e+00" gbyte="3.77716e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.32999e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000072145a557214711461" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82590e+01" utime="5.01545e+01" stime="6.88756e+00" mtime="3.32999e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.32999e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4926e+08" > 3.4933e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4830e+08" > 3.1142e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9612e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4276e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0041e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0270e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4944e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2494e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9898e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7183e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9771e+01 </func>
</region>
</regions>
<internal rank="423" log_i="1723713849.499397" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="424" mpi_size="768" stamp_init="1723713791.146185" stamp_final="1723713849.513925" username="apac4" allocationname="unknown" flags="0" pid="3090897" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83677e+01" utime="4.57225e+01" stime="7.99714e+00" mtime="3.18689e+01" gflop="0.00000e+00" gbyte="3.77247e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18689e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b015af1516" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82695e+01" utime="4.56918e+01" stime="7.98614e+00" mtime="3.18689e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18689e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4845e+08" > 7.2222e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4891e+08" > 7.6301e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0831e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4892e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3900e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0272e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4934e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2613e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9612e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7241e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7814e+01 </func>
</region>
</regions>
<internal rank="424" log_i="1723713849.513925" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="425" mpi_size="768" stamp_init="1723713791.153325" stamp_final="1723713849.515251" username="apac4" allocationname="unknown" flags="0" pid="3090898" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83619e+01" utime="4.99634e+01" stime="7.06247e+00" mtime="3.24965e+01" gflop="0.00000e+00" gbyte="3.78044e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24965e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009c159b153c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82707e+01" utime="4.99328e+01" stime="7.05106e+00" mtime="3.24965e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24965e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4762e+08" > 4.7881e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4777e+08" > 3.6036e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6863e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4165e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8358e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7182e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4921e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2723e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1495e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7170e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9125e+01 </func>
</region>
</regions>
<internal rank="425" log_i="1723713849.515251" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="426" mpi_size="768" stamp_init="1723713791.152212" stamp_final="1723713849.513781" username="apac4" allocationname="unknown" flags="0" pid="3090899" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83616e+01" utime="4.87670e+01" stime="7.44718e+00" mtime="3.19612e+01" gflop="0.00000e+00" gbyte="3.76545e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19612e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f614f814f914dc55f914f9145b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82696e+01" utime="4.87348e+01" stime="7.43872e+00" mtime="3.19612e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19612e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4767e+08" > 5.3828e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4903e+08" > 4.4611e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9280e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4028e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8315e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7241e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4929e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2642e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5095e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7237e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8292e+01 </func>
</region>
</regions>
<internal rank="426" log_i="1723713849.513781" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="427" mpi_size="768" stamp_init="1723713791.156189" stamp_final="1723713849.500508" username="apac4" allocationname="unknown" flags="0" pid="3090900" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83443e+01" utime="4.97657e+01" stime="7.01211e+00" mtime="3.20790e+01" gflop="0.00000e+00" gbyte="3.77075e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20790e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a315a415a5157056a515a51505" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82555e+01" utime="4.97316e+01" stime="7.00553e+00" mtime="3.20790e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20790e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4844e+08" > 4.8720e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4879e+08" > 3.1328e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1835e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4197e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7221e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4932e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2619e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0804e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6646e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9204e+01 </func>
</region>
</regions>
<internal rank="427" log_i="1723713849.500508" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="428" mpi_size="768" stamp_init="1723713791.156767" stamp_final="1723713849.507198" username="apac4" allocationname="unknown" flags="0" pid="3090901" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83504e+01" utime="4.85772e+01" stime="7.42351e+00" mtime="3.21655e+01" gflop="0.00000e+00" gbyte="3.77663e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21655e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c915cb15cc15a855cc15cb1515" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82588e+01" utime="4.85445e+01" stime="7.41502e+00" mtime="3.21655e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21655e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4787e+08" > 5.5470e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4860e+08" > 4.0723e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2944e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4260e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2129e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7155e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4931e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2621e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8300e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7228e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9092e+01 </func>
</region>
</regions>
<internal rank="428" log_i="1723713849.507198" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="429" mpi_size="768" stamp_init="1723713791.158062" stamp_final="1723713849.510350" username="apac4" allocationname="unknown" flags="0" pid="3090902" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83523e+01" utime="4.98891e+01" stime="7.16596e+00" mtime="3.32275e+01" gflop="0.00000e+00" gbyte="3.77018e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.32275e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000161416146f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82643e+01" utime="4.98523e+01" stime="7.16232e+00" mtime="3.32275e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.32275e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4940e+08" > 4.7523e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4804e+08" > 3.0574e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8635e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4305e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7895e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7874e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4909e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2862e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1686e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7177e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9673e+01 </func>
</region>
</regions>
<internal rank="429" log_i="1723713849.510350" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="430" mpi_size="768" stamp_init="1723713791.159803" stamp_final="1723713849.513870" username="apac4" allocationname="unknown" flags="0" pid="3090903" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83541e+01" utime="4.79721e+01" stime="7.66658e+00" mtime="3.21624e+01" gflop="0.00000e+00" gbyte="3.74832e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21624e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000023158c552315221517" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82682e+01" utime="4.79374e+01" stime="7.66101e+00" mtime="3.21624e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21624e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4889e+08" > 6.3735e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5008e+08" > 5.0468e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6309e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4040e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.6968e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7216e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4924e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2731e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4189e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7217e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8685e+01 </func>
</region>
</regions>
<internal rank="430" log_i="1723713849.513870" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="431" mpi_size="768" stamp_init="1723713791.164649" stamp_final="1723713849.506900" username="apac4" allocationname="unknown" flags="0" pid="3090904" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="5.83423e+01" utime="4.96291e+01" stime="7.44997e+00" mtime="3.30579e+01" gflop="0.00000e+00" gbyte="3.76526e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.30579e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003415341524" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82593e+01" utime="4.96018e+01" stime="7.43745e+00" mtime="3.30579e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.30579e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4909e+08" > 4.7703e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4985e+08" > 3.5992e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.4100e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4208e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8820e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4918e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2756e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3283e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7167e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8958e+01 </func>
</region>
</regions>
<internal rank="431" log_i="1723713849.506900" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="432" mpi_size="768" stamp_init="1723713791.124124" stamp_final="1723713849.505053" username="apac4" allocationname="unknown" flags="0" pid="984141" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83809e+01" utime="4.10608e+01" stime="1.30523e+01" mtime="3.01613e+01" gflop="0.00000e+00" gbyte="3.85704e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.01613e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004814285548144814b2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83018e+01" utime="4.10286e+01" stime="1.30435e+01" mtime="3.01613e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.01613e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4853e+08" > 9.1556e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4849e+08" > 9.5267e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.6179e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3968e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.5269e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6871e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4917e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2802e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7299e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7342e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7374e+01 </func>
</region>
</regions>
<internal rank="432" log_i="1723713849.505053" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="433" mpi_size="768" stamp_init="1723713791.123979" stamp_final="1723713849.520532" username="apac4" allocationname="unknown" flags="0" pid="984142" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83966e+01" utime="4.95802e+01" stime="6.53883e+00" mtime="3.19386e+01" gflop="0.00000e+00" gbyte="3.76831e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19386e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf41b151c151d1545551d151d1540" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83101e+01" utime="4.95479e+01" stime="6.52951e+00" mtime="3.19386e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19386e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4921e+08" > 4.8891e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4945e+08" > 3.5073e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1402e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4903e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4796e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0059e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4915e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2817e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6011e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7311e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9121e+01 </func>
</region>
</regions>
<internal rank="433" log_i="1723713849.520532" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="434" mpi_size="768" stamp_init="1723713791.124093" stamp_final="1723713849.520771" username="apac4" allocationname="unknown" flags="0" pid="984143" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83967e+01" utime="4.82222e+01" stime="7.41026e+00" mtime="3.11185e+01" gflop="0.00000e+00" gbyte="3.75515e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.11185e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a315a2150e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83207e+01" utime="4.81916e+01" stime="7.39974e+00" mtime="3.11185e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.11185e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5104e+08" > 6.9146e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4959e+08" > 5.2278e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0081e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4080e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.4611e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0209e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4909e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2838e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1710e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7338e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8200e+01 </func>
</region>
</regions>
<internal rank="434" log_i="1723713849.520771" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="435" mpi_size="768" stamp_init="1723713791.123974" stamp_final="1723713849.509198" username="apac4" allocationname="unknown" flags="0" pid="984144" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83852e+01" utime="4.92261e+01" stime="6.87763e+00" mtime="3.23494e+01" gflop="0.00000e+00" gbyte="3.77388e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23494e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83040e+01" utime="4.91972e+01" stime="6.86479e+00" mtime="3.23494e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23494e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4949e+08" > 4.7592e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5063e+08" > 2.7162e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3912e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5151e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0060e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4909e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2885e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7800e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7286e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9277e+01 </func>
</region>
</regions>
<internal rank="435" log_i="1723713849.509198" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="436" mpi_size="768" stamp_init="1723713791.130040" stamp_final="1723713849.505590" username="apac4" allocationname="unknown" flags="0" pid="984145" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83756e+01" utime="4.79957e+01" stime="7.54729e+00" mtime="3.17731e+01" gflop="0.00000e+00" gbyte="3.78178e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17731e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004214db56421441145d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82785e+01" utime="4.79646e+01" stime="7.53364e+00" mtime="3.17731e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17731e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4926e+08" > 6.7728e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5037e+08" > 4.5539e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3959e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4136e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7571e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6950e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4908e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2894e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7990e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7337e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8486e+01 </func>
</region>
</regions>
<internal rank="436" log_i="1723713849.505590" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="437" mpi_size="768" stamp_init="1723713791.127624" stamp_final="1723713849.511668" username="apac4" allocationname="unknown" flags="0" pid="984146" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83840e+01" utime="5.02537e+01" stime="6.84954e+00" mtime="3.23670e+01" gflop="0.00000e+00" gbyte="3.77064e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23670e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000046144614dd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82970e+01" utime="5.02239e+01" stime="6.83873e+00" mtime="3.23670e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23670e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5082e+08" > 4.6088e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4775e+08" > 3.0371e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0264e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4180e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0255e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4910e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2935e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1590e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7294e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9671e+01 </func>
</region>
</regions>
<internal rank="437" log_i="1723713849.511668" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="438" mpi_size="768" stamp_init="1723713791.129281" stamp_final="1723713849.515516" username="apac4" allocationname="unknown" flags="0" pid="984147" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83862e+01" utime="4.74753e+01" stime="7.82711e+00" mtime="3.15881e+01" gflop="0.00000e+00" gbyte="3.76575e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15881e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4301432143314cb553314331464" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82939e+01" utime="4.74447e+01" stime="7.81607e+00" mtime="3.15881e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15881e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4802e+08" > 7.0369e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5002e+08" > 4.5822e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3045e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4265e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3940e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0468e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4899e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2937e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8396e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7331e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8344e+01 </func>
</region>
</regions>
<internal rank="438" log_i="1723713849.515516" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="439" mpi_size="768" stamp_init="1723713791.132051" stamp_final="1723713849.509584" username="apac4" allocationname="unknown" flags="0" pid="984148" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83775e+01" utime="5.02457e+01" stime="6.86230e+00" mtime="3.20006e+01" gflop="0.00000e+00" gbyte="3.74653e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20006e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82809e+01" utime="5.02165e+01" stime="6.84961e+00" mtime="3.20006e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20006e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4997e+08" > 4.8113e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4897e+08" > 3.0897e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2653e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4138e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4080e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0467e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4899e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2983e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3998e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7288e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9047e+01 </func>
</region>
</regions>
<internal rank="439" log_i="1723713849.509584" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="440" mpi_size="768" stamp_init="1723713791.139844" stamp_final="1723713849.515512" username="apac4" allocationname="unknown" flags="0" pid="984149" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83757e+01" utime="4.78929e+01" stime="7.84940e+00" mtime="3.16366e+01" gflop="0.00000e+00" gbyte="3.77941e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16366e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82800e+01" utime="4.78640e+01" stime="7.83623e+00" mtime="3.16366e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16366e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4542e+08" > 5.9951e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4892e+08" > 4.8224e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.3601e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4048e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.3669e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0247e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4900e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2975e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9802e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7297e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7460e+01 </func>
</region>
</regions>
<internal rank="440" log_i="1723713849.515512" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="441" mpi_size="768" stamp_init="1723713791.135542" stamp_final="1723713849.514651" username="apac4" allocationname="unknown" flags="0" pid="984150" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83791e+01" utime="4.99872e+01" stime="6.95293e+00" mtime="3.23574e+01" gflop="0.00000e+00" gbyte="3.75885e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23574e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006f156e1507" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82848e+01" utime="4.99580e+01" stime="6.94015e+00" mtime="3.23574e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23574e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4820e+08" > 4.8099e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4711e+08" > 2.7424e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6914e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4277e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0725e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4895e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3020e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3402e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7320e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8965e+01 </func>
</region>
</regions>
<internal rank="441" log_i="1723713849.514651" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="442" mpi_size="768" stamp_init="1723713791.140351" stamp_final="1723713849.513510" username="apac4" allocationname="unknown" flags="0" pid="984151" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83732e+01" utime="4.67155e+01" stime="8.23987e+00" mtime="3.18493e+01" gflop="0.00000e+00" gbyte="3.75504e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18493e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82795e+01" utime="4.66858e+01" stime="8.22719e+00" mtime="3.18493e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18493e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4769e+08" > 8.9175e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4901e+08" > 5.7312e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8147e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4214e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8260e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0248e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4892e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3014e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9492e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7296e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7903e+01 </func>
</region>
</regions>
<internal rank="442" log_i="1723713849.513510" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="443" mpi_size="768" stamp_init="1723713791.145539" stamp_final="1723713849.502700" username="apac4" allocationname="unknown" flags="0" pid="984152" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83572e+01" utime="4.99937e+01" stime="6.98989e+00" mtime="3.24841e+01" gflop="0.00000e+00" gbyte="3.74481e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24841e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c014c214c314e055c314c2147d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82582e+01" utime="4.99599e+01" stime="6.98218e+00" mtime="3.24841e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24841e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4871e+08" > 5.0423e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4776e+08" > 3.2606e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7410e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4136e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9087e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0246e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4887e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3071e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2711e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7295e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9033e+01 </func>
</region>
</regions>
<internal rank="443" log_i="1723713849.502700" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="444" mpi_size="768" stamp_init="1723713791.143432" stamp_final="1723713849.509975" username="apac4" allocationname="unknown" flags="0" pid="984153" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83665e+01" utime="4.69942e+01" stime="8.08435e+00" mtime="3.17688e+01" gflop="0.00000e+00" gbyte="3.77876e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17688e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008f148f14bd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82683e+01" utime="4.69650e+01" stime="8.07194e+00" mtime="3.17688e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17688e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4812e+08" > 7.3535e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4774e+08" > 6.6987e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4626e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4268e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.5321e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0342e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4883e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3140e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5787e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7324e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8312e+01 </func>
</region>
</regions>
<internal rank="444" log_i="1723713849.509975" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="445" mpi_size="768" stamp_init="1723713791.145959" stamp_final="1723713849.509038" username="apac4" allocationname="unknown" flags="0" pid="984154" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83631e+01" utime="5.01964e+01" stime="6.89448e+00" mtime="3.24474e+01" gflop="0.00000e+00" gbyte="3.77090e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24474e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000021142114a8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82666e+01" utime="5.01663e+01" stime="6.88303e+00" mtime="3.24474e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24474e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4787e+08" > 4.7766e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4902e+08" > 2.7819e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3640e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4248e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0333e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4883e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3147e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0613e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7291e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9392e+01 </func>
</region>
</regions>
<internal rank="445" log_i="1723713849.509038" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="446" mpi_size="768" stamp_init="1723713791.149865" stamp_final="1723713849.508253" username="apac4" allocationname="unknown" flags="0" pid="984155" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83584e+01" utime="4.73650e+01" stime="8.07149e+00" mtime="3.21154e+01" gflop="0.00000e+00" gbyte="3.76091e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21154e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82643e+01" utime="4.73337e+01" stime="8.06128e+00" mtime="3.21154e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21154e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4813e+08" > 7.0802e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4878e+08" > 4.4779e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1014e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4251e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.3685e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0395e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4878e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3205e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0208e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7337e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8075e+01 </func>
</region>
</regions>
<internal rank="446" log_i="1723713849.508253" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="447" mpi_size="768" stamp_init="1723713791.156165" stamp_final="1723713849.503336" username="apac4" allocationname="unknown" flags="0" pid="984156" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83472e+01" utime="4.93292e+01" stime="6.49422e+00" mtime="3.20972e+01" gflop="0.00000e+00" gbyte="3.74775e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20972e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf448154a154b1528564b154a1545" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82447e+01" utime="4.92964e+01" stime="6.48660e+00" mtime="3.20972e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20972e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4901e+08" > 5.0175e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4784e+08" > 2.8231e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0448e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5273e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0069e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4878e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3202e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0613e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7277e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9331e+01 </func>
</region>
</regions>
<internal rank="447" log_i="1723713849.503336" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="448" mpi_size="768" stamp_init="1723713791.155830" stamp_final="1723713849.515925" username="apac4" allocationname="unknown" flags="0" pid="984157" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83601e+01" utime="4.56562e+01" stime="8.19215e+00" mtime="3.16467e+01" gflop="0.00000e+00" gbyte="3.74470e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16467e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4da14db14dc145c56dc14dc14fd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82676e+01" utime="4.56233e+01" stime="8.18376e+00" mtime="3.16467e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16467e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4869e+08" > 9.8770e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4898e+08" > 5.5804e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5003e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4406e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6958e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0635e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4865e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3316e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9111e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7332e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7924e+01 </func>
</region>
</regions>
<internal rank="448" log_i="1723713849.515925" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="449" mpi_size="768" stamp_init="1723713791.157423" stamp_final="1723713849.504161" username="apac4" allocationname="unknown" flags="0" pid="984158" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83467e+01" utime="5.01325e+01" stime="6.99132e+00" mtime="3.23748e+01" gflop="0.00000e+00" gbyte="3.77132e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23748e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e015e01553" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82550e+01" utime="5.00993e+01" stime="6.98366e+00" mtime="3.23748e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23748e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5030e+08" > 5.8703e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4869e+08" > 3.0437e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5700e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4180e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0258e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4861e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3370e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9707e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7267e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9009e+01 </func>
</region>
</regions>
<internal rank="449" log_i="1723713849.504161" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="450" mpi_size="768" stamp_init="1723713791.180253" stamp_final="1723713849.505445" username="apac4" allocationname="unknown" flags="0" pid="984159" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83252e+01" utime="4.68402e+01" stime="7.69632e+00" mtime="3.16545e+01" gflop="0.00000e+00" gbyte="3.77327e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16545e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82522e+01" utime="4.68089e+01" stime="7.68802e+00" mtime="3.16545e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16545e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127164" bytes="8.4842e+08" > 8.1941e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4779e+08" > 5.9693e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3667e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4536e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.7500e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0653e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4863e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3348e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8682e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7334e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8232e+01 </func>
</region>
</regions>
<internal rank="450" log_i="1723713849.505445" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="451" mpi_size="768" stamp_init="1723713791.167401" stamp_final="1723713849.514887" username="apac4" allocationname="unknown" flags="0" pid="984160" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83475e+01" utime="5.00000e+01" stime="7.15457e+00" mtime="3.24657e+01" gflop="0.00000e+00" gbyte="3.77838e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24657e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82624e+01" utime="4.99641e+01" stime="7.15018e+00" mtime="3.24657e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24657e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4733e+08" > 5.9427e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5054e+08" > 3.2863e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5049e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4193e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3842e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9938e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4860e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3342e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2187e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7270e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9157e+01 </func>
</region>
</regions>
<internal rank="451" log_i="1723713849.514887" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="452" mpi_size="768" stamp_init="1723713791.166083" stamp_final="1723713849.515475" username="apac4" allocationname="unknown" flags="0" pid="984161" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83494e+01" utime="4.63661e+01" stime="7.71061e+00" mtime="3.12609e+01" gflop="0.00000e+00" gbyte="3.76595e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.12609e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007f147a1496" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82650e+01" utime="4.63350e+01" stime="7.70174e+00" mtime="3.12609e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.12609e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5106e+08" > 9.7493e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4884e+08" > 5.6404e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7142e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4381e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3836e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0636e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4851e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3462e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0708e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7311e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8343e+01 </func>
</region>
</regions>
<internal rank="452" log_i="1723713849.515475" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="453" mpi_size="768" stamp_init="1723713791.167401" stamp_final="1723713849.509007" username="apac4" allocationname="unknown" flags="0" pid="984162" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83416e+01" utime="5.01087e+01" stime="7.00302e+00" mtime="3.25486e+01" gflop="0.00000e+00" gbyte="3.76263e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25486e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006c14ca556c146b1469" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82588e+01" utime="5.00774e+01" stime="6.99518e+00" mtime="3.25486e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25486e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4817e+08" > 5.9546e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4790e+08" > 3.6880e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3812e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4230e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0079e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4860e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3385e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3879e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7267e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9354e+01 </func>
</region>
</regions>
<internal rank="453" log_i="1723713849.509007" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="454" mpi_size="768" stamp_init="1723713791.184147" stamp_final="1723713849.508754" username="apac4" allocationname="unknown" flags="0" pid="984163" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83246e+01" utime="4.70584e+01" stime="7.51314e+00" mtime="3.13113e+01" gflop="0.00000e+00" gbyte="3.76507e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.13113e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4271529152a1565552a152a1506" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82520e+01" utime="4.70263e+01" stime="7.50617e+00" mtime="3.13113e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.13113e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4784e+08" > 8.5159e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4816e+08" > 4.5636e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8879e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4445e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0636e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4847e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3472e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3498e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7315e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8361e+01 </func>
</region>
</regions>
<internal rank="454" log_i="1723713849.508754" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="455" mpi_size="768" stamp_init="1723713791.171100" stamp_final="1723713849.522492" username="apac4" allocationname="unknown" flags="0" pid="984164" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="5.83514e+01" utime="5.03080e+01" stime="6.84644e+00" mtime="3.23605e+01" gflop="0.00000e+00" gbyte="3.77441e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23605e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b414b614b714cc55b714b71477" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82685e+01" utime="5.02733e+01" stime="6.84167e+00" mtime="3.23605e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23605e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4906e+08" > 5.8905e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4978e+08" > 2.9125e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4569e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4161e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0725e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4849e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3463e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2210e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7262e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9105e+01 </func>
</region>
</regions>
<internal rank="455" log_i="1723713849.522492" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="456" mpi_size="768" stamp_init="1723713791.107808" stamp_final="1723713849.503696" username="apac4" allocationname="unknown" flags="0" pid="533953" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83959e+01" utime="4.36868e+01" stime="1.23461e+01" mtime="3.16758e+01" gflop="0.00000e+00" gbyte="3.86143e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16758e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003d143c14ed" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83425e+01" utime="4.36527e+01" stime="1.23404e+01" mtime="3.16758e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16758e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4827e+08" > 5.8367e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4870e+08" > 4.9366e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8494e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4081e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1641e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1584e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4854e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3449e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9993e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2194e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7995e+01 </func>
</region>
</regions>
<internal rank="456" log_i="1723713849.503696" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="457" mpi_size="768" stamp_init="1723713791.109750" stamp_final="1723713849.518701" username="apac4" allocationname="unknown" flags="0" pid="533954" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.84090e+01" utime="5.04155e+01" stime="6.75035e+00" mtime="3.22329e+01" gflop="0.00000e+00" gbyte="3.77838e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22329e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ee15f015f1158d56f115f11501" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83447e+01" utime="5.03813e+01" stime="6.74315e+00" mtime="3.22329e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22329e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4776e+08" > 4.4878e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4834e+08" > 3.7848e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3822e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3974e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1753e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4854e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3459e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9898e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2249e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9177e+01 </func>
</region>
</regions>
<internal rank="457" log_i="1723713849.518701" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="458" mpi_size="768" stamp_init="1723713791.109686" stamp_final="1723713849.510241" username="apac4" allocationname="unknown" flags="0" pid="533955" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.84006e+01" utime="4.79382e+01" stime="7.68916e+00" mtime="3.16557e+01" gflop="0.00000e+00" gbyte="3.76472e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16557e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83496e+01" utime="4.79052e+01" stime="7.68354e+00" mtime="3.16557e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16557e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4851e+08" > 7.0044e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4819e+08" > 6.0103e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3776e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3979e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.3864e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1708e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4851e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3463e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0708e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2203e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8326e+01 </func>
</region>
</regions>
<internal rank="458" log_i="1723713849.510241" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="459" mpi_size="768" stamp_init="1723713791.114214" stamp_final="1723713849.508699" username="apac4" allocationname="unknown" flags="0" pid="533956" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83945e+01" utime="5.02239e+01" stime="6.94361e+00" mtime="3.22906e+01" gflop="0.00000e+00" gbyte="3.76099e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22906e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006f14d6556f146f14c1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83207e+01" utime="5.01915e+01" stime="6.93543e+00" mtime="3.22906e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22906e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4925e+08" > 4.4108e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4920e+08" > 3.3503e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4483e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4248e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1707e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4851e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3483e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1686e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2259e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9153e+01 </func>
</region>
</regions>
<internal rank="459" log_i="1723713849.508699" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="460" mpi_size="768" stamp_init="1723713791.115548" stamp_final="1723713849.517758" username="apac4" allocationname="unknown" flags="0" pid="533957" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.84022e+01" utime="4.73180e+01" stime="7.95443e+00" mtime="3.19159e+01" gflop="0.00000e+00" gbyte="3.76434e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19159e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000871485558714821497" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83289e+01" utime="4.72858e+01" stime="7.94627e+00" mtime="3.19159e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19159e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4965e+08" > 6.9283e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4902e+08" > 5.6223e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7180e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4168e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.2729e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1586e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4847e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3497e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1805e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2204e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8240e+01 </func>
</region>
</regions>
<internal rank="460" log_i="1723713849.517758" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="461" mpi_size="768" stamp_init="1723713791.117878" stamp_final="1723713849.508491" username="apac4" allocationname="unknown" flags="0" pid="533958" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83906e+01" utime="5.07079e+01" stime="6.48332e+00" mtime="3.22907e+01" gflop="0.00000e+00" gbyte="3.76530e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22907e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f814f8149a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83178e+01" utime="5.06773e+01" stime="6.47307e+00" mtime="3.22907e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22907e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4898e+08" > 4.4364e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4992e+08" > 3.3244e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1047e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4272e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2027e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4842e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3577e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1185e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2241e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9489e+01 </func>
</region>
</regions>
<internal rank="461" log_i="1723713849.508491" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="462" mpi_size="768" stamp_init="1723713791.121071" stamp_final="1723713849.503389" username="apac4" allocationname="unknown" flags="0" pid="533959" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83823e+01" utime="4.78952e+01" stime="7.60958e+00" mtime="3.17199e+01" gflop="0.00000e+00" gbyte="3.76343e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17199e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82967e+01" utime="4.78584e+01" stime="7.60523e+00" mtime="3.17199e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17199e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4987e+08" > 6.2847e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4838e+08" > 4.5220e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3878e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4117e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4030e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1588e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4838e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3602e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7609e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2199e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8445e+01 </func>
</region>
</regions>
<internal rank="462" log_i="1723713849.503389" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="463" mpi_size="768" stamp_init="1723713791.124808" stamp_final="1723713849.508424" username="apac4" allocationname="unknown" flags="0" pid="533960" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83836e+01" utime="5.02938e+01" stime="6.85380e+00" mtime="3.23120e+01" gflop="0.00000e+00" gbyte="3.77247e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23120e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ed143c55ed14ed145b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82876e+01" utime="5.02644e+01" stime="6.84016e+00" mtime="3.23120e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23120e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4833e+08" > 4.3633e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4885e+08" > 3.6474e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4444e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3983e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1587e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4842e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3575e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0708e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2246e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9208e+01 </func>
</region>
</regions>
<internal rank="463" log_i="1723713849.508424" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="464" mpi_size="768" stamp_init="1723713791.125999" stamp_final="1723713849.525697" username="apac4" allocationname="unknown" flags="0" pid="533961" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83997e+01" utime="4.87090e+01" stime="7.53968e+00" mtime="3.25447e+01" gflop="0.00000e+00" gbyte="3.76480e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25447e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001314fd55131413149a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83065e+01" utime="4.86743e+01" stime="7.53257e+00" mtime="3.25447e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25447e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4722e+08" > 4.6829e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4802e+08" > 3.8588e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.4342e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4255e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2727e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1095e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4826e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3730e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9111e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2208e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8382e+01 </func>
</region>
</regions>
<internal rank="464" log_i="1723713849.525697" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="465" mpi_size="768" stamp_init="1723713791.128087" stamp_final="1723713849.512780" username="apac4" allocationname="unknown" flags="0" pid="533962" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83847e+01" utime="5.05433e+01" stime="6.58872e+00" mtime="3.24478e+01" gflop="0.00000e+00" gbyte="3.75923e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24478e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003e1460563e143e1459" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82867e+01" utime="5.05122e+01" stime="6.57770e+00" mtime="3.24478e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24478e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4793e+08" > 3.4581e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4889e+08" > 2.6824e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3749e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4165e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2858e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4823e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3734e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5596e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2232e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9483e+01 </func>
</region>
</regions>
<internal rank="465" log_i="1723713849.512780" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="466" mpi_size="768" stamp_init="1723713791.132105" stamp_final="1723713849.500040" username="apac4" allocationname="unknown" flags="0" pid="533963" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83679e+01" utime="4.80616e+01" stime="7.58826e+00" mtime="3.23887e+01" gflop="0.00000e+00" gbyte="3.78010e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23887e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a214a114d6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82710e+01" utime="4.80349e+01" stime="7.57204e+00" mtime="3.23887e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23887e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4906e+08" > 4.8109e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4766e+08" > 4.1141e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0413e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4331e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0054e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2185e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4819e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3785e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8992e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2266e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8598e+01 </func>
</region>
</regions>
<internal rank="466" log_i="1723713849.500040" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="467" mpi_size="768" stamp_init="1723713791.133827" stamp_final="1723713849.503662" username="apac4" allocationname="unknown" flags="0" pid="533964" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83698e+01" utime="5.04123e+01" stime="6.70358e+00" mtime="3.24849e+01" gflop="0.00000e+00" gbyte="3.76450e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24849e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82770e+01" utime="5.03800e+01" stime="6.69422e+00" mtime="3.24849e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24849e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4761e+08" > 3.5812e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4903e+08" > 2.6150e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4569e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4245e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2417e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4819e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3816e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8300e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2193e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9423e+01 </func>
</region>
</regions>
<internal rank="467" log_i="1723713849.503662" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="468" mpi_size="768" stamp_init="1723713791.136071" stamp_final="1723713849.509263" username="apac4" allocationname="unknown" flags="0" pid="533965" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83732e+01" utime="4.88910e+01" stime="7.24259e+00" mtime="3.22199e+01" gflop="0.00000e+00" gbyte="3.76541e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22199e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e314e314c6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82810e+01" utime="4.88612e+01" stime="7.23127e+00" mtime="3.22199e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22199e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4942e+08" > 4.5402e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4839e+08" > 3.6069e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8414e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4258e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.4040e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2171e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4809e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3881e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1590e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2265e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8660e+01 </func>
</region>
</regions>
<internal rank="468" log_i="1723713849.509263" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="469" mpi_size="768" stamp_init="1723713791.138399" stamp_final="1723713849.513490" username="apac4" allocationname="unknown" flags="0" pid="533966" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83751e+01" utime="5.03956e+01" stime="6.75628e+00" mtime="3.25503e+01" gflop="0.00000e+00" gbyte="3.76755e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25503e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a015a0152e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82826e+01" utime="5.03616e+01" stime="6.74860e+00" mtime="3.25503e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25503e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4985e+08" > 3.5939e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4927e+08" > 2.7147e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2004e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4369e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2698e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4813e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3878e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4809e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2188e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9727e+01 </func>
</region>
</regions>
<internal rank="469" log_i="1723713849.513490" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="470" mpi_size="768" stamp_init="1723713791.140704" stamp_final="1723713849.510364" username="apac4" allocationname="unknown" flags="0" pid="533967" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83697e+01" utime="4.85660e+01" stime="7.40124e+00" mtime="3.22118e+01" gflop="0.00000e+00" gbyte="3.77056e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22118e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4bf15c015c1158b56c115c11541" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82783e+01" utime="4.85284e+01" stime="7.39761e+00" mtime="3.22118e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22118e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4936e+08" > 4.5427e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4935e+08" > 3.9459e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8837e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3910e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.5817e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2172e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4812e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3873e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0994e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2225e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8649e+01 </func>
</region>
</regions>
<internal rank="470" log_i="1723713849.510364" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="471" mpi_size="768" stamp_init="1723713791.143212" stamp_final="1723713849.508743" username="apac4" allocationname="unknown" flags="0" pid="533968" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83655e+01" utime="5.03565e+01" stime="6.83732e+00" mtime="3.25525e+01" gflop="0.00000e+00" gbyte="3.78075e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25525e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82757e+01" utime="5.03258e+01" stime="6.82673e+00" mtime="3.25525e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25525e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4829e+08" > 3.5521e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4723e+08" > 2.8562e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5492e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4252e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2284e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4799e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3985e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4213e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2191e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9399e+01 </func>
</region>
</regions>
<internal rank="471" log_i="1723713849.508743" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="472" mpi_size="768" stamp_init="1723713791.147211" stamp_final="1723713849.518330" username="apac4" allocationname="unknown" flags="0" pid="533969" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83711e+01" utime="4.66478e+01" stime="8.23355e+00" mtime="3.19812e+01" gflop="0.00000e+00" gbyte="3.77789e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19812e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006f146e14b8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82840e+01" utime="4.66141e+01" stime="8.22617e+00" mtime="3.19812e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19812e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4847e+08" > 7.3389e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4765e+08" > 9.0618e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1315e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4216e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.1406e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1090e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4792e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4050e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9492e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2258e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7812e+01 </func>
</region>
</regions>
<internal rank="472" log_i="1723713849.518330" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="473" mpi_size="768" stamp_init="1723713791.150754" stamp_final="1723713849.500847" username="apac4" allocationname="unknown" flags="0" pid="533970" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83501e+01" utime="5.01344e+01" stime="7.00761e+00" mtime="3.26659e+01" gflop="0.00000e+00" gbyte="3.76442e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26659e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82654e+01" utime="5.01031e+01" stime="6.99812e+00" mtime="3.26659e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26659e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4885e+08" > 4.8067e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4747e+08" > 2.9003e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5767e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4040e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2025e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4802e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3995e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4308e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2197e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9383e+01 </func>
</region>
</regions>
<internal rank="473" log_i="1723713849.500847" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="474" mpi_size="768" stamp_init="1723713791.154519" stamp_final="1723713849.512779" username="apac4" allocationname="unknown" flags="0" pid="533971" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83583e+01" utime="4.86109e+01" stime="7.33968e+00" mtime="3.19545e+01" gflop="0.00000e+00" gbyte="3.76736e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19545e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49315951596153956961596154e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82761e+01" utime="4.85841e+01" stime="7.32622e+00" mtime="3.19545e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19545e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4810e+08" > 5.7177e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4869e+08" > 4.3752e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5976e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4225e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.7234e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1551e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4793e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4086e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5201e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2253e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8531e+01 </func>
</region>
</regions>
<internal rank="474" log_i="1723713849.512779" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="475" mpi_size="768" stamp_init="1723713791.155661" stamp_final="1723713849.513569" username="apac4" allocationname="unknown" flags="0" pid="533972" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83579e+01" utime="5.05276e+01" stime="6.62846e+00" mtime="3.22506e+01" gflop="0.00000e+00" gbyte="3.77602e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22506e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f414f314da" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82740e+01" utime="5.04944e+01" stime="6.62076e+00" mtime="3.22506e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22506e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4863e+08" > 4.9929e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4917e+08" > 3.0407e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5992e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4121e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2615e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4800e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3974e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1495e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2179e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8911e+01 </func>
</region>
</regions>
<internal rank="475" log_i="1723713849.513569" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="476" mpi_size="768" stamp_init="1723713791.156952" stamp_final="1723713849.510227" username="apac4" allocationname="unknown" flags="0" pid="533973" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83533e+01" utime="4.67379e+01" stime="8.02123e+00" mtime="3.10101e+01" gflop="0.00000e+00" gbyte="3.76610e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.10101e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000014141414f7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82715e+01" utime="4.67043e+01" stime="8.01506e+00" mtime="3.10101e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.10101e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4881e+08" > 7.3258e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4849e+08" > 5.7492e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0029e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4207e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9749e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1113e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4785e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4110e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9087e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2246e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7993e+01 </func>
</region>
</regions>
<internal rank="476" log_i="1723713849.510227" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="477" mpi_size="768" stamp_init="1723713791.162723" stamp_final="1723713849.518537" username="apac4" allocationname="unknown" flags="0" pid="533974" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83558e+01" utime="5.03961e+01" stime="6.85392e+00" mtime="3.22275e+01" gflop="0.00000e+00" gbyte="3.75252e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22275e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4241525152615de562615261555" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82780e+01" utime="5.03609e+01" stime="6.84927e+00" mtime="3.22275e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22275e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4873e+08" > 4.8514e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4870e+08" > 3.2918e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0641e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4044e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7657e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1166e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4790e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4115e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3402e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2177e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9457e+01 </func>
</region>
</regions>
<internal rank="477" log_i="1723713849.518537" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="478" mpi_size="768" stamp_init="1723713791.161209" stamp_final="1723713849.503866" username="apac4" allocationname="unknown" flags="0" pid="533975" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83427e+01" utime="4.86082e+01" stime="7.59616e+00" mtime="3.22181e+01" gflop="0.00000e+00" gbyte="3.77674e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22181e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82639e+01" utime="4.85756e+01" stime="7.58920e+00" mtime="3.22181e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22181e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5007e+08" > 5.5523e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4893e+08" > 4.2946e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7843e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4216e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.5498e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1096e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4790e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4106e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8300e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2248e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8624e+01 </func>
</region>
</regions>
<internal rank="478" log_i="1723713849.503866" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="479" mpi_size="768" stamp_init="1723713791.169697" stamp_final="1723713849.513770" username="apac4" allocationname="unknown" flags="0" pid="533976" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="5.83441e+01" utime="5.02810e+01" stime="6.91160e+00" mtime="3.26152e+01" gflop="0.00000e+00" gbyte="3.77621e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26152e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ea147955ea14ea14d0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82722e+01" utime="5.02499e+01" stime="6.90328e+00" mtime="3.26152e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26152e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4830e+08" > 4.8308e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5058e+08" > 3.6822e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8614e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4042e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4319e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1099e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4781e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4209e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2401e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2171e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9046e+01 </func>
</region>
</regions>
<internal rank="479" log_i="1723713849.513770" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="480" mpi_size="768" stamp_init="1723713791.112497" stamp_final="1723713849.509341" username="apac4" allocationname="unknown" flags="0" pid="778041" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83968e+01" utime="4.19336e+01" stime="1.27717e+01" mtime="3.13245e+01" gflop="0.00000e+00" gbyte="3.85895e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.13245e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000071147114ae" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83518e+01" utime="4.18990e+01" stime="1.27665e+01" mtime="3.13245e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.13245e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4940e+08" > 8.2899e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4899e+08" > 5.6677e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4669e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4257e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6316e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9843e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4770e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4287e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0398e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2156e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7758e+01 </func>
</region>
</regions>
<internal rank="480" log_i="1723713849.509341" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="481" mpi_size="768" stamp_init="1723713791.114867" stamp_final="1723713849.507754" username="apac4" allocationname="unknown" flags="0" pid="778042" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83929e+01" utime="5.04846e+01" stime="6.64706e+00" mtime="3.22916e+01" gflop="0.00000e+00" gbyte="3.78071e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22916e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001114de5611141114c5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83466e+01" utime="5.04533e+01" stime="6.63892e+00" mtime="3.22916e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22916e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4920e+08" > 4.5069e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4916e+08" > 3.1943e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3552e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4207e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9550e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0723e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4777e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4237e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2187e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2140e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9253e+01 </func>
</region>
</regions>
<internal rank="481" log_i="1723713849.507754" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="482" mpi_size="768" stamp_init="1723713791.114793" stamp_final="1723713849.501557" username="apac4" allocationname="unknown" flags="0" pid="778043" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83868e+01" utime="4.87482e+01" stime="7.24054e+00" mtime="3.18205e+01" gflop="0.00000e+00" gbyte="3.76644e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18205e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f3141655f314f314b9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83441e+01" utime="4.87212e+01" stime="7.22915e+00" mtime="3.18205e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18205e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4823e+08" > 6.1840e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4920e+08" > 3.5545e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2423e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4172e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2478e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0863e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4766e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4340e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3784e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2162e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8724e+01 </func>
</region>
</regions>
<internal rank="482" log_i="1723713849.501557" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="483" mpi_size="768" stamp_init="1723713791.118205" stamp_final="1723713849.515066" username="apac4" allocationname="unknown" flags="0" pid="778044" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83969e+01" utime="5.02903e+01" stime="6.83560e+00" mtime="3.24035e+01" gflop="0.00000e+00" gbyte="3.76881e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24035e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000aa15a91508" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83486e+01" utime="5.02548e+01" stime="6.83231e+00" mtime="3.24035e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24035e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4835e+08" > 4.4003e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4823e+08" > 2.8610e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7590e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4113e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3603e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0865e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4771e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4263e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3498e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2111e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8984e+01 </func>
</region>
</regions>
<internal rank="483" log_i="1723713849.515066" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="484" mpi_size="768" stamp_init="1723713791.121167" stamp_final="1723713849.507574" username="apac4" allocationname="unknown" flags="0" pid="778045" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83864e+01" utime="4.87538e+01" stime="7.28596e+00" mtime="3.18066e+01" gflop="0.00000e+00" gbyte="3.77956e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18066e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4161418141914965519141914ff" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83318e+01" utime="4.87228e+01" stime="7.27684e+00" mtime="3.18066e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18066e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4847e+08" > 6.0312e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4862e+08" > 4.2280e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7551e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4148e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0273e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0001e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4771e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4252e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2187e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2171e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8209e+01 </func>
</region>
</regions>
<internal rank="484" log_i="1723713849.507574" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="485" mpi_size="768" stamp_init="1723713791.125554" stamp_final="1723713849.503604" username="apac4" allocationname="unknown" flags="0" pid="778046" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83780e+01" utime="5.05603e+01" stime="6.59113e+00" mtime="3.21430e+01" gflop="0.00000e+00" gbyte="3.77453e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21430e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf47515761577156855771577152f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83018e+01" utime="5.05278e+01" stime="6.58283e+00" mtime="3.21430e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21430e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4933e+08" > 4.3745e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4852e+08" > 2.8901e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0083e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4161e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1156e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4760e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4408e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1090e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2128e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9468e+01 </func>
</region>
</regions>
<internal rank="485" log_i="1723713849.503604" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="486" mpi_size="768" stamp_init="1723713791.127544" stamp_final="1723713849.501973" username="apac4" allocationname="unknown" flags="0" pid="778047" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83744e+01" utime="4.87415e+01" stime="7.31052e+00" mtime="3.23012e+01" gflop="0.00000e+00" gbyte="3.76446e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23012e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000cb14cb145b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82932e+01" utime="4.87132e+01" stime="7.29793e+00" mtime="3.23012e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23012e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4666e+08" > 5.3276e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4928e+08" > 3.5675e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7245e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4120e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1026e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0000e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4764e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4352e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0494e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2168e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8813e+01 </func>
</region>
</regions>
<internal rank="486" log_i="1723713849.501973" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="487" mpi_size="768" stamp_init="1723713791.136514" stamp_final="1723713849.510852" username="apac4" allocationname="unknown" flags="0" pid="778048" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83743e+01" utime="5.03350e+01" stime="6.78218e+00" mtime="3.24359e+01" gflop="0.00000e+00" gbyte="3.77235e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24359e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003014301465" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82861e+01" utime="5.03018e+01" stime="6.77420e+00" mtime="3.24359e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24359e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4932e+08" > 4.3996e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4792e+08" > 2.5785e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4673e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4072e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9998e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4760e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4415e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3998e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2115e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9323e+01 </func>
</region>
</regions>
<internal rank="487" log_i="1723713849.510852" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="488" mpi_size="768" stamp_init="1723713791.134655" stamp_final="1723713849.513347" username="apac4" allocationname="unknown" flags="0" pid="778049" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83787e+01" utime="4.74129e+01" stime="7.84428e+00" mtime="3.19011e+01" gflop="0.00000e+00" gbyte="3.77857e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19011e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c7148255c714c714d1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82922e+01" utime="4.73827e+01" stime="7.83193e+00" mtime="3.19011e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19011e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4733e+08" > 5.1917e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4955e+08" > 5.0102e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.3799e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3267e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.0874e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.0217e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4751e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4480e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3808e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2162e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7743e+01 </func>
</region>
</regions>
<internal rank="488" log_i="1723713849.513347" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="489" mpi_size="768" stamp_init="1723713791.136508" stamp_final="1723713849.503593" username="apac4" allocationname="unknown" flags="0" pid="778050" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83671e+01" utime="5.02053e+01" stime="6.92882e+00" mtime="3.25674e+01" gflop="0.00000e+00" gbyte="3.75786e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25674e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82773e+01" utime="5.01691e+01" stime="6.92393e+00" mtime="3.25674e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25674e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4900e+08" > 3.5612e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4805e+08" > 2.9227e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0001e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4249e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1432e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4741e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4626e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.6693e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2122e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8970e+01 </func>
</region>
</regions>
<internal rank="489" log_i="1723713849.503593" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="490" mpi_size="768" stamp_init="1723713791.137225" stamp_final="1723713849.498806" username="apac4" allocationname="unknown" flags="0" pid="778051" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83616e+01" utime="4.82595e+01" stime="7.56345e+00" mtime="3.25445e+01" gflop="0.00000e+00" gbyte="3.77186e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25445e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d115d0154a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82750e+01" utime="4.82263e+01" stime="7.55544e+00" mtime="3.25445e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25445e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4759e+08" > 4.7313e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4867e+08" > 4.2727e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.4067e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4301e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.9571e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0701e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4752e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4474e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1686e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2154e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8406e+01 </func>
</region>
</regions>
<internal rank="490" log_i="1723713849.498806" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="491" mpi_size="768" stamp_init="1723713791.145062" stamp_final="1723713849.512927" username="apac4" allocationname="unknown" flags="0" pid="778052" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83679e+01" utime="5.03269e+01" stime="6.58445e+00" mtime="3.25109e+01" gflop="0.00000e+00" gbyte="3.77716e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25109e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005115511516" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82775e+01" utime="5.02951e+01" stime="6.57483e+00" mtime="3.25109e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25109e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4898e+08" > 3.5161e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4734e+08" > 3.0435e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4867e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4177e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0700e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4752e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4457e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.7503e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2114e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9445e+01 </func>
</region>
</regions>
<internal rank="491" log_i="1723713849.512927" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="492" mpi_size="768" stamp_init="1723713791.143535" stamp_final="1723713849.512866" username="apac4" allocationname="unknown" flags="0" pid="778053" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83693e+01" utime="4.73564e+01" stime="7.93608e+00" mtime="3.22822e+01" gflop="0.00000e+00" gbyte="3.77983e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22822e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005b1563555b155b1552" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82783e+01" utime="4.73276e+01" stime="7.92221e+00" mtime="3.22822e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22822e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4858e+08" > 5.0768e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4830e+08" > 3.9526e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0066e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4126e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5524e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1022e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4748e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4509e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1400e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2149e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8530e+01 </func>
</region>
</regions>
<internal rank="492" log_i="1723713849.512866" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="493" mpi_size="768" stamp_init="1723713791.145374" stamp_final="1723713849.510862" username="apac4" allocationname="unknown" flags="0" pid="778054" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83655e+01" utime="5.03304e+01" stime="6.74976e+00" mtime="3.27170e+01" gflop="0.00000e+00" gbyte="3.74928e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27170e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005314eb555314521471" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82761e+01" utime="5.02979e+01" stime="6.74157e+00" mtime="3.27170e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27170e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4875e+08" > 3.5051e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4858e+08" > 2.9518e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4840e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4233e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7895e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1326e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4742e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4559e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5882e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2133e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9644e+01 </func>
</region>
</regions>
<internal rank="493" log_i="1723713849.510862" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="494" mpi_size="768" stamp_init="1723713791.148190" stamp_final="1723713849.502276" username="apac4" allocationname="unknown" flags="0" pid="778055" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83541e+01" utime="4.86575e+01" stime="7.41252e+00" mtime="3.26263e+01" gflop="0.00000e+00" gbyte="3.74641e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26263e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b314b255b314b21472" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82654e+01" utime="4.86267e+01" stime="7.40227e+00" mtime="3.26263e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26263e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4858e+08" > 4.8488e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5035e+08" > 4.1886e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0052e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4235e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0800e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1023e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4744e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4564e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1519e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2146e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8888e+01 </func>
</region>
</regions>
<internal rank="494" log_i="1723713849.502276" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="495" mpi_size="768" stamp_init="1723713791.150147" stamp_final="1723713849.502814" username="apac4" allocationname="unknown" flags="0" pid="778056" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83527e+01" utime="5.01964e+01" stime="6.68911e+00" mtime="3.23607e+01" gflop="0.00000e+00" gbyte="3.76961e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23607e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4bd15bf15c015b355c015bf151a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82666e+01" utime="5.01636e+01" stime="6.68178e+00" mtime="3.23607e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23607e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4788e+08" > 3.9169e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4870e+08" > 3.2002e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6859e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4085e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1420e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4746e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4563e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.7909e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2116e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9056e+01 </func>
</region>
</regions>
<internal rank="495" log_i="1723713849.502814" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="496" mpi_size="768" stamp_init="1723713791.159593" stamp_final="1723713849.509157" username="apac4" allocationname="unknown" flags="0" pid="778057" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83496e+01" utime="4.68394e+01" stime="8.06426e+00" mtime="3.14410e+01" gflop="0.00000e+00" gbyte="3.77171e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.14410e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82675e+01" utime="4.68090e+01" stime="8.05406e+00" mtime="3.14410e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.14410e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4943e+08" > 6.7926e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4869e+08" > 4.4377e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9906e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4191e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4509e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9869e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4743e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4541e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2091e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2148e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7535e+01 </func>
</region>
</regions>
<internal rank="496" log_i="1723713849.509157" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="497" mpi_size="768" stamp_init="1723713791.162265" stamp_final="1723713849.503831" username="apac4" allocationname="unknown" flags="0" pid="778058" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83416e+01" utime="5.01509e+01" stime="6.99201e+00" mtime="3.24090e+01" gflop="0.00000e+00" gbyte="3.77083e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24090e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48b148d148e1435558e148d14a9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82584e+01" utime="5.01163e+01" stime="6.98710e+00" mtime="3.24090e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24090e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4913e+08" > 4.6308e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4939e+08" > 2.7378e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0780e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4118e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0487e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4736e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4657e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3283e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2124e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8651e+01 </func>
</region>
</regions>
<internal rank="497" log_i="1723713849.503831" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="498" mpi_size="768" stamp_init="1723713791.159585" stamp_final="1723713849.512010" username="apac4" allocationname="unknown" flags="0" pid="778059" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83524e+01" utime="4.76142e+01" stime="7.96109e+00" mtime="3.21690e+01" gflop="0.00000e+00" gbyte="3.77098e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21690e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c914ca14cb14eb55cb14cb14ca" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82703e+01" utime="4.75836e+01" stime="7.95134e+00" mtime="3.21690e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21690e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5085e+08" > 6.4108e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5007e+08" > 3.7908e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.4434e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4195e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0109e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1332e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4732e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4656e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0494e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2139e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7842e+01 </func>
</region>
</regions>
<internal rank="498" log_i="1723713849.512010" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="499" mpi_size="768" stamp_init="1723713791.162258" stamp_final="1723713849.507482" username="apac4" allocationname="unknown" flags="0" pid="778060" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83452e+01" utime="4.98945e+01" stime="7.15666e+00" mtime="3.23455e+01" gflop="0.00000e+00" gbyte="3.75732e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23455e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82619e+01" utime="4.98624e+01" stime="7.14805e+00" mtime="3.23455e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23455e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4883e+08" > 4.6617e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5003e+08" > 3.2766e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8272e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4109e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9802e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1418e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4725e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4779e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5095e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2112e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8821e+01 </func>
</region>
</regions>
<internal rank="499" log_i="1723713849.507482" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="500" mpi_size="768" stamp_init="1723713791.164619" stamp_final="1723713849.504785" username="apac4" allocationname="unknown" flags="0" pid="778061" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83402e+01" utime="4.81030e+01" stime="7.78410e+00" mtime="3.24558e+01" gflop="0.00000e+00" gbyte="3.76602e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24558e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005b14a1555b145b1456" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82617e+01" utime="4.80668e+01" stime="7.78113e+00" mtime="3.24558e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24558e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4921e+08" > 6.8500e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4851e+08" > 5.3439e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.6003e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4136e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2016e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0970e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4732e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4697e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3784e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2150e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7921e+01 </func>
</region>
</regions>
<internal rank="500" log_i="1723713849.504785" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="501" mpi_size="768" stamp_init="1723713791.165678" stamp_final="1723713849.505937" username="apac4" allocationname="unknown" flags="0" pid="778062" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83403e+01" utime="5.00644e+01" stime="7.02238e+00" mtime="3.25709e+01" gflop="0.00000e+00" gbyte="3.77876e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25709e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82601e+01" utime="5.00373e+01" stime="7.00986e+00" mtime="3.25709e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25709e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 4.9125e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5024e+08" > 3.1544e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5094e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4295e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2187e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1433e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4716e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4874e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1614e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2111e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9322e+01 </func>
</region>
</regions>
<internal rank="501" log_i="1723713849.505937" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="502" mpi_size="768" stamp_init="1723713791.167979" stamp_final="1723713849.513329" username="apac4" allocationname="unknown" flags="0" pid="778063" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83454e+01" utime="4.85412e+01" stime="7.51792e+00" mtime="3.19370e+01" gflop="0.00000e+00" gbyte="3.77686e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19370e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009c1524559c159b153f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82680e+01" utime="4.85093e+01" stime="7.51053e+00" mtime="3.19370e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19370e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4930e+08" > 5.4796e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4900e+08" > 3.6049e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9560e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4160e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.1008e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0970e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4719e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4827e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2091e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2129e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8199e+01 </func>
</region>
</regions>
<internal rank="502" log_i="1723713849.513329" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="503" mpi_size="768" stamp_init="1723713791.170626" stamp_final="1723713849.513683" username="apac4" allocationname="unknown" flags="0" pid="778064" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="5.83431e+01" utime="4.99269e+01" stime="7.21384e+00" mtime="3.24559e+01" gflop="0.00000e+00" gbyte="3.77647e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24559e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001c1465561c141c1491" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82659e+01" utime="4.98947e+01" stime="7.20723e+00" mtime="3.24559e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24559e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4945e+08" > 4.7351e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4831e+08" > 3.1760e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1785e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4115e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1125e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4720e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4824e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.6693e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2097e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8577e+01 </func>
</region>
</regions>
<internal rank="503" log_i="1723713849.513683" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="504" mpi_size="768" stamp_init="1723713791.135327" stamp_final="1723713849.505914" username="apac4" allocationname="unknown" flags="0" pid="784612" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83706e+01" utime="4.07016e+01" stime="1.27049e+01" mtime="3.18283e+01" gflop="0.00000e+00" gbyte="3.85418e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18283e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000075147514af" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82578e+01" utime="4.06702e+01" stime="1.26945e+01" mtime="3.18283e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18283e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4752e+08" > 9.4600e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4711e+08" > 5.6365e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2579e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4841e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6898e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0392e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4720e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4843e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0208e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2201e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7369e+01 </func>
</region>
</regions>
<internal rank="504" log_i="1723713849.505914" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="505" mpi_size="768" stamp_init="1723713791.135348" stamp_final="1723713849.511672" username="apac4" allocationname="unknown" flags="0" pid="784613" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83763e+01" utime="4.92786e+01" stime="6.85184e+00" mtime="3.21831e+01" gflop="0.00000e+00" gbyte="3.77991e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21831e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82573e+01" utime="4.92441e+01" stime="6.84300e+00" mtime="3.21831e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21831e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4892e+08" > 5.6250e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4667e+08" > 3.0266e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7696e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4986e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9789e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0392e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4725e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4781e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0398e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1614e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8611e+01 </func>
</region>
</regions>
<internal rank="505" log_i="1723713849.511672" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="506" mpi_size="768" stamp_init="1723713791.135344" stamp_final="1723713849.516914" username="apac4" allocationname="unknown" flags="0" pid="784614" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83816e+01" utime="4.81034e+01" stime="7.75744e+00" mtime="3.21128e+01" gflop="0.00000e+00" gbyte="3.76225e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21128e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a615a715a8151d55a815a8152a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82736e+01" utime="4.80665e+01" stime="7.75221e+00" mtime="3.21128e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21128e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4781e+08" > 6.7511e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4827e+08" > 4.4487e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.5127e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3823e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0705e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0780e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4721e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4821e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5191e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2188e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7717e+01 </func>
</region>
</regions>
<internal rank="506" log_i="1723713849.516914" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="507" mpi_size="768" stamp_init="1723713791.135316" stamp_final="1723713849.510321" username="apac4" allocationname="unknown" flags="0" pid="784615" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83750e+01" utime="5.00750e+01" stime="7.06821e+00" mtime="3.25573e+01" gflop="0.00000e+00" gbyte="3.76358e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25573e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82643e+01" utime="5.00426e+01" stime="7.05918e+00" mtime="3.25573e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25573e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4838e+08" > 5.4018e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4791e+08" > 2.7394e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9953e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4029e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0868e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4715e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4840e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.6788e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2109e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8810e+01 </func>
</region>
</regions>
<internal rank="507" log_i="1723713849.510321" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="508" mpi_size="768" stamp_init="1723713791.135365" stamp_final="1723713849.513993" username="apac4" allocationname="unknown" flags="0" pid="784616" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83786e+01" utime="4.73494e+01" stime="7.95463e+00" mtime="3.16497e+01" gflop="0.00000e+00" gbyte="3.75973e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16497e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002f14c6552f142f14f5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82670e+01" utime="4.73137e+01" stime="7.94912e+00" mtime="3.16497e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16497e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4887e+08" > 8.3862e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4932e+08" > 4.3986e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7538e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3984e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.3930e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0769e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4708e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4898e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0208e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2194e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7834e+01 </func>
</region>
</regions>
<internal rank="508" log_i="1723713849.513993" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="509" mpi_size="768" stamp_init="1723713791.135318" stamp_final="1723713849.504000" username="apac4" allocationname="unknown" flags="0" pid="784617" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83687e+01" utime="5.00572e+01" stime="7.05954e+00" mtime="3.24748e+01" gflop="0.00000e+00" gbyte="3.76606e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24748e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82571e+01" utime="5.00233e+01" stime="7.05127e+00" mtime="3.24748e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24748e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4828e+08" > 5.4429e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4829e+08" > 2.8187e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4086e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4145e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0805e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4710e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4897e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0303e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2112e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9299e+01 </func>
</region>
</regions>
<internal rank="509" log_i="1723713849.504000" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="510" mpi_size="768" stamp_init="1723713791.135329" stamp_final="1723713849.505620" username="apac4" allocationname="unknown" flags="0" pid="784618" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83703e+01" utime="4.67966e+01" stime="8.23785e+00" mtime="3.15597e+01" gflop="0.00000e+00" gbyte="3.76160e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15597e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002d1423562d142c14e3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82584e+01" utime="4.67658e+01" stime="8.22645e+00" mtime="3.15597e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15597e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4821e+08" > 8.1662e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4756e+08" > 5.0245e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7395e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4026e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.1975e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0770e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4709e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4900e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0780e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2186e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7770e+01 </func>
</region>
</regions>
<internal rank="510" log_i="1723713849.505620" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="511" mpi_size="768" stamp_init="1723713791.135379" stamp_final="1723713849.511016" username="apac4" allocationname="unknown" flags="0" pid="784619" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83756e+01" utime="4.98925e+01" stime="7.21516e+00" mtime="3.24957e+01" gflop="0.00000e+00" gbyte="3.76656e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24957e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82661e+01" utime="4.98640e+01" stime="7.20160e+00" mtime="3.24957e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24957e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4769e+08" > 5.5007e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4954e+08" > 2.7621e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1175e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4201e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3842e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0884e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4709e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4947e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9516e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2100e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8599e+01 </func>
</region>
</regions>
<internal rank="511" log_i="1723713849.511016" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="512" mpi_size="768" stamp_init="1723713791.144687" stamp_final="1723713849.516267" username="apac4" allocationname="unknown" flags="0" pid="784620" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83716e+01" utime="4.73823e+01" stime="7.90376e+00" mtime="3.20450e+01" gflop="0.00000e+00" gbyte="3.76625e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20450e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004a154a153a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82543e+01" utime="4.73482e+01" stime="7.89625e+00" mtime="3.20450e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20450e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4824e+08" > 6.5217e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4901e+08" > 4.6484e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5396e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4446e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.9887e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4667e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5335e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4189e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2191e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8620e+01 </func>
</region>
</regions>
<internal rank="512" log_i="1723713849.516267" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="513" mpi_size="768" stamp_init="1723713791.143774" stamp_final="1723713849.514360" username="apac4" allocationname="unknown" flags="0" pid="784621" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83706e+01" utime="5.00833e+01" stime="6.87465e+00" mtime="3.26768e+01" gflop="0.00000e+00" gbyte="3.76472e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26768e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82541e+01" utime="5.00508e+01" stime="6.86557e+00" mtime="3.26768e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26768e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4926e+08" > 4.2122e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4941e+08" > 3.6095e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6383e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4187e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0788e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4661e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5425e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.7694e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2105e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9382e+01 </func>
</region>
</regions>
<internal rank="513" log_i="1723713849.514360" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="514" mpi_size="768" stamp_init="1723713791.143592" stamp_final="1723713849.504051" username="apac4" allocationname="unknown" flags="0" pid="784622" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83605e+01" utime="4.73548e+01" stime="7.81681e+00" mtime="3.20609e+01" gflop="0.00000e+00" gbyte="3.76774e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20609e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82435e+01" utime="4.73192e+01" stime="7.80995e+00" mtime="3.20609e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20609e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4821e+08" > 6.8288e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4860e+08" > 5.1975e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4431e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4142e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2708e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0839e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4666e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5370e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0899e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2185e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8687e+01 </func>
</region>
</regions>
<internal rank="514" log_i="1723713849.504051" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="515" mpi_size="768" stamp_init="1723713791.143590" stamp_final="1723713849.511097" username="apac4" allocationname="unknown" flags="0" pid="784623" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83675e+01" utime="5.05236e+01" stime="6.59633e+00" mtime="3.23481e+01" gflop="0.00000e+00" gbyte="3.76156e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23481e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a915ab15ac155c55ac15ac1550" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82506e+01" utime="5.04941e+01" stime="6.58412e+00" mtime="3.23481e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23481e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4835e+08" > 4.3290e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4887e+08" > 3.6734e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0910e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4376e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0041e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0851e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4659e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5411e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1590e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1614e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9569e+01 </func>
</region>
</regions>
<internal rank="515" log_i="1723713849.511097" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="516" mpi_size="768" stamp_init="1723713791.143594" stamp_final="1723713849.502299" username="apac4" allocationname="unknown" flags="0" pid="784624" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83587e+01" utime="4.64351e+01" stime="7.83817e+00" mtime="3.20811e+01" gflop="0.00000e+00" gbyte="3.74592e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20811e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49c149d149e149e559e149e145c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82404e+01" utime="4.64011e+01" stime="7.83054e+00" mtime="3.20811e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20811e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5138e+08" > 6.5133e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4940e+08" > 4.8859e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4346e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4931e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5539e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0397e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4658e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5419e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9087e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2186e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8737e+01 </func>
</region>
</regions>
<internal rank="516" log_i="1723713849.502299" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="517" mpi_size="768" stamp_init="1723713791.143597" stamp_final="1723713849.510141" username="apac4" allocationname="unknown" flags="0" pid="784625" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83665e+01" utime="5.01405e+01" stime="6.94741e+00" mtime="3.29772e+01" gflop="0.00000e+00" gbyte="3.74870e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.29772e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000040144014cf" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82493e+01" utime="5.01095e+01" stime="6.93662e+00" mtime="3.29772e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.29772e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4938e+08" > 4.3039e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4973e+08" > 3.2097e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5142e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4258e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0822e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4658e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5460e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5214e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2102e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9794e+01 </func>
</region>
</regions>
<internal rank="517" log_i="1723713849.510141" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="518" mpi_size="768" stamp_init="1723713791.143589" stamp_final="1723713849.511538" username="apac4" allocationname="unknown" flags="0" pid="784626" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83679e+01" utime="4.73704e+01" stime="7.44225e+00" mtime="3.24012e+01" gflop="0.00000e+00" gbyte="3.77823e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24012e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b014b214b314d856b314b314e2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82497e+01" utime="4.73384e+01" stime="7.43219e+00" mtime="3.24012e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24012e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4964e+08" > 5.3598e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4822e+08" > 4.2749e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5872e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4789e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0456e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0397e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4650e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5539e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4714e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2181e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9041e+01 </func>
</region>
</regions>
<internal rank="518" log_i="1723713849.511538" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="519" mpi_size="768" stamp_init="1723713791.147542" stamp_final="1723713849.503601" username="apac4" allocationname="unknown" flags="0" pid="784627" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83561e+01" utime="4.95292e+01" stime="6.54432e+00" mtime="3.26127e+01" gflop="0.00000e+00" gbyte="3.77846e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26127e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000096149614d7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82403e+01" utime="4.94983e+01" stime="6.53323e+00" mtime="3.26127e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26127e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4800e+08" > 4.3248e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4930e+08" > 3.4047e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1867e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4603e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0397e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4650e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5542e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8515e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2097e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9787e+01 </func>
</region>
</regions>
<internal rank="519" log_i="1723713849.503601" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="520" mpi_size="768" stamp_init="1723713791.145185" stamp_final="1723713849.506016" username="apac4" allocationname="unknown" flags="0" pid="784628" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83608e+01" utime="4.75970e+01" stime="7.86368e+00" mtime="3.17047e+01" gflop="0.00000e+00" gbyte="3.77884e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17047e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82428e+01" utime="4.75658e+01" stime="7.85258e+00" mtime="3.17047e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17047e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4827e+08" > 7.2334e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4961e+08" > 5.5525e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6218e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4072e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0487e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0483e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4644e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5569e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9707e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2157e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8117e+01 </func>
</region>
</regions>
<internal rank="520" log_i="1723713849.506016" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="521" mpi_size="768" stamp_init="1723713791.147223" stamp_final="1723713849.515497" username="apac4" allocationname="unknown" flags="0" pid="784629" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83683e+01" utime="4.94171e+01" stime="6.70305e+00" mtime="3.27009e+01" gflop="0.00000e+00" gbyte="3.77434e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27009e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003e1411553e1439145a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82511e+01" utime="4.93842e+01" stime="6.69425e+00" mtime="3.27009e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27009e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4989e+08" > 4.8727e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4808e+08" > 3.1582e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6193e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4841e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0399e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4640e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5608e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4380e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2100e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9366e+01 </func>
</region>
</regions>
<internal rank="521" log_i="1723713849.515497" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="522" mpi_size="768" stamp_init="1723713791.149870" stamp_final="1723713849.502191" username="apac4" allocationname="unknown" flags="0" pid="784630" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83523e+01" utime="4.78078e+01" stime="7.97662e+00" mtime="3.25074e+01" gflop="0.00000e+00" gbyte="3.76904e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25074e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a714a814a9146555a914a9148d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82371e+01" utime="4.77752e+01" stime="7.96792e+00" mtime="3.25074e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25074e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4928e+08" > 6.7656e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5003e+08" > 4.4441e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2907e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4123e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0657e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0825e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4640e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5641e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9683e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2146e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8301e+01 </func>
</region>
</regions>
<internal rank="522" log_i="1723713849.502191" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="523" mpi_size="768" stamp_init="1723713791.152636" stamp_final="1723713849.506001" username="apac4" allocationname="unknown" flags="0" pid="784631" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83534e+01" utime="5.01777e+01" stime="6.80540e+00" mtime="3.23700e+01" gflop="0.00000e+00" gbyte="3.76072e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23700e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf47314741475141d557514751472" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82418e+01" utime="5.01439e+01" stime="6.79851e+00" mtime="3.23700e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23700e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4992e+08" > 4.8924e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4786e+08" > 3.1003e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6261e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4091e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0827e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4634e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5708e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5119e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2098e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9033e+01 </func>
</region>
</regions>
<internal rank="523" log_i="1723713849.506001" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="524" mpi_size="768" stamp_init="1723713791.154548" stamp_final="1723713849.508934" username="apac4" allocationname="unknown" flags="0" pid="784632" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83544e+01" utime="4.82184e+01" stime="7.66530e+00" mtime="3.22490e+01" gflop="0.00000e+00" gbyte="3.76904e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22490e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000e8152c55e815e81531" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82428e+01" utime="4.81858e+01" stime="7.65678e+00" mtime="3.22490e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22490e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4728e+08" > 6.1114e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5062e+08" > 3.9047e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8077e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4195e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0582e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0524e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4635e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5686e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0494e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2146e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8590e+01 </func>
</region>
</regions>
<internal rank="524" log_i="1723713849.508934" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="525" mpi_size="768" stamp_init="1723713791.162531" stamp_final="1723713849.504536" username="apac4" allocationname="unknown" flags="0" pid="784633" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83420e+01" utime="5.05424e+01" stime="6.64090e+00" mtime="3.21819e+01" gflop="0.00000e+00" gbyte="3.77316e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21819e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000bc158455bc15bc1518" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82376e+01" utime="5.05112e+01" stime="6.63122e+00" mtime="3.21819e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21819e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4776e+08" > 4.7371e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4904e+08" > 3.2686e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1938e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4276e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0524e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4634e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5705e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3498e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2096e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9276e+01 </func>
</region>
</regions>
<internal rank="525" log_i="1723713849.504536" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="526" mpi_size="768" stamp_init="1723713791.159480" stamp_final="1723713849.508983" username="apac4" allocationname="unknown" flags="0" pid="784634" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83495e+01" utime="4.81238e+01" stime="7.63765e+00" mtime="3.17798e+01" gflop="0.00000e+00" gbyte="3.76385e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17798e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82412e+01" utime="4.80881e+01" stime="7.63286e+00" mtime="3.17798e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17798e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5021e+08" > 6.3553e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4847e+08" > 4.4799e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5241e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4269e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.4690e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0765e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4626e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5783e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8610e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2143e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8368e+01 </func>
</region>
</regions>
<internal rank="526" log_i="1723713849.508983" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="527" mpi_size="768" stamp_init="1723713791.161980" stamp_final="1723713849.503093" username="apac4" allocationname="unknown" flags="0" pid="784635" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="5.83411e+01" utime="4.98107e+01" stime="6.99505e+00" mtime="3.24066e+01" gflop="0.00000e+00" gbyte="3.76343e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24066e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007b15761519" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82375e+01" utime="4.97852e+01" stime="6.98010e+00" mtime="3.24066e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24066e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4986e+08" > 4.8189e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4985e+08" > 3.4018e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5646e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4072e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3842e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0765e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4629e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5725e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3188e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2101e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9139e+01 </func>
</region>
</regions>
<internal rank="527" log_i="1723713849.503093" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="528" mpi_size="768" stamp_init="1723713791.112307" stamp_final="1723713849.517217" username="apac4" allocationname="unknown" flags="0" pid="3145750" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.84049e+01" utime="4.24541e+01" stime="1.29401e+01" mtime="3.26228e+01" gflop="0.00000e+00" gbyte="3.86227e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26228e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000551454149b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83572e+01" utime="4.24177e+01" stime="1.29374e+01" mtime="3.26228e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26228e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4741e+08" > 7.3479e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4698e+08" > 4.0162e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2939e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4207e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.7859e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1087e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4613e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5873e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.3985e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2255e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8343e+01 </func>
</region>
</regions>
<internal rank="528" log_i="1723713849.517217" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="529" mpi_size="768" stamp_init="1723713791.112356" stamp_final="1723713849.509694" username="apac4" allocationname="unknown" flags="0" pid="3145751" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83973e+01" utime="5.00005e+01" stime="6.85510e+00" mtime="3.22793e+01" gflop="0.00000e+00" gbyte="3.78208e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22793e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ba14bc14bd14a956bd14bd14c4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83441e+01" utime="4.99696e+01" stime="6.84564e+00" mtime="3.22793e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22793e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4816e+08" > 4.6202e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4850e+08" > 3.1836e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1300e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4148e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8849e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1120e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4625e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5760e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.4104e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2308e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9457e+01 </func>
</region>
</regions>
<internal rank="529" log_i="1723713849.509694" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="530" mpi_size="768" stamp_init="1723713791.114453" stamp_final="1723713849.508731" username="apac4" allocationname="unknown" flags="0" pid="3145752" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83943e+01" utime="4.73890e+01" stime="7.84558e+00" mtime="3.18769e+01" gflop="0.00000e+00" gbyte="3.77678e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18769e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005914b55559145914f2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83422e+01" utime="4.73629e+01" stime="7.83308e+00" mtime="3.18769e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18769e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.5104e+08" > 7.7714e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4741e+08" > 6.7037e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3451e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4155e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.7311e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1310e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4617e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5864e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9302e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2262e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8477e+01 </func>
</region>
</regions>
<internal rank="530" log_i="1723713849.508731" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="531" mpi_size="768" stamp_init="1723713791.117229" stamp_final="1723713849.511528" username="apac4" allocationname="unknown" flags="0" pid="3145753" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83943e+01" utime="4.99545e+01" stime="7.13077e+00" mtime="3.27807e+01" gflop="0.00000e+00" gbyte="3.77846e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27807e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83335e+01" utime="4.99220e+01" stime="7.12284e+00" mtime="3.27807e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27807e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4720e+08" > 4.6499e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5059e+08" > 2.7833e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8399e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4248e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1309e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4627e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5782e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9898e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2269e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9237e+01 </func>
</region>
</regions>
<internal rank="531" log_i="1723713849.511528" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="532" mpi_size="768" stamp_init="1723713791.121548" stamp_final="1723713849.512533" username="apac4" allocationname="unknown" flags="0" pid="3145754" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83910e+01" utime="4.72640e+01" stime="7.97125e+00" mtime="3.19716e+01" gflop="0.00000e+00" gbyte="3.77819e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19716e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b5153d55b515b5151b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83196e+01" utime="4.72318e+01" stime="7.96281e+00" mtime="3.19716e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19716e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4993e+08" > 7.7497e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4937e+08" > 4.5498e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4918e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4227e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.7248e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1311e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4621e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5845e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.4319e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2277e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8450e+01 </func>
</region>
</regions>
<internal rank="532" log_i="1723713849.512533" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="533" mpi_size="768" stamp_init="1723713791.122697" stamp_final="1723713849.515213" username="apac4" allocationname="unknown" flags="0" pid="3145755" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83925e+01" utime="5.01175e+01" stime="7.00016e+00" mtime="3.28879e+01" gflop="0.00000e+00" gbyte="3.76198e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28879e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d414d41492" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83169e+01" utime="5.00818e+01" stime="6.99531e+00" mtime="3.28879e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28879e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4841e+08" > 4.4488e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5062e+08" > 2.9759e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3737e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4163e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1308e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4619e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5861e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5892e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2251e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9837e+01 </func>
</region>
</regions>
<internal rank="533" log_i="1723713849.515213" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="534" mpi_size="768" stamp_init="1723713791.125913" stamp_final="1723713849.508229" username="apac4" allocationname="unknown" flags="0" pid="3145756" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83823e+01" utime="4.84577e+01" stime="7.66429e+00" mtime="3.22511e+01" gflop="0.00000e+00" gbyte="3.76514e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22511e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ec14ec14a6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83006e+01" utime="4.84195e+01" stime="7.66187e+00" mtime="3.22511e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22511e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4692e+08" > 5.5291e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4909e+08" > 3.7090e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7241e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4179e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.2021e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1450e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4607e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5946e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6202e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1776e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8727e+01 </func>
</region>
</regions>
<internal rank="534" log_i="1723713849.508229" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="535" mpi_size="768" stamp_init="1723713791.127494" stamp_final="1723713849.516555" username="apac4" allocationname="unknown" flags="0" pid="3145757" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83891e+01" utime="5.02479e+01" stime="6.89798e+00" mtime="3.29225e+01" gflop="0.00000e+00" gbyte="3.78010e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.29225e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002c14dc562c142b14b6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83126e+01" utime="5.02153e+01" stime="6.88904e+00" mtime="3.29225e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.29225e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4917e+08" > 4.4181e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4611e+08" > 2.8655e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7193e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4180e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1450e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4604e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5973e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7704e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1686e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9528e+01 </func>
</region>
</regions>
<internal rank="535" log_i="1723713849.516555" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="536" mpi_size="768" stamp_init="1723713791.134865" stamp_final="1723713849.502001" username="apac4" allocationname="unknown" flags="0" pid="3145758" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83671e+01" utime="4.81239e+01" stime="7.62848e+00" mtime="3.25940e+01" gflop="0.00000e+00" gbyte="3.78025e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25940e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82826e+01" utime="4.80895e+01" stime="7.62084e+00" mtime="3.25940e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25940e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4974e+08" > 4.8395e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4839e+08" > 4.4924e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.3361e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4219e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5950e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1091e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4606e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5986e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8706e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2285e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8523e+01 </func>
</region>
</regions>
<internal rank="536" log_i="1723713849.502001" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="537" mpi_size="768" stamp_init="1723713791.137790" stamp_final="1723713849.504978" username="apac4" allocationname="unknown" flags="0" pid="3145759" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83672e+01" utime="5.05041e+01" stime="6.60182e+00" mtime="3.26914e+01" gflop="0.00000e+00" gbyte="3.76289e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26914e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82841e+01" utime="5.04717e+01" stime="6.59348e+00" mtime="3.26914e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26914e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4984e+08" > 3.6999e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5032e+08" > 2.4496e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3989e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4263e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1265e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4598e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6034e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6703e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2239e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9686e+01 </func>
</region>
</regions>
<internal rank="537" log_i="1723713849.504978" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="538" mpi_size="768" stamp_init="1723713791.135119" stamp_final="1723713849.512354" username="apac4" allocationname="unknown" flags="0" pid="3145760" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83772e+01" utime="4.79090e+01" stime="7.85656e+00" mtime="3.28455e+01" gflop="0.00000e+00" gbyte="3.77651e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28455e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49f14a014a2146f56a214a1145a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82932e+01" utime="4.78758e+01" stime="7.84843e+00" mtime="3.28455e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28455e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4876e+08" > 4.7534e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4842e+08" > 3.9996e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.4899e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4119e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.5102e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1262e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4600e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6028e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5010e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2311e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8643e+01 </func>
</region>
</regions>
<internal rank="538" log_i="1723713849.512354" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="539" mpi_size="768" stamp_init="1723713791.137454" stamp_final="1723713849.508296" username="apac4" allocationname="unknown" flags="0" pid="3145761" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83708e+01" utime="5.04350e+01" stime="6.70752e+00" mtime="3.25965e+01" gflop="0.00000e+00" gbyte="3.76858e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25965e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006f14d3566f146f14ca" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82874e+01" utime="5.04021e+01" stime="6.69883e+00" mtime="3.25965e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25965e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4882e+08" > 3.6352e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4855e+08" > 2.8285e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6864e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4176e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1417e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4587e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6208e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2997e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2238e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9313e+01 </func>
</region>
</regions>
<internal rank="539" log_i="1723713849.508296" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="540" mpi_size="768" stamp_init="1723713791.139902" stamp_final="1723713849.510196" username="apac4" allocationname="unknown" flags="0" pid="3145762" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83703e+01" utime="4.83975e+01" stime="7.44123e+00" mtime="3.20453e+01" gflop="0.00000e+00" gbyte="3.74371e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20453e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf47a157b157c15b2557c157c151b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82897e+01" utime="4.83623e+01" stime="7.43603e+00" mtime="3.20453e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20453e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1683e-05 </func>
<func name="MPI_Isend" count="127164" bytes="8.4834e+08" > 4.6758e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4961e+08" > 4.1092e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8974e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4227e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0336e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1266e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4591e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6129e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8896e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2290e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8420e+01 </func>
</region>
</regions>
<internal rank="540" log_i="1723713849.510196" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="541" mpi_size="768" stamp_init="1723713791.141637" stamp_final="1723713849.504772" username="apac4" allocationname="unknown" flags="0" pid="3145763" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83631e+01" utime="5.01090e+01" stime="6.90452e+00" mtime="3.30699e+01" gflop="0.00000e+00" gbyte="3.75908e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.30699e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000e614d556e614e6145a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82804e+01" utime="5.00756e+01" stime="6.89683e+00" mtime="3.30699e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.30699e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4951e+08" > 3.6086e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4896e+08" > 2.8843e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7698e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4242e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1880e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4592e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6127e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8110e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2251e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9694e+01 </func>
</region>
</regions>
<internal rank="541" log_i="1723713849.504772" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="542" mpi_size="768" stamp_init="1723713791.145255" stamp_final="1723713849.501639" username="apac4" allocationname="unknown" flags="0" pid="3145764" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83564e+01" utime="4.88603e+01" stime="7.33651e+00" mtime="3.23509e+01" gflop="0.00000e+00" gbyte="3.77132e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23509e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf491149214941463569414931484" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82763e+01" utime="4.88268e+01" stime="7.32953e+00" mtime="3.23509e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23509e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4938e+08" > 4.5208e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4760e+08" > 3.1186e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0878e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4274e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0288e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1271e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4581e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6223e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7704e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2307e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8557e+01 </func>
</region>
</regions>
<internal rank="542" log_i="1723713849.501639" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="543" mpi_size="768" stamp_init="1723713791.147571" stamp_final="1723713849.509949" username="apac4" allocationname="unknown" flags="0" pid="3145765" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83624e+01" utime="4.99380e+01" stime="6.96337e+00" mtime="3.30939e+01" gflop="0.00000e+00" gbyte="3.76942e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.30939e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c914c914ce" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82837e+01" utime="4.99012e+01" stime="6.95993e+00" mtime="3.30939e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.30939e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4647e+08" > 3.5173e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5008e+08" > 3.0452e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8163e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4282e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1266e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4581e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6243e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.5010e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2243e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9682e+01 </func>
</region>
</regions>
<internal rank="543" log_i="1723713849.509949" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="544" mpi_size="768" stamp_init="1723713791.155590" stamp_final="1723713849.504494" username="apac4" allocationname="unknown" flags="0" pid="3145766" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83489e+01" utime="4.81888e+01" stime="7.57367e+00" mtime="3.18175e+01" gflop="0.00000e+00" gbyte="3.76781e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18175e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009e14ff559e149e1488" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82735e+01" utime="4.81576e+01" stime="7.56475e+00" mtime="3.18175e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18175e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127164" bytes="8.4864e+08" > 6.0281e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4928e+08" > 6.0724e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6997e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3912e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3750e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8746e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4571e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6312e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.3985e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2302e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8278e+01 </func>
</region>
</regions>
<internal rank="544" log_i="1723713849.504494" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="545" mpi_size="768" stamp_init="1723713791.152498" stamp_final="1723713849.500453" username="apac4" allocationname="unknown" flags="0" pid="3145767" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83480e+01" utime="4.99848e+01" stime="7.11899e+00" mtime="3.26745e+01" gflop="0.00000e+00" gbyte="3.77441e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26745e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82715e+01" utime="4.99465e+01" stime="7.11719e+00" mtime="3.26745e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26745e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4955e+08" > 4.7981e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4967e+08" > 2.6745e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7878e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3824e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1239e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4576e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6307e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1400e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2246e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9212e+01 </func>
</region>
</regions>
<internal rank="545" log_i="1723713849.500453" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="546" mpi_size="768" stamp_init="1723713791.155326" stamp_final="1723713849.504261" username="apac4" allocationname="unknown" flags="0" pid="3145768" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83489e+01" utime="4.84097e+01" stime="7.46848e+00" mtime="3.17935e+01" gflop="0.00000e+00" gbyte="3.76957e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17935e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005b1561565b155a1535" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82731e+01" utime="4.83777e+01" stime="7.46092e+00" mtime="3.17935e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17935e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4880e+08" > 5.6721e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4828e+08" > 4.3552e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3465e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4195e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3855e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1353e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4567e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6390e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.4414e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2299e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8630e+01 </func>
</region>
</regions>
<internal rank="546" log_i="1723713849.504261" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="547" mpi_size="768" stamp_init="1723713791.157132" stamp_final="1723713849.503170" username="apac4" allocationname="unknown" flags="0" pid="3145769" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83460e+01" utime="5.01838e+01" stime="6.98174e+00" mtime="3.22701e+01" gflop="0.00000e+00" gbyte="3.74935e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22701e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ab14e555ab14a614c2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82741e+01" utime="5.01515e+01" stime="6.97461e+00" mtime="3.22701e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22701e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4843e+08" > 4.9242e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4867e+08" > 3.4137e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5361e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4026e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2100e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4567e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6353e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0994e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2237e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9011e+01 </func>
</region>
</regions>
<internal rank="547" log_i="1723713849.503170" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="548" mpi_size="768" stamp_init="1723713791.159977" stamp_final="1723713849.514229" username="apac4" allocationname="unknown" flags="0" pid="3145770" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83543e+01" utime="4.63274e+01" stime="8.25397e+00" mtime="3.14766e+01" gflop="0.00000e+00" gbyte="3.77197e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.14766e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003e147e563e143914d8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82813e+01" utime="4.62994e+01" stime="8.24288e+00" mtime="3.14766e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.14766e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4825e+08" > 8.2742e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4777e+08" > 6.8761e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4572e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4130e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.8719e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0994e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4566e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6402e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.4319e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2310e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7919e+01 </func>
</region>
</regions>
<internal rank="548" log_i="1723713849.514229" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="549" mpi_size="768" stamp_init="1723713791.161341" stamp_final="1723713849.510079" username="apac4" allocationname="unknown" flags="0" pid="3145771" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83487e+01" utime="5.01332e+01" stime="7.02653e+00" mtime="3.26531e+01" gflop="0.00000e+00" gbyte="3.77430e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26531e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82782e+01" utime="5.01000e+01" stime="7.02062e+00" mtime="3.26531e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26531e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4876e+08" > 4.7786e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4746e+08" > 3.1788e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4700e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4246e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1703e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4554e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6499e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8610e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1687e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9459e+01 </func>
</region>
</regions>
<internal rank="549" log_i="1723713849.510079" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="550" mpi_size="768" stamp_init="1723713791.170425" stamp_final="1723713849.512267" username="apac4" allocationname="unknown" flags="0" pid="3145772" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83418e+01" utime="4.83306e+01" stime="7.57054e+00" mtime="3.18116e+01" gflop="0.00000e+00" gbyte="3.76938e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18116e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003014a8553014301459" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82766e+01" utime="4.82992e+01" stime="7.56302e+00" mtime="3.18116e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18116e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4599e+08" > 6.1779e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4704e+08" > 4.7951e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3362e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4173e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.9101e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0993e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4559e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6473e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.3699e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2300e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8609e+01 </func>
</region>
</regions>
<internal rank="550" log_i="1723713849.512267" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="551" mpi_size="768" stamp_init="1723713791.165208" stamp_final="1723713849.505682" username="apac4" allocationname="unknown" flags="0" pid="3145773" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="5.83405e+01" utime="4.99583e+01" stime="7.14901e+00" mtime="3.27575e+01" gflop="0.00000e+00" gbyte="3.74462e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27575e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004d149c554d144d14dd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82731e+01" utime="4.99232e+01" stime="7.14556e+00" mtime="3.27575e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27575e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4879e+08" > 5.0255e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4745e+08" > 3.3905e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8829e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4253e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4796e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1325e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4553e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6497e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6989e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1684e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9127e+01 </func>
</region>
</regions>
<internal rank="551" log_i="1723713849.505682" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="552" mpi_size="768" stamp_init="1723713791.103805" stamp_final="1723713849.507669" username="apac4" allocationname="unknown" flags="0" pid="867156" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.84039e+01" utime="4.36894e+01" stime="1.22889e+01" mtime="3.24833e+01" gflop="0.00000e+00" gbyte="3.85891e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24833e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000df15de1508" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83411e+01" utime="4.36594e+01" stime="1.22778e+01" mtime="3.24833e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24833e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4859e+08" > 5.3191e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4855e+08" > 3.5487e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.4450e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4174e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9670e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9504e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4561e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6466e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2592e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2455e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8280e+01 </func>
</region>
</regions>
<internal rank="552" log_i="1723713849.507669" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="553" mpi_size="768" stamp_init="1723713791.103798" stamp_final="1723713849.507982" username="apac4" allocationname="unknown" flags="0" pid="867157" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.84042e+01" utime="5.01818e+01" stime="6.94959e+00" mtime="3.24698e+01" gflop="0.00000e+00" gbyte="3.74973e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24698e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000cb14ca1489" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83331e+01" utime="5.01521e+01" stime="6.93842e+00" mtime="3.24698e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24698e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4935e+08" > 4.5707e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4862e+08" > 3.1700e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5997e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4241e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6464e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0172e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4556e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6476e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8706e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2466e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9182e+01 </func>
</region>
</regions>
<internal rank="553" log_i="1723713849.507982" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="554" mpi_size="768" stamp_init="1723713791.106090" stamp_final="1723713849.513552" username="apac4" allocationname="unknown" flags="0" pid="867158" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.84075e+01" utime="4.80887e+01" stime="7.73448e+00" mtime="3.19612e+01" gflop="0.00000e+00" gbyte="3.74474e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19612e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000db151155db15da1508" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83441e+01" utime="4.80558e+01" stime="7.72780e+00" mtime="3.19612e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19612e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4686e+08" > 6.4834e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4949e+08" > 4.3552e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6979e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4119e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.4611e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9695e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4554e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6481e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0184e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2475e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8387e+01 </func>
</region>
</regions>
<internal rank="554" log_i="1723713849.513552" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="555" mpi_size="768" stamp_init="1723713791.108581" stamp_final="1723713849.499211" username="apac4" allocationname="unknown" flags="0" pid="867159" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83906e+01" utime="4.93530e+01" stime="6.67835e+00" mtime="3.23597e+01" gflop="0.00000e+00" gbyte="3.74695e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23597e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf488148a148b141d568b148a1469" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83181e+01" utime="4.93207e+01" stime="6.67017e+00" mtime="3.23597e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23597e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4898e+08" > 4.6397e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4844e+08" > 2.9176e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4614e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4707e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3842e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0584e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4553e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6531e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4285e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2443e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9203e+01 </func>
</region>
</regions>
<internal rank="555" log_i="1723713849.499211" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="556" mpi_size="768" stamp_init="1723713791.112399" stamp_final="1723713849.519308" username="apac4" allocationname="unknown" flags="0" pid="867160" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.84069e+01" utime="4.70726e+01" stime="8.16968e+00" mtime="3.20868e+01" gflop="0.00000e+00" gbyte="3.76545e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20868e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008414bf55841484147c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83165e+01" utime="4.70449e+01" stime="8.15499e+00" mtime="3.20868e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20868e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4886e+08" > 7.8210e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4912e+08" > 5.5866e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8110e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4160e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3080e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0008e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4543e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6581e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2091e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2490e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8247e+01 </func>
</region>
</regions>
<internal rank="556" log_i="1723713849.519308" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="557" mpi_size="768" stamp_init="1723713791.115443" stamp_final="1723713849.514567" username="apac4" allocationname="unknown" flags="0" pid="867161" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83991e+01" utime="5.01118e+01" stime="6.94811e+00" mtime="3.26147e+01" gflop="0.00000e+00" gbyte="3.77277e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26147e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83039e+01" utime="5.00822e+01" stime="6.93543e+00" mtime="3.26147e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26147e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4949e+08" > 4.6336e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4890e+08" > 2.8410e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3170e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3943e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4080e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0339e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4549e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6575e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0708e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2435e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9635e+01 </func>
</region>
</regions>
<internal rank="557" log_i="1723713849.514567" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="558" mpi_size="768" stamp_init="1723713791.117642" stamp_final="1723713849.513264" username="apac4" allocationname="unknown" flags="0" pid="867162" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83956e+01" utime="4.80794e+01" stime="7.60914e+00" mtime="3.20506e+01" gflop="0.00000e+00" gbyte="3.76213e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20506e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82962e+01" utime="4.80495e+01" stime="7.59528e+00" mtime="3.20506e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20506e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4870e+08" > 6.0060e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4877e+08" > 4.5944e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4153e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3988e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.8248e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0010e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4547e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6566e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0494e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2471e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8808e+01 </func>
</region>
</regions>
<internal rank="558" log_i="1723713849.513264" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="559" mpi_size="768" stamp_init="1723713791.120373" stamp_final="1723713849.514898" username="apac4" allocationname="unknown" flags="0" pid="867163" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83945e+01" utime="5.03568e+01" stime="6.78169e+00" mtime="3.19840e+01" gflop="0.00000e+00" gbyte="3.77266e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19840e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82907e+01" utime="5.03220e+01" stime="6.77346e+00" mtime="3.19840e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19840e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4974e+08" > 4.5288e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4718e+08" > 3.0857e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0681e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3583e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0010e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4541e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6622e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2496e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2433e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9301e+01 </func>
</region>
</regions>
<internal rank="559" log_i="1723713849.514898" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="560" mpi_size="768" stamp_init="1723713791.122144" stamp_final="1723713849.512111" username="apac4" allocationname="unknown" flags="0" pid="867164" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83900e+01" utime="4.76855e+01" stime="7.72098e+00" mtime="3.20088e+01" gflop="0.00000e+00" gbyte="3.76087e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20088e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82854e+01" utime="4.76530e+01" stime="7.71072e+00" mtime="3.20088e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20088e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4922e+08" > 5.4351e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4830e+08" > 3.8844e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8377e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4172e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2170e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8773e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4539e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6630e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4499e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2484e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8397e+01 </func>
</region>
</regions>
<internal rank="560" log_i="1723713849.512111" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="561" mpi_size="768" stamp_init="1723713791.124474" stamp_final="1723713849.518924" username="apac4" allocationname="unknown" flags="0" pid="867165" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83944e+01" utime="5.00548e+01" stime="7.05839e+00" mtime="3.26891e+01" gflop="0.00000e+00" gbyte="3.76621e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26891e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001a141a1482" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82905e+01" utime="5.00218e+01" stime="7.04870e+00" mtime="3.26891e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26891e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4913e+08" > 3.5553e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4933e+08" > 2.7013e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1181e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4091e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9556e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4536e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6711e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8491e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2424e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9006e+01 </func>
</region>
</regions>
<internal rank="561" log_i="1723713849.518924" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="562" mpi_size="768" stamp_init="1723713791.126599" stamp_final="1723713849.507603" username="apac4" allocationname="unknown" flags="0" pid="867166" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83810e+01" utime="4.78152e+01" stime="7.80096e+00" mtime="3.22894e+01" gflop="0.00000e+00" gbyte="3.78220e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22894e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4fe141e14141442561414ee14d2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82732e+01" utime="4.77787e+01" stime="7.79335e+00" mtime="3.22894e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22894e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4982e+08" > 5.7532e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4854e+08" > 3.9824e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1637e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4108e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.9169e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0779e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4532e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6744e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3712e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2461e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8318e+01 </func>
</region>
</regions>
<internal rank="562" log_i="1723713849.507603" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="563" mpi_size="768" stamp_init="1723713791.129095" stamp_final="1723713849.512256" username="apac4" allocationname="unknown" flags="0" pid="867167" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83832e+01" utime="5.01671e+01" stime="6.92794e+00" mtime="3.26086e+01" gflop="0.00000e+00" gbyte="3.75351e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26086e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4281429142a14e7562a142a1482" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82769e+01" utime="5.01365e+01" stime="6.91554e+00" mtime="3.26086e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26086e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4830e+08" > 3.5544e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4957e+08" > 2.6910e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8101e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4167e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0782e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4532e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6751e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3498e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2413e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9218e+01 </func>
</region>
</regions>
<internal rank="563" log_i="1723713849.512256" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="564" mpi_size="768" stamp_init="1723713791.132423" stamp_final="1723713849.519339" username="apac4" allocationname="unknown" flags="0" pid="867168" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83869e+01" utime="4.79344e+01" stime="7.79780e+00" mtime="3.23239e+01" gflop="0.00000e+00" gbyte="3.77411e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23239e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000089155d55891589150b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82802e+01" utime="4.78966e+01" stime="7.79224e+00" mtime="3.23239e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23239e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5077e+08" > 4.9595e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4934e+08" > 4.6889e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.4454e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4037e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2994e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0078e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4527e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6802e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1900e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2479e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8158e+01 </func>
</region>
</regions>
<internal rank="564" log_i="1723713849.519339" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="565" mpi_size="768" stamp_init="1723713791.134137" stamp_final="1723713849.512440" username="apac4" allocationname="unknown" flags="0" pid="867169" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83783e+01" utime="5.02750e+01" stime="6.83612e+00" mtime="3.26971e+01" gflop="0.00000e+00" gbyte="3.78010e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26971e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49a159c159d1564559d159c151b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82733e+01" utime="5.02438e+01" stime="6.82457e+00" mtime="3.26971e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26971e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4902e+08" > 3.6058e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4991e+08" > 2.4787e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4202e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4301e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8597e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0197e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4515e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6934e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4809e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2394e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9686e+01 </func>
</region>
</regions>
<internal rank="565" log_i="1723713849.512440" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="566" mpi_size="768" stamp_init="1723713791.136728" stamp_final="1723713849.512523" username="apac4" allocationname="unknown" flags="0" pid="867170" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83758e+01" utime="4.84575e+01" stime="7.49259e+00" mtime="3.22753e+01" gflop="0.00000e+00" gbyte="3.75835e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22753e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82727e+01" utime="4.84241e+01" stime="7.48401e+00" mtime="3.22753e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22753e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4933e+08" > 4.7602e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4837e+08" > 4.0861e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8242e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4160e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.3195e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0309e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4519e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6877e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9016e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2476e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8742e+01 </func>
</region>
</regions>
<internal rank="566" log_i="1723713849.512523" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="567" mpi_size="768" stamp_init="1723713791.138511" stamp_final="1723713849.512495" username="apac4" allocationname="unknown" flags="0" pid="867171" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83740e+01" utime="5.02716e+01" stime="6.85807e+00" mtime="3.25062e+01" gflop="0.00000e+00" gbyte="3.77098e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25062e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e314e414e6141e55e614e51470" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82722e+01" utime="5.02428e+01" stime="6.84535e+00" mtime="3.25062e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25062e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4855e+08" > 3.5676e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4935e+08" > 2.6501e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7096e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4073e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0768e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4514e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6901e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8110e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2395e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9225e+01 </func>
</region>
</regions>
<internal rank="567" log_i="1723713849.512495" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="568" mpi_size="768" stamp_init="1723713791.143202" stamp_final="1723713849.514016" username="apac4" allocationname="unknown" flags="0" pid="867172" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83708e+01" utime="4.80133e+01" stime="7.78297e+00" mtime="3.18287e+01" gflop="0.00000e+00" gbyte="3.76919e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18287e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000017141714a7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82733e+01" utime="4.79847e+01" stime="7.76980e+00" mtime="3.18287e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18287e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4841e+08" > 5.5140e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4882e+08" > 4.0476e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0970e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4124e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.1772e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8782e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4517e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6901e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7490e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2482e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7949e+01 </func>
</region>
</regions>
<internal rank="568" log_i="1723713849.514016" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="569" mpi_size="768" stamp_init="1723713791.144942" stamp_final="1723713849.512201" username="apac4" allocationname="unknown" flags="0" pid="867173" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83673e+01" utime="4.99305e+01" stime="7.15261e+00" mtime="3.26350e+01" gflop="0.00000e+00" gbyte="3.77308e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26350e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4521453145514bd5555145414d0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82666e+01" utime="4.98979e+01" stime="7.14505e+00" mtime="3.26350e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26350e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4880e+08" > 4.7167e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4960e+08" > 2.7782e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0333e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4081e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9814e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4507e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6966e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3402e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2385e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8921e+01 </func>
</region>
</regions>
<internal rank="569" log_i="1723713849.512201" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="570" mpi_size="768" stamp_init="1723713791.147710" stamp_final="1723713849.513852" username="apac4" allocationname="unknown" flags="0" pid="867174" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83661e+01" utime="4.67375e+01" stime="8.18450e+00" mtime="3.17443e+01" gflop="0.00000e+00" gbyte="3.76301e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17443e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4411443144414f4564414431459" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82716e+01" utime="4.67051e+01" stime="8.17613e+00" mtime="3.17443e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17443e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4942e+08" > 6.8743e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4901e+08" > 4.8967e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1369e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4159e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.6960e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0196e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4497e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7048e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8181e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2464e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7674e+01 </func>
</region>
</regions>
<internal rank="570" log_i="1723713849.513852" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="571" mpi_size="768" stamp_init="1723713791.149556" stamp_final="1723713849.512492" username="apac4" allocationname="unknown" flags="0" pid="867175" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83629e+01" utime="4.97406e+01" stime="7.02202e+00" mtime="3.19764e+01" gflop="0.00000e+00" gbyte="3.77785e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19764e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000251425145f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82687e+01" utime="4.97075e+01" stime="7.01372e+00" mtime="3.19764e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19764e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4946e+08" > 4.8253e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4979e+08" > 2.6734e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7353e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4166e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0196e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4501e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7063e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1114e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2390e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8540e+01 </func>
</region>
</regions>
<internal rank="571" log_i="1723713849.512492" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="572" mpi_size="768" stamp_init="1723713791.151944" stamp_final="1723713849.513863" username="apac4" allocationname="unknown" flags="0" pid="867176" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83619e+01" utime="4.80883e+01" stime="7.80084e+00" mtime="3.15476e+01" gflop="0.00000e+00" gbyte="3.77769e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15476e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002d14d9562d142c1463" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82689e+01" utime="4.80560e+01" stime="7.79217e+00" mtime="3.15476e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15476e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4894e+08" > 5.5421e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5058e+08" > 4.0698e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8060e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4187e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8456e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8801e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4500e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7079e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7204e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2478e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7954e+01 </func>
</region>
</regions>
<internal rank="572" log_i="1723713849.513863" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="573" mpi_size="768" stamp_init="1723713791.153815" stamp_final="1723713849.505822" username="apac4" allocationname="unknown" flags="0" pid="867177" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83520e+01" utime="5.02039e+01" stime="6.94751e+00" mtime="3.28355e+01" gflop="0.00000e+00" gbyte="3.77361e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28355e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4131414141514185515141514ef" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82606e+01" utime="5.01719e+01" stime="6.93899e+00" mtime="3.28355e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28355e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4985e+08" > 4.9079e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4844e+08" > 2.8757e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9529e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4220e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4544e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8790e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4497e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7116e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8896e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2395e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9169e+01 </func>
</region>
</regions>
<internal rank="573" log_i="1723713849.505822" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="574" mpi_size="768" stamp_init="1723713791.158225" stamp_final="1723713849.513549" username="apac4" allocationname="unknown" flags="0" pid="867178" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83553e+01" utime="4.74392e+01" stime="7.93874e+00" mtime="3.19080e+01" gflop="0.00000e+00" gbyte="3.76431e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19080e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49b159c159d1575569d159d1520" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82671e+01" utime="4.74064e+01" stime="7.93135e+00" mtime="3.19080e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19080e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4982e+08" > 6.5763e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4922e+08" > 4.2482e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9251e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4270e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.8876e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9650e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4490e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7143e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8992e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1959e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8084e+01 </func>
</region>
</regions>
<internal rank="574" log_i="1723713849.513549" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="575" mpi_size="768" stamp_init="1723713791.158442" stamp_final="1723713849.499201" username="apac4" allocationname="unknown" flags="0" pid="867179" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="5.83408e+01" utime="4.99704e+01" stime="7.14954e+00" mtime="3.24009e+01" gflop="0.00000e+00" gbyte="3.77949e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24009e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000039143914a4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82519e+01" utime="4.99473e+01" stime="7.13245e+00" mtime="3.24009e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24009e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4938e+08" > 4.6525e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4990e+08" > 2.9338e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5575e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4011e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0779e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4497e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7105e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7800e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2386e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9166e+01 </func>
</region>
</regions>
<internal rank="575" log_i="1723713849.499201" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="576" mpi_size="768" stamp_init="1723713791.133198" stamp_final="1723713849.509673" username="apac4" allocationname="unknown" flags="0" pid="1823457" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83765e+01" utime="4.19338e+01" stime="1.24943e+01" mtime="3.18665e+01" gflop="0.00000e+00" gbyte="3.86036e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18665e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82707e+01" utime="4.19019e+01" stime="1.24848e+01" mtime="3.18665e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18665e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4785e+08" > 6.3981e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4739e+08" > 4.8340e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4872e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4739e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.5378e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0227e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4471e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7347e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4308e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2410e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8476e+01 </func>
</region>
</regions>
<internal rank="576" log_i="1723713849.509673" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="577" mpi_size="768" stamp_init="1723713791.133175" stamp_final="1723713849.508066" username="apac4" allocationname="unknown" flags="0" pid="1823458" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83749e+01" utime="4.94835e+01" stime="6.58466e+00" mtime="3.29142e+01" gflop="0.00000e+00" gbyte="3.77087e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.29142e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82670e+01" utime="4.94508e+01" stime="6.57509e+00" mtime="3.29142e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.29142e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4893e+08" > 4.5217e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4848e+08" > 3.5237e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4906e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4698e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0564e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4486e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7208e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7609e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2317e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9737e+01 </func>
</region>
</regions>
<internal rank="577" log_i="1723713849.508066" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="578" mpi_size="768" stamp_init="1723713791.133188" stamp_final="1723713849.504055" username="apac4" allocationname="unknown" flags="0" pid="1823459" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83709e+01" utime="4.82865e+01" stime="7.72304e+00" mtime="3.23166e+01" gflop="0.00000e+00" gbyte="3.76690e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23166e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a214a2149c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82635e+01" utime="4.82567e+01" stime="7.71189e+00" mtime="3.23166e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23166e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5003e+08" > 5.8781e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4818e+08" > 4.9245e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7615e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4250e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2994e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0961e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4486e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7181e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1495e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2385e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8710e+01 </func>
</region>
</regions>
<internal rank="578" log_i="1723713849.504055" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="579" mpi_size="768" stamp_init="1723713791.136412" stamp_final="1723713849.507462" username="apac4" allocationname="unknown" flags="0" pid="1823460" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83711e+01" utime="4.97192e+01" stime="7.11477e+00" mtime="3.26855e+01" gflop="0.00000e+00" gbyte="3.74725e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26855e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c614c614e0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82640e+01" utime="4.96840e+01" stime="7.10877e+00" mtime="3.26855e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26855e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4907e+08" > 4.4442e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4948e+08" > 2.8254e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6661e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3935e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1000e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4486e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7223e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9898e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2317e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9370e+01 </func>
</region>
</regions>
<internal rank="579" log_i="1723713849.507462" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="580" mpi_size="768" stamp_init="1723713791.133191" stamp_final="1723713849.512811" username="apac4" allocationname="unknown" flags="0" pid="1823461" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83796e+01" utime="4.81350e+01" stime="7.53750e+00" mtime="3.16762e+01" gflop="0.00000e+00" gbyte="3.77979e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16762e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001714171471" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82750e+01" utime="4.81085e+01" stime="7.52167e+00" mtime="3.16762e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16762e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4923e+08" > 5.9591e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4974e+08" > 4.9434e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0986e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3111e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.8560e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.1314e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4481e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7261e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8396e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2384e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8729e+01 </func>
</region>
</regions>
<internal rank="580" log_i="1723713849.512811" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="581" mpi_size="768" stamp_init="1723713791.133170" stamp_final="1723713849.507882" username="apac4" allocationname="unknown" flags="0" pid="1823462" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83747e+01" utime="5.02252e+01" stime="6.74592e+00" mtime="3.26011e+01" gflop="0.00000e+00" gbyte="3.76156e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26011e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a515a615a7156856a715a71535" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82698e+01" utime="5.01934e+01" stime="6.73655e+00" mtime="3.26011e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26011e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4885e+08" > 4.3438e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4853e+08" > 3.3417e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1496e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4178e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0992e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4488e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7200e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0088e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2311e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9783e+01 </func>
</region>
</regions>
<internal rank="581" log_i="1723713849.507882" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="582" mpi_size="768" stamp_init="1723713791.133223" stamp_final="1723713849.514491" username="apac4" allocationname="unknown" flags="0" pid="1823463" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83813e+01" utime="4.69912e+01" stime="7.64630e+00" mtime="3.15783e+01" gflop="0.00000e+00" gbyte="3.77750e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15783e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d114d214d4149f55d414d314ab" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82746e+01" utime="4.69564e+01" stime="7.63877e+00" mtime="3.15783e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15783e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4774e+08" > 6.5471e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4754e+08" > 4.8564e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8387e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4605e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8518e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0573e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4475e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7276e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8586e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1879e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8844e+01 </func>
</region>
</regions>
<internal rank="582" log_i="1723713849.514491" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="583" mpi_size="768" stamp_init="1723713791.133173" stamp_final="1723713849.516727" username="apac4" allocationname="unknown" flags="0" pid="1823464" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83836e+01" utime="4.94386e+01" stime="6.61128e+00" mtime="3.25895e+01" gflop="0.00000e+00" gbyte="3.76972e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25895e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000e5141e56e514e514ae" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82768e+01" utime="4.94097e+01" stime="6.59856e+00" mtime="3.25895e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25895e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4608e+08" > 4.5827e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4792e+08" > 2.9623e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1162e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4692e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0543e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4481e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7269e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9588e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2316e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9788e+01 </func>
</region>
</regions>
<internal rank="583" log_i="1723713849.516727" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="584" mpi_size="768" stamp_init="1723713791.133194" stamp_final="1723713849.507225" username="apac4" allocationname="unknown" flags="0" pid="1823465" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83740e+01" utime="4.74587e+01" stime="8.05059e+00" mtime="3.21000e+01" gflop="0.00000e+00" gbyte="3.77201e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21000e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003e14d7553e143e1477" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82699e+01" utime="4.74270e+01" stime="8.04045e+00" mtime="3.21000e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21000e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4851e+08" > 5.3092e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4917e+08" > 3.8848e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1445e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4291e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.2021e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0957e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4473e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7316e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9302e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2377e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8169e+01 </func>
</region>
</regions>
<internal rank="584" log_i="1723713849.507225" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="585" mpi_size="768" stamp_init="1723713791.141013" stamp_final="1723713849.502313" username="apac4" allocationname="unknown" flags="0" pid="1823466" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83613e+01" utime="5.04361e+01" stime="6.72945e+00" mtime="3.23947e+01" gflop="0.00000e+00" gbyte="3.76213e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23947e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82526e+01" utime="5.04042e+01" stime="6.72025e+00" mtime="3.23947e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23947e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4790e+08" > 3.5103e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4848e+08" > 3.1179e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2228e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4285e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1204e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4475e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7322e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2687e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2295e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9576e+01 </func>
</region>
</regions>
<internal rank="585" log_i="1723713849.502313" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="586" mpi_size="768" stamp_init="1723713791.142783" stamp_final="1723713849.512719" username="apac4" allocationname="unknown" flags="0" pid="1823467" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83699e+01" utime="4.87542e+01" stime="7.35192e+00" mtime="3.19998e+01" gflop="0.00000e+00" gbyte="3.76850e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19998e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82612e+01" utime="4.87208e+01" stime="7.34346e+00" mtime="3.19998e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19998e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4939e+08" > 4.4741e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5029e+08" > 4.1135e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7348e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4216e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.3937e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0954e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4468e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7364e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6703e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1900e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8573e+01 </func>
</region>
</regions>
<internal rank="586" log_i="1723713849.512719" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="587" mpi_size="768" stamp_init="1723713791.141663" stamp_final="1723713849.514413" username="apac4" allocationname="unknown" flags="0" pid="1823468" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83728e+01" utime="4.99370e+01" stime="6.92627e+00" mtime="3.28421e+01" gflop="0.00000e+00" gbyte="3.75648e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28421e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c314c21497" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82621e+01" utime="4.99014e+01" stime="6.92047e+00" mtime="3.28421e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28421e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127164" bytes="8.4965e+08" > 3.4879e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4938e+08" > 2.8720e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9944e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4229e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1365e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4462e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7464e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8682e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2287e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9260e+01 </func>
</region>
</regions>
<internal rank="587" log_i="1723713849.514413" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="588" mpi_size="768" stamp_init="1723713791.142785" stamp_final="1723713849.508396" username="apac4" allocationname="unknown" flags="0" pid="1823469" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83656e+01" utime="4.83932e+01" stime="7.53488e+00" mtime="3.25405e+01" gflop="0.00000e+00" gbyte="3.74939e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25405e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003a153a1536" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82565e+01" utime="4.83590e+01" stime="7.52779e+00" mtime="3.25405e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25405e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4930e+08" > 4.4649e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4992e+08" > 4.0934e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0753e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4252e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2589e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0953e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4462e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7417e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4380e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2364e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8770e+01 </func>
</region>
</regions>
<internal rank="588" log_i="1723713849.508396" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="589" mpi_size="768" stamp_init="1723713791.141310" stamp_final="1723713849.502240" username="apac4" allocationname="unknown" flags="0" pid="1823470" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83609e+01" utime="5.03799e+01" stime="6.74856e+00" mtime="3.26739e+01" gflop="0.00000e+00" gbyte="3.76678e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26739e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007014395570147014df" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82528e+01" utime="5.03467e+01" stime="6.74080e+00" mtime="3.26739e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26739e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4988e+08" > 3.3853e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4911e+08" > 2.7921e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3580e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4190e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4305e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1352e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4461e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7471e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5691e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2297e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9743e+01 </func>
</region>
</regions>
<internal rank="589" log_i="1723713849.502240" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="590" mpi_size="768" stamp_init="1723713791.142782" stamp_final="1723713849.501301" username="apac4" allocationname="unknown" flags="0" pid="1823471" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83585e+01" utime="4.81360e+01" stime="7.59770e+00" mtime="3.17267e+01" gflop="0.00000e+00" gbyte="3.76717e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17267e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a614a714a8142555a814a814d5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82493e+01" utime="4.81033e+01" stime="7.58926e+00" mtime="3.17267e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17267e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4861e+08" > 5.0401e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4925e+08" > 4.5166e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3830e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4167e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6083e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1133e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4452e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7522e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1805e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2358e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8593e+01 </func>
</region>
</regions>
<internal rank="590" log_i="1723713849.501301" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="591" mpi_size="768" stamp_init="1723713791.143405" stamp_final="1723713849.510181" username="apac4" allocationname="unknown" flags="0" pid="1823472" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83668e+01" utime="5.01469e+01" stime="7.01668e+00" mtime="3.28435e+01" gflop="0.00000e+00" gbyte="3.77644e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28435e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000050144f14f4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82576e+01" utime="5.01137e+01" stime="7.00866e+00" mtime="3.28435e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28435e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4916e+08" > 3.4531e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4920e+08" > 2.7550e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7167e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4240e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3842e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1128e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4449e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7551e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7800e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2280e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9546e+01 </func>
</region>
</regions>
<internal rank="591" log_i="1723713849.510181" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="592" mpi_size="768" stamp_init="1723713791.147092" stamp_final="1723713849.504310" username="apac4" allocationname="unknown" flags="0" pid="1823473" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83572e+01" utime="4.53480e+01" stime="8.22197e+00" mtime="3.13562e+01" gflop="0.00000e+00" gbyte="3.78048e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.13562e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49a159c159d15e9559d159c154e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82506e+01" utime="4.53174e+01" stime="8.21077e+00" mtime="3.13562e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.13562e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4742e+08" > 7.6386e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4830e+08" > 5.9415e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3764e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4628e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.3983e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0240e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4453e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7534e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4714e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2373e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7953e+01 </func>
</region>
</regions>
<internal rank="592" log_i="1723713849.504310" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="593" mpi_size="768" stamp_init="1723713791.148946" stamp_final="1723713849.514109" username="apac4" allocationname="unknown" flags="0" pid="1823474" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83652e+01" utime="5.00929e+01" stime="7.04633e+00" mtime="3.26717e+01" gflop="0.00000e+00" gbyte="3.76621e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26717e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82596e+01" utime="5.00620e+01" stime="7.03643e+00" mtime="3.26717e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26717e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4517e+08" > 4.7031e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4816e+08" > 2.8934e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6305e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4162e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0985e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4445e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7635e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7585e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2288e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9343e+01 </func>
</region>
</regions>
<internal rank="593" log_i="1723713849.514109" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="594" mpi_size="768" stamp_init="1723713791.152081" stamp_final="1723713849.508432" username="apac4" allocationname="unknown" flags="0" pid="1823475" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83564e+01" utime="4.85869e+01" stime="7.57476e+00" mtime="3.20702e+01" gflop="0.00000e+00" gbyte="3.76919e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20702e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000551550153e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82535e+01" utime="4.85572e+01" stime="7.56351e+00" mtime="3.20702e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20702e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4900e+08" > 5.6300e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4901e+08" > 4.3007e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5205e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4242e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6003e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1006e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4442e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7643e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5787e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2360e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8734e+01 </func>
</region>
</regions>
<internal rank="594" log_i="1723713849.508432" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="595" mpi_size="768" stamp_init="1723713791.154068" stamp_final="1723713849.507766" username="apac4" allocationname="unknown" flags="0" pid="1823476" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83537e+01" utime="4.99676e+01" stime="7.15371e+00" mtime="3.26414e+01" gflop="0.00000e+00" gbyte="3.77369e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26414e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82524e+01" utime="4.99383e+01" stime="7.14218e+00" mtime="3.26414e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26414e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5085e+08" > 4.7075e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4850e+08" > 2.6426e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8656e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4079e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1696e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1006e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4443e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7660e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2020e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2292e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9087e+01 </func>
</region>
</regions>
<internal rank="595" log_i="1723713849.507766" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="596" mpi_size="768" stamp_init="1723713791.156834" stamp_final="1723713849.517158" username="apac4" allocationname="unknown" flags="0" pid="1823477" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83603e+01" utime="4.65545e+01" stime="7.97016e+00" mtime="3.21509e+01" gflop="0.00000e+00" gbyte="3.77655e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21509e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005b145b1456" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82589e+01" utime="4.65256e+01" stime="7.95809e+00" mtime="3.21509e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21509e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4926e+08" > 6.0112e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4974e+08" > 4.3910e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8738e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4523e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1991e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0560e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4441e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7673e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2496e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2352e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8438e+01 </func>
</region>
</regions>
<internal rank="596" log_i="1723713849.517158" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="597" mpi_size="768" stamp_init="1723713791.169759" stamp_final="1723713849.507922" username="apac4" allocationname="unknown" flags="0" pid="1823478" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83382e+01" utime="4.91414e+01" stime="6.92657e+00" mtime="3.25838e+01" gflop="0.00000e+00" gbyte="3.77708e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25838e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c715c815c9153656c915c91504" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82468e+01" utime="4.91111e+01" stime="6.91660e+00" mtime="3.25838e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25838e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4855e+08" > 4.7013e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4843e+08" > 3.4910e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5529e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4356e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8597e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0562e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4433e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7718e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4618e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2284e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9361e+01 </func>
</region>
</regions>
<internal rank="597" log_i="1723713849.507922" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="598" mpi_size="768" stamp_init="1723713791.167364" stamp_final="1723713849.513386" username="apac4" allocationname="unknown" flags="0" pid="1823479" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83460e+01" utime="4.68899e+01" stime="7.64422e+00" mtime="3.17111e+01" gflop="0.00000e+00" gbyte="3.77647e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17111e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b814b914ba149956ba14ba146e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82541e+01" utime="4.68544e+01" stime="7.63944e+00" mtime="3.17111e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17111e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4890e+08" > 6.5647e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4682e+08" > 4.8631e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2005e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4504e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.5592e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0560e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4434e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7704e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3522e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2349e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8625e+01 </func>
</region>
</regions>
<internal rank="598" log_i="1723713849.513386" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="599" mpi_size="768" stamp_init="1723713791.173627" stamp_final="1723713849.508288" username="apac4" allocationname="unknown" flags="0" pid="1823480" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="5.83347e+01" utime="4.93606e+01" stime="6.77518e+00" mtime="3.27807e+01" gflop="0.00000e+00" gbyte="3.74115e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27807e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82468e+01" utime="4.93304e+01" stime="6.76614e+00" mtime="3.27807e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27807e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4827e+08" > 4.5259e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4858e+08" > 2.9027e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5422e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4788e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4319e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0560e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4428e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7776e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9516e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2274e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9549e+01 </func>
</region>
</regions>
<internal rank="599" log_i="1723713849.508288" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="600" mpi_size="768" stamp_init="1723713791.145583" stamp_final="1723713849.512279" username="apac4" allocationname="unknown" flags="0" pid="3304679" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83667e+01" utime="4.24204e+01" stime="1.22191e+01" mtime="3.22474e+01" gflop="0.00000e+00" gbyte="3.86749e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22474e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf46e14701471149e5571147114b5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82656e+01" utime="4.23844e+01" stime="1.22136e+01" mtime="3.22474e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22474e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4959e+08" > 5.8954e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4839e+08" > 5.3272e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7960e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4313e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.9980e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0146e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4422e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7808e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8396e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1984e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8686e+01 </func>
</region>
</regions>
<internal rank="600" log_i="1723713849.512279" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="601" mpi_size="768" stamp_init="1723713791.145255" stamp_final="1723713849.508695" username="apac4" allocationname="unknown" flags="0" pid="3304680" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83634e+01" utime="4.94614e+01" stime="6.64631e+00" mtime="3.23566e+01" gflop="0.00000e+00" gbyte="3.76507e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23566e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82568e+01" utime="4.94270e+01" stime="6.63815e+00" mtime="3.23566e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23566e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4770e+08" > 4.5456e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4939e+08" > 2.9735e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0487e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4752e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0196e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4424e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7821e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6989e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2517e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9655e+01 </func>
</region>
</regions>
<internal rank="601" log_i="1723713849.508695" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="602" mpi_size="768" stamp_init="1723713791.145281" stamp_final="1723713849.510827" username="apac4" allocationname="unknown" flags="0" pid="3304681" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83655e+01" utime="4.80097e+01" stime="6.90635e+00" mtime="3.21380e+01" gflop="0.00000e+00" gbyte="3.77048e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21380e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000201548562015201555" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82632e+01" utime="4.79805e+01" stime="6.89427e+00" mtime="3.21380e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21380e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1683e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4884e+08" > 5.8533e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4847e+08" > 4.0583e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5914e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4741e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6192e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0145e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4424e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7841e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8896e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2500e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8758e+01 </func>
</region>
</regions>
<internal rank="602" log_i="1723713849.510827" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="603" mpi_size="768" stamp_init="1723713791.145134" stamp_final="1723713849.503727" username="apac4" allocationname="unknown" flags="0" pid="3304682" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83586e+01" utime="4.96802e+01" stime="6.38232e+00" mtime="3.26007e+01" gflop="0.00000e+00" gbyte="3.76717e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26007e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82563e+01" utime="4.96499e+01" stime="6.37160e+00" mtime="3.26007e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26007e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4808e+08" > 4.3885e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4936e+08" > 2.5285e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3336e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4773e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0146e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4427e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7832e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1281e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2510e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9637e+01 </func>
</region>
</regions>
<internal rank="603" log_i="1723713849.503727" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="604" mpi_size="768" stamp_init="1723713791.145311" stamp_final="1723713849.513643" username="apac4" allocationname="unknown" flags="0" pid="3304683" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83683e+01" utime="4.73511e+01" stime="7.34475e+00" mtime="3.23713e+01" gflop="0.00000e+00" gbyte="3.77739e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23713e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002b147b552b142a1491" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82677e+01" utime="4.73160e+01" stime="7.33812e+00" mtime="3.23713e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23713e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4976e+08" > 5.7340e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4726e+08" > 4.2689e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7157e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4751e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2589e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0169e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4421e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7876e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7990e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1996e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8874e+01 </func>
</region>
</regions>
<internal rank="604" log_i="1723713849.513643" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="605" mpi_size="768" stamp_init="1723713791.145155" stamp_final="1723713849.506934" username="apac4" allocationname="unknown" flags="0" pid="3304684" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83618e+01" utime="4.97557e+01" stime="6.35277e+00" mtime="3.26520e+01" gflop="0.00000e+00" gbyte="3.77106e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26520e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009414645594149414e9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82608e+01" utime="4.97228e+01" stime="6.34445e+00" mtime="3.26520e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26520e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4852e+08" > 4.5485e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5004e+08" > 2.7368e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1886e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4658e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0180e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4418e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7912e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7800e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2488e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9824e+01 </func>
</region>
</regions>
<internal rank="605" log_i="1723713849.506934" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="606" mpi_size="768" stamp_init="1723713791.145170" stamp_final="1723713849.512809" username="apac4" allocationname="unknown" flags="0" pid="3304685" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83676e+01" utime="4.60050e+01" stime="7.63736e+00" mtime="3.17382e+01" gflop="0.00000e+00" gbyte="3.77769e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17382e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82669e+01" utime="4.59756e+01" stime="7.62501e+00" mtime="3.17382e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17382e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4885e+08" > 8.4228e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4892e+08" > 6.7138e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1392e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4716e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.8889e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0192e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4406e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7958e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9087e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2504e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8526e+01 </func>
</region>
</regions>
<internal rank="606" log_i="1723713849.512809" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="607" mpi_size="768" stamp_init="1723713791.145143" stamp_final="1723713849.512210" username="apac4" allocationname="unknown" flags="0" pid="3304686" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83671e+01" utime="4.95050e+01" stime="6.57399e+00" mtime="3.29523e+01" gflop="0.00000e+00" gbyte="3.77434e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.29523e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d414d855d414d3147e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82651e+01" utime="4.94732e+01" stime="6.56394e+00" mtime="3.29523e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.29523e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4832e+08" > 4.5601e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4885e+08" > 3.1936e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6572e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4859e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0196e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4416e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7937e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7204e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2463e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9628e+01 </func>
</region>
</regions>
<internal rank="607" log_i="1723713849.512210" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="608" mpi_size="768" stamp_init="1723713791.145156" stamp_final="1723713849.510686" username="apac4" allocationname="unknown" flags="0" pid="3304687" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83655e+01" utime="4.79948e+01" stime="7.03473e+00" mtime="3.22965e+01" gflop="0.00000e+00" gbyte="3.76568e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22965e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009c14bf569c149c1498" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82645e+01" utime="4.79557e+01" stime="7.03274e+00" mtime="3.22965e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22965e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4890e+08" > 5.0750e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4683e+08" > 3.9917e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8572e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4810e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5489e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0134e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4401e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8043e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3903e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2517e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8721e+01 </func>
</region>
</regions>
<internal rank="608" log_i="1723713849.510686" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="609" mpi_size="768" stamp_init="1723713791.145141" stamp_final="1723713849.512853" username="apac4" allocationname="unknown" flags="0" pid="3304688" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83677e+01" utime="4.97845e+01" stime="6.29862e+00" mtime="3.24276e+01" gflop="0.00000e+00" gbyte="3.78475e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24276e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82663e+01" utime="4.97516e+01" stime="6.28983e+00" mtime="3.24276e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24276e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4868e+08" > 3.6529e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4841e+08" > 2.8774e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2754e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4795e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0171e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4403e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8035e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3092e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2462e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9588e+01 </func>
</region>
</regions>
<internal rank="609" log_i="1723713849.512853" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="610" mpi_size="768" stamp_init="1723713791.145162" stamp_final="1723713849.510631" username="apac4" allocationname="unknown" flags="0" pid="3304689" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83655e+01" utime="4.79062e+01" stime="7.08519e+00" mtime="3.23922e+01" gflop="0.00000e+00" gbyte="3.77800e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23922e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4dd14df14e014be55e014df14ff" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82641e+01" utime="4.78743e+01" stime="7.07519e+00" mtime="3.23922e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23922e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4728e+08" > 4.8786e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4958e+08" > 3.7542e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9689e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4828e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9387e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0163e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4407e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7994e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1495e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2523e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8722e+01 </func>
</region>
</regions>
<internal rank="610" log_i="1723713849.510631" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="611" mpi_size="768" stamp_init="1723713791.145128" stamp_final="1723713849.507819" username="apac4" allocationname="unknown" flags="0" pid="3304690" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83627e+01" utime="4.96297e+01" stime="6.33711e+00" mtime="3.24931e+01" gflop="0.00000e+00" gbyte="3.78139e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24931e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82618e+01" utime="4.95974e+01" stime="6.32767e+00" mtime="3.24931e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24931e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4916e+08" > 3.5559e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4950e+08" > 2.9396e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3659e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4615e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0165e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4399e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8110e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2401e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2458e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9590e+01 </func>
</region>
</regions>
<internal rank="611" log_i="1723713849.507819" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="612" mpi_size="768" stamp_init="1723713791.161887" stamp_final="1723713849.503282" username="apac4" allocationname="unknown" flags="0" pid="3304691" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83414e+01" utime="4.77729e+01" stime="7.20153e+00" mtime="3.25983e+01" gflop="0.00000e+00" gbyte="3.77312e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25983e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43a153c153d15a1553d153c1520" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82411e+01" utime="4.77424e+01" stime="7.19139e+00" mtime="3.25983e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25983e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4954e+08" > 5.0074e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4912e+08" > 4.4087e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.4404e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4801e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.9860e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0139e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4393e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8124e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4404e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2507e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8440e+01 </func>
</region>
</regions>
<internal rank="612" log_i="1723713849.503282" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="613" mpi_size="768" stamp_init="1723713791.161679" stamp_final="1723713849.506541" username="apac4" allocationname="unknown" flags="0" pid="3304692" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83449e+01" utime="4.97065e+01" stime="6.00970e+00" mtime="3.26574e+01" gflop="0.00000e+00" gbyte="3.77563e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26574e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f8157855f815f31535" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82441e+01" utime="4.96707e+01" stime="6.00433e+00" mtime="3.26574e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26574e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4865e+08" > 3.6095e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4902e+08" > 2.8893e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3026e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4871e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0169e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4396e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8143e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2687e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2459e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9787e+01 </func>
</region>
</regions>
<internal rank="613" log_i="1723713849.506541" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="614" mpi_size="768" stamp_init="1723713791.161946" stamp_final="1723713849.506857" username="apac4" allocationname="unknown" flags="0" pid="3304693" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83449e+01" utime="4.76451e+01" stime="7.16651e+00" mtime="3.23514e+01" gflop="0.00000e+00" gbyte="3.78040e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23514e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a714a914aa143455aa14aa14ab" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82446e+01" utime="4.76152e+01" stime="7.15500e+00" mtime="3.23514e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23514e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4594e+08" > 5.0460e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4556e+08" > 4.5222e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7523e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4877e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.7220e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0137e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4390e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8159e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8014e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2500e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8874e+01 </func>
</region>
</regions>
<internal rank="614" log_i="1723713849.506857" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="615" mpi_size="768" stamp_init="1723713791.161681" stamp_final="1723713849.500734" username="apac4" allocationname="unknown" flags="0" pid="3304694" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83391e+01" utime="4.95338e+01" stime="6.50671e+00" mtime="3.26109e+01" gflop="0.00000e+00" gbyte="3.74390e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26109e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82390e+01" utime="4.95024e+01" stime="6.49804e+00" mtime="3.26109e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26109e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4620e+08" > 3.5699e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4750e+08" > 2.8697e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4599e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4737e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0198e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4391e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8190e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8706e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2424e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9598e+01 </func>
</region>
</regions>
<internal rank="615" log_i="1723713849.500734" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="616" mpi_size="768" stamp_init="1723713791.162146" stamp_final="1723713849.512057" username="apac4" allocationname="unknown" flags="0" pid="3304695" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83499e+01" utime="4.65882e+01" stime="7.53930e+00" mtime="3.18679e+01" gflop="0.00000e+00" gbyte="3.76064e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18679e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004e15c6554e154d1535" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82504e+01" utime="4.65548e+01" stime="7.53232e+00" mtime="3.18679e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18679e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4791e+08" > 7.9063e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4764e+08" > 7.9863e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7731e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4733e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.4921e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0135e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4376e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8261e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8086e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2510e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8058e+01 </func>
</region>
</regions>
<internal rank="616" log_i="1723713849.512057" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="617" mpi_size="768" stamp_init="1723713791.162065" stamp_final="1723713849.504166" username="apac4" allocationname="unknown" flags="0" pid="3304696" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83421e+01" utime="4.95595e+01" stime="6.46429e+00" mtime="3.24186e+01" gflop="0.00000e+00" gbyte="3.77296e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24186e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000241524150d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82411e+01" utime="4.95235e+01" stime="6.45934e+00" mtime="3.24186e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24186e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4997e+08" > 4.8242e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4952e+08" > 3.5831e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7724e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4735e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9789e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0183e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4373e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8337e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9898e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2413e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8963e+01 </func>
</region>
</regions>
<internal rank="617" log_i="1723713849.504166" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="618" mpi_size="768" stamp_init="1723713791.162138" stamp_final="1723713849.511997" username="apac4" allocationname="unknown" flags="0" pid="3304697" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83499e+01" utime="4.70601e+01" stime="7.60282e+00" mtime="3.17146e+01" gflop="0.00000e+00" gbyte="3.77117e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17146e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005a14b8555a145a1483" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82500e+01" utime="4.70278e+01" stime="7.59422e+00" mtime="3.17146e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17146e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4931e+08" > 6.3077e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4916e+08" > 5.0180e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4972e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4740e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.4538e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0135e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4380e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8308e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2187e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2520e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8369e+01 </func>
</region>
</regions>
<internal rank="618" log_i="1723713849.511997" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="619" mpi_size="768" stamp_init="1723713791.168982" stamp_final="1723713849.513031" username="apac4" allocationname="unknown" flags="0" pid="3304698" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83440e+01" utime="4.90026e+01" stime="6.86361e+00" mtime="3.25986e+01" gflop="0.00000e+00" gbyte="3.76740e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25986e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f614f614f9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82485e+01" utime="4.89738e+01" stime="6.85248e+00" mtime="3.25986e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25986e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4914e+08" > 5.0198e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4874e+08" > 2.9047e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4914e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4761e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0135e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4381e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8293e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7490e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2413e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9413e+01 </func>
</region>
</regions>
<internal rank="619" log_i="1723713849.513031" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="620" mpi_size="768" stamp_init="1723713791.166847" stamp_final="1723713849.504025" username="apac4" allocationname="unknown" flags="0" pid="3304699" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83372e+01" utime="4.67966e+01" stime="7.61688e+00" mtime="3.17681e+01" gflop="0.00000e+00" gbyte="3.76820e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17681e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82391e+01" utime="4.67673e+01" stime="7.60563e+00" mtime="3.17681e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17681e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-05 </func>
<func name="MPI_Isend" count="127164" bytes="8.4830e+08" > 6.6086e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4788e+08" > 4.0895e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5245e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4626e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.9759e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0156e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4379e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8296e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0804e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2495e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8386e+01 </func>
</region>
</regions>
<internal rank="620" log_i="1723713849.504025" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="621" mpi_size="768" stamp_init="1723713791.168732" stamp_final="1723713849.515115" username="apac4" allocationname="unknown" flags="0" pid="3304700" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83464e+01" utime="4.93852e+01" stime="6.71757e+00" mtime="3.24186e+01" gflop="0.00000e+00" gbyte="3.77129e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24186e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82510e+01" utime="4.93519e+01" stime="6.71076e+00" mtime="3.24186e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24186e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4822e+08" > 4.7569e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4694e+08" > 2.9921e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2304e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4831e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0156e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4371e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8402e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8706e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2410e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9510e+01 </func>
</region>
</regions>
<internal rank="621" log_i="1723713849.515115" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="622" mpi_size="768" stamp_init="1723713791.175800" stamp_final="1723713849.507515" username="apac4" allocationname="unknown" flags="0" pid="3304701" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83317e+01" utime="4.76537e+01" stime="7.42995e+00" mtime="3.23345e+01" gflop="0.00000e+00" gbyte="3.77872e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23345e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82418e+01" utime="4.76236e+01" stime="7.41983e+00" mtime="3.23345e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23345e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4714e+08" > 5.4870e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4883e+08" > 3.6640e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8978e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4761e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.7936e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0189e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4372e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8389e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2401e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2493e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8682e+01 </func>
</region>
</regions>
<internal rank="622" log_i="1723713849.507515" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="623" mpi_size="768" stamp_init="1723713791.173533" stamp_final="1723713849.508108" username="apac4" allocationname="unknown" flags="0" pid="3304702" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="5.83346e+01" utime="4.92565e+01" stime="6.77753e+00" mtime="3.23224e+01" gflop="0.00000e+00" gbyte="3.76945e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23224e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003d1451563d143d145c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82431e+01" utime="4.92220e+01" stime="6.77231e+00" mtime="3.23224e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23224e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4926e+08" > 5.0690e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4860e+08" > 2.9033e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2768e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4737e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0189e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4359e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8513e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1209e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2415e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9343e+01 </func>
</region>
</regions>
<internal rank="623" log_i="1723713849.508108" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="624" mpi_size="768" stamp_init="1723713791.111620" stamp_final="1723713849.512401" username="apac4" allocationname="unknown" flags="0" pid="1222829" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.84008e+01" utime="4.19171e+01" stime="1.21427e+01" mtime="3.15736e+01" gflop="0.00000e+00" gbyte="3.86765e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15736e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001915191525" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83510e+01" utime="4.18849e+01" stime="1.21357e+01" mtime="3.15736e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15736e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4896e+08" > 7.3737e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4642e+08" > 6.1393e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4172e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4741e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.3260e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0099e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4363e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8501e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7299e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2537e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8190e+01 </func>
</region>
</regions>
<internal rank="624" log_i="1723713849.512401" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="625" mpi_size="768" stamp_init="1723713791.109008" stamp_final="1723713849.504353" username="apac4" allocationname="unknown" flags="0" pid="1222830" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83953e+01" utime="5.03492e+01" stime="6.77609e+00" mtime="3.23136e+01" gflop="0.00000e+00" gbyte="3.78006e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23136e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000011146056111411148e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83467e+01" utime="5.03204e+01" stime="6.76435e+00" mtime="3.23136e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23136e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 4.5556e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4989e+08" > 3.3858e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7623e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4078e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7180e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0786e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4366e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8417e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6679e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2475e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8873e+01 </func>
</region>
</regions>
<internal rank="625" log_i="1723713849.504353" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="626" mpi_size="768" stamp_init="1723713791.111459" stamp_final="1723713849.510007" username="apac4" allocationname="unknown" flags="0" pid="1222831" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83985e+01" utime="4.78809e+01" stime="7.44869e+00" mtime="3.19330e+01" gflop="0.00000e+00" gbyte="3.77079e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19330e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000da143055da14d91483" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83471e+01" utime="4.78475e+01" stime="7.44223e+00" mtime="3.19330e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19330e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4894e+08" > 7.1420e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5026e+08" > 4.2235e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6385e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4317e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3403e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1235e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4361e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8498e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3092e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2538e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8318e+01 </func>
</region>
</regions>
<internal rank="626" log_i="1723713849.510007" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="627" mpi_size="768" stamp_init="1723713791.112446" stamp_final="1723713849.503335" username="apac4" allocationname="unknown" flags="0" pid="1222832" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83909e+01" utime="5.03207e+01" stime="6.55482e+00" mtime="3.23457e+01" gflop="0.00000e+00" gbyte="3.77499e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23457e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4831485148614795686148514c6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83371e+01" utime="5.02889e+01" stime="6.54733e+00" mtime="3.23457e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23457e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5120e+08" > 4.5417e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4863e+08" > 3.6271e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3238e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4040e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1233e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4358e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8500e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0899e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2470e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9342e+01 </func>
</region>
</regions>
<internal rank="627" log_i="1723713849.503335" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="628" mpi_size="768" stamp_init="1723713791.116673" stamp_final="1723713849.512990" username="apac4" allocationname="unknown" flags="0" pid="1222833" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83963e+01" utime="4.73220e+01" stime="7.13322e+00" mtime="3.18797e+01" gflop="0.00000e+00" gbyte="3.74519e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18797e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b615b6150c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83401e+01" utime="4.72922e+01" stime="7.12416e+00" mtime="3.18797e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18797e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4834e+08" > 6.2217e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5014e+08" > 4.7281e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5799e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5173e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8028e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0099e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4351e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8594e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2091e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2545e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8426e+01 </func>
</region>
</regions>
<internal rank="628" log_i="1723713849.512990" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="629" mpi_size="768" stamp_init="1723713791.119006" stamp_final="1723713849.503888" username="apac4" allocationname="unknown" flags="0" pid="1222834" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83849e+01" utime="5.06471e+01" stime="6.45876e+00" mtime="3.26284e+01" gflop="0.00000e+00" gbyte="3.76209e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26284e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004f144f14ec" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83182e+01" utime="5.06209e+01" stime="6.44430e+00" mtime="3.26284e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26284e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4882e+08" > 4.5597e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4861e+08" > 3.3568e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4009e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4231e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1435e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4347e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8607e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7084e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2391e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9527e+01 </func>
</region>
</regions>
<internal rank="629" log_i="1723713849.503888" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="630" mpi_size="768" stamp_init="1723713791.126865" stamp_final="1723713849.516229" username="apac4" allocationname="unknown" flags="0" pid="1222835" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83894e+01" utime="4.73013e+01" stime="7.19549e+00" mtime="3.18532e+01" gflop="0.00000e+00" gbyte="3.76595e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18532e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf486158815891526558915891504" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83149e+01" utime="4.72727e+01" stime="7.18320e+00" mtime="3.18532e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18532e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4917e+08" > 6.5066e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4799e+08" > 4.5042e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4626e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5000e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2016e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0099e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4346e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8620e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7990e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2529e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8511e+01 </func>
</region>
</regions>
<internal rank="630" log_i="1723713849.516229" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="631" mpi_size="768" stamp_init="1723713791.124733" stamp_final="1723713849.502998" username="apac4" allocationname="unknown" flags="0" pid="1222836" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83783e+01" utime="5.05400e+01" stime="6.59439e+00" mtime="3.25618e+01" gflop="0.00000e+00" gbyte="3.76404e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25618e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82989e+01" utime="5.05065e+01" stime="6.58608e+00" mtime="3.25618e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25618e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4750e+08" > 4.4682e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4879e+08" > 3.9899e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5987e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4202e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1438e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4347e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8644e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8896e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2372e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9268e+01 </func>
</region>
</regions>
<internal rank="631" log_i="1723713849.502998" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="632" mpi_size="768" stamp_init="1723713791.126861" stamp_final="1723713849.512258" username="apac4" allocationname="unknown" flags="0" pid="1222837" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83854e+01" utime="4.63358e+01" stime="8.25836e+00" mtime="3.21405e+01" gflop="0.00000e+00" gbyte="3.75275e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21405e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ce15dc55ce15ce150a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83072e+01" utime="4.63018e+01" stime="8.25068e+00" mtime="3.21405e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21405e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4852e+08" > 7.0058e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4832e+08" > 5.9374e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.6078e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4325e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8659e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0965e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4329e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8741e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8610e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2532e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7556e+01 </func>
</region>
</regions>
<internal rank="632" log_i="1723713849.512258" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="633" mpi_size="768" stamp_init="1723713791.128547" stamp_final="1723713849.508871" username="apac4" allocationname="unknown" flags="0" pid="1222838" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83803e+01" utime="5.05750e+01" stime="6.55096e+00" mtime="3.26901e+01" gflop="0.00000e+00" gbyte="3.78094e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26901e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000082148114a4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82911e+01" utime="5.05408e+01" stime="6.54345e+00" mtime="3.26901e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26901e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127164" bytes="8.5054e+08" > 4.6446e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4801e+08" > 2.8607e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0884e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4121e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1221e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4340e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8720e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6679e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2389e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8910e+01 </func>
</region>
</regions>
<internal rank="633" log_i="1723713849.508871" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="634" mpi_size="768" stamp_init="1723713791.142879" stamp_final="1723713849.501845" username="apac4" allocationname="unknown" flags="0" pid="1222839" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83590e+01" utime="4.81767e+01" stime="7.58362e+00" mtime="3.22747e+01" gflop="0.00000e+00" gbyte="3.75984e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22747e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ce14e714f9145d55f914f414f8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82781e+01" utime="4.81424e+01" stime="7.57736e+00" mtime="3.22747e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22747e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4954e+08" > 6.3448e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5070e+08" > 4.2273e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.5227e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4285e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.1301e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1009e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4337e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8754e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8086e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2527e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7863e+01 </func>
</region>
</regions>
<internal rank="634" log_i="1723713849.501845" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="635" mpi_size="768" stamp_init="1723713791.133568" stamp_final="1723713849.515276" username="apac4" allocationname="unknown" flags="0" pid="1222840" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83817e+01" utime="5.02205e+01" stime="6.75323e+00" mtime="3.24075e+01" gflop="0.00000e+00" gbyte="3.77014e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24075e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006c147b566c146c14dd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82975e+01" utime="5.01848e+01" stime="6.74712e+00" mtime="3.24075e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24075e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4970e+08" > 4.6186e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4905e+08" > 2.8960e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7934e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4262e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1440e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4339e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8733e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3593e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2384e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8909e+01 </func>
</region>
</regions>
<internal rank="635" log_i="1723713849.515276" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="636" mpi_size="768" stamp_init="1723713791.137091" stamp_final="1723713849.513200" username="apac4" allocationname="unknown" flags="0" pid="1222841" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83761e+01" utime="4.81676e+01" stime="7.59701e+00" mtime="3.24418e+01" gflop="0.00000e+00" gbyte="3.76160e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24418e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e115e11542" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82964e+01" utime="4.81367e+01" stime="7.58659e+00" mtime="3.24418e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24418e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4957e+08" > 6.5571e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4909e+08" > 4.2099e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.4976e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4060e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.7503e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0962e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4333e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8793e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1400e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2529e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8052e+01 </func>
</region>
</regions>
<internal rank="636" log_i="1723713849.513200" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="637" mpi_size="768" stamp_init="1723713791.144696" stamp_final="1723713849.511311" username="apac4" allocationname="unknown" flags="0" pid="1222842" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83666e+01" utime="5.05054e+01" stime="6.63548e+00" mtime="3.29423e+01" gflop="0.00000e+00" gbyte="3.77693e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.29423e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001014375610141014d1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82854e+01" utime="5.04711e+01" stime="6.62818e+00" mtime="3.29423e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.29423e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4834e+08" > 4.5767e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4962e+08" > 2.6414e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0312e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4179e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0958e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4329e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8830e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1996e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2382e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9226e+01 </func>
</region>
</regions>
<internal rank="637" log_i="1723713849.511311" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="638" mpi_size="768" stamp_init="1723713791.143293" stamp_final="1723713849.511275" username="apac4" allocationname="unknown" flags="0" pid="1222843" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83680e+01" utime="4.80543e+01" stime="7.63199e+00" mtime="3.17790e+01" gflop="0.00000e+00" gbyte="3.78178e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17790e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82868e+01" utime="4.80195e+01" stime="7.62508e+00" mtime="3.17790e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17790e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4829e+08" > 5.7974e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4836e+08" > 5.8444e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7634e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4234e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3566e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1223e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4325e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8839e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3498e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2520e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8168e+01 </func>
</region>
</regions>
<internal rank="638" log_i="1723713849.511275" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="639" mpi_size="768" stamp_init="1723713791.144997" stamp_final="1723713849.510596" username="apac4" allocationname="unknown" flags="0" pid="1222844" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83656e+01" utime="5.03554e+01" stime="6.77018e+00" mtime="3.27448e+01" gflop="0.00000e+00" gbyte="3.77834e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27448e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d114f655d114d114fc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82874e+01" utime="5.03245e+01" stime="6.76026e+00" mtime="3.27448e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27448e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4813e+08" > 4.5982e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4902e+08" > 2.4022e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9973e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4211e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4796e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1424e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4324e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8851e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3092e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2396e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9055e+01 </func>
</region>
</regions>
<internal rank="639" log_i="1723713849.510596" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="640" mpi_size="768" stamp_init="1723713791.154594" stamp_final="1723713849.516362" username="apac4" allocationname="unknown" flags="0" pid="1222845" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83618e+01" utime="4.64356e+01" stime="8.26805e+00" mtime="3.18756e+01" gflop="0.00000e+00" gbyte="3.76904e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18756e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ff15cb151e15b0551e15191516" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82909e+01" utime="4.64048e+01" stime="8.25873e+00" mtime="3.18756e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18756e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4844e+08" > 9.9577e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4864e+08" > 5.8205e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6399e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4072e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.1712e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0337e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4286e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9247e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1686e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2518e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7989e+01 </func>
</region>
</regions>
<internal rank="640" log_i="1723713849.516362" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="641" mpi_size="768" stamp_init="1723713791.151533" stamp_final="1723713849.504792" username="apac4" allocationname="unknown" flags="0" pid="1222846" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83533e+01" utime="5.03922e+01" stime="6.72546e+00" mtime="3.26558e+01" gflop="0.00000e+00" gbyte="3.76011e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26558e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82758e+01" utime="5.03624e+01" stime="6.71450e+00" mtime="3.26558e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26558e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4884e+08" > 5.6557e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4873e+08" > 2.9681e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8222e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4252e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0966e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4291e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9223e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7704e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2396e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9030e+01 </func>
</region>
</regions>
<internal rank="641" log_i="1723713849.504792" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="642" mpi_size="768" stamp_init="1723713791.154284" stamp_final="1723713849.516303" username="apac4" allocationname="unknown" flags="0" pid="1222847" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83620e+01" utime="4.79200e+01" stime="7.87701e+00" mtime="3.19216e+01" gflop="0.00000e+00" gbyte="3.76152e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19216e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48f1590159115bd55911591150a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82880e+01" utime="4.78872e+01" stime="7.87024e+00" mtime="3.19216e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19216e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5003e+08" > 7.9644e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4932e+08" > 4.9666e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2698e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4155e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.6798e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0953e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4290e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9226e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9492e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2521e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8606e+01 </func>
</region>
</regions>
<internal rank="642" log_i="1723713849.516303" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="643" mpi_size="768" stamp_init="1723713791.155556" stamp_final="1723713849.507727" username="apac4" allocationname="unknown" flags="0" pid="1222848" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83522e+01" utime="4.98452e+01" stime="7.22784e+00" mtime="3.26443e+01" gflop="0.00000e+00" gbyte="3.76438e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26443e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82802e+01" utime="4.98164e+01" stime="7.21696e+00" mtime="3.26443e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26443e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4886e+08" > 5.7186e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5082e+08" > 3.3752e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9257e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4242e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1307e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4285e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9244e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2806e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2390e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8902e+01 </func>
</region>
</regions>
<internal rank="643" log_i="1723713849.507727" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="644" mpi_size="768" stamp_init="1723713791.167594" stamp_final="1723713849.521224" username="apac4" allocationname="unknown" flags="0" pid="1222849" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83536e+01" utime="4.66333e+01" stime="7.73661e+00" mtime="3.20573e+01" gflop="0.00000e+00" gbyte="3.77258e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20573e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82871e+01" utime="4.66005e+01" stime="7.73003e+00" mtime="3.20573e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20573e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4974e+08" > 8.7718e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4843e+08" > 5.0523e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5753e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5150e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.8944e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0098e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4284e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9285e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1781e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2493e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8355e+01 </func>
</region>
</regions>
<internal rank="644" log_i="1723713849.521224" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="645" mpi_size="768" stamp_init="1723713791.163055" stamp_final="1723713849.509162" username="apac4" allocationname="unknown" flags="0" pid="1222850" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83461e+01" utime="4.99805e+01" stime="7.13392e+00" mtime="3.27202e+01" gflop="0.00000e+00" gbyte="3.76595e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27202e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82784e+01" utime="4.99485e+01" stime="7.12647e+00" mtime="3.27202e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27202e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 5.6401e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4785e+08" > 2.9518e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5018e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4200e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0949e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4283e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9309e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6298e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2390e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9421e+01 </func>
</region>
</regions>
<internal rank="645" log_i="1723713849.509162" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="646" mpi_size="768" stamp_init="1723713791.167597" stamp_final="1723713849.520730" username="apac4" allocationname="unknown" flags="0" pid="1222851" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83531e+01" utime="4.79397e+01" stime="7.78565e+00" mtime="3.15694e+01" gflop="0.00000e+00" gbyte="3.76163e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15694e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009f149e14e9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82923e+01" utime="4.79116e+01" stime="7.77532e+00" mtime="3.15694e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15694e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4821e+08" > 7.8903e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4855e+08" > 4.8288e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8965e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4069e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.1489e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0839e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4283e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9295e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8110e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2489e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8641e+01 </func>
</region>
</regions>
<internal rank="646" log_i="1723713849.520730" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="647" mpi_size="768" stamp_init="1723713791.172222" stamp_final="1723713849.507640" username="apac4" allocationname="unknown" flags="0" pid="1222852" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="5.83354e+01" utime="5.03676e+01" stime="6.77952e+00" mtime="3.23112e+01" gflop="0.00000e+00" gbyte="3.75603e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23112e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004f144e146b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82712e+01" utime="5.03356e+01" stime="6.77175e+00" mtime="3.23112e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23112e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4866e+08" > 5.6047e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4734e+08" > 2.3621e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1506e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4243e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3590e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0838e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4282e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9316e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7108e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2389e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9370e+01 </func>
</region>
</regions>
<internal rank="647" log_i="1723713849.507640" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="648" mpi_size="768" stamp_init="1723713791.122249" stamp_final="1723713849.512468" username="apac4" allocationname="unknown" flags="0" pid="1951035" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83902e+01" utime="4.17203e+01" stime="1.30723e+01" mtime="3.16014e+01" gflop="0.00000e+00" gbyte="3.86631e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16014e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83076e+01" utime="4.16863e+01" stime="1.30658e+01" mtime="3.16014e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16014e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4958e+08" > 8.1137e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4848e+08" > 7.0116e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3967e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4141e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4150e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6616e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4274e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9389e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9302e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2442e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8137e+01 </func>
</region>
</regions>
<internal rank="648" log_i="1723713849.512468" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="649" mpi_size="768" stamp_init="1723713791.121081" stamp_final="1723713849.499663" username="apac4" allocationname="unknown" flags="0" pid="1951036" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83786e+01" utime="5.00949e+01" stime="6.98608e+00" mtime="3.25422e+01" gflop="0.00000e+00" gbyte="3.78178e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25422e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002c142b1466" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82951e+01" utime="5.00620e+01" stime="6.97719e+00" mtime="3.25422e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25422e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4848e+08" > 4.8749e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4864e+08" > 3.0215e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4880e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4289e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0994e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0137e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4272e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9412e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6703e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7666e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9347e+01 </func>
</region>
</regions>
<internal rank="649" log_i="1723713849.499663" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="650" mpi_size="768" stamp_init="1723713791.121054" stamp_final="1723713849.514232" username="apac4" allocationname="unknown" flags="0" pid="1951037" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83932e+01" utime="4.91879e+01" stime="6.97059e+00" mtime="3.18228e+01" gflop="0.00000e+00" gbyte="3.74329e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18228e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000093145f569314921472" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83119e+01" utime="4.91542e+01" stime="6.96380e+00" mtime="3.18228e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18228e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4981e+08" > 5.6135e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4810e+08" > 4.9014e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3710e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4096e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0411e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6669e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4282e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9311e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8706e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2377e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8659e+01 </func>
</region>
</regions>
<internal rank="650" log_i="1723713849.514232" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="651" mpi_size="768" stamp_init="1723713791.121075" stamp_final="1723713849.509504" username="apac4" allocationname="unknown" flags="0" pid="1951038" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83884e+01" utime="5.04800e+01" stime="6.56635e+00" mtime="3.24584e+01" gflop="0.00000e+00" gbyte="3.77731e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24584e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006c146c14f6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83088e+01" utime="5.04514e+01" stime="6.55355e+00" mtime="3.24584e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24584e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127164" bytes="8.4925e+08" > 4.4951e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5121e+08" > 2.8756e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4271e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4299e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6668e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4275e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9378e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8205e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7659e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9367e+01 </func>
</region>
</regions>
<internal rank="651" log_i="1723713849.509504" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="652" mpi_size="768" stamp_init="1723713791.121055" stamp_final="1723713849.519900" username="apac4" allocationname="unknown" flags="0" pid="1951039" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83988e+01" utime="4.81755e+01" stime="7.41475e+00" mtime="3.19088e+01" gflop="0.00000e+00" gbyte="3.76820e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19088e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4fb14fc14fe14c055fe14fd1464" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83224e+01" utime="4.81423e+01" stime="7.40719e+00" mtime="3.19088e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19088e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5082e+08" > 6.5174e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4949e+08" > 4.4303e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8117e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4084e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5831e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6657e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4273e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9394e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6321e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2373e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8222e+01 </func>
</region>
</regions>
<internal rank="652" log_i="1723713849.519900" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="653" mpi_size="768" stamp_init="1723713791.121582" stamp_final="1723713849.499323" username="apac4" allocationname="unknown" flags="0" pid="1951040" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83777e+01" utime="5.04965e+01" stime="6.61249e+00" mtime="3.26792e+01" gflop="0.00000e+00" gbyte="3.74649e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26792e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000df14de149d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82988e+01" utime="5.04611e+01" stime="6.60742e+00" mtime="3.26792e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26792e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4962e+08" > 4.4989e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4958e+08" > 2.8114e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5258e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4229e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9550e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0132e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4262e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9514e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9421e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7672e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9492e+01 </func>
</region>
</regions>
<internal rank="653" log_i="1723713849.499323" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="654" mpi_size="768" stamp_init="1723713791.124177" stamp_final="1723713849.505399" username="apac4" allocationname="unknown" flags="0" pid="1951041" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83812e+01" utime="4.74278e+01" stime="7.60796e+00" mtime="3.15462e+01" gflop="0.00000e+00" gbyte="3.76591e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15462e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ec15eb1551" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83039e+01" utime="4.73983e+01" stime="7.59756e+00" mtime="3.15462e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15462e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4868e+08" > 7.5430e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4983e+08" > 4.9081e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0320e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4074e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.7683e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6751e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4261e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9485e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7204e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2375e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8527e+01 </func>
</region>
</regions>
<internal rank="654" log_i="1723713849.505399" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="655" mpi_size="768" stamp_init="1723713791.126940" stamp_final="1723713849.513869" username="apac4" allocationname="unknown" flags="0" pid="1951042" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83869e+01" utime="5.07499e+01" stime="6.34608e+00" mtime="3.21672e+01" gflop="0.00000e+00" gbyte="3.77968e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21672e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82993e+01" utime="5.07177e+01" stime="6.33681e+00" mtime="3.21672e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21672e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4799e+08" > 4.5347e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4804e+08" > 3.0509e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0328e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4159e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6721e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4252e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9614e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2401e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7662e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9478e+01 </func>
</region>
</regions>
<internal rank="655" log_i="1723713849.513869" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="656" mpi_size="768" stamp_init="1723713791.128389" stamp_final="1723713849.511271" username="apac4" allocationname="unknown" flags="0" pid="1951043" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83829e+01" utime="4.70650e+01" stime="7.22530e+00" mtime="3.20217e+01" gflop="0.00000e+00" gbyte="3.75778e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20217e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f214f114d7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82958e+01" utime="4.70351e+01" stime="7.21441e+00" mtime="3.20217e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20217e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4832e+08" > 4.8452e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4675e+08" > 4.4909e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6310e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4394e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.3112e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0659e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4240e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9680e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9302e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1913e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8674e+01 </func>
</region>
</regions>
<internal rank="656" log_i="1723713849.511271" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="657" mpi_size="768" stamp_init="1723713791.131191" stamp_final="1723713849.508993" username="apac4" allocationname="unknown" flags="0" pid="1951044" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83778e+01" utime="5.03421e+01" stime="6.59939e+00" mtime="3.27829e+01" gflop="0.00000e+00" gbyte="3.76320e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27829e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d414d455d414d4146d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82926e+01" utime="5.03144e+01" stime="6.58632e+00" mtime="3.27829e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27829e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4734e+08" > 3.5656e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4709e+08" > 2.7769e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6484e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4086e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9443e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4247e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9676e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1185e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7666e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9583e+01 </func>
</region>
</regions>
<internal rank="657" log_i="1723713849.508993" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="658" mpi_size="768" stamp_init="1723713791.133911" stamp_final="1723713849.505616" username="apac4" allocationname="unknown" flags="0" pid="1951045" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83717e+01" utime="4.84456e+01" stime="7.18774e+00" mtime="3.22537e+01" gflop="0.00000e+00" gbyte="3.76049e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22537e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000481490554814481478" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82832e+01" utime="4.84127e+01" stime="7.17926e+00" mtime="3.22537e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22537e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4884e+08" > 5.3549e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4833e+08" > 4.4062e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5444e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4199e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0633e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0114e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4238e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9710e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3522e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2343e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8935e+01 </func>
</region>
</regions>
<internal rank="658" log_i="1723713849.505616" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="659" mpi_size="768" stamp_init="1723713791.136517" stamp_final="1723713849.513432" username="apac4" allocationname="unknown" flags="0" pid="1951046" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83769e+01" utime="5.09177e+01" stime="6.17446e+00" mtime="3.24746e+01" gflop="0.00000e+00" gbyte="3.76255e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24746e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005b14cf555b145b1471" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82905e+01" utime="5.08846e+01" stime="6.16664e+00" mtime="3.24746e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24746e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4941e+08" > 3.5148e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4912e+08" > 3.0129e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3939e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4149e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0140e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4246e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9676e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4404e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7652e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9523e+01 </func>
</region>
</regions>
<internal rank="659" log_i="1723713849.513432" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="660" mpi_size="768" stamp_init="1723713791.138671" stamp_final="1723713849.514694" username="apac4" allocationname="unknown" flags="0" pid="1951047" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83760e+01" utime="4.71583e+01" stime="7.73739e+00" mtime="3.20112e+01" gflop="0.00000e+00" gbyte="3.74626e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20112e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000073156e1534" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82883e+01" utime="4.71277e+01" stime="7.72739e+00" mtime="3.20112e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20112e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 6.3345e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4967e+08" > 4.0792e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8768e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4202e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3995e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7595e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4236e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9739e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3712e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.2122e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8269e+01 </func>
</region>
</regions>
<internal rank="660" log_i="1723713849.514694" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="661" mpi_size="768" stamp_init="1723713791.142469" stamp_final="1723713849.513384" username="apac4" allocationname="unknown" flags="0" pid="1951048" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83709e+01" utime="5.06332e+01" stime="6.50198e+00" mtime="3.26826e+01" gflop="0.00000e+00" gbyte="3.76900e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26826e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009414941458" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82858e+01" utime="5.06055e+01" stime="6.48881e+00" mtime="3.26826e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26826e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4875e+08" > 3.5518e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4838e+08" > 2.8889e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3770e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4135e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0118e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4235e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9788e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1400e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7652e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9747e+01 </func>
</region>
</regions>
<internal rank="661" log_i="1723713849.513384" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="662" mpi_size="768" stamp_init="1723713791.144268" stamp_final="1723713849.504847" username="apac4" allocationname="unknown" flags="0" pid="1951049" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83606e+01" utime="4.85940e+01" stime="7.14383e+00" mtime="3.21455e+01" gflop="0.00000e+00" gbyte="3.76129e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21455e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e214e214b3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82769e+01" utime="4.85594e+01" stime="7.13889e+00" mtime="3.21455e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21455e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4782e+08" > 5.1068e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4941e+08" > 4.4096e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3466e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4023e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5060e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9434e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4224e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9860e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9206e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1853e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9067e+01 </func>
</region>
</regions>
<internal rank="662" log_i="1723713849.504847" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="663" mpi_size="768" stamp_init="1723713791.146599" stamp_final="1723713849.509247" username="apac4" allocationname="unknown" flags="0" pid="1951050" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83626e+01" utime="5.08580e+01" stime="6.24710e+00" mtime="3.27403e+01" gflop="0.00000e+00" gbyte="3.77174e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27403e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008114811457" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82789e+01" utime="5.08258e+01" stime="6.23908e+00" mtime="3.27403e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27403e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4903e+08" > 3.5130e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4646e+08" > 2.6530e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0919e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4338e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4080e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9437e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4232e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9780e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4785e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7624e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 2.0078e+01 </func>
</region>
</regions>
<internal rank="663" log_i="1723713849.509247" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="664" mpi_size="768" stamp_init="1723713791.150215" stamp_final="1723713849.511119" username="apac4" allocationname="unknown" flags="0" pid="1951051" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83609e+01" utime="4.78732e+01" stime="6.78444e+00" mtime="3.14853e+01" gflop="0.00000e+00" gbyte="3.76423e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.14853e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e715e915ea15e655ea15ea1502" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82783e+01" utime="4.78404e+01" stime="6.77754e+00" mtime="3.14853e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.14853e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4809e+08" > 6.1907e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4934e+08" > 4.2644e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8521e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4384e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8481e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0659e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4215e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9963e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8014e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1858e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8791e+01 </func>
</region>
</regions>
<internal rank="664" log_i="1723713849.511119" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="665" mpi_size="768" stamp_init="1723713791.154173" stamp_final="1723713849.509418" username="apac4" allocationname="unknown" flags="0" pid="1951052" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83552e+01" utime="5.06560e+01" stime="6.45878e+00" mtime="3.21252e+01" gflop="0.00000e+00" gbyte="3.77396e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21252e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a315a3154d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82752e+01" utime="5.06220e+01" stime="6.45302e+00" mtime="3.21252e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21252e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4908e+08" > 4.8403e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4869e+08" > 3.3807e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0567e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4097e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8001e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4219e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9951e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7514e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7626e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9384e+01 </func>
</region>
</regions>
<internal rank="665" log_i="1723713849.509418" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="666" mpi_size="768" stamp_init="1723713791.156563" stamp_final="1723713849.513223" username="apac4" allocationname="unknown" flags="0" pid="1951053" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83567e+01" utime="4.89442e+01" stime="7.47777e+00" mtime="3.21605e+01" gflop="0.00000e+00" gbyte="3.74302e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21605e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48615871588157555881588154c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82803e+01" utime="4.89106e+01" stime="7.47269e+00" mtime="3.21605e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21605e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4910e+08" > 5.3744e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4945e+08" > 4.0451e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6158e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4193e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.1301e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0083e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4212e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0002e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2616e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1170e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8774e+01 </func>
</region>
</regions>
<internal rank="666" log_i="1723713849.513223" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="667" mpi_size="768" stamp_init="1723713791.157927" stamp_final="1723713849.504477" username="apac4" allocationname="unknown" flags="0" pid="1951054" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83465e+01" utime="4.98871e+01" stime="7.21252e+00" mtime="3.25061e+01" gflop="0.00000e+00" gbyte="3.77342e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25061e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000da14f055da14d9146e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82705e+01" utime="4.98510e+01" stime="7.20906e+00" mtime="3.25061e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25061e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4887e+08" > 4.9795e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5064e+08" > 3.3430e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4685e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4222e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4796e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0116e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4221e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9896e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8896e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7188e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9324e+01 </func>
</region>
</regions>
<internal rank="667" log_i="1723713849.504477" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="668" mpi_size="768" stamp_init="1723713791.159844" stamp_final="1723713849.511626" username="apac4" allocationname="unknown" flags="0" pid="1951055" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83518e+01" utime="4.77161e+01" stime="7.36790e+00" mtime="3.22123e+01" gflop="0.00000e+00" gbyte="3.76450e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22123e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000e714ae55e714e7149c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82756e+01" utime="4.76874e+01" stime="7.35785e+00" mtime="3.22123e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22123e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4902e+08" > 6.1413e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4796e+08" > 4.2813e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6811e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4324e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1233e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0659e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4220e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9937e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0208e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.1494e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8700e+01 </func>
</region>
</regions>
<internal rank="668" log_i="1723713849.511626" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="669" mpi_size="768" stamp_init="1723713791.161959" stamp_final="1723713849.503854" username="apac4" allocationname="unknown" flags="0" pid="1951056" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83419e+01" utime="5.02151e+01" stime="6.88131e+00" mtime="3.24382e+01" gflop="0.00000e+00" gbyte="3.76358e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24382e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c614d055c614c514d6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82696e+01" utime="5.01773e+01" stime="6.88074e+00" mtime="3.24382e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24382e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4751e+08" > 4.7694e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4735e+08" > 3.0040e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2277e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3953e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8133e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9397e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4216e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9977e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1590e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7566e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9549e+01 </func>
</region>
</regions>
<internal rank="669" log_i="1723713849.503854" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="670" mpi_size="768" stamp_init="1723713791.164706" stamp_final="1723713849.505460" username="apac4" allocationname="unknown" flags="0" pid="1951057" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83408e+01" utime="4.75640e+01" stime="7.36918e+00" mtime="3.25105e+01" gflop="0.00000e+00" gbyte="3.77834e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25105e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004314431472" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82686e+01" utime="4.75359e+01" stime="7.35827e+00" mtime="3.25105e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25105e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4734e+08" > 6.0396e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4971e+08" > 3.7712e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5818e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4426e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3804e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0659e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4212e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0007e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8110e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7672e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9119e+01 </func>
</region>
</regions>
<internal rank="670" log_i="1723713849.505460" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="671" mpi_size="768" stamp_init="1723713791.165851" stamp_final="1723713849.499329" username="apac4" allocationname="unknown" flags="0" pid="1951058" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="5.83335e+01" utime="5.02397e+01" stime="6.88426e+00" mtime="3.19657e+01" gflop="0.00000e+00" gbyte="3.74107e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19657e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005f148b565f145e1499" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82629e+01" utime="5.02111e+01" stime="6.87442e+00" mtime="3.19657e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19657e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4921e+08" > 4.8842e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4816e+08" > 3.3690e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9442e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4193e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1696e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9611e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4210e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0047e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2282e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7549e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9321e+01 </func>
</region>
</regions>
<internal rank="671" log_i="1723713849.499329" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="672" mpi_size="768" stamp_init="1723713791.109390" stamp_final="1723713849.508012" username="apac4" allocationname="unknown" flags="0" pid="1050175" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83986e+01" utime="4.32345e+01" stime="1.26488e+01" mtime="3.17411e+01" gflop="0.00000e+00" gbyte="3.85818e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17411e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000be15be1516" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83442e+01" utime="4.32005e+01" stime="1.26425e+01" mtime="3.17411e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17411e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4897e+08" > 6.2368e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4859e+08" > 3.9852e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3504e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4076e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4877e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0173e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4208e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0029e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2997e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7557e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8596e+01 </func>
</region>
</regions>
<internal rank="672" log_i="1723713849.508012" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="673" mpi_size="768" stamp_init="1723713791.109397" stamp_final="1723713849.512982" username="apac4" allocationname="unknown" flags="0" pid="1050176" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.84036e+01" utime="5.02522e+01" stime="6.64948e+00" mtime="3.22756e+01" gflop="0.00000e+00" gbyte="3.78242e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22756e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83464e+01" utime="5.02192e+01" stime="6.64206e+00" mtime="3.22756e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22756e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4789e+08" > 4.6324e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4972e+08" > 3.3296e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2707e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4059e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7180e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9530e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4205e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0057e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1304e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7552e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9384e+01 </func>
</region>
</regions>
<internal rank="673" log_i="1723713849.512982" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="674" mpi_size="768" stamp_init="1723713791.109386" stamp_final="1723713849.513525" username="apac4" allocationname="unknown" flags="0" pid="1050177" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.84041e+01" utime="4.73774e+01" stime="7.70263e+00" mtime="3.17836e+01" gflop="0.00000e+00" gbyte="3.77853e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17836e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf466146814691443556914681493" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83499e+01" utime="4.73493e+01" stime="7.69099e+00" mtime="3.17836e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17836e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4953e+08" > 7.6017e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4745e+08" > 4.1879e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2357e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4131e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3853e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8602e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4200e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0060e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0494e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7565e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8612e+01 </func>
</region>
</regions>
<internal rank="674" log_i="1723713849.513525" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="675" mpi_size="768" stamp_init="1723713791.118189" stamp_final="1723713849.515013" username="apac4" allocationname="unknown" flags="0" pid="1050178" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83968e+01" utime="5.02562e+01" stime="6.66791e+00" mtime="3.28765e+01" gflop="0.00000e+00" gbyte="3.75324e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.28765e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009115901524" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83251e+01" utime="5.02248e+01" stime="6.65854e+00" mtime="3.28765e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.28765e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4911e+08" > 4.5303e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5029e+08" > 2.9969e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0379e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4059e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8581e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4198e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0164e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9993e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7548e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9232e+01 </func>
</region>
</regions>
<internal rank="675" log_i="1723713849.515013" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="676" mpi_size="768" stamp_init="1723713791.119001" stamp_final="1723713849.514261" username="apac4" allocationname="unknown" flags="0" pid="1050179" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83953e+01" utime="4.79672e+01" stime="7.31072e+00" mtime="3.13771e+01" gflop="0.00000e+00" gbyte="3.75542e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.13771e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005d155c1553" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83179e+01" utime="4.79382e+01" stime="7.29796e+00" mtime="3.13771e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.13771e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.5763e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4965e+08" > 7.1944e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4812e+08" > 4.7108e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1728e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4065e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5916e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.2523e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4196e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0179e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1304e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7560e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8329e+01 </func>
</region>
</regions>
<internal rank="676" log_i="1723713849.514261" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="677" mpi_size="768" stamp_init="1723713791.118200" stamp_final="1723713849.515023" username="apac4" allocationname="unknown" flags="0" pid="1050180" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83968e+01" utime="4.92744e+01" stime="6.37222e+00" mtime="3.27501e+01" gflop="0.00000e+00" gbyte="3.76995e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27501e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000221518562215221509" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83253e+01" utime="4.92412e+01" stime="6.36515e+00" mtime="3.27501e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27501e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4668e+08" > 4.4431e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4677e+08" > 3.0364e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7624e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4421e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0228e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4198e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0172e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1686e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7539e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9389e+01 </func>
</region>
</regions>
<internal rank="677" log_i="1723713849.515023" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="678" mpi_size="768" stamp_init="1723713791.120840" stamp_final="1723713849.512030" username="apac4" allocationname="unknown" flags="0" pid="1050181" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83912e+01" utime="4.89937e+01" stime="7.06787e+00" mtime="3.19140e+01" gflop="0.00000e+00" gbyte="3.76499e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19140e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f6142456f614f61482" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83154e+01" utime="4.89671e+01" stime="7.05427e+00" mtime="3.19140e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19140e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4504e+08" > 5.6757e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4320e+08" > 4.5555e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3119e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4138e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1915e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2655e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4195e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0196e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1281e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7558e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8850e+01 </func>
</region>
</regions>
<internal rank="678" log_i="1723713849.512030" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="679" mpi_size="768" stamp_init="1723713791.125214" stamp_final="1723713849.505356" username="apac4" allocationname="unknown" flags="0" pid="1050182" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83801e+01" utime="5.07219e+01" stime="6.35665e+00" mtime="3.26735e+01" gflop="0.00000e+00" gbyte="3.77293e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26735e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008e148d14d8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82922e+01" utime="5.06921e+01" stime="6.34486e+00" mtime="3.26735e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26735e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4536e+08" > 4.4308e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4640e+08" > 2.6868e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4876e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4065e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7643e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2672e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4189e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0263e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2115e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7525e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9588e+01 </func>
</region>
</regions>
<internal rank="679" log_i="1723713849.505356" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="680" mpi_size="768" stamp_init="1723713791.126325" stamp_final="1723713849.506729" username="apac4" allocationname="unknown" flags="0" pid="1050183" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83804e+01" utime="4.72220e+01" stime="7.74401e+00" mtime="3.19004e+01" gflop="0.00000e+00" gbyte="3.78086e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19004e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002d142d145a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82937e+01" utime="4.71954e+01" stime="7.72831e+00" mtime="3.19004e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19004e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4719e+08" > 5.7514e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4858e+08" > 4.5621e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7307e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4305e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4561e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.7861e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4174e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0294e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1805e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7575e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8398e+01 </func>
</region>
</regions>
<internal rank="680" log_i="1723713849.506729" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="681" mpi_size="768" stamp_init="1723713791.128348" stamp_final="1723713849.502887" username="apac4" allocationname="unknown" flags="0" pid="1050184" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83745e+01" utime="4.96780e+01" stime="6.25474e+00" mtime="3.27676e+01" gflop="0.00000e+00" gbyte="3.76263e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27676e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82908e+01" utime="4.96490e+01" stime="6.24280e+00" mtime="3.27676e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27676e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4974e+08" > 3.4517e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4805e+08" > 3.1025e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7849e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4593e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0284e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4185e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0265e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3784e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7529e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9460e+01 </func>
</region>
</regions>
<internal rank="681" log_i="1723713849.502887" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="682" mpi_size="768" stamp_init="1723713791.131550" stamp_final="1723713849.507382" username="apac4" allocationname="unknown" flags="0" pid="1050185" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83758e+01" utime="4.77999e+01" stime="6.94340e+00" mtime="3.16567e+01" gflop="0.00000e+00" gbyte="3.76530e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16567e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000e415d856e415e31528" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82895e+01" utime="4.77717e+01" stime="6.93007e+00" mtime="3.16567e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16567e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.3828e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4776e+08" > 4.8251e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4920e+08" > 4.3567e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3681e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4495e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.1076e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0282e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4180e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0323e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.6597e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7570e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8626e+01 </func>
</region>
</regions>
<internal rank="682" log_i="1723713849.507382" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="683" mpi_size="768" stamp_init="1723713791.133138" stamp_final="1723713849.512297" username="apac4" allocationname="unknown" flags="0" pid="1050186" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83792e+01" utime="4.99508e+01" stime="6.05707e+00" mtime="3.27440e+01" gflop="0.00000e+00" gbyte="3.76854e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27440e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000141414148e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82926e+01" utime="4.99199e+01" stime="6.04635e+00" mtime="3.27440e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27440e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4863e+08" > 3.5516e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4904e+08" > 2.7864e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5860e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4571e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0283e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4183e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0322e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5715e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7536e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9630e+01 </func>
</region>
</regions>
<internal rank="683" log_i="1723713849.512297" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="684" mpi_size="768" stamp_init="1723713791.142690" stamp_final="1723713849.501627" username="apac4" allocationname="unknown" flags="0" pid="1050187" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83589e+01" utime="4.75656e+01" stime="7.56177e+00" mtime="3.18182e+01" gflop="0.00000e+00" gbyte="3.76678e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18182e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c114e256c114c11465" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82729e+01" utime="4.75322e+01" stime="7.55373e+00" mtime="3.18182e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18182e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4891e+08" > 6.0341e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4813e+08" > 5.2380e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5951e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4170e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2920e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.7886e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4169e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0390e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9683e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7561e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8431e+01 </func>
</region>
</regions>
<internal rank="684" log_i="1723713849.501627" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="685" mpi_size="768" stamp_init="1723713791.140227" stamp_final="1723713849.513500" username="apac4" allocationname="unknown" flags="0" pid="1050188" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83733e+01" utime="5.06144e+01" stime="6.36362e+00" mtime="3.25544e+01" gflop="0.00000e+00" gbyte="3.75542e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25544e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82844e+01" utime="5.05825e+01" stime="6.35375e+00" mtime="3.25544e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25544e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4761e+08" > 3.6732e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4861e+08" > 2.4715e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5265e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4063e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9789e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.1209e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4173e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0419e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0184e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7535e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9509e+01 </func>
</region>
</regions>
<internal rank="685" log_i="1723713849.513500" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="686" mpi_size="768" stamp_init="1723713791.142689" stamp_final="1723713849.506761" username="apac4" allocationname="unknown" flags="0" pid="1050189" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83641e+01" utime="4.86361e+01" stime="7.28691e+00" mtime="3.21631e+01" gflop="0.00000e+00" gbyte="3.77926e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21631e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005115a455511551150a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82775e+01" utime="4.86049e+01" stime="7.27670e+00" mtime="3.21631e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21631e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4613e+08" > 4.4921e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4574e+08" > 3.4857e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8980e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4225e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.0572e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.7884e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4171e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0449e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.7003e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7575e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8651e+01 </func>
</region>
</regions>
<internal rank="686" log_i="1723713849.506761" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="687" mpi_size="768" stamp_init="1723713791.145155" stamp_final="1723713849.507925" username="apac4" allocationname="unknown" flags="0" pid="1050190" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83628e+01" utime="4.97231e+01" stime="6.14410e+00" mtime="3.26032e+01" gflop="0.00000e+00" gbyte="3.77335e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26032e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001a14e1551a141a1457" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82785e+01" utime="4.96924e+01" stime="6.13436e+00" mtime="3.26032e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26032e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4803e+08" > 3.5964e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4874e+08" > 2.6794e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4066e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4544e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0242e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4166e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0497e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0804e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7522e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9672e+01 </func>
</region>
</regions>
<internal rank="687" log_i="1723713849.507925" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="688" mpi_size="768" stamp_init="1723713791.148540" stamp_final="1723713849.517117" username="apac4" allocationname="unknown" flags="0" pid="1050191" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83686e+01" utime="4.81673e+01" stime="7.39822e+00" mtime="3.13482e+01" gflop="0.00000e+00" gbyte="3.76068e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.13482e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82865e+01" utime="4.81378e+01" stime="7.38749e+00" mtime="3.13482e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.13482e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4818e+08" > 6.6205e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4777e+08" > 4.7485e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4955e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4018e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5984e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0392e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4164e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0454e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8682e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7578e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8016e+01 </func>
</region>
</regions>
<internal rank="688" log_i="1723713849.517117" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="689" mpi_size="768" stamp_init="1723713791.150871" stamp_final="1723713849.512333" username="apac4" allocationname="unknown" flags="0" pid="1050192" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83615e+01" utime="4.95460e+01" stime="6.41826e+00" mtime="3.25712e+01" gflop="0.00000e+00" gbyte="3.76610e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25712e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000df14df14f3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82818e+01" utime="4.95157e+01" stime="6.40844e+00" mtime="3.25712e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25712e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4850e+08" > 4.8769e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4882e+08" > 3.5663e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9490e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4463e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4796e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0284e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4167e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0490e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5691e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7523e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8965e+01 </func>
</region>
</regions>
<internal rank="689" log_i="1723713849.512333" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="690" mpi_size="768" stamp_init="1723713791.157177" stamp_final="1723713849.508103" username="apac4" allocationname="unknown" flags="0" pid="1050193" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83509e+01" utime="4.80376e+01" stime="7.75156e+00" mtime="3.13197e+01" gflop="0.00000e+00" gbyte="3.77064e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.13197e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004e14f1554e144e147a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82726e+01" utime="4.80054e+01" stime="7.74348e+00" mtime="3.13197e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.13197e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4831e+08" > 6.4363e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4923e+08" > 4.5647e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1878e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3976e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.6744e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0488e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4158e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0573e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0708e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7578e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8345e+01 </func>
</region>
</regions>
<internal rank="690" log_i="1723713849.508103" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="691" mpi_size="768" stamp_init="1723713791.155884" stamp_final="1723713849.506171" username="apac4" allocationname="unknown" flags="0" pid="1050194" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83503e+01" utime="4.90974e+01" stime="6.90774e+00" mtime="3.26735e+01" gflop="0.00000e+00" gbyte="3.74054e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26735e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f014f114f3149556f314f214ce" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82728e+01" utime="4.90672e+01" stime="6.89863e+00" mtime="3.26735e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26735e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4916e+08" > 4.6638e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4892e+08" > 3.1624e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8679e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4540e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0281e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4153e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0633e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2997e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7526e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9166e+01 </func>
</region>
</regions>
<internal rank="691" log_i="1723713849.506171" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="692" mpi_size="768" stamp_init="1723713791.157573" stamp_final="1723713849.508269" username="apac4" allocationname="unknown" flags="0" pid="1050195" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83507e+01" utime="4.85442e+01" stime="7.43959e+00" mtime="3.17808e+01" gflop="0.00000e+00" gbyte="3.77159e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17808e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c814fe55c814c814c5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82755e+01" utime="4.85122e+01" stime="7.43219e+00" mtime="3.17808e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17808e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.5015e+08" > 5.7701e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4829e+08" > 4.4041e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5262e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4132e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9600e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8372e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4158e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0579e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8586e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7566e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8501e+01 </func>
</region>
</regions>
<internal rank="692" log_i="1723713849.508269" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="693" mpi_size="768" stamp_init="1723713791.160313" stamp_final="1723713849.507773" username="apac4" allocationname="unknown" flags="0" pid="1050196" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83475e+01" utime="5.00675e+01" stime="6.91345e+00" mtime="3.24244e+01" gflop="0.00000e+00" gbyte="3.74512e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24244e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82730e+01" utime="5.00331e+01" stime="6.90884e+00" mtime="3.24244e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24244e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4753e+08" > 4.7749e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4876e+08" > 3.1800e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5561e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4116e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.1051e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4155e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0602e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0899e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7528e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9227e+01 </func>
</region>
</regions>
<internal rank="693" log_i="1723713849.507773" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="694" mpi_size="768" stamp_init="1723713791.162945" stamp_final="1723713849.513992" username="apac4" allocationname="unknown" flags="0" pid="1050197" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83510e+01" utime="4.71007e+01" stime="7.53731e+00" mtime="3.17584e+01" gflop="0.00000e+00" gbyte="3.77350e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17584e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ae14ae1493" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82787e+01" utime="4.70745e+01" stime="7.52456e+00" mtime="3.17584e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17584e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4754e+08" > 6.1414e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4729e+08" > 4.5233e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6745e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4578e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.1498e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0243e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4148e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0678e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9898e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7543e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8283e+01 </func>
</region>
</regions>
<internal rank="694" log_i="1723713849.513992" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="695" mpi_size="768" stamp_init="1723713791.164854" stamp_final="1723713849.508090" username="apac4" allocationname="unknown" flags="0" pid="1050198" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="5.83432e+01" utime="4.92172e+01" stime="6.71798e+00" mtime="3.25794e+01" gflop="0.00000e+00" gbyte="3.77293e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25794e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e914101411142c551114101480" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82666e+01" utime="4.91868e+01" stime="6.70964e+00" mtime="3.25794e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25794e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4907e+08" > 4.7468e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4813e+08" > 3.3169e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7460e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4384e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0282e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4148e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0689e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2306e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7516e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9199e+01 </func>
</region>
</regions>
<internal rank="695" log_i="1723713849.508090" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="696" mpi_size="768" stamp_init="1723713791.107191" stamp_final="1723713849.508819" username="apac4" allocationname="unknown" flags="0" pid="1943553" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.84016e+01" utime="4.27077e+01" stime="1.27282e+01" mtime="3.18109e+01" gflop="0.00000e+00" gbyte="3.86162e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18109e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d014d214d314a555d314d314fe" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83478e+01" utime="4.26779e+01" stime="1.27171e+01" mtime="3.18109e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18109e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4883e+08" > 7.1999e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4964e+08" > 7.2843e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2184e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4045e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6001e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6783e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4145e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0718e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7204e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7487e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7639e+01 </func>
</region>
</regions>
<internal rank="696" log_i="1723713849.508819" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="697" mpi_size="768" stamp_init="1723713791.108911" stamp_final="1723713849.509097" username="apac4" allocationname="unknown" flags="0" pid="1943554" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.84002e+01" utime="4.99465e+01" stime="7.14743e+00" mtime="3.21733e+01" gflop="0.00000e+00" gbyte="3.77953e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21733e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a415a515a6155355a615a6151d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83436e+01" utime="4.99137e+01" stime="7.13956e+00" mtime="3.21733e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21733e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4987e+08" > 5.7077e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4851e+08" > 2.9022e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5416e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4145e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7872e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4147e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0699e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6798e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7515e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8860e+01 </func>
</region>
</regions>
<internal rank="697" log_i="1723713849.509097" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="698" mpi_size="768" stamp_init="1723713791.110679" stamp_final="1723713849.509165" username="apac4" allocationname="unknown" flags="0" pid="1943555" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83985e+01" utime="4.72347e+01" stime="8.06459e+00" mtime="3.16233e+01" gflop="0.00000e+00" gbyte="3.76488e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16233e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000bd14bd14d6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83419e+01" utime="4.72035e+01" stime="8.05649e+00" mtime="3.16233e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16233e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4916e+08" > 8.2362e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4969e+08" > 4.7972e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4287e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4265e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.0810e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0542e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4140e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0737e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1185e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7474e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8132e+01 </func>
</region>
</regions>
<internal rank="698" log_i="1723713849.509165" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="699" mpi_size="768" stamp_init="1723713791.113115" stamp_final="1723713849.503807" username="apac4" allocationname="unknown" flags="0" pid="1943556" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83907e+01" utime="4.92133e+01" stime="6.85468e+00" mtime="3.25063e+01" gflop="0.00000e+00" gbyte="3.76530e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25063e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000af14af14e0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83248e+01" utime="4.91775e+01" stime="6.85052e+00" mtime="3.25063e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25063e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4951e+08" > 5.5489e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4877e+08" > 2.7547e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9344e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4730e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0400e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4141e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0758e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9492e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7504e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8817e+01 </func>
</region>
</regions>
<internal rank="699" log_i="1723713849.503807" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="700" mpi_size="768" stamp_init="1723713791.120724" stamp_final="1723713849.521475" username="apac4" allocationname="unknown" flags="0" pid="1943557" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.84008e+01" utime="4.75698e+01" stime="7.92565e+00" mtime="3.17768e+01" gflop="0.00000e+00" gbyte="3.76202e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17768e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004415441508" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83169e+01" utime="4.75316e+01" stime="7.92170e+00" mtime="3.17768e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17768e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.0729e-05 </func>
<func name="MPI_Isend" count="127164" bytes="8.4804e+08" > 7.8214e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4956e+08" > 6.5611e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9803e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4189e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6840e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4132e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0800e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3498e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7480e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7774e+01 </func>
</region>
</regions>
<internal rank="700" log_i="1723713849.521475" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="701" mpi_size="768" stamp_init="1723713791.119613" stamp_final="1723713849.517870" username="apac4" allocationname="unknown" flags="0" pid="1943558" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83983e+01" utime="5.00384e+01" stime="7.10078e+00" mtime="3.22890e+01" gflop="0.00000e+00" gbyte="3.76682e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22890e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000631562152d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83195e+01" utime="5.00068e+01" stime="7.09081e+00" mtime="3.22890e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22890e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4893e+08" > 5.5151e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4768e+08" > 2.8515e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4674e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4041e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6959e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4133e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0837e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9612e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7510e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9081e+01 </func>
</region>
</regions>
<internal rank="701" log_i="1723713849.517870" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="702" mpi_size="768" stamp_init="1723713791.122047" stamp_final="1723713849.510026" username="apac4" allocationname="unknown" flags="0" pid="1943559" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83880e+01" utime="4.72564e+01" stime="8.18208e+00" mtime="3.14350e+01" gflop="0.00000e+00" gbyte="3.77151e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.14350e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83027e+01" utime="4.72267e+01" stime="8.17038e+00" mtime="3.14350e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.14350e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.2875e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4937e+08" > 8.0489e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4893e+08" > 4.7918e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2152e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4221e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2898e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0456e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4133e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0836e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2377e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7483e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8181e+01 </func>
</region>
</regions>
<internal rank="702" log_i="1723713849.510026" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="703" mpi_size="768" stamp_init="1723713791.124470" stamp_final="1723713849.517961" username="apac4" allocationname="unknown" flags="0" pid="1943560" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83935e+01" utime="4.93571e+01" stime="6.73842e+00" mtime="3.24174e+01" gflop="0.00000e+00" gbyte="3.76953e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24174e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002615211514" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82996e+01" utime="4.93231e+01" stime="6.72979e+00" mtime="3.24174e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24174e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4992e+08" > 5.5619e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4956e+08" > 2.4214e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3726e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4852e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0401e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4133e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0845e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8706e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7498e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9280e+01 </func>
</region>
</regions>
<internal rank="703" log_i="1723713849.517961" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="704" mpi_size="768" stamp_init="1723713791.126085" stamp_final="1723713849.510764" username="apac4" allocationname="unknown" flags="0" pid="1943561" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83847e+01" utime="4.60574e+01" stime="7.82190e+00" mtime="3.18014e+01" gflop="0.00000e+00" gbyte="3.77663e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18014e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4211422142314585623142314e5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82908e+01" utime="4.60291e+01" stime="7.80785e+00" mtime="3.18014e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18014e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4864e+08" > 7.6825e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4806e+08" > 6.0922e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5459e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4761e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5255e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0136e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4115e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0978e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7490e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7472e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8252e+01 </func>
</region>
</regions>
<internal rank="704" log_i="1723713849.510764" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="705" mpi_size="768" stamp_init="1723713791.132993" stamp_final="1723713849.507266" username="apac4" allocationname="unknown" flags="0" pid="1943562" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83743e+01" utime="5.08063e+01" stime="6.32847e+00" mtime="3.23629e+01" gflop="0.00000e+00" gbyte="3.76587e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23629e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4fc1411141214835512141214ec" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82772e+01" utime="5.07758e+01" stime="6.31656e+00" mtime="3.23629e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23629e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4929e+08" > 4.4603e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4966e+08" > 3.4850e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1786e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4184e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8021e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4104e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1103e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5191e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7110e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9528e+01 </func>
</region>
</regions>
<internal rank="705" log_i="1723713849.507266" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="706" mpi_size="768" stamp_init="1723713791.131300" stamp_final="1723713849.499903" username="apac4" allocationname="unknown" flags="0" pid="1943563" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83686e+01" utime="4.82555e+01" stime="7.26947e+00" mtime="3.15110e+01" gflop="0.00000e+00" gbyte="3.77666e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15110e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c814ca14cb14ca55cb14cb14ed" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82695e+01" utime="4.82247e+01" stime="7.25662e+00" mtime="3.15110e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15110e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4895e+08" > 6.6317e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4951e+08" > 4.7293e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.6514e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4168e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2088e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0258e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4106e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1054e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0279e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7501e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8971e+01 </func>
</region>
</regions>
<internal rank="706" log_i="1723713849.499903" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="707" mpi_size="768" stamp_init="1723713791.133358" stamp_final="1723713849.496976" username="apac4" allocationname="unknown" flags="0" pid="1943564" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83636e+01" utime="5.04289e+01" stime="6.70286e+00" mtime="3.29402e+01" gflop="0.00000e+00" gbyte="3.78132e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.29402e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003715371512" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82680e+01" utime="5.03984e+01" stime="6.69192e+00" mtime="3.29402e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.29402e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4909e+08" > 4.2160e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4840e+08" > 3.5302e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6971e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4320e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0516e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4110e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1074e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1281e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7460e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9589e+01 </func>
</region>
</regions>
<internal rank="707" log_i="1723713849.496976" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="708" mpi_size="768" stamp_init="1723713791.136458" stamp_final="1723713849.504093" username="apac4" allocationname="unknown" flags="0" pid="1943565" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83676e+01" utime="4.79664e+01" stime="7.51513e+00" mtime="3.21255e+01" gflop="0.00000e+00" gbyte="3.74168e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21255e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006614d85566146614ca" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82723e+01" utime="4.79345e+01" stime="7.50500e+00" mtime="3.21255e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21255e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4927e+08" > 6.4144e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4908e+08" > 4.6965e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5002e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4034e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.3078e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0174e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4100e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1168e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3998e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7510e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8766e+01 </func>
</region>
</regions>
<internal rank="708" log_i="1723713849.504093" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="709" mpi_size="768" stamp_init="1723713791.138902" stamp_final="1723713849.508125" username="apac4" allocationname="unknown" flags="0" pid="1943566" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83692e+01" utime="5.07479e+01" stime="6.35408e+00" mtime="3.22289e+01" gflop="0.00000e+00" gbyte="3.77995e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22289e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4551457145814615558145814df" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82735e+01" utime="5.07148e+01" stime="6.34551e+00" mtime="3.22289e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22289e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4877e+08" > 4.2273e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4915e+08" > 3.2028e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.6724e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4220e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0168e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4097e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1203e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9588e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7465e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9919e+01 </func>
</region>
</regions>
<internal rank="709" log_i="1723713849.508125" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="710" mpi_size="768" stamp_init="1723713791.141843" stamp_final="1723713849.504109" username="apac4" allocationname="unknown" flags="0" pid="1943567" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83623e+01" utime="4.82132e+01" stime="7.28273e+00" mtime="3.18080e+01" gflop="0.00000e+00" gbyte="3.77205e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18080e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000931567559315921534" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82650e+01" utime="4.81762e+01" stime="7.27781e+00" mtime="3.18080e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18080e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4823e+08" > 6.4386e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4866e+08" > 4.0598e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9565e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4239e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0220e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0206e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4101e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1162e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5715e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7495e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8979e+01 </func>
</region>
</regions>
<internal rank="710" log_i="1723713849.504109" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="711" mpi_size="768" stamp_init="1723713791.144040" stamp_final="1723713849.514317" username="apac4" allocationname="unknown" flags="0" pid="1943568" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83703e+01" utime="5.02266e+01" stime="6.92515e+00" mtime="3.23116e+01" gflop="0.00000e+00" gbyte="3.77750e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23116e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82774e+01" utime="5.01946e+01" stime="6.91586e+00" mtime="3.23116e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23116e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4786e+08" > 4.3044e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4757e+08" > 3.4037e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8974e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4127e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1696e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0208e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4097e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1197e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.5787e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7468e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9775e+01 </func>
</region>
</regions>
<internal rank="711" log_i="1723713849.514317" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="712" mpi_size="768" stamp_init="1723713791.148127" stamp_final="1723713849.509314" username="apac4" allocationname="unknown" flags="0" pid="1943569" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83612e+01" utime="4.66208e+01" stime="7.71876e+00" mtime="3.20047e+01" gflop="0.00000e+00" gbyte="3.77766e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20047e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f314f514f6148955f614f614e0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82700e+01" utime="4.65889e+01" stime="7.70887e+00" mtime="3.20047e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20047e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4895e+08" > 6.5650e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4886e+08" > 5.3328e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9909e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4819e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0637e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0163e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4087e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1261e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0494e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7505e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8126e+01 </func>
</region>
</regions>
<internal rank="712" log_i="1723713849.509314" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="713" mpi_size="768" stamp_init="1723713791.149736" stamp_final="1723713849.516364" username="apac4" allocationname="unknown" flags="0" pid="1943570" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83666e+01" utime="4.90443e+01" stime="7.04245e+00" mtime="3.24885e+01" gflop="0.00000e+00" gbyte="3.77941e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24885e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c314c414c5144255c514c51468" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82783e+01" utime="4.90098e+01" stime="7.03591e+00" mtime="3.24885e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24885e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4767e+08" > 4.8878e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4926e+08" > 3.3329e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8014e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4824e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0163e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4086e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1324e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4809e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7463e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9007e+01 </func>
</region>
</regions>
<internal rank="713" log_i="1723713849.516364" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="714" mpi_size="768" stamp_init="1723713791.157067" stamp_final="1723713849.504369" username="apac4" allocationname="unknown" flags="0" pid="1943571" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83473e+01" utime="4.67174e+01" stime="7.60498e+00" mtime="3.12357e+01" gflop="0.00000e+00" gbyte="3.75858e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.12357e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008b15cb558b158a151b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82647e+01" utime="4.66863e+01" stime="7.59528e+00" mtime="3.12357e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.12357e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4887e+08" > 7.4003e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4879e+08" > 5.6699e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8049e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4743e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.2257e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0278e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4089e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1283e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2210e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7514e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8468e+01 </func>
</region>
</regions>
<internal rank="714" log_i="1723713849.504369" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="715" mpi_size="768" stamp_init="1723713791.160121" stamp_final="1723713849.513063" username="apac4" allocationname="unknown" flags="0" pid="1943572" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83529e+01" utime="4.96530e+01" stime="6.41733e+00" mtime="3.20211e+01" gflop="0.00000e+00" gbyte="3.74950e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20211e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e615e715e8151855e815e8150a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82668e+01" utime="4.96220e+01" stime="6.40919e+00" mtime="3.20211e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20211e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4884e+08" > 4.7981e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4770e+08" > 3.0061e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0575e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4901e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0279e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4076e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1431e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2091e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7450e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9276e+01 </func>
</region>
</regions>
<internal rank="715" log_i="1723713849.513063" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="716" mpi_size="768" stamp_init="1723713791.157064" stamp_final="1723713849.507991" username="apac4" allocationname="unknown" flags="0" pid="1943573" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83509e+01" utime="4.68502e+01" stime="8.03937e+00" mtime="3.14685e+01" gflop="0.00000e+00" gbyte="3.76682e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.14685e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003b153b1539" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82674e+01" utime="4.68161e+01" stime="8.03355e+00" mtime="3.14685e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.14685e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.3447e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4840e+08" > 8.0740e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4919e+08" > 5.9330e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0724e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4172e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.9502e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0083e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4085e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1332e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0994e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7517e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8351e+01 </func>
</region>
</regions>
<internal rank="716" log_i="1723713849.507991" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="717" mpi_size="768" stamp_init="1723713791.160376" stamp_final="1723713849.503983" username="apac4" allocationname="unknown" flags="0" pid="1943574" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83436e+01" utime="5.03781e+01" stime="6.74199e+00" mtime="3.24582e+01" gflop="0.00000e+00" gbyte="3.76389e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24582e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000033144a5533143314a1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82643e+01" utime="5.03470e+01" stime="6.73320e+00" mtime="3.24582e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24582e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4864e+08" > 4.8976e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4932e+08" > 3.2490e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2807e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4268e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0543e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4076e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1424e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3283e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7417e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9463e+01 </func>
</region>
</regions>
<internal rank="717" log_i="1723713849.503983" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="718" mpi_size="768" stamp_init="1723713791.163109" stamp_final="1723713849.511056" username="apac4" allocationname="unknown" flags="0" pid="1943575" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83479e+01" utime="4.79009e+01" stime="7.67171e+00" mtime="3.17827e+01" gflop="0.00000e+00" gbyte="3.76736e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17827e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82683e+01" utime="4.78702e+01" stime="7.66284e+00" mtime="3.17827e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17827e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4969e+08" > 6.9892e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4828e+08" > 4.6798e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3991e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3827e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.0361e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0082e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4075e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1434e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7800e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7517e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8495e+01 </func>
</region>
</regions>
<internal rank="718" log_i="1723713849.511056" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="719" mpi_size="768" stamp_init="1723713791.163863" stamp_final="1723713849.511917" username="apac4" allocationname="unknown" flags="0" pid="1943576" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="5.83481e+01" utime="4.95285e+01" stime="6.58683e+00" mtime="3.22518e+01" gflop="0.00000e+00" gbyte="3.74390e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22518e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4901491149214fc55921492148f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82688e+01" utime="4.95006e+01" stime="6.57500e+00" mtime="3.22518e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22518e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4902e+08" > 4.8016e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4932e+08" > 2.8703e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1209e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4865e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0394e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4073e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1457e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1304e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7402e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9437e+01 </func>
</region>
</regions>
<internal rank="719" log_i="1723713849.511917" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="720" mpi_size="768" stamp_init="1723713791.102535" stamp_final="1723713849.502123" username="apac4" allocationname="unknown" flags="0" pid="1094353" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83996e+01" utime="4.27569e+01" stime="1.21206e+01" mtime="3.17028e+01" gflop="0.00000e+00" gbyte="3.85941e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17028e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83446e+01" utime="4.27287e+01" stime="1.21081e+01" mtime="3.17028e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17028e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4749e+08" > 5.8514e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4848e+08" > 4.0629e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8322e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5031e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1492e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0072e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4050e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1642e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8801e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7407e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9064e+01 </func>
</region>
</regions>
<internal rank="720" log_i="1723713849.502123" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="721" mpi_size="768" stamp_init="1723713791.102567" stamp_final="1723713849.514686" username="apac4" allocationname="unknown" flags="0" pid="1094354" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.84121e+01" utime="4.94163e+01" stime="6.67562e+00" mtime="3.25162e+01" gflop="0.00000e+00" gbyte="3.78632e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25162e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c6148d55c614c114c1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83550e+01" utime="4.93864e+01" stime="6.66464e+00" mtime="3.25162e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25162e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4873e+08" > 4.6144e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4985e+08" > 3.3673e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4254e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4884e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5749e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0177e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4064e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1510e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7800e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7148e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9431e+01 </func>
</region>
</regions>
<internal rank="721" log_i="1723713849.514686" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="722" mpi_size="768" stamp_init="1723713791.103353" stamp_final="1723713849.512346" username="apac4" allocationname="unknown" flags="0" pid="1094355" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.84090e+01" utime="4.78513e+01" stime="7.08528e+00" mtime="3.17617e+01" gflop="0.00000e+00" gbyte="3.77697e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17617e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e515e51509" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83538e+01" utime="4.78228e+01" stime="7.07487e+00" mtime="3.17617e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17617e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5039e+08" > 5.6042e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4993e+08" > 4.7148e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7564e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4912e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.7697e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0177e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4064e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1505e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0112e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7405e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9230e+01 </func>
</region>
</regions>
<internal rank="722" log_i="1723713849.512346" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="723" mpi_size="768" stamp_init="1723713791.106949" stamp_final="1723713849.506177" username="apac4" allocationname="unknown" flags="0" pid="1094356" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83992e+01" utime="5.03496e+01" stime="6.77561e+00" mtime="3.25194e+01" gflop="0.00000e+00" gbyte="3.76316e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25194e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000be14bd1483" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83426e+01" utime="5.03203e+01" stime="6.76566e+00" mtime="3.25194e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25194e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4942e+08" > 4.5041e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4929e+08" > 2.7881e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6380e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4258e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9821e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4063e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1548e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0899e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7103e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9218e+01 </func>
</region>
</regions>
<internal rank="723" log_i="1723713849.506177" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="724" mpi_size="768" stamp_init="1723713791.109782" stamp_final="1723713849.505855" username="apac4" allocationname="unknown" flags="0" pid="1094357" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83961e+01" utime="4.69847e+01" stime="7.98641e+00" mtime="3.15745e+01" gflop="0.00000e+00" gbyte="3.76919e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15745e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001d151c151f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83294e+01" utime="4.69510e+01" stime="7.97951e+00" mtime="3.15745e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15745e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4815e+08" > 7.4437e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4890e+08" > 7.0775e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9726e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4023e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6562e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4064e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1527e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.3689e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7403e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8627e+01 </func>
</region>
</regions>
<internal rank="724" log_i="1723713849.505855" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="725" mpi_size="768" stamp_init="1723713791.113163" stamp_final="1723713849.509709" username="apac4" allocationname="unknown" flags="0" pid="1094358" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83965e+01" utime="5.02883e+01" stime="6.85570e+00" mtime="3.27691e+01" gflop="0.00000e+00" gbyte="3.77811e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27691e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a014a0149f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83231e+01" utime="5.02557e+01" stime="6.84678e+00" mtime="3.27691e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27691e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4756e+08" > 4.4935e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4875e+08" > 3.0398e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3237e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4219e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6790e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4054e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1605e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8300e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7080e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9788e+01 </func>
</region>
</regions>
<internal rank="725" log_i="1723713849.509709" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="726" mpi_size="768" stamp_init="1723713791.115473" stamp_final="1723713849.510343" username="apac4" allocationname="unknown" flags="0" pid="1094359" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83949e+01" utime="4.83361e+01" stime="7.33633e+00" mtime="3.16025e+01" gflop="0.00000e+00" gbyte="3.77235e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16025e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c2143755c214c11483" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83149e+01" utime="4.83051e+01" stime="7.32540e+00" mtime="3.16025e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16025e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4936e+08" > 6.2729e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4743e+08" > 4.7457e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.5884e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4166e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.2448e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6578e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4058e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1608e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4094e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7392e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9162e+01 </func>
</region>
</regions>
<internal rank="726" log_i="1723713849.510343" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="727" mpi_size="768" stamp_init="1723713791.119061" stamp_final="1723713849.519918" username="apac4" allocationname="unknown" flags="0" pid="1094360" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.84009e+01" utime="5.03044e+01" stime="6.77169e+00" mtime="3.25876e+01" gflop="0.00000e+00" gbyte="3.76274e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25876e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d215d315d4158a55d415d41549" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83151e+01" utime="5.02766e+01" stime="6.75748e+00" mtime="3.25876e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25876e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4840e+08" > 4.5122e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4819e+08" > 2.7543e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3010e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4039e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6570e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4052e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1660e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8396e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7083e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9648e+01 </func>
</region>
</regions>
<internal rank="727" log_i="1723713849.519918" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="728" mpi_size="768" stamp_init="1723713791.120291" stamp_final="1723713849.515892" username="apac4" allocationname="unknown" flags="0" pid="1094361" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83956e+01" utime="4.71083e+01" stime="7.36063e+00" mtime="3.18966e+01" gflop="0.00000e+00" gbyte="3.77186e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18966e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43315341535154f553515351512" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83096e+01" utime="4.70820e+01" stime="7.34506e+00" mtime="3.18966e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18966e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4867e+08" > 5.7340e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4902e+08" > 4.1758e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3020e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4783e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1940e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0072e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4043e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1720e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0780e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7362e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8823e+01 </func>
</region>
</regions>
<internal rank="728" log_i="1723713849.515892" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="729" mpi_size="768" stamp_init="1723713791.123424" stamp_final="1723713849.506705" username="apac4" allocationname="unknown" flags="0" pid="1094362" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83833e+01" utime="5.05580e+01" stime="6.59719e+00" mtime="3.22361e+01" gflop="0.00000e+00" gbyte="3.77228e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22361e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f014cb56f014f01464" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82955e+01" utime="5.05272e+01" stime="6.58570e+00" mtime="3.22361e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22361e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4997e+08" > 3.5568e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4829e+08" > 2.4992e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9148e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4174e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0000e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4048e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1707e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9492e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7041e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9764e+01 </func>
</region>
</regions>
<internal rank="729" log_i="1723713849.506705" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="730" mpi_size="768" stamp_init="1723713791.140468" stamp_final="1723713849.501274" username="apac4" allocationname="unknown" flags="0" pid="1094363" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83608e+01" utime="4.77589e+01" stime="7.25741e+00" mtime="3.22239e+01" gflop="0.00000e+00" gbyte="3.75294e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22239e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009015f6569015901514" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82748e+01" utime="4.77287e+01" stime="7.24657e+00" mtime="3.22239e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22239e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4837e+08" > 4.6452e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4990e+08" > 4.4458e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5630e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4902e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0324e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0071e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4037e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1780e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.4904e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7309e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8995e+01 </func>
</region>
</regions>
<internal rank="730" log_i="1723713849.501274" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="731" mpi_size="768" stamp_init="1723713791.128991" stamp_final="1723713849.512703" username="apac4" allocationname="unknown" flags="0" pid="1094364" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83837e+01" utime="5.06611e+01" stime="6.48809e+00" mtime="3.27895e+01" gflop="0.00000e+00" gbyte="3.77018e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27895e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82899e+01" utime="5.06302e+01" stime="6.47652e+00" mtime="3.27895e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27895e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4919e+08" > 3.5800e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4896e+08" > 2.4995e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3566e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4222e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3365e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6783e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4041e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1780e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1519e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7028e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9872e+01 </func>
</region>
</regions>
<internal rank="731" log_i="1723713849.512703" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="732" mpi_size="768" stamp_init="1723713791.134940" stamp_final="1723713849.511919" username="apac4" allocationname="unknown" flags="0" pid="1094365" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83770e+01" utime="4.66022e+01" stime="7.64368e+00" mtime="3.21815e+01" gflop="0.00000e+00" gbyte="3.77979e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21815e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007114245671147014c0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82861e+01" utime="4.65698e+01" stime="7.63277e+00" mtime="3.21815e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21815e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4857e+08" > 5.9131e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4832e+08" > 4.9018e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5633e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4959e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3614e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0174e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4038e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1795e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9802e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7311e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8805e+01 </func>
</region>
</regions>
<internal rank="732" log_i="1723713849.511919" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="733" mpi_size="768" stamp_init="1723713791.133115" stamp_final="1723713849.509728" username="apac4" allocationname="unknown" flags="0" pid="1094366" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83766e+01" utime="5.04178e+01" stime="6.57581e+00" mtime="3.27062e+01" gflop="0.00000e+00" gbyte="3.75851e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27062e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf41c151e151f15b3551f151e1504" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82857e+01" utime="5.03862e+01" stime="6.56457e+00" mtime="3.27062e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27062e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4843e+08" > 3.6522e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4804e+08" > 2.5197e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1786e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4229e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8033e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4034e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1821e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9898e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6983e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9957e+01 </func>
</region>
</regions>
<internal rank="733" log_i="1723713849.509728" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="734" mpi_size="768" stamp_init="1723713791.135511" stamp_final="1723713849.505763" username="apac4" allocationname="unknown" flags="0" pid="1094367" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83703e+01" utime="4.74309e+01" stime="7.39421e+00" mtime="3.22637e+01" gflop="0.00000e+00" gbyte="3.78086e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22637e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000fe154d55fe15fe1520" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82794e+01" utime="4.73972e+01" stime="7.38614e+00" mtime="3.22637e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22637e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4915e+08" > 5.1002e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4867e+08" > 3.6269e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4109e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4880e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6621e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0182e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4038e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1809e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9802e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7285e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9141e+01 </func>
</region>
</regions>
<internal rank="734" log_i="1723713849.505763" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="735" mpi_size="768" stamp_init="1723713791.138664" stamp_final="1723713849.501489" username="apac4" allocationname="unknown" flags="0" pid="1094368" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83628e+01" utime="4.94664e+01" stime="6.39345e+00" mtime="3.21842e+01" gflop="0.00000e+00" gbyte="3.77617e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21842e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf459145b145c149f555c145c14cc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82742e+01" utime="4.94291e+01" stime="6.38923e+00" mtime="3.21842e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21842e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4855e+08" > 3.6016e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4956e+08" > 3.0007e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1082e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4878e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9312e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0182e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4031e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1885e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2187e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6978e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9521e+01 </func>
</region>
</regions>
<internal rank="735" log_i="1723713849.501489" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="736" mpi_size="768" stamp_init="1723713791.156829" stamp_final="1723713849.501699" username="apac4" allocationname="unknown" flags="0" pid="1094369" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83449e+01" utime="4.71855e+01" stime="7.43146e+00" mtime="3.19907e+01" gflop="0.00000e+00" gbyte="3.76564e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19907e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a414a414fb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82696e+01" utime="4.71572e+01" stime="7.41939e+00" mtime="3.19907e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19907e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5035e+08" > 6.7836e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4832e+08" > 4.8625e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2856e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5050e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0539e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0038e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4022e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1944e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9397e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7244e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8807e+01 </func>
</region>
</regions>
<internal rank="736" log_i="1723713849.501699" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="737" mpi_size="768" stamp_init="1723713791.144419" stamp_final="1723713849.507181" username="apac4" allocationname="unknown" flags="0" pid="1094370" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83628e+01" utime="5.00682e+01" stime="7.01219e+00" mtime="3.24065e+01" gflop="0.00000e+00" gbyte="3.76507e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24065e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000055145514fe" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82815e+01" utime="5.00358e+01" stime="7.00350e+00" mtime="3.24065e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24065e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4829e+08" > 4.7731e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4859e+08" > 3.0089e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3882e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4199e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2650e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6973e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4009e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2069e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7680e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6951e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9335e+01 </func>
</region>
</regions>
<internal rank="737" log_i="1723713849.507181" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="738" mpi_size="768" stamp_init="1723713791.146796" stamp_final="1723713849.516643" username="apac4" allocationname="unknown" flags="0" pid="1094371" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83698e+01" utime="4.74854e+01" stime="7.22628e+00" mtime="3.15860e+01" gflop="0.00000e+00" gbyte="3.77792e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15860e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009d1593569d159d1540" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82888e+01" utime="4.74530e+01" stime="7.21766e+00" mtime="3.15860e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15860e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4744e+08" > 6.0007e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4900e+08" > 3.7530e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9672e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4936e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.1127e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0181e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4012e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2012e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8110e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7239e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8807e+01 </func>
</region>
</regions>
<internal rank="738" log_i="1723713849.516643" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="739" mpi_size="768" stamp_init="1723713791.149757" stamp_final="1723713849.513026" username="apac4" allocationname="unknown" flags="0" pid="1094372" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83633e+01" utime="4.94904e+01" stime="6.62550e+00" mtime="3.22562e+01" gflop="0.00000e+00" gbyte="3.76465e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22562e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f8153155f815f81506" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82839e+01" utime="4.94580e+01" stime="6.61762e+00" mtime="3.22562e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22562e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4832e+08" > 4.6941e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4904e+08" > 2.8481e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1796e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4872e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0182e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4016e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2040e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7609e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6942e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9414e+01 </func>
</region>
</regions>
<internal rank="739" log_i="1723713849.513026" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="740" mpi_size="768" stamp_init="1723713791.155943" stamp_final="1723713849.506248" username="apac4" allocationname="unknown" flags="0" pid="1094373" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83503e+01" utime="4.74089e+01" stime="7.26605e+00" mtime="3.16956e+01" gflop="0.00000e+00" gbyte="3.77827e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.16956e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000da14da14ad" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82725e+01" utime="4.73779e+01" stime="7.25666e+00" mtime="3.16956e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.16956e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4741e+08" > 6.2878e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4833e+08" > 4.6065e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1011e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4999e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3811e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0038e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4010e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2054e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9898e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7217e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8743e+01 </func>
</region>
</regions>
<internal rank="740" log_i="1723713849.506248" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="741" mpi_size="768" stamp_init="1723713791.154068" stamp_final="1723713849.516612" username="apac4" allocationname="unknown" flags="0" pid="1094374" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83625e+01" utime="5.01993e+01" stime="6.95896e+00" mtime="3.29005e+01" gflop="0.00000e+00" gbyte="3.76537e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.29005e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e614e714e814e655e814e81474" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82850e+01" utime="5.01700e+01" stime="6.94843e+00" mtime="3.29005e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.29005e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4744e+08" > 4.8173e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4668e+08" > 2.8883e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7358e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4134e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0266e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8661e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4012e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2076e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2115e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6946e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9483e+01 </func>
</region>
</regions>
<internal rank="741" log_i="1723713849.516612" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="742" mpi_size="768" stamp_init="1723713791.155935" stamp_final="1723713849.512122" username="apac4" allocationname="unknown" flags="0" pid="1094375" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83562e+01" utime="4.74102e+01" stime="7.20937e+00" mtime="3.17703e+01" gflop="0.00000e+00" gbyte="3.74859e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17703e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b415b515b6151755b615b61540" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82801e+01" utime="4.73805e+01" stime="7.19910e+00" mtime="3.17703e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17703e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4645e+08" > 6.4770e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4583e+08" > 4.3665e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8222e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5005e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3947e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0038e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4008e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2118e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1400e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.7148e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9079e+01 </func>
</region>
</regions>
<internal rank="742" log_i="1723713849.512122" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="743" mpi_size="768" stamp_init="1723713791.174474" stamp_final="1723713849.513589" username="apac4" allocationname="unknown" flags="0" pid="1094376" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="5.83391e+01" utime="4.95439e+01" stime="6.52510e+00" mtime="3.22732e+01" gflop="0.00000e+00" gbyte="3.76369e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22732e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82763e+01" utime="4.95108e+01" stime="6.51864e+00" mtime="3.22732e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22732e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4682e+08" > 4.7000e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4668e+08" > 2.8896e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1282e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.4785e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4080e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0038e+00 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4011e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2092e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1996e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6942e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9504e+01 </func>
</region>
</regions>
<internal rank="743" log_i="1723713849.513589" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="744" mpi_size="768" stamp_init="1723713791.098768" stamp_final="1723713849.516120" username="apac4" allocationname="unknown" flags="0" pid="922648" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.84174e+01" utime="4.29997e+01" stime="1.27703e+01" mtime="3.18370e+01" gflop="0.00000e+00" gbyte="3.86337e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18370e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d014d01473" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83554e+01" utime="4.29693e+01" stime="1.27611e+01" mtime="3.18370e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18370e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.0014e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4879e+08" > 6.2683e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4829e+08" > 4.2893e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4631e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4250e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.3728e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0763e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4000e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2201e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2592e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6907e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8507e+01 </func>
</region>
</regions>
<internal rank="744" log_i="1723713849.516120" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="745" mpi_size="768" stamp_init="1723713791.098918" stamp_final="1723713849.501249" username="apac4" allocationname="unknown" flags="0" pid="922649" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.84023e+01" utime="5.01346e+01" stime="6.77803e+00" mtime="3.21649e+01" gflop="0.00000e+00" gbyte="3.76751e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21649e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005f14e2555f145f1482" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83357e+01" utime="5.01033e+01" stime="6.76893e+00" mtime="3.21649e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21649e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4922e+08" > 4.3917e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4942e+08" > 2.6913e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0670e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4294e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0777e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4001e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2176e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.4986e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6248e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9436e+01 </func>
</region>
</regions>
<internal rank="745" log_i="1723713849.501249" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="746" mpi_size="768" stamp_init="1723713791.099978" stamp_final="1723713849.498661" username="apac4" allocationname="unknown" flags="0" pid="922650" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83987e+01" utime="4.72319e+01" stime="7.78089e+00" mtime="3.13289e+01" gflop="0.00000e+00" gbyte="3.77842e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.13289e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ba14b914dc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83389e+01" utime="4.72033e+01" stime="7.77060e+00" mtime="3.13289e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.13289e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.4796e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4941e+08" > 8.6570e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4951e+08" > 5.4917e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7702e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3954e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.6028e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1008e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3991e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2271e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6417e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6913e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8474e+01 </func>
</region>
</regions>
<internal rank="746" log_i="1723713849.498661" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="747" mpi_size="768" stamp_init="1723713791.101419" stamp_final="1723713849.501208" username="apac4" allocationname="unknown" flags="0" pid="922651" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83998e+01" utime="5.03244e+01" stime="6.81645e+00" mtime="3.23128e+01" gflop="0.00000e+00" gbyte="3.76598e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.23128e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf45f1461146214ba5562146214f7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83307e+01" utime="5.02920e+01" stime="6.80928e+00" mtime="3.23128e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.23128e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5039e+08" > 4.4286e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4791e+08" > 2.7035e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9950e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4079e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7180e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1301e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3994e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2210e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1710e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6230e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9669e+01 </func>
</region>
</regions>
<internal rank="747" log_i="1723713849.501208" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="748" mpi_size="768" stamp_init="1723713791.106725" stamp_final="1723713849.507362" username="apac4" allocationname="unknown" flags="0" pid="922652" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.84006e+01" utime="4.90143e+01" stime="7.23031e+00" mtime="3.18495e+01" gflop="0.00000e+00" gbyte="3.77819e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.18495e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000037143714c9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83175e+01" utime="4.89832e+01" stime="7.22014e+00" mtime="3.18495e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.18495e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4736e+08" > 5.4446e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4929e+08" > 3.4134e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1327e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4207e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6594e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0969e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3991e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2273e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7585e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6916e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8949e+01 </func>
</region>
</regions>
<internal rank="748" log_i="1723713849.507362" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="749" mpi_size="768" stamp_init="1723713791.107252" stamp_final="1723713849.502139" username="apac4" allocationname="unknown" flags="0" pid="922653" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83949e+01" utime="5.02677e+01" stime="6.75540e+00" mtime="3.25856e+01" gflop="0.00000e+00" gbyte="3.77930e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25856e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83105e+01" utime="5.02377e+01" stime="6.74335e+00" mtime="3.25856e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25856e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4855e+08" > 4.2857e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4819e+08" > 2.7864e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1559e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4287e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1215e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3995e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2246e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6798e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6253e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9774e+01 </func>
</region>
</regions>
<internal rank="749" log_i="1723713849.502139" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="750" mpi_size="768" stamp_init="1723713791.110483" stamp_final="1723713849.515721" username="apac4" allocationname="unknown" flags="0" pid="922654" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.84052e+01" utime="4.71233e+01" stime="8.00266e+00" mtime="3.17456e+01" gflop="0.00000e+00" gbyte="3.77480e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.17456e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83259e+01" utime="4.70956e+01" stime="7.98963e+00" mtime="3.17456e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.17456e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4691e+08" > 7.2432e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4783e+08" > 4.9692e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0449e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3923e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6070e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0971e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3987e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2302e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2711e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6846e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8762e+01 </func>
</region>
</regions>
<internal rank="750" log_i="1723713849.515721" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="751" mpi_size="768" stamp_init="1723713791.111969" stamp_final="1723713849.506011" username="apac4" allocationname="unknown" flags="0" pid="922655" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83940e+01" utime="5.02123e+01" stime="6.91573e+00" mtime="3.25498e+01" gflop="0.00000e+00" gbyte="3.77617e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25498e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000aa142e55aa14a91482" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83069e+01" utime="5.01811e+01" stime="6.90488e+00" mtime="3.25498e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25498e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4737e+08" > 4.5141e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4935e+08" > 2.9003e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4256e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4153e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0955e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3990e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2259e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0494e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6247e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9461e+01 </func>
</region>
</regions>
<internal rank="751" log_i="1723713849.506011" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="752" mpi_size="768" stamp_init="1723713791.113688" stamp_final="1723713849.508060" username="apac4" allocationname="unknown" flags="0" pid="922656" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83944e+01" utime="4.66029e+01" stime="8.17595e+00" mtime="3.15639e+01" gflop="0.00000e+00" gbyte="3.77220e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.15639e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83088e+01" utime="4.65689e+01" stime="8.16918e+00" mtime="3.15639e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.15639e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4856e+08" > 6.4998e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4840e+08" > 5.4474e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5112e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4174e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.7272e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0708e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3976e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2405e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1710e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6829e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8158e+01 </func>
</region>
</regions>
<internal rank="752" log_i="1723713849.508060" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="753" mpi_size="768" stamp_init="1723713791.115749" stamp_final="1723713849.500909" username="apac4" allocationname="unknown" flags="0" pid="922657" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83852e+01" utime="5.04415e+01" stime="6.63795e+00" mtime="3.24087e+01" gflop="0.00000e+00" gbyte="3.76358e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.24087e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4771479147a142c557a147914d0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82971e+01" utime="5.04083e+01" stime="6.62994e+00" mtime="3.24087e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.24087e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4825e+08" > 3.6100e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4992e+08" > 2.9045e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4889e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4304e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1123e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3966e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2516e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8300e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6241e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9330e+01 </func>
</region>
</regions>
<internal rank="753" log_i="1723713849.500909" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="754" mpi_size="768" stamp_init="1723713791.118557" stamp_final="1723713849.512273" username="apac4" allocationname="unknown" flags="0" pid="922658" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83937e+01" utime="4.84560e+01" stime="7.56963e+00" mtime="3.19864e+01" gflop="0.00000e+00" gbyte="3.76633e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19864e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43a143c143d14de553d143c14d7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83050e+01" utime="4.84232e+01" stime="7.56122e+00" mtime="3.19864e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19864e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4887e+08" > 4.5144e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4937e+08" > 4.0115e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6841e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4203e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6809e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1270e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3963e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2534e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2616e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6809e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8619e+01 </func>
</region>
</regions>
<internal rank="754" log_i="1723713849.512273" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="755" mpi_size="768" stamp_init="1723713791.121234" stamp_final="1723713849.508291" username="apac4" allocationname="unknown" flags="0" pid="922659" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83871e+01" utime="5.02112e+01" stime="6.93360e+00" mtime="3.27674e+01" gflop="0.00000e+00" gbyte="3.74451e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27674e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82997e+01" utime="5.01790e+01" stime="6.92449e+00" mtime="3.27674e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27674e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4956e+08" > 3.5129e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4903e+08" > 2.8842e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7962e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4100e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1300e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3974e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2460e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.2616e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6208e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9410e+01 </func>
</region>
</regions>
<internal rank="755" log_i="1723713849.508291" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="756" mpi_size="768" stamp_init="1723713791.123138" stamp_final="1723713849.496359" username="apac4" allocationname="unknown" flags="0" pid="922660" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83732e+01" utime="4.89227e+01" stime="7.20586e+00" mtime="3.25346e+01" gflop="0.00000e+00" gbyte="3.76183e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25346e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000087148714d4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82875e+01" utime="4.88916e+01" stime="7.19646e+00" mtime="3.25346e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25346e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4857e+08" > 4.5120e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4843e+08" > 4.2171e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9108e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4353e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7810e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1123e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3971e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2455e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9707e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6780e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8923e+01 </func>
</region>
</regions>
<internal rank="756" log_i="1723713849.496359" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="757" mpi_size="768" stamp_init="1723713791.124723" stamp_final="1723713849.504235" username="apac4" allocationname="unknown" flags="0" pid="922661" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83795e+01" utime="4.99543e+01" stime="6.96865e+00" mtime="3.27374e+01" gflop="0.00000e+00" gbyte="3.76614e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.27374e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82915e+01" utime="4.99195e+01" stime="6.96273e+00" mtime="3.27374e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.27374e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4851e+08" > 3.6649e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4802e+08" > 2.5780e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4388e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4214e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5749e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1301e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3966e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2497e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9206e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.5836e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9714e+01 </func>
</region>
</regions>
<internal rank="757" log_i="1723713849.504235" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="758" mpi_size="768" stamp_init="1723713791.127259" stamp_final="1723713849.515827" username="apac4" allocationname="unknown" flags="0" pid="922662" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83886e+01" utime="4.82231e+01" stime="7.40938e+00" mtime="3.21543e+01" gflop="0.00000e+00" gbyte="3.76846e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21543e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.83025e+01" utime="4.81916e+01" stime="7.40009e+00" mtime="3.21543e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21543e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4793e+08" > 5.0641e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4875e+08" > 4.3600e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5826e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4270e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6856e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1124e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3964e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2509e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.1900e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6778e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8823e+01 </func>
</region>
</regions>
<internal rank="758" log_i="1723713849.515827" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="759" mpi_size="768" stamp_init="1723713791.130038" stamp_final="1723713849.501078" username="apac4" allocationname="unknown" flags="0" pid="922663" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83710e+01" utime="5.01517e+01" stime="6.89080e+00" mtime="3.30129e+01" gflop="0.00000e+00" gbyte="3.77167e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.30129e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000cf143956cf14cf147f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82870e+01" utime="5.01151e+01" stime="6.88691e+00" mtime="3.30129e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.30129e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4839e+08" > 3.5063e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4740e+08" > 2.6957e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8772e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4279e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1125e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3962e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2575e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6202e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6215e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9560e+01 </func>
</region>
</regions>
<internal rank="759" log_i="1723713849.501078" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="760" mpi_size="768" stamp_init="1723713791.134319" stamp_final="1723713849.511215" username="apac4" allocationname="unknown" flags="0" pid="922664" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83769e+01" utime="4.66390e+01" stime="8.01074e+00" mtime="3.13949e+01" gflop="0.00000e+00" gbyte="3.76572e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.13949e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ff14ff145a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82963e+01" utime="4.66105e+01" stime="7.99909e+00" mtime="3.13949e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.13949e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4887e+08" > 8.1676e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4982e+08" > 6.1904e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3173e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4093e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.2791e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0712e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3951e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2629e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8014e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6391e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8016e+01 </func>
</region>
</regions>
<internal rank="760" log_i="1723713849.511215" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="761" mpi_size="768" stamp_init="1723713791.135820" stamp_final="1723713849.512629" username="apac4" allocationname="unknown" flags="0" pid="922665" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83768e+01" utime="5.00663e+01" stime="7.06409e+00" mtime="3.25427e+01" gflop="0.00000e+00" gbyte="3.75778e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.25427e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005f147a565f145e14f9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82950e+01" utime="5.00302e+01" stime="7.05997e+00" mtime="3.25427e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.25427e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4832e+08" > 4.7646e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4979e+08" > 2.6096e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9209e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4174e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7643e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0824e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3954e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2663e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6703e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6222e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8935e+01 </func>
</region>
</regions>
<internal rank="761" log_i="1723713849.512629" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="762" mpi_size="768" stamp_init="1723713791.140031" stamp_final="1723713849.505315" username="apac4" allocationname="unknown" flags="0" pid="922666" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83653e+01" utime="4.88364e+01" stime="7.36214e+00" mtime="3.22364e+01" gflop="0.00000e+00" gbyte="3.76842e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.22364e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf47f158115821556558215821509" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82870e+01" utime="4.88032e+01" stime="7.35610e+00" mtime="3.22364e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.22364e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4952e+08" > 5.4158e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4922e+08" > 3.4686e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0206e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4167e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.6028e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0862e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3954e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2665e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8515e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6750e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8455e+01 </func>
</region>
</regions>
<internal rank="762" log_i="1723713849.505315" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="763" mpi_size="768" stamp_init="1723713791.140447" stamp_final="1723713849.508890" username="apac4" allocationname="unknown" flags="0" pid="922667" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83684e+01" utime="5.02930e+01" stime="6.74904e+00" mtime="3.21346e+01" gflop="0.00000e+00" gbyte="3.77884e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.21346e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f915111512157855121511154f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82910e+01" utime="5.02607e+01" stime="6.74157e+00" mtime="3.21346e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.21346e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.5011e+08" > 4.4922e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4852e+08" > 2.3885e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1819e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3992e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1212e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3944e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2758e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.8515e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6229e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9310e+01 </func>
</region>
</regions>
<internal rank="763" log_i="1723713849.508890" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="764" mpi_size="768" stamp_init="1723713791.142698" stamp_final="1723713849.505470" username="apac4" allocationname="unknown" flags="0" pid="922668" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83628e+01" utime="4.86855e+01" stime="7.38615e+00" mtime="3.19719e+01" gflop="0.00000e+00" gbyte="3.77422e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.19719e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000cf15cf1536" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82880e+01" utime="4.86540e+01" stime="7.37846e+00" mtime="3.19719e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.19719e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4847e+08" > 6.0563e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4976e+08" > 4.1008e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7693e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.3783e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.6411e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0851e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3943e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2738e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 3.0899e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6246e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8406e+01 </func>
</region>
</regions>
<internal rank="764" log_i="1723713849.505470" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="765" mpi_size="768" stamp_init="1723713791.143842" stamp_final="1723713849.504424" username="apac4" allocationname="unknown" flags="0" pid="922669" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83606e+01" utime="5.00563e+01" stime="7.02532e+00" mtime="3.26237e+01" gflop="0.00000e+00" gbyte="3.77956e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26237e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ce14cd148f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82845e+01" utime="5.00185e+01" stime="7.02368e+00" mtime="3.26237e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26237e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4740e+08" > 4.5252e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4834e+08" > 2.4124e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6874e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4000e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1188e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3949e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2718e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.6011e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6214e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9289e+01 </func>
</region>
</regions>
<internal rank="765" log_i="1723713849.504424" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="766" mpi_size="768" stamp_init="1723713791.148610" stamp_final="1723713849.515971" username="apac4" allocationname="unknown" flags="0" pid="922670" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83674e+01" utime="4.82992e+01" stime="7.45578e+00" mtime="3.20982e+01" gflop="0.00000e+00" gbyte="3.75431e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.20982e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000033143214b0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82953e+01" utime="4.82645e+01" stime="7.45166e+00" mtime="3.20982e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.20982e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.3447e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4924e+08" > 5.9368e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4842e+08" > 3.8986e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7918e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4099e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.2943e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0852e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3939e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2810e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.9111e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6246e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8497e+01 </func>
</region>
</regions>
<internal rank="766" log_i="1723713849.515971" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="767" mpi_size="768" stamp_init="1723713791.149939" stamp_final="1723713849.512320" username="apac4" allocationname="unknown" flags="0" pid="922671" >
<job nhosts="32" ntasks="768" start="1723713791" final="1723713849" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="5.83624e+01" utime="5.03003e+01" stime="6.80161e+00" mtime="3.26263e+01" gflop="0.00000e+00" gbyte="3.76736e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.26263e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005514175655145514ca" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="5.82909e+01" utime="5.02674e+01" stime="6.79606e+00" mtime="3.26263e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.26263e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4979e+08" > 4.6309e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4885e+08" > 2.6376e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9149e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 4.4029e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0941e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3933e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2874e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 2.7084e-04 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 3.6219e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9051e+01 </func>
</region>
</regions>
<internal rank="767" log_i="1723713849.512320" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723713791.80061.ipm.xml" logrank="0" ></internal>
</task>
</ipm_job_profile>
