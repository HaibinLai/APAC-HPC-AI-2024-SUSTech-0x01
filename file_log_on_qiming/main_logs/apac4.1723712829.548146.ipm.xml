<?xml version="1.0" encoding="iso-8859-1"?>
<ipm_job_profile>
<calltable nsections="1" >
<section module="MPI" nentries="69" >
<entry name="MPI_Init" />
<entry name="MPI_Init_thread" />
<entry name="MPI_Finalize" />
<entry name="MPI_Comm_rank" />
<entry name="MPI_Comm_size" />
<entry name="MPI_Send" />
<entry name="MPI_Ssend" />
<entry name="MPI_Rsend" />
<entry name="MPI_Bsend" />
<entry name="MPI_Isend" />
<entry name="MPI_Issend" />
<entry name="MPI_Irsend" />
<entry name="MPI_Ibsend" />
<entry name="MPI_Recv" />
<entry name="MPI_Irecv" />
<entry name="MPI_Sendrecv" />
<entry name="MPI_Sendrecv_replace" />
<entry name="MPI_Wait" />
<entry name="MPI_Waitany" />
<entry name="MPI_Waitall" />
<entry name="MPI_Waitsome" />
<entry name="MPI_Probe" />
<entry name="MPI_Iprobe" />
<entry name="MPI_Send_init" />
<entry name="MPI_Ssend_init" />
<entry name="MPI_Rsend_init" />
<entry name="MPI_Bsend_init" />
<entry name="MPI_Recv_init" />
<entry name="MPI_Buffer_attach" />
<entry name="MPI_Buffer_detach" />
<entry name="MPI_Test" />
<entry name="MPI_Testany" />
<entry name="MPI_Testall" />
<entry name="MPI_Testsome" />
<entry name="MPI_Start" />
<entry name="MPI_Startall" />
<entry name="MPI_Bcast" />
<entry name="MPI_Reduce" />
<entry name="MPI_Reduce_scatter" />
<entry name="MPI_Barrier" />
<entry name="MPI_Gather" />
<entry name="MPI_Gatherv" />
<entry name="MPI_Scatter" />
<entry name="MPI_Scatterv" />
<entry name="MPI_Scan" />
<entry name="MPI_Allgather" />
<entry name="MPI_Allgatherv" />
<entry name="MPI_Allreduce" />
<entry name="MPI_Alltoall" />
<entry name="MPI_Alltoallv" />
<entry name="MPI_Comm_group" />
<entry name="MPI_Comm_compare" />
<entry name="MPI_Comm_dup" />
<entry name="MPI_Comm_create" />
<entry name="MPI_Comm_split" />
<entry name="MPI_Comm_free" />
<entry name="MPI_Ibcast" />
<entry name="MPI_Ireduce" />
<entry name="MPI_Ireduce_scatter" />
<entry name="MPI_Igather" />
<entry name="MPI_Igatherv" />
<entry name="MPI_Iscatter" />
<entry name="MPI_Iscatterv" />
<entry name="MPI_Iscan" />
<entry name="MPI_Iallgather" />
<entry name="MPI_Iallgatherv" />
<entry name="MPI_Iallreduce" />
<entry name="MPI_Ialltoall" />
<entry name="MPI_Ialltoallv" />
</section>
</calltable>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="0" mpi_size="768" stamp_init="1723712829.548146" stamp_final="1723712895.690476" username="apac4" allocationname="unknown" flags="0" pid="684268" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61423e+01" utime="4.50850e+01" stime="1.24286e+01" mtime="2.73969e+01" gflop="0.00000e+00" gbyte="9.56520e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.73969e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d814d714aa" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60072e+01" utime="4.50567e+01" stime="1.24145e+01" mtime="2.73969e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.73969e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.2159e-05 </func>
<func name="MPI_Comm_size" count="43" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4859e+08" > 5.7064e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4748e+08" > 5.1909e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5872e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.2665e-02 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3173e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3868e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 8.3613e-04 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2387e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0448e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1394e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7496e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383315" nkey="200" >
<hent key="02400100000000000000080000000001" call="MPI_Isend" bytes="2048" orank="1" region="0" commid="0" count="19" tid="0" op="" dtype="" >8.3923e-05 3.8147e-06 5.0068e-06</hent>
<hent key="02400100000000000000080000000007" call="MPI_Isend" bytes="2048" orank="7" region="0" commid="0" count="13" tid="0" op="" dtype="" >3.1233e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000008" call="MPI_Isend" bytes="2048" orank="8" region="0" commid="0" count="12" tid="0" op="" dtype="" >4.3869e-05 1.9073e-06 1.2875e-05</hent>
<hent key="02400100000000000000080000000038" call="MPI_Isend" bytes="2048" orank="56" region="0" commid="0" count="17" tid="0" op="" dtype="" >9.6083e-05 5.0068e-06 6.1989e-06</hent>
<hent key="02400100000000000000080000000040" call="MPI_Isend" bytes="2048" orank="64" region="0" commid="0" count="3352" tid="0" op="" dtype="" >1.4768e-02 9.5367e-07 3.3140e-05</hent>
<hent key="024001000000000000000800000002C0" call="MPI_Isend" bytes="2048" orank="704" region="0" commid="0" count="3355" tid="0" op="" dtype="" >1.3011e-02 9.5367e-07 3.0994e-05</hent>
<hent key="038001000000000000000E0000000008" call="MPI_Irecv" bytes="3584" orank="8" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000E00000002C0" call="MPI_Irecv" bytes="3584" orank="704" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000E0000000040" call="MPI_Isend" bytes="3584" orank="64" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.1921e-05 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000001" call="MPI_Irecv" bytes="640" orank="1" region="0" commid="0" count="404" tid="0" op="" dtype="" >1.6022e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000028000000007" call="MPI_Irecv" bytes="640" orank="7" region="0" commid="0" count="412" tid="0" op="" dtype="" >1.6308e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000008" call="MPI_Irecv" bytes="640" orank="8" region="0" commid="0" count="414" tid="0" op="" dtype="" >1.4830e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000028000000038" call="MPI_Irecv" bytes="640" orank="56" region="0" commid="0" count="420" tid="0" op="" dtype="" >1.6737e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000028000000040" call="MPI_Irecv" bytes="640" orank="64" region="0" commid="0" count="294" tid="0" op="" dtype="" >1.3518e-04 0.0000e+00 2.8610e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1086e-04 1.1086e-04 1.1086e-04</hent>
<hent key="038001000000000000000280000002C0" call="MPI_Irecv" bytes="640" orank="704" region="0" commid="0" count="304" tid="0" op="" dtype="" >1.4305e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000028000000001" call="MPI_Isend" bytes="640" orank="1" region="0" commid="0" count="403" tid="0" op="" dtype="" >1.3852e-03 1.9073e-06 6.9141e-06</hent>
<hent key="02400100000000000000028000000007" call="MPI_Isend" bytes="640" orank="7" region="0" commid="0" count="406" tid="0" op="" dtype="" >6.0964e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000028000000008" call="MPI_Isend" bytes="640" orank="8" region="0" commid="0" count="404" tid="0" op="" dtype="" >5.4169e-04 9.5367e-07 7.8678e-06</hent>
<hent key="02400100000000000000028000000038" call="MPI_Isend" bytes="640" orank="56" region="0" commid="0" count="399" tid="0" op="" dtype="" >2.2337e-03 3.8147e-06 2.0027e-05</hent>
<hent key="02400100000000000000028000000040" call="MPI_Isend" bytes="640" orank="64" region="0" commid="0" count="284" tid="0" op="" dtype="" >1.5337e-03 3.8147e-06 1.7881e-05</hent>
<hent key="024001000000000000000280000002C0" call="MPI_Isend" bytes="640" orank="704" region="0" commid="0" count="278" tid="0" op="" dtype="" >1.3874e-03 3.0994e-06 1.4067e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="03800100000000000000014000000001" call="MPI_Irecv" bytes="320" orank="1" region="0" commid="0" count="325" tid="0" op="" dtype="" >1.1015e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000014000000007" call="MPI_Irecv" bytes="320" orank="7" region="0" commid="0" count="405" tid="0" op="" dtype="" >1.7643e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000008" call="MPI_Irecv" bytes="320" orank="8" region="0" commid="0" count="373" tid="0" op="" dtype="" >1.1802e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000014000000038" call="MPI_Irecv" bytes="320" orank="56" region="0" commid="0" count="343" tid="0" op="" dtype="" >1.0991e-04 0.0000e+00 3.8147e-06</hent>
<hent key="03800100000000000000014000000040" call="MPI_Irecv" bytes="320" orank="64" region="0" commid="0" count="168" tid="0" op="" dtype="" >8.5115e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000140000002C0" call="MPI_Irecv" bytes="320" orank="704" region="0" commid="0" count="159" tid="0" op="" dtype="" >7.6294e-05 0.0000e+00 8.8215e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >8.2321e-03 1.8959e-03 2.5370e-03</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.5872e+00 0.0000e+00 1.8243e-01</hent>
<hent key="03800100000000000000400000000040" call="MPI_Irecv" bytes="16384" orank="64" region="0" commid="0" count="12280" tid="0" op="" dtype="" >1.4079e-02 0.0000e+00 2.4080e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6479e-02 2.1458e-06 1.6474e-02</hent>
<hent key="038001000000000000004000000002C0" call="MPI_Irecv" bytes="16384" orank="704" region="0" commid="0" count="12126" tid="0" op="" dtype="" >3.9430e-03 0.0000e+00 1.5974e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.4199e-04 6.1989e-05 1.1492e-04</hent>
<hent key="02400100000000000000014000000001" call="MPI_Isend" bytes="320" orank="1" region="0" commid="0" count="342" tid="0" op="" dtype="" >1.0920e-03 1.9073e-06 8.1062e-06</hent>
<hent key="02400100000000000000014000000007" call="MPI_Isend" bytes="320" orank="7" region="0" commid="0" count="399" tid="0" op="" dtype="" >4.8089e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000014000000008" call="MPI_Isend" bytes="320" orank="8" region="0" commid="0" count="349" tid="0" op="" dtype="" >4.3511e-04 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000014000000038" call="MPI_Isend" bytes="320" orank="56" region="0" commid="0" count="371" tid="0" op="" dtype="" >1.8668e-03 3.8147e-06 1.4067e-05</hent>
<hent key="02400100000000000000014000000040" call="MPI_Isend" bytes="320" orank="64" region="0" commid="0" count="181" tid="0" op="" dtype="" >8.9788e-04 3.0994e-06 9.0599e-06</hent>
<hent key="03800100000000000000200000000001" call="MPI_Irecv" bytes="8192" orank="1" region="0" commid="0" count="3774" tid="0" op="" dtype="" >8.3447e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000200000000007" call="MPI_Irecv" bytes="8192" orank="7" region="0" commid="0" count="3470" tid="0" op="" dtype="" >1.5678e-03 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000200000000008" call="MPI_Irecv" bytes="8192" orank="8" region="0" commid="0" count="12550" tid="0" op="" dtype="" >7.4430e-03 0.0000e+00 3.1948e-05</hent>
<hent key="03800100000000000000200000000038" call="MPI_Irecv" bytes="8192" orank="56" region="0" commid="0" count="12635" tid="0" op="" dtype="" >2.1727e-03 0.0000e+00 1.0967e-05</hent>
<hent key="024001000000000000000140000002C0" call="MPI_Isend" bytes="320" orank="704" region="0" commid="0" count="186" tid="0" op="" dtype="" >8.8835e-04 2.8610e-06 1.0967e-05</hent>
<hent key="03800100000000000000000000000001" call="MPI_Irecv" bytes="0" orank="1" region="0" commid="0" count="263" tid="0" op="" dtype="" >7.3910e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000000000000007" call="MPI_Irecv" bytes="0" orank="7" region="0" commid="0" count="281" tid="0" op="" dtype="" >1.1301e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000008" call="MPI_Irecv" bytes="0" orank="8" region="0" commid="0" count="254" tid="0" op="" dtype="" >9.4175e-05 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000000000000038" call="MPI_Irecv" bytes="0" orank="56" region="0" commid="0" count="241" tid="0" op="" dtype="" >6.0797e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000000000040" call="MPI_Irecv" bytes="0" orank="64" region="0" commid="0" count="162" tid="0" op="" dtype="" >5.1260e-05 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000400000000040" call="MPI_Isend" bytes="16384" orank="64" region="0" commid="0" count="12233" tid="0" op="" dtype="" >1.1112e-01 4.0531e-06 2.6011e-04</hent>
<hent key="038001000000000000000000000002C0" call="MPI_Irecv" bytes="0" orank="704" region="0" commid="0" count="151" tid="0" op="" dtype="" >4.1485e-05 0.0000e+00 4.0531e-06</hent>
<hent key="024001000000000000004000000002C0" call="MPI_Isend" bytes="16384" orank="704" region="0" commid="0" count="12309" tid="0" op="" dtype="" >9.6950e-02 2.8610e-06 1.4400e-04</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >5.7936e-05 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000200000000001" call="MPI_Isend" bytes="8192" orank="1" region="0" commid="0" count="3680" tid="0" op="" dtype="" >7.7031e-03 0.0000e+00 6.5088e-05</hent>
<hent key="02400100000000000000200000000007" call="MPI_Isend" bytes="8192" orank="7" region="0" commid="0" count="4139" tid="0" op="" dtype="" >4.8761e-03 0.0000e+00 1.6928e-05</hent>
<hent key="02400100000000000000200000000008" call="MPI_Isend" bytes="8192" orank="8" region="0" commid="0" count="12411" tid="0" op="" dtype="" >1.0114e-02 0.0000e+00 9.7036e-05</hent>
<hent key="02400100000000000000200000000038" call="MPI_Isend" bytes="8192" orank="56" region="0" commid="0" count="12677" tid="0" op="" dtype="" >1.2388e-01 3.8147e-06 1.1992e-04</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.2875e-05 1.2875e-05 1.2875e-05</hent>
<hent key="03800100000000000000030000000007" call="MPI_Irecv" bytes="768" orank="7" region="0" commid="0" count="6" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000000000000001" call="MPI_Isend" bytes="0" orank="1" region="0" commid="0" count="289" tid="0" op="" dtype="" >6.5494e-04 9.5367e-07 3.0041e-05</hent>
<hent key="02400100000000000000000000000007" call="MPI_Isend" bytes="0" orank="7" region="0" commid="0" count="248" tid="0" op="" dtype="" >1.7571e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000008" call="MPI_Isend" bytes="0" orank="8" region="0" commid="0" count="257" tid="0" op="" dtype="" >2.1315e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000000000000038" call="MPI_Isend" bytes="0" orank="56" region="0" commid="0" count="256" tid="0" op="" dtype="" >1.0993e-03 1.9073e-06 1.2875e-05</hent>
<hent key="02400100000000000000000000000040" call="MPI_Isend" bytes="0" orank="64" region="0" commid="0" count="150" tid="0" op="" dtype="" >6.2776e-04 9.5367e-07 1.0014e-05</hent>
<hent key="024001000000000000000000000002C0" call="MPI_Isend" bytes="0" orank="704" region="0" commid="0" count="146" tid="0" op="" dtype="" >6.1607e-04 1.9073e-06 1.0014e-05</hent>
<hent key="03800100000000000000060000000001" call="MPI_Irecv" bytes="1536" orank="1" region="0" commid="0" count="114" tid="0" op="" dtype="" >4.7207e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000060000000007" call="MPI_Irecv" bytes="1536" orank="7" region="0" commid="0" count="88" tid="0" op="" dtype="" >3.2187e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000008" call="MPI_Irecv" bytes="1536" orank="8" region="0" commid="0" count="101" tid="0" op="" dtype="" >3.7670e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000038" call="MPI_Irecv" bytes="1536" orank="56" region="0" commid="0" count="99" tid="0" op="" dtype="" >3.3855e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000040" call="MPI_Irecv" bytes="1536" orank="64" region="0" commid="0" count="240" tid="0" op="" dtype="" >1.1230e-04 0.0000e+00 3.0994e-06</hent>
<hent key="038001000000000000000600000002C0" call="MPI_Irecv" bytes="1536" orank="704" region="0" commid="0" count="210" tid="0" op="" dtype="" >7.5817e-05 0.0000e+00 2.1458e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="43" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000001" call="MPI_Isend" bytes="1536" orank="1" region="0" commid="0" count="101" tid="0" op="" dtype="" >4.2295e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000060000000007" call="MPI_Isend" bytes="1536" orank="7" region="0" commid="0" count="95" tid="0" op="" dtype="" >1.9956e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000008" call="MPI_Isend" bytes="1536" orank="8" region="0" commid="0" count="105" tid="0" op="" dtype="" >1.8597e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000060000000038" call="MPI_Isend" bytes="1536" orank="56" region="0" commid="0" count="95" tid="0" op="" dtype="" >5.9271e-04 4.0531e-06 1.5974e-05</hent>
<hent key="02400100000000000000060000000040" call="MPI_Isend" bytes="1536" orank="64" region="0" commid="0" count="221" tid="0" op="" dtype="" >1.1938e-03 3.8147e-06 1.2875e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C0" call="MPI_Isend" bytes="1536" orank="704" region="0" commid="0" count="226" tid="0" op="" dtype="" >1.1926e-03 3.8147e-06 1.5974e-05</hent>
<hent key="038001000000000000000C0000000007" call="MPI_Irecv" bytes="3072" orank="7" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C0000000008" call="MPI_Irecv" bytes="3072" orank="8" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C0000000040" call="MPI_Irecv" bytes="3072" orank="64" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C0" call="MPI_Irecv" bytes="3072" orank="704" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C0000000040" call="MPI_Isend" bytes="3072" orank="64" region="0" commid="0" count="11" tid="0" op="" dtype="" >7.2479e-05 5.0068e-06 8.8215e-06</hent>
<hent key="024001000000000000000C00000002C0" call="MPI_Isend" bytes="3072" orank="704" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.9564e-05 5.0068e-06 6.1989e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0901e-02 1.0901e-02 1.0901e-02</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.2714e-02 7.4320e-03 7.7500e-03</hent>
<hent key="03800100000000000000038000000001" call="MPI_Irecv" bytes="896" orank="1" region="0" commid="0" count="2723" tid="0" op="" dtype="" >6.0105e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000038000000007" call="MPI_Irecv" bytes="896" orank="7" region="0" commid="0" count="2747" tid="0" op="" dtype="" >5.0187e-04 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000038000000008" call="MPI_Irecv" bytes="896" orank="8" region="0" commid="0" count="311" tid="0" op="" dtype="" >1.1444e-04 0.0000e+00 2.8610e-06</hent>
<hent key="03800100000000000000038000000038" call="MPI_Irecv" bytes="896" orank="56" region="0" commid="0" count="329" tid="0" op="" dtype="" >1.4162e-04 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000038000000040" call="MPI_Irecv" bytes="896" orank="64" region="0" commid="0" count="279" tid="0" op="" dtype="" >1.2898e-04 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000380000002C0" call="MPI_Irecv" bytes="896" orank="704" region="0" commid="0" count="331" tid="0" op="" dtype="" >1.3852e-04 0.0000e+00 4.0531e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.9141e-06 6.9141e-06 6.9141e-06</hent>
<hent key="03800100000000000000380000000040" call="MPI_Irecv" bytes="14336" orank="64" region="0" commid="0" count="419" tid="0" op="" dtype="" >4.8399e-04 0.0000e+00 1.3113e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000003800000002C0" call="MPI_Irecv" bytes="14336" orank="704" region="0" commid="0" count="573" tid="0" op="" dtype="" >2.2459e-04 0.0000e+00 3.4094e-05</hent>
<hent key="03800100000000000000180000000007" call="MPI_Irecv" bytes="6144" orank="7" region="0" commid="0" count="22" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000038000000001" call="MPI_Isend" bytes="896" orank="1" region="0" commid="0" count="2723" tid="0" op="" dtype="" >3.0317e-03 0.0000e+00 1.1206e-05</hent>
<hent key="02400100000000000000038000000007" call="MPI_Isend" bytes="896" orank="7" region="0" commid="0" count="2584" tid="0" op="" dtype="" >1.9000e-03 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000038000000008" call="MPI_Isend" bytes="896" orank="8" region="0" commid="0" count="346" tid="0" op="" dtype="" >5.6219e-04 9.5367e-07 1.1206e-05</hent>
<hent key="02400100000000000000038000000038" call="MPI_Isend" bytes="896" orank="56" region="0" commid="0" count="295" tid="0" op="" dtype="" >1.6966e-03 3.8147e-06 1.7881e-05</hent>
<hent key="02400100000000000000038000000040" call="MPI_Isend" bytes="896" orank="64" region="0" commid="0" count="309" tid="0" op="" dtype="" >1.6599e-03 3.8147e-06 1.4067e-05</hent>
<hent key="024001000000000000000380000002C0" call="MPI_Isend" bytes="896" orank="704" region="0" commid="0" count="342" tid="0" op="" dtype="" >1.6985e-03 2.8610e-06 1.0967e-05</hent>
<hent key="02400100000000000000380000000040" call="MPI_Isend" bytes="14336" orank="64" region="0" commid="0" count="466" tid="0" op="" dtype="" >4.1466e-03 5.0068e-06 3.0994e-05</hent>
<hent key="024001000000000000003800000002C0" call="MPI_Isend" bytes="14336" orank="704" region="0" commid="0" count="390" tid="0" op="" dtype="" >2.8820e-03 3.8147e-06 2.9087e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >6.9563e+00 1.2159e-05 2.0306e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.2108e-03 3.2108e-03 3.2108e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0448e-02 1.0448e-02 1.0448e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.9230e-02 2.9230e-02 2.9230e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.4373e-01 1.1320e-03 2.2611e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.0422e-04 3.0422e-04 3.0422e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.5082e-03 1.8952e-03 3.6130e-03</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >8.3613e-04 1.5020e-05 9.7036e-05</hent>
<hent key="03800100000000000000040000000001" call="MPI_Irecv" bytes="1024" orank="1" region="0" commid="0" count="1012" tid="0" op="" dtype="" >1.9050e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000040000000007" call="MPI_Irecv" bytes="1024" orank="7" region="0" commid="0" count="934" tid="0" op="" dtype="" >1.1849e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000040000000008" call="MPI_Irecv" bytes="1024" orank="8" region="0" commid="0" count="3358" tid="0" op="" dtype="" >1.1380e-03 0.0000e+00 1.1921e-05</hent>
<hent key="03800100000000000000040000000038" call="MPI_Irecv" bytes="1024" orank="56" region="0" commid="0" count="3382" tid="0" op="" dtype="" >5.0926e-04 0.0000e+00 1.1921e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.3173e+00 9.5367e-07 2.2650e+00</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="03800100000000000000070000000001" call="MPI_Irecv" bytes="1792" orank="1" region="0" commid="0" count="35" tid="0" op="" dtype="" >1.3351e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000007" call="MPI_Irecv" bytes="1792" orank="7" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.2398e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000008" call="MPI_Irecv" bytes="1792" orank="8" region="0" commid="0" count="36" tid="0" op="" dtype="" >1.0014e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000038" call="MPI_Irecv" bytes="1792" orank="56" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.3828e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000040" call="MPI_Irecv" bytes="1792" orank="64" region="0" commid="0" count="232" tid="0" op="" dtype="" >6.8188e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002C0" call="MPI_Irecv" bytes="1792" orank="704" region="0" commid="0" count="288" tid="0" op="" dtype="" >1.0014e-04 0.0000e+00 8.1062e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >2.5749e-05 1.9073e-06 4.0531e-06</hent>
<hent key="02400100000000000000040000000001" call="MPI_Isend" bytes="1024" orank="1" region="0" commid="0" count="984" tid="0" op="" dtype="" >8.3923e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000040000000007" call="MPI_Isend" bytes="1024" orank="7" region="0" commid="0" count="1114" tid="0" op="" dtype="" >7.0882e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000040000000008" call="MPI_Isend" bytes="1024" orank="8" region="0" commid="0" count="3314" tid="0" op="" dtype="" >1.8854e-03 0.0000e+00 1.7166e-05</hent>
<hent key="02400100000000000000040000000038" call="MPI_Isend" bytes="1024" orank="56" region="0" commid="0" count="3392" tid="0" op="" dtype="" >1.0942e-02 9.5367e-07 2.5988e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >5.9605e-06 1.9073e-06 4.0531e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >6.0608e-03 1.9073e-06 5.1980e-03</hent>
<hent key="038001000000000000000A0000000001" call="MPI_Irecv" bytes="2560" orank="1" region="0" commid="0" count="7" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000007" call="MPI_Irecv" bytes="2560" orank="7" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000008" call="MPI_Irecv" bytes="2560" orank="8" region="0" commid="0" count="8" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000038" call="MPI_Irecv" bytes="2560" orank="56" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000040" call="MPI_Irecv" bytes="2560" orank="64" region="0" commid="0" count="54" tid="0" op="" dtype="" >3.3617e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002C0" call="MPI_Irecv" bytes="2560" orank="704" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.3828e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000001" call="MPI_Isend" bytes="1792" orank="1" region="0" commid="0" count="45" tid="0" op="" dtype="" >2.0146e-04 2.8610e-06 1.5974e-05</hent>
<hent key="02400100000000000000070000000007" call="MPI_Isend" bytes="1792" orank="7" region="0" commid="0" count="37" tid="0" op="" dtype="" >8.7976e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000008" call="MPI_Isend" bytes="1792" orank="8" region="0" commid="0" count="37" tid="0" op="" dtype="" >6.3181e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000070000000038" call="MPI_Isend" bytes="1792" orank="56" region="0" commid="0" count="49" tid="0" op="" dtype="" >3.0112e-04 5.0068e-06 1.5020e-05</hent>
<hent key="02400100000000000000070000000040" call="MPI_Isend" bytes="1792" orank="64" region="0" commid="0" count="252" tid="0" op="" dtype="" >1.2825e-03 9.5367e-07 1.4067e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1394e-01 1.1394e-01 1.1394e-01</hent>
<hent key="024001000000000000000700000002C0" call="MPI_Isend" bytes="1792" orank="704" region="0" commid="0" count="218" tid="0" op="" dtype="" >1.0409e-03 9.5367e-07 1.5020e-05</hent>
<hent key="024001000000000000000A0000000001" call="MPI_Isend" bytes="2560" orank="1" region="0" commid="0" count="9" tid="0" op="" dtype="" >4.1962e-05 3.8147e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000007" call="MPI_Isend" bytes="2560" orank="7" region="0" commid="0" count="3" tid="0" op="" dtype="" >6.6757e-06 1.9073e-06 2.8610e-06</hent>
<hent key="024001000000000000000A0000000008" call="MPI_Isend" bytes="2560" orank="8" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.0967e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000038" call="MPI_Isend" bytes="2560" orank="56" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.8835e-05 5.9605e-06 1.2875e-05</hent>
<hent key="024001000000000000000A0000000040" call="MPI_Isend" bytes="2560" orank="64" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.5630e-04 5.0068e-06 1.0967e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.3930e-02 4.4780e-03 4.8201e-03</hent>
<hent key="024001000000000000000A00000002C0" call="MPI_Isend" bytes="2560" orank="704" region="0" commid="0" count="39" tid="0" op="" dtype="" >2.4652e-04 4.0531e-06 1.3113e-05</hent>
<hent key="03800100000000000000100000000040" call="MPI_Irecv" bytes="4096" orank="64" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.3671e-02 2.6515e-02 2.7156e-02</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.0339e-03 2.0339e-03 2.0339e-03</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >6.9141e-06 2.8610e-06 4.0531e-06</hent>
<hent key="03800100000000000000000400000001" call="MPI_Irecv" bytes="4" orank="1" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.9615e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000000400000007" call="MPI_Irecv" bytes="4" orank="7" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3039e-03 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000400000008" call="MPI_Irecv" bytes="4" orank="8" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2374e-03 0.0000e+00 1.4067e-05</hent>
<hent key="03800100000000000000000400000038" call="MPI_Irecv" bytes="4" orank="56" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.7711e-04 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000000400000040" call="MPI_Irecv" bytes="4" orank="64" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2984e-03 0.0000e+00 1.6928e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.0891e-01 2.0147e-02 4.6985e-02</hent>
<hent key="038001000000000000001C0000000001" call="MPI_Irecv" bytes="7168" orank="1" region="0" commid="0" count="8925" tid="0" op="" dtype="" >2.0514e-03 0.0000e+00 6.9141e-06</hent>
<hent key="038001000000000000001C0000000007" call="MPI_Irecv" bytes="7168" orank="7" region="0" commid="0" count="9207" tid="0" op="" dtype="" >4.2012e-03 0.0000e+00 8.1062e-06</hent>
<hent key="038001000000000000000004000002C0" call="MPI_Irecv" bytes="4" orank="704" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1647e-03 0.0000e+00 1.7881e-05</hent>
<hent key="02400100000000000000000400000001" call="MPI_Isend" bytes="4" orank="1" region="0" commid="0" count="3398" tid="0" op="" dtype="" >3.1610e-03 0.0000e+00 1.2875e-05</hent>
<hent key="02400100000000000000000400000007" call="MPI_Isend" bytes="4" orank="7" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6100e-03 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000000400000008" call="MPI_Isend" bytes="4" orank="8" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4544e-03 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000000400000038" call="MPI_Isend" bytes="4" orank="56" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3983e-02 3.8147e-06 3.7909e-05</hent>
<hent key="02400100000000000000000400000040" call="MPI_Isend" bytes="4" orank="64" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.2726e-02 3.8147e-06 3.3140e-05</hent>
<hent key="024001000000000000001C0000000001" call="MPI_Isend" bytes="7168" orank="1" region="0" commid="0" count="9019" tid="0" op="" dtype="" >3.0141e-02 9.5367e-07 2.0981e-05</hent>
<hent key="024001000000000000001C0000000007" call="MPI_Isend" bytes="7168" orank="7" region="0" commid="0" count="8560" tid="0" op="" dtype="" >2.0264e-02 9.5367e-07 2.0981e-05</hent>
<hent key="024001000000000000000004000002C0" call="MPI_Isend" bytes="4" orank="704" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.2177e-02 2.8610e-06 2.5034e-05</hent>
<hent key="03800100000000000000050000000001" call="MPI_Irecv" bytes="1280" orank="1" region="0" commid="0" count="201" tid="0" op="" dtype="" >8.5592e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000007" call="MPI_Irecv" bytes="1280" orank="7" region="0" commid="0" count="177" tid="0" op="" dtype="" >7.4387e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000008" call="MPI_Irecv" bytes="1280" orank="8" region="0" commid="0" count="230" tid="0" op="" dtype="" >7.2479e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000050000000038" call="MPI_Irecv" bytes="1280" orank="56" region="0" commid="0" count="226" tid="0" op="" dtype="" >8.8930e-05 0.0000e+00 8.8215e-06</hent>
<hent key="03800100000000000000050000000040" call="MPI_Irecv" bytes="1280" orank="64" region="0" commid="0" count="311" tid="0" op="" dtype="" >1.7953e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0263e+01 9.0599e-06 1.5539e-01</hent>
<hent key="03800100000000000000280000000008" call="MPI_Irecv" bytes="10240" orank="8" region="0" commid="0" count="149" tid="0" op="" dtype="" >7.0572e-05 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000280000000038" call="MPI_Irecv" bytes="10240" orank="56" region="0" commid="0" count="64" tid="0" op="" dtype="" >1.9550e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000500000002C0" call="MPI_Irecv" bytes="1280" orank="704" region="0" commid="0" count="292" tid="0" op="" dtype="" >1.2183e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000080000000001" call="MPI_Irecv" bytes="2048" orank="1" region="0" commid="0" count="13" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000080000000007" call="MPI_Irecv" bytes="2048" orank="7" region="0" commid="0" count="10" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000008" call="MPI_Irecv" bytes="2048" orank="8" region="0" commid="0" count="10" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000038" call="MPI_Irecv" bytes="2048" orank="56" region="0" commid="0" count="15" tid="0" op="" dtype="" >1.0014e-05 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000080000000040" call="MPI_Irecv" bytes="2048" orank="64" region="0" commid="0" count="3351" tid="0" op="" dtype="" >9.2745e-04 0.0000e+00 4.4823e-05</hent>
<hent key="038001000000000000000800000002C0" call="MPI_Irecv" bytes="2048" orank="704" region="0" commid="0" count="3315" tid="0" op="" dtype="" >8.3232e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000050000000001" call="MPI_Isend" bytes="1280" orank="1" region="0" commid="0" count="182" tid="0" op="" dtype="" >7.1907e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000050000000007" call="MPI_Isend" bytes="1280" orank="7" region="0" commid="0" count="198" tid="0" op="" dtype="" >3.8815e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000008" call="MPI_Isend" bytes="1280" orank="8" region="0" commid="0" count="269" tid="0" op="" dtype="" >3.9220e-04 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000050000000038" call="MPI_Isend" bytes="1280" orank="56" region="0" commid="0" count="221" tid="0" op="" dtype="" >1.3013e-03 9.5367e-07 1.8120e-05</hent>
<hent key="02400100000000000000050000000040" call="MPI_Isend" bytes="1280" orank="64" region="0" commid="0" count="292" tid="0" op="" dtype="" >1.6260e-03 3.8147e-06 1.5020e-05</hent>
<hent key="02400100000000000000280000000008" call="MPI_Isend" bytes="10240" orank="8" region="0" commid="0" count="288" tid="0" op="" dtype="" >1.7071e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000280000000038" call="MPI_Isend" bytes="10240" orank="56" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.8787e-04 5.0068e-06 1.5974e-05</hent>
<hent key="024001000000000000000500000002C0" call="MPI_Isend" bytes="1280" orank="704" region="0" commid="0" count="302" tid="0" op="" dtype="" >1.5182e-03 2.8610e-06 1.1921e-05</hent>
</hash>
<internal rank="0" log_i="1723712895.690476" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="1" mpi_size="768" stamp_init="1723712829.548172" stamp_final="1723712895.689771" username="apac4" allocationname="unknown" flags="0" pid="684269" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61416e+01" utime="5.12560e+01" stime="7.44606e+00" mtime="3.01750e+01" gflop="0.00000e+00" gbyte="3.78616e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.01750e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60044e+01" utime="5.12209e+01" stime="7.43702e+00" mtime="3.01750e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.01750e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4952e+08" > 4.6925e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4806e+08" > 3.2022e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7201e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5776e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8147e-06 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5915e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.0105e-04 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0379e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1364e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8660e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="196" >
<hent key="02400100000000000000080000000000" call="MPI_Isend" bytes="2048" orank="0" region="0" commid="0" count="13" tid="0" op="" dtype="" >2.9087e-05 1.9073e-06 2.8610e-06</hent>
<hent key="02400100000000000000080000000002" call="MPI_Isend" bytes="2048" orank="2" region="0" commid="0" count="14" tid="0" op="" dtype="" >6.6280e-05 3.8147e-06 6.1989e-06</hent>
<hent key="02400100000000000000080000000009" call="MPI_Isend" bytes="2048" orank="9" region="0" commid="0" count="13" tid="0" op="" dtype="" >5.3644e-05 9.5367e-07 1.2875e-05</hent>
<hent key="02400100000000000000080000000039" call="MPI_Isend" bytes="2048" orank="57" region="0" commid="0" count="11" tid="0" op="" dtype="" >6.2943e-05 5.0068e-06 6.1989e-06</hent>
<hent key="02400100000000000000080000000041" call="MPI_Isend" bytes="2048" orank="65" region="0" commid="0" count="3473" tid="0" op="" dtype="" >1.1670e-02 9.5367e-07 2.1935e-05</hent>
<hent key="024001000000000000000800000002C1" call="MPI_Isend" bytes="2048" orank="705" region="0" commid="0" count="3445" tid="0" op="" dtype="" >1.1321e-02 9.5367e-07 1.2875e-05</hent>
<hent key="038001000000000000000E00000002C1" call="MPI_Irecv" bytes="3584" orank="705" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000E0000000041" call="MPI_Isend" bytes="3584" orank="65" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="03800100000000000000028000000000" call="MPI_Irecv" bytes="640" orank="0" region="0" commid="0" count="403" tid="0" op="" dtype="" >1.4758e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000002" call="MPI_Irecv" bytes="640" orank="2" region="0" commid="0" count="421" tid="0" op="" dtype="" >2.4700e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000028000000009" call="MPI_Irecv" bytes="640" orank="9" region="0" commid="0" count="392" tid="0" op="" dtype="" >1.0037e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000028000000039" call="MPI_Irecv" bytes="640" orank="57" region="0" commid="0" count="416" tid="0" op="" dtype="" >1.4734e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000041" call="MPI_Irecv" bytes="640" orank="65" region="0" commid="0" count="267" tid="0" op="" dtype="" >7.9155e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002C1" call="MPI_Irecv" bytes="640" orank="705" region="0" commid="0" count="282" tid="0" op="" dtype="" >6.1035e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000000" call="MPI_Isend" bytes="640" orank="0" region="0" commid="0" count="404" tid="0" op="" dtype="" >5.9175e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000028000000002" call="MPI_Isend" bytes="640" orank="2" region="0" commid="0" count="422" tid="0" op="" dtype="" >1.6112e-03 2.8610e-06 1.0014e-05</hent>
<hent key="02400100000000000000028000000009" call="MPI_Isend" bytes="640" orank="9" region="0" commid="0" count="390" tid="0" op="" dtype="" >4.7612e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000028000000039" call="MPI_Isend" bytes="640" orank="57" region="0" commid="0" count="413" tid="0" op="" dtype="" >1.9369e-03 3.8147e-06 8.8215e-06</hent>
<hent key="02400100000000000000028000000041" call="MPI_Isend" bytes="640" orank="65" region="0" commid="0" count="281" tid="0" op="" dtype="" >1.2093e-03 2.8610e-06 1.0967e-05</hent>
<hent key="024001000000000000000280000002C1" call="MPI_Isend" bytes="640" orank="705" region="0" commid="0" count="274" tid="0" op="" dtype="" >1.1752e-03 3.0994e-06 5.9605e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.1948e-05 3.1948e-05 3.1948e-05</hent>
<hent key="03800100000000000000014000000000" call="MPI_Irecv" bytes="320" orank="0" region="0" commid="0" count="342" tid="0" op="" dtype="" >1.5020e-04 0.0000e+00 3.8147e-06</hent>
<hent key="03800100000000000000014000000002" call="MPI_Irecv" bytes="320" orank="2" region="0" commid="0" count="369" tid="0" op="" dtype="" >1.9979e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000014000000009" call="MPI_Irecv" bytes="320" orank="9" region="0" commid="0" count="370" tid="0" op="" dtype="" >9.8944e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000039" call="MPI_Irecv" bytes="320" orank="57" region="0" commid="0" count="351" tid="0" op="" dtype="" >9.6083e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000014000000041" call="MPI_Irecv" bytes="320" orank="65" region="0" commid="0" count="172" tid="0" op="" dtype="" >4.9591e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002C1" call="MPI_Irecv" bytes="320" orank="705" region="0" commid="0" count="167" tid="0" op="" dtype="" >3.6478e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >4.0531e-06 9.5367e-07 1.1921e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.7201e+00 0.0000e+00 1.8395e-01</hent>
<hent key="03800100000000000000400000000041" call="MPI_Irecv" bytes="16384" orank="65" region="0" commid="0" count="12691" tid="0" op="" dtype="" >3.0091e-03 0.0000e+00 7.1526e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6446e-02 1.1921e-06 1.6428e-02</hent>
<hent key="038001000000000000004000000002C1" call="MPI_Irecv" bytes="16384" orank="705" region="0" commid="0" count="12699" tid="0" op="" dtype="" >2.3849e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000000" call="MPI_Isend" bytes="320" orank="0" region="0" commid="0" count="325" tid="0" op="" dtype="" >3.5048e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000002" call="MPI_Isend" bytes="320" orank="2" region="0" commid="0" count="338" tid="0" op="" dtype="" >1.1756e-03 9.5367e-07 8.8215e-06</hent>
<hent key="02400100000000000000014000000009" call="MPI_Isend" bytes="320" orank="9" region="0" commid="0" count="355" tid="0" op="" dtype="" >3.6335e-04 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000014000000039" call="MPI_Isend" bytes="320" orank="57" region="0" commid="0" count="352" tid="0" op="" dtype="" >1.5655e-03 3.8147e-06 5.9605e-06</hent>
<hent key="02400100000000000000014000000041" call="MPI_Isend" bytes="320" orank="65" region="0" commid="0" count="164" tid="0" op="" dtype="" >7.0286e-04 3.8147e-06 1.0014e-05</hent>
<hent key="03800100000000000000200000000000" call="MPI_Irecv" bytes="8192" orank="0" region="0" commid="0" count="3680" tid="0" op="" dtype="" >1.5392e-03 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000200000000002" call="MPI_Irecv" bytes="8192" orank="2" region="0" commid="0" count="2473" tid="0" op="" dtype="" >2.9135e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000200000000009" call="MPI_Irecv" bytes="8192" orank="9" region="0" commid="0" count="12684" tid="0" op="" dtype="" >7.7486e-03 0.0000e+00 2.0981e-05</hent>
<hent key="03800100000000000000200000000039" call="MPI_Irecv" bytes="8192" orank="57" region="0" commid="0" count="12691" tid="0" op="" dtype="" >1.3490e-03 0.0000e+00 6.1989e-06</hent>
<hent key="024001000000000000000140000002C1" call="MPI_Isend" bytes="320" orank="705" region="0" commid="0" count="181" tid="0" op="" dtype="" >7.5316e-04 2.8610e-06 5.0068e-06</hent>
<hent key="03800100000000000000000000000000" call="MPI_Irecv" bytes="0" orank="0" region="0" commid="0" count="289" tid="0" op="" dtype="" >9.8944e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000002" call="MPI_Irecv" bytes="0" orank="2" region="0" commid="0" count="248" tid="0" op="" dtype="" >9.7990e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000009" call="MPI_Irecv" bytes="0" orank="9" region="0" commid="0" count="289" tid="0" op="" dtype="" >7.7009e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000000000000039" call="MPI_Irecv" bytes="0" orank="57" region="0" commid="0" count="258" tid="0" op="" dtype="" >6.1750e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000041" call="MPI_Irecv" bytes="0" orank="65" region="0" commid="0" count="140" tid="0" op="" dtype="" >3.6478e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000041" call="MPI_Isend" bytes="16384" orank="65" region="0" commid="0" count="12685" tid="0" op="" dtype="" >8.9480e-02 4.0531e-06 6.1989e-05</hent>
<hent key="038001000000000000000000000002C1" call="MPI_Irecv" bytes="0" orank="705" region="0" commid="0" count="140" tid="0" op="" dtype="" >2.2888e-05 0.0000e+00 1.9073e-06</hent>
<hent key="024001000000000000004000000002C1" call="MPI_Isend" bytes="16384" orank="705" region="0" commid="0" count="12645" tid="0" op="" dtype="" >9.1200e-02 3.8147e-06 6.6996e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.1866e-04 0.0000e+00 2.7680e-04</hent>
<hent key="02400100000000000000200000000000" call="MPI_Isend" bytes="8192" orank="0" region="0" commid="0" count="3774" tid="0" op="" dtype="" >4.7703e-03 0.0000e+00 5.6028e-05</hent>
<hent key="02400100000000000000200000000002" call="MPI_Isend" bytes="8192" orank="2" region="0" commid="0" count="3615" tid="0" op="" dtype="" >7.9319e-03 0.0000e+00 4.6015e-05</hent>
<hent key="02400100000000000000200000000009" call="MPI_Isend" bytes="8192" orank="9" region="0" commid="0" count="12632" tid="0" op="" dtype="" >8.1048e-03 0.0000e+00 9.2030e-05</hent>
<hent key="02400100000000000000200000000039" call="MPI_Isend" bytes="8192" orank="57" region="0" commid="0" count="12613" tid="0" op="" dtype="" >8.0425e-02 3.8147e-06 1.3304e-04</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.0027e-05 2.0027e-05 2.0027e-05</hent>
<hent key="02400100000000000000000000000000" call="MPI_Isend" bytes="0" orank="0" region="0" commid="0" count="263" tid="0" op="" dtype="" >1.8573e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000000000002" call="MPI_Isend" bytes="0" orank="2" region="0" commid="0" count="264" tid="0" op="" dtype="" >6.1965e-04 9.5367e-07 5.2929e-05</hent>
<hent key="02400100000000000000000000000009" call="MPI_Isend" bytes="0" orank="9" region="0" commid="0" count="265" tid="0" op="" dtype="" >2.1505e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000000000000039" call="MPI_Isend" bytes="0" orank="57" region="0" commid="0" count="255" tid="0" op="" dtype="" >1.0080e-03 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000000000000041" call="MPI_Isend" bytes="0" orank="65" region="0" commid="0" count="151" tid="0" op="" dtype="" >5.4073e-04 9.5367e-07 6.1989e-06</hent>
<hent key="024001000000000000000000000002C1" call="MPI_Isend" bytes="0" orank="705" region="0" commid="0" count="145" tid="0" op="" dtype="" >5.5647e-04 9.5367e-07 7.1526e-06</hent>
<hent key="03800100000000000000060000000000" call="MPI_Irecv" bytes="1536" orank="0" region="0" commid="0" count="101" tid="0" op="" dtype="" >5.2452e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000002" call="MPI_Irecv" bytes="1536" orank="2" region="0" commid="0" count="90" tid="0" op="" dtype="" >7.2241e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000060000000009" call="MPI_Irecv" bytes="1536" orank="9" region="0" commid="0" count="113" tid="0" op="" dtype="" >2.5749e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000039" call="MPI_Irecv" bytes="1536" orank="57" region="0" commid="0" count="104" tid="0" op="" dtype="" >3.6001e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000041" call="MPI_Irecv" bytes="1536" orank="65" region="0" commid="0" count="241" tid="0" op="" dtype="" >9.4652e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002C1" call="MPI_Irecv" bytes="1536" orank="705" region="0" commid="0" count="217" tid="0" op="" dtype="" >4.3869e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000000" call="MPI_Isend" bytes="1536" orank="0" region="0" commid="0" count="114" tid="0" op="" dtype="" >2.2721e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000002" call="MPI_Isend" bytes="1536" orank="2" region="0" commid="0" count="105" tid="0" op="" dtype="" >4.6134e-04 3.8147e-06 5.0068e-06</hent>
<hent key="02400100000000000000060000000009" call="MPI_Isend" bytes="1536" orank="9" region="0" commid="0" count="104" tid="0" op="" dtype="" >1.8978e-04 9.5367e-07 1.5020e-05</hent>
<hent key="02400100000000000000060000000039" call="MPI_Isend" bytes="1536" orank="57" region="0" commid="0" count="90" tid="0" op="" dtype="" >4.7708e-04 4.0531e-06 6.9141e-06</hent>
<hent key="02400100000000000000060000000041" call="MPI_Isend" bytes="1536" orank="65" region="0" commid="0" count="200" tid="0" op="" dtype="" >9.7322e-04 3.8147e-06 1.1921e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C1" call="MPI_Isend" bytes="1536" orank="705" region="0" commid="0" count="204" tid="0" op="" dtype="" >9.6226e-04 3.8147e-06 9.0599e-06</hent>
<hent key="038001000000000000000C0000000039" call="MPI_Irecv" bytes="3072" orank="57" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.1921e-06 1.1921e-06 1.1921e-06</hent>
<hent key="038001000000000000000C0000000041" call="MPI_Irecv" bytes="3072" orank="65" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C1" call="MPI_Irecv" bytes="3072" orank="705" region="0" commid="0" count="5" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000C0000000002" call="MPI_Isend" bytes="3072" orank="2" region="0" commid="0" count="1" tid="0" op="" dtype="" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="024001000000000000000C0000000041" call="MPI_Isend" bytes="3072" orank="65" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.6955e-05 5.0068e-06 1.0967e-05</hent>
<hent key="024001000000000000000C00000002C1" call="MPI_Isend" bytes="3072" orank="705" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.5020e-05 5.0068e-06 5.0068e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.1962e-05 4.1962e-05 4.1962e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >8.9884e-05 2.8849e-05 3.0994e-05</hent>
<hent key="03800100000000000000038000000000" call="MPI_Irecv" bytes="896" orank="0" region="0" commid="0" count="2723" tid="0" op="" dtype="" >4.3368e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000002" call="MPI_Irecv" bytes="896" orank="2" region="0" commid="0" count="3072" tid="0" op="" dtype="" >6.8092e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000038000000009" call="MPI_Irecv" bytes="896" orank="9" region="0" commid="0" count="311" tid="0" op="" dtype="" >7.0333e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000039" call="MPI_Irecv" bytes="896" orank="57" region="0" commid="0" count="324" tid="0" op="" dtype="" >1.0514e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000041" call="MPI_Irecv" bytes="896" orank="65" region="0" commid="0" count="320" tid="0" op="" dtype="" >1.1444e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002C1" call="MPI_Irecv" bytes="896" orank="705" region="0" commid="0" count="326" tid="0" op="" dtype="" >8.1539e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.3828e-05 1.3828e-05 1.3828e-05</hent>
<hent key="03800100000000000000380000000041" call="MPI_Irecv" bytes="14336" orank="65" region="0" commid="0" count="8" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="02400100000000000000038000000000" call="MPI_Isend" bytes="896" orank="0" region="0" commid="0" count="2723" tid="0" op="" dtype="" >2.0134e-03 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000038000000002" call="MPI_Isend" bytes="896" orank="2" region="0" commid="0" count="2772" tid="0" op="" dtype="" >2.9798e-03 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000038000000009" call="MPI_Isend" bytes="896" orank="9" region="0" commid="0" count="353" tid="0" op="" dtype="" >5.0974e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000038000000039" call="MPI_Isend" bytes="896" orank="57" region="0" commid="0" count="319" tid="0" op="" dtype="" >1.5819e-03 3.8147e-06 9.0599e-06</hent>
<hent key="02400100000000000000038000000041" call="MPI_Isend" bytes="896" orank="65" region="0" commid="0" count="314" tid="0" op="" dtype="" >1.4219e-03 3.8147e-06 1.0967e-05</hent>
<hent key="024001000000000000000380000002C1" call="MPI_Isend" bytes="896" orank="705" region="0" commid="0" count="374" tid="0" op="" dtype="" >1.6425e-03 3.0994e-06 8.8215e-06</hent>
<hent key="02400100000000000000380000000041" call="MPI_Isend" bytes="14336" orank="65" region="0" commid="0" count="14" tid="0" op="" dtype="" >9.9182e-05 5.9605e-06 8.8215e-06</hent>
<hent key="024001000000000000003800000002C1" call="MPI_Isend" bytes="14336" orank="705" region="0" commid="0" count="54" tid="0" op="" dtype="" >3.9053e-04 5.0068e-06 1.2875e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.7742e+00 1.1921e-05 2.0293e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.2029e-03 3.2029e-03 3.2029e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0379e-02 1.0379e-02 1.0379e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.4438e-02 2.4438e-02 2.4438e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.0324e-01 1.9650e-03 1.8521e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.5810e-04 3.5810e-04 3.5810e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 9.5367e-07 1.9073e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5915e+00 3.6597e-04 2.5145e-01</hent>
<hent key="03800100000000000000040000000000" call="MPI_Irecv" bytes="1024" orank="0" region="0" commid="0" count="984" tid="0" op="" dtype="" >1.0562e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000002" call="MPI_Irecv" bytes="1024" orank="2" region="0" commid="0" count="668" tid="0" op="" dtype="" >9.7036e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000009" call="MPI_Irecv" bytes="1024" orank="9" region="0" commid="0" count="3394" tid="0" op="" dtype="" >8.0466e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000040000000039" call="MPI_Irecv" bytes="1024" orank="57" region="0" commid="0" count="3396" tid="0" op="" dtype="" >2.8777e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.3127e-05 0.0000e+00 1.0014e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.9101e-05 3.9101e-05 3.9101e-05</hent>
<hent key="03800100000000000000070000000000" call="MPI_Irecv" bytes="1792" orank="0" region="0" commid="0" count="45" tid="0" op="" dtype="" >2.0504e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000002" call="MPI_Irecv" bytes="1792" orank="2" region="0" commid="0" count="43" tid="0" op="" dtype="" >3.0279e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000070000000009" call="MPI_Irecv" bytes="1792" orank="9" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.7166e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000039" call="MPI_Irecv" bytes="1792" orank="57" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.6451e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000041" call="MPI_Irecv" bytes="1792" orank="65" region="0" commid="0" count="128" tid="0" op="" dtype="" >5.0545e-05 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000700000002C1" call="MPI_Irecv" bytes="1792" orank="705" region="0" commid="0" count="133" tid="0" op="" dtype="" >3.0994e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >6.4373e-05 0.0000e+00 2.7180e-05</hent>
<hent key="02400100000000000000040000000000" call="MPI_Isend" bytes="1024" orank="0" region="0" commid="0" count="1012" tid="0" op="" dtype="" >6.1750e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000040000000002" call="MPI_Isend" bytes="1024" orank="2" region="0" commid="0" count="954" tid="0" op="" dtype="" >7.3552e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000040000000009" call="MPI_Isend" bytes="1024" orank="9" region="0" commid="0" count="3378" tid="0" op="" dtype="" >1.7974e-03 0.0000e+00 1.6928e-05</hent>
<hent key="02400100000000000000040000000039" call="MPI_Isend" bytes="1024" orank="57" region="0" commid="0" count="3374" tid="0" op="" dtype="" >8.8553e-03 9.5367e-07 6.9141e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >5.0068e-06 1.9073e-06 3.0994e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5606e+00 0.0000e+00 2.2679e+00</hent>
<hent key="038001000000000000000A0000000000" call="MPI_Irecv" bytes="2560" orank="0" region="0" commid="0" count="9" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000002" call="MPI_Irecv" bytes="2560" orank="2" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000000A0000000009" call="MPI_Irecv" bytes="2560" orank="9" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000039" call="MPI_Irecv" bytes="2560" orank="57" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000041" call="MPI_Irecv" bytes="2560" orank="65" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.8358e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002C1" call="MPI_Irecv" bytes="2560" orank="705" region="0" commid="0" count="51" tid="0" op="" dtype="" >1.8120e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000000" call="MPI_Isend" bytes="1792" orank="0" region="0" commid="0" count="35" tid="0" op="" dtype="" >7.8678e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000002" call="MPI_Isend" bytes="1792" orank="2" region="0" commid="0" count="43" tid="0" op="" dtype="" >1.9431e-04 3.8147e-06 6.1989e-06</hent>
<hent key="02400100000000000000070000000009" call="MPI_Isend" bytes="1792" orank="9" region="0" commid="0" count="40" tid="0" op="" dtype="" >9.4652e-05 9.5367e-07 1.5974e-05</hent>
<hent key="02400100000000000000070000000039" call="MPI_Isend" bytes="1792" orank="57" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.6713e-04 5.0068e-06 6.1989e-06</hent>
<hent key="02400100000000000000070000000041" call="MPI_Isend" bytes="1792" orank="65" region="0" commid="0" count="148" tid="0" op="" dtype="" >7.3266e-04 2.1458e-06 6.9141e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1364e-01 1.1364e-01 1.1364e-01</hent>
<hent key="024001000000000000000700000002C1" call="MPI_Isend" bytes="1792" orank="705" region="0" commid="0" count="124" tid="0" op="" dtype="" >5.6624e-04 9.5367e-07 6.1989e-06</hent>
<hent key="024001000000000000000A0000000000" call="MPI_Isend" bytes="2560" orank="0" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.5020e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000002" call="MPI_Isend" bytes="2560" orank="2" region="0" commid="0" count="6" tid="0" op="" dtype="" >4.6253e-05 4.0531e-06 2.2173e-05</hent>
<hent key="024001000000000000000A0000000009" call="MPI_Isend" bytes="2560" orank="9" region="0" commid="0" count="5" tid="0" op="" dtype="" >4.9829e-05 1.9073e-06 1.5974e-05</hent>
<hent key="024001000000000000000A0000000039" call="MPI_Isend" bytes="2560" orank="57" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.8849e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000041" call="MPI_Isend" bytes="2560" orank="65" region="0" commid="0" count="45" tid="0" op="" dtype="" >2.3842e-04 4.0531e-06 6.1989e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >7.8917e-05 1.8120e-05 3.3855e-05</hent>
<hent key="024001000000000000000A00000002C1" call="MPI_Isend" bytes="2560" orank="705" region="0" commid="0" count="49" tid="0" op="" dtype="" >2.4581e-04 3.8147e-06 6.1989e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.5497e-04 6.6042e-05 8.8930e-05</hent>
<hent key="024001000000000000001000000002C1" call="MPI_Isend" bytes="4096" orank="705" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >6.1989e-06 2.1458e-06 4.0531e-06</hent>
<hent key="03800100000000000000000400000000" call="MPI_Irecv" bytes="4" orank="0" region="0" commid="0" count="3398" tid="0" op="" dtype="" >9.2554e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000000400000002" call="MPI_Irecv" bytes="4" orank="2" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.5896e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000009" call="MPI_Irecv" bytes="4" orank="9" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.8998e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000039" call="MPI_Irecv" bytes="4" orank="57" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.5323e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000041" call="MPI_Irecv" bytes="4" orank="65" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.4445e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.2840e-04 4.7207e-05 6.6042e-05</hent>
<hent key="038001000000000000001C0000000000" call="MPI_Irecv" bytes="7168" orank="0" region="0" commid="0" count="9019" tid="0" op="" dtype="" >3.9690e-03 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000001C0000000002" call="MPI_Irecv" bytes="7168" orank="2" region="0" commid="0" count="10226" tid="0" op="" dtype="" >1.2019e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000004000002C1" call="MPI_Irecv" bytes="4" orank="705" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.3682e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000000400000000" call="MPI_Isend" bytes="4" orank="0" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1289e-03 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000400000002" call="MPI_Isend" bytes="4" orank="2" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.7890e-03 0.0000e+00 5.3883e-05</hent>
<hent key="02400100000000000000000400000009" call="MPI_Isend" bytes="4" orank="9" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2813e-03 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000000400000039" call="MPI_Isend" bytes="4" orank="57" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.9799e-02 3.8147e-06 1.0681e-04</hent>
<hent key="02400100000000000000000400000041" call="MPI_Isend" bytes="4" orank="65" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.9228e-02 3.8147e-06 1.4067e-05</hent>
<hent key="024001000000000000001C0000000000" call="MPI_Isend" bytes="7168" orank="0" region="0" commid="0" count="8925" tid="0" op="" dtype="" >2.0080e-02 9.5367e-07 1.5020e-05</hent>
<hent key="024001000000000000001C0000000002" call="MPI_Isend" bytes="7168" orank="2" region="0" commid="0" count="9084" tid="0" op="" dtype="" >3.1720e-02 9.5367e-07 1.5974e-05</hent>
<hent key="024001000000000000000004000002C1" call="MPI_Isend" bytes="4" orank="705" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.8512e-02 2.8610e-06 8.7976e-05</hent>
<hent key="03800100000000000000050000000000" call="MPI_Irecv" bytes="1280" orank="0" region="0" commid="0" count="182" tid="0" op="" dtype="" >7.9632e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000002" call="MPI_Irecv" bytes="1280" orank="2" region="0" commid="0" count="168" tid="0" op="" dtype="" >1.2112e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000050000000009" call="MPI_Irecv" bytes="1280" orank="9" region="0" commid="0" count="164" tid="0" op="" dtype="" >4.9353e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000039" call="MPI_Irecv" bytes="1280" orank="57" region="0" commid="0" count="190" tid="0" op="" dtype="" >6.2704e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000041" call="MPI_Irecv" bytes="1280" orank="65" region="0" commid="0" count="305" tid="0" op="" dtype="" >1.0443e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0654e+01 7.8678e-06 1.5591e-01</hent>
<hent key="03800100000000000000280000000009" call="MPI_Irecv" bytes="10240" orank="9" region="0" commid="0" count="15" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000280000000039" call="MPI_Irecv" bytes="10240" orank="57" region="0" commid="0" count="8" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000500000002C1" call="MPI_Irecv" bytes="1280" orank="705" region="0" commid="0" count="311" tid="0" op="" dtype="" >8.1301e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000000" call="MPI_Irecv" bytes="2048" orank="0" region="0" commid="0" count="19" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000002" call="MPI_Irecv" bytes="2048" orank="2" region="0" commid="0" count="11" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000080000000009" call="MPI_Irecv" bytes="2048" orank="9" region="0" commid="0" count="12" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000039" call="MPI_Irecv" bytes="2048" orank="57" region="0" commid="0" count="16" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000041" call="MPI_Irecv" bytes="2048" orank="65" region="0" commid="0" count="3476" tid="0" op="" dtype="" >5.1737e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002C1" call="MPI_Irecv" bytes="2048" orank="705" region="0" commid="0" count="3464" tid="0" op="" dtype="" >5.3477e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000050000000000" call="MPI_Isend" bytes="1280" orank="0" region="0" commid="0" count="201" tid="0" op="" dtype="" >3.7766e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000002" call="MPI_Isend" bytes="1280" orank="2" region="0" commid="0" count="178" tid="0" op="" dtype="" >7.6628e-04 3.0994e-06 8.1062e-06</hent>
<hent key="02400100000000000000050000000009" call="MPI_Isend" bytes="1280" orank="9" region="0" commid="0" count="194" tid="0" op="" dtype="" >3.0518e-04 0.0000e+00 3.0994e-05</hent>
<hent key="02400100000000000000050000000039" call="MPI_Isend" bytes="1280" orank="57" region="0" commid="0" count="246" tid="0" op="" dtype="" >1.2176e-03 9.5367e-07 1.0014e-05</hent>
<hent key="02400100000000000000050000000041" call="MPI_Isend" bytes="1280" orank="65" region="0" commid="0" count="314" tid="0" op="" dtype="" >1.5197e-03 3.8147e-06 6.3896e-05</hent>
<hent key="02400100000000000000280000000009" call="MPI_Isend" bytes="10240" orank="9" region="0" commid="0" count="67" tid="0" op="" dtype="" >3.7432e-05 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000280000000039" call="MPI_Isend" bytes="10240" orank="57" region="0" commid="0" count="86" tid="0" op="" dtype="" >5.4955e-04 5.0068e-06 9.0599e-06</hent>
<hent key="024001000000000000000500000002C1" call="MPI_Isend" bytes="1280" orank="705" region="0" commid="0" count="297" tid="0" op="" dtype="" >1.3313e-03 3.8147e-06 5.9605e-06</hent>
</hash>
<internal rank="1" log_i="1723712895.689771" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="2" mpi_size="768" stamp_init="1723712829.548132" stamp_final="1723712895.689684" username="apac4" allocationname="unknown" flags="0" pid="684270" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61416e+01" utime="4.89642e+01" stime="8.28314e+00" mtime="2.96515e+01" gflop="0.00000e+00" gbyte="3.74966e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.96515e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60094e+01" utime="4.89274e+01" stime="8.27696e+00" mtime="2.96515e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.96515e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4859e+08" > 6.1369e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4958e+08" > 4.6390e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.5709e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5867e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4505e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9073e-06 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5912e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.9169e-04 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0410e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1386e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8116e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="190" >
<hent key="02400100000000000000080000000001" call="MPI_Isend" bytes="2048" orank="1" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.5749e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000003" call="MPI_Isend" bytes="2048" orank="3" region="0" commid="0" count="13" tid="0" op="" dtype="" >5.2452e-05 3.8147e-06 5.0068e-06</hent>
<hent key="0240010000000000000008000000000A" call="MPI_Isend" bytes="2048" orank="10" region="0" commid="0" count="15" tid="0" op="" dtype="" >5.2929e-05 9.5367e-07 1.3113e-05</hent>
<hent key="0240010000000000000008000000003A" call="MPI_Isend" bytes="2048" orank="58" region="0" commid="0" count="10" tid="0" op="" dtype="" >5.4359e-05 5.0068e-06 6.1989e-06</hent>
<hent key="02400100000000000000080000000042" call="MPI_Isend" bytes="2048" orank="66" region="0" commid="0" count="3466" tid="0" op="" dtype="" >1.7038e-02 9.5367e-07 5.4836e-05</hent>
<hent key="024001000000000000000800000002C2" call="MPI_Isend" bytes="2048" orank="706" region="0" commid="0" count="3475" tid="0" op="" dtype="" >1.4683e-02 9.5367e-07 7.3910e-05</hent>
<hent key="038001000000000000000E00000002C2" call="MPI_Irecv" bytes="3584" orank="706" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000E0000000042" call="MPI_Isend" bytes="3584" orank="66" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.3113e-05 5.9605e-06 7.1526e-06</hent>
<hent key="03800100000000000000028000000001" call="MPI_Irecv" bytes="640" orank="1" region="0" commid="0" count="422" tid="0" op="" dtype="" >1.8501e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000003" call="MPI_Irecv" bytes="640" orank="3" region="0" commid="0" count="429" tid="0" op="" dtype="" >3.2949e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000002800000000A" call="MPI_Irecv" bytes="640" orank="10" region="0" commid="0" count="412" tid="0" op="" dtype="" >1.0562e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000003A" call="MPI_Irecv" bytes="640" orank="58" region="0" commid="0" count="404" tid="0" op="" dtype="" >1.8501e-04 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000028000000042" call="MPI_Irecv" bytes="640" orank="66" region="0" commid="0" count="281" tid="0" op="" dtype="" >1.5521e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000280000002C2" call="MPI_Irecv" bytes="640" orank="706" region="0" commid="0" count="275" tid="0" op="" dtype="" >8.9884e-05 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000028000000001" call="MPI_Isend" bytes="640" orank="1" region="0" commid="0" count="421" tid="0" op="" dtype="" >5.6505e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000028000000003" call="MPI_Isend" bytes="640" orank="3" region="0" commid="0" count="428" tid="0" op="" dtype="" >1.3924e-03 1.9073e-06 1.0014e-05</hent>
<hent key="0240010000000000000002800000000A" call="MPI_Isend" bytes="640" orank="10" region="0" commid="0" count="407" tid="0" op="" dtype="" >5.3453e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0240010000000000000002800000003A" call="MPI_Isend" bytes="640" orank="58" region="0" commid="0" count="420" tid="0" op="" dtype="" >1.9701e-03 3.8147e-06 2.0981e-05</hent>
<hent key="02400100000000000000028000000042" call="MPI_Isend" bytes="640" orank="66" region="0" commid="0" count="273" tid="0" op="" dtype="" >1.4436e-03 2.8610e-06 1.7881e-05</hent>
<hent key="024001000000000000000280000002C2" call="MPI_Isend" bytes="640" orank="706" region="0" commid="0" count="270" tid="0" op="" dtype="" >1.2720e-03 2.8610e-06 1.5020e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-05 3.0994e-05 3.0994e-05</hent>
<hent key="03800100000000000000014000000001" call="MPI_Irecv" bytes="320" orank="1" region="0" commid="0" count="338" tid="0" op="" dtype="" >1.2112e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000003" call="MPI_Irecv" bytes="320" orank="3" region="0" commid="0" count="382" tid="0" op="" dtype="" >2.6274e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000001400000000A" call="MPI_Irecv" bytes="320" orank="10" region="0" commid="0" count="357" tid="0" op="" dtype="" >1.0228e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000003A" call="MPI_Irecv" bytes="320" orank="58" region="0" commid="0" count="398" tid="0" op="" dtype="" >1.4520e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000042" call="MPI_Irecv" bytes="320" orank="66" region="0" commid="0" count="198" tid="0" op="" dtype="" >8.8692e-05 0.0000e+00 6.9141e-06</hent>
<hent key="038001000000000000000140000002C2" call="MPI_Irecv" bytes="320" orank="706" region="0" commid="0" count="166" tid="0" op="" dtype="" >5.2214e-05 0.0000e+00 3.0994e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 9.5367e-07 2.1458e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.5709e+00 0.0000e+00 1.5463e-01</hent>
<hent key="03800100000000000000400000000042" call="MPI_Irecv" bytes="16384" orank="66" region="0" commid="0" count="12699" tid="0" op="" dtype="" >6.9263e-03 0.0000e+00 3.6001e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6478e-02 9.5367e-07 1.6464e-02</hent>
<hent key="038001000000000000004000000002C2" call="MPI_Irecv" bytes="16384" orank="706" region="0" commid="0" count="12699" tid="0" op="" dtype="" >4.4968e-03 0.0000e+00 4.1008e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="02400100000000000000014000000001" call="MPI_Isend" bytes="320" orank="1" region="0" commid="0" count="369" tid="0" op="" dtype="" >3.8171e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000014000000003" call="MPI_Isend" bytes="320" orank="3" region="0" commid="0" count="364" tid="0" op="" dtype="" >1.0824e-03 1.9073e-06 1.7881e-05</hent>
<hent key="0240010000000000000001400000000A" call="MPI_Isend" bytes="320" orank="10" region="0" commid="0" count="358" tid="0" op="" dtype="" >3.8409e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0240010000000000000001400000003A" call="MPI_Isend" bytes="320" orank="58" region="0" commid="0" count="375" tid="0" op="" dtype="" >1.6615e-03 2.8610e-06 1.1206e-05</hent>
<hent key="02400100000000000000014000000042" call="MPI_Isend" bytes="320" orank="66" region="0" commid="0" count="166" tid="0" op="" dtype="" >8.5735e-04 2.8610e-06 1.4067e-05</hent>
<hent key="03800100000000000000200000000001" call="MPI_Irecv" bytes="8192" orank="1" region="0" commid="0" count="3615" tid="0" op="" dtype="" >1.4229e-03 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000200000000003" call="MPI_Irecv" bytes="8192" orank="3" region="0" commid="0" count="3956" tid="0" op="" dtype="" >8.9192e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0380010000000000000020000000000A" call="MPI_Irecv" bytes="8192" orank="10" region="0" commid="0" count="12699" tid="0" op="" dtype="" >9.6810e-03 0.0000e+00 3.0994e-05</hent>
<hent key="0380010000000000000020000000003A" call="MPI_Irecv" bytes="8192" orank="58" region="0" commid="0" count="12644" tid="0" op="" dtype="" >2.7125e-03 0.0000e+00 2.3842e-05</hent>
<hent key="024001000000000000000140000002C2" call="MPI_Isend" bytes="320" orank="706" region="0" commid="0" count="185" tid="0" op="" dtype="" >8.5878e-04 2.8610e-06 1.4067e-05</hent>
<hent key="03800100000000000000000000000001" call="MPI_Irecv" bytes="0" orank="1" region="0" commid="0" count="264" tid="0" op="" dtype="" >8.1301e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000003" call="MPI_Irecv" bytes="0" orank="3" region="0" commid="0" count="260" tid="0" op="" dtype="" >1.1730e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000000000000000A" call="MPI_Irecv" bytes="0" orank="10" region="0" commid="0" count="260" tid="0" op="" dtype="" >9.2268e-05 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000000000000003A" call="MPI_Irecv" bytes="0" orank="58" region="0" commid="0" count="269" tid="0" op="" dtype="" >1.0085e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000000000000042" call="MPI_Irecv" bytes="0" orank="66" region="0" commid="0" count="143" tid="0" op="" dtype="" >5.8651e-05 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000400000000042" call="MPI_Isend" bytes="16384" orank="66" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.4883e-01 3.8147e-06 1.5807e-04</hent>
<hent key="038001000000000000000000000002C2" call="MPI_Irecv" bytes="0" orank="706" region="0" commid="0" count="149" tid="0" op="" dtype="" >4.4584e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002C2" call="MPI_Isend" bytes="16384" orank="706" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.3016e-01 2.8610e-06 1.1611e-04</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.0174e-04 0.0000e+00 2.6488e-04</hent>
<hent key="02400100000000000000200000000001" call="MPI_Isend" bytes="8192" orank="1" region="0" commid="0" count="2473" tid="0" op="" dtype="" >3.4494e-03 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000200000000003" call="MPI_Isend" bytes="8192" orank="3" region="0" commid="0" count="4145" tid="0" op="" dtype="" >7.6652e-03 0.0000e+00 1.5974e-05</hent>
<hent key="0240010000000000000020000000000A" call="MPI_Isend" bytes="8192" orank="10" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.2698e-02 0.0000e+00 4.8876e-05</hent>
<hent key="0240010000000000000020000000003A" call="MPI_Isend" bytes="8192" orank="58" region="0" commid="0" count="12642" tid="0" op="" dtype="" >1.0434e-01 3.8147e-06 7.8201e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.9087e-05 2.9087e-05 2.9087e-05</hent>
<hent key="02400100000000000000000000000001" call="MPI_Isend" bytes="0" orank="1" region="0" commid="0" count="248" tid="0" op="" dtype="" >1.3924e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000003" call="MPI_Isend" bytes="0" orank="3" region="0" commid="0" count="262" tid="0" op="" dtype="" >5.3740e-04 9.5367e-07 3.0994e-05</hent>
<hent key="0240010000000000000000000000000A" call="MPI_Isend" bytes="0" orank="10" region="0" commid="0" count="252" tid="0" op="" dtype="" >1.8048e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000000000000003A" call="MPI_Isend" bytes="0" orank="58" region="0" commid="0" count="252" tid="0" op="" dtype="" >9.8515e-04 1.9073e-06 1.1206e-05</hent>
<hent key="02400100000000000000000000000042" call="MPI_Isend" bytes="0" orank="66" region="0" commid="0" count="156" tid="0" op="" dtype="" >6.7949e-04 9.5367e-07 1.7166e-05</hent>
<hent key="024001000000000000000000000002C2" call="MPI_Isend" bytes="0" orank="706" region="0" commid="0" count="148" tid="0" op="" dtype="" >5.9104e-04 2.1458e-06 1.1921e-05</hent>
<hent key="03800100000000000000060000000001" call="MPI_Irecv" bytes="1536" orank="1" region="0" commid="0" count="105" tid="0" op="" dtype="" >2.8610e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000003" call="MPI_Irecv" bytes="1536" orank="3" region="0" commid="0" count="101" tid="0" op="" dtype="" >1.0109e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000006000000000A" call="MPI_Irecv" bytes="1536" orank="10" region="0" commid="0" count="130" tid="0" op="" dtype="" >4.0293e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000003A" call="MPI_Irecv" bytes="1536" orank="58" region="0" commid="0" count="75" tid="0" op="" dtype="" >4.6730e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000060000000042" call="MPI_Irecv" bytes="1536" orank="66" region="0" commid="0" count="211" tid="0" op="" dtype="" >1.2779e-04 0.0000e+00 7.1526e-06</hent>
<hent key="038001000000000000000600000002C2" call="MPI_Irecv" bytes="1536" orank="706" region="0" commid="0" count="212" tid="0" op="" dtype="" >7.7724e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000001" call="MPI_Isend" bytes="1536" orank="1" region="0" commid="0" count="90" tid="0" op="" dtype="" >1.7834e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000060000000003" call="MPI_Isend" bytes="1536" orank="3" region="0" commid="0" count="87" tid="0" op="" dtype="" >3.4094e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000006000000000A" call="MPI_Isend" bytes="1536" orank="10" region="0" commid="0" count="99" tid="0" op="" dtype="" >1.9407e-04 9.5367e-07 1.0967e-05</hent>
<hent key="0240010000000000000006000000003A" call="MPI_Isend" bytes="1536" orank="58" region="0" commid="0" count="71" tid="0" op="" dtype="" >3.8481e-04 3.8147e-06 1.5974e-05</hent>
<hent key="02400100000000000000060000000042" call="MPI_Isend" bytes="1536" orank="66" region="0" commid="0" count="213" tid="0" op="" dtype="" >1.2264e-03 3.8147e-06 1.5974e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C2" call="MPI_Isend" bytes="1536" orank="706" region="0" commid="0" count="225" tid="0" op="" dtype="" >1.1868e-03 3.8147e-06 1.7881e-05</hent>
<hent key="038001000000000000000C0000000001" call="MPI_Irecv" bytes="3072" orank="1" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000C0000000042" call="MPI_Irecv" bytes="3072" orank="66" region="0" commid="0" count="7" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C2" call="MPI_Irecv" bytes="3072" orank="706" region="0" commid="0" count="7" tid="0" op="" dtype="" >8.8215e-06 0.0000e+00 5.9605e-06</hent>
<hent key="024001000000000000000C000000003A" call="MPI_Isend" bytes="3072" orank="58" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="024001000000000000000C0000000042" call="MPI_Isend" bytes="3072" orank="66" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.1206e-05 5.0068e-06 6.1989e-06</hent>
<hent key="024001000000000000000C00000002C2" call="MPI_Isend" bytes="3072" orank="706" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.6941e-05 5.0068e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.6982e-05 5.6982e-05 5.6982e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.3542e-04 4.3154e-05 4.9114e-05</hent>
<hent key="03800100000000000000038000000001" call="MPI_Irecv" bytes="896" orank="1" region="0" commid="0" count="2772" tid="0" op="" dtype="" >3.6407e-04 0.0000e+00 2.8610e-06</hent>
<hent key="03800100000000000000038000000003" call="MPI_Irecv" bytes="896" orank="3" region="0" commid="0" count="2641" tid="0" op="" dtype="" >7.6318e-04 0.0000e+00 2.4080e-05</hent>
<hent key="0380010000000000000003800000000A" call="MPI_Irecv" bytes="896" orank="10" region="0" commid="0" count="300" tid="0" op="" dtype="" >9.6560e-05 0.0000e+00 9.0599e-06</hent>
<hent key="0380010000000000000003800000003A" call="MPI_Irecv" bytes="896" orank="58" region="0" commid="0" count="313" tid="0" op="" dtype="" >1.3137e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000038000000042" call="MPI_Irecv" bytes="896" orank="66" region="0" commid="0" count="318" tid="0" op="" dtype="" >1.8430e-04 0.0000e+00 1.0967e-05</hent>
<hent key="038001000000000000000380000002C2" call="MPI_Irecv" bytes="896" orank="706" region="0" commid="0" count="337" tid="0" op="" dtype="" >1.4472e-04 0.0000e+00 7.1526e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.4067e-05 1.4067e-05 1.4067e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="02400100000000000000038000000001" call="MPI_Isend" bytes="896" orank="1" region="0" commid="0" count="3072" tid="0" op="" dtype="" >2.2044e-03 0.0000e+00 2.1935e-05</hent>
<hent key="02400100000000000000038000000003" call="MPI_Isend" bytes="896" orank="3" region="0" commid="0" count="2588" tid="0" op="" dtype="" >2.8338e-03 0.0000e+00 1.5974e-05</hent>
<hent key="0240010000000000000003800000000A" call="MPI_Isend" bytes="896" orank="10" region="0" commid="0" count="321" tid="0" op="" dtype="" >5.0497e-04 9.5367e-07 7.1526e-06</hent>
<hent key="0240010000000000000003800000003A" call="MPI_Isend" bytes="896" orank="58" region="0" commid="0" count="317" tid="0" op="" dtype="" >1.5168e-03 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000038000000042" call="MPI_Isend" bytes="896" orank="66" region="0" commid="0" count="336" tid="0" op="" dtype="" >1.8981e-03 2.8610e-06 7.3195e-05</hent>
<hent key="024001000000000000000380000002C2" call="MPI_Isend" bytes="896" orank="706" region="0" commid="0" count="304" tid="0" op="" dtype="" >1.4114e-03 2.8610e-06 1.5974e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.4709e+00 3.6955e-05 2.0314e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.1891e-03 3.1891e-03 3.1891e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0410e-02 1.0410e-02 1.0410e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.4892e-02 2.4892e-02 2.4892e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.9765e-01 2.1560e-03 1.7904e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.6907e-04 3.6907e-04 3.6907e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 9.5367e-07 2.1458e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5912e+00 4.5991e-04 2.5139e-01</hent>
<hent key="03800100000000000000040000000001" call="MPI_Irecv" bytes="1024" orank="1" region="0" commid="0" count="954" tid="0" op="" dtype="" >1.0705e-04 0.0000e+00 1.4067e-05</hent>
<hent key="03800100000000000000040000000003" call="MPI_Irecv" bytes="1024" orank="3" region="0" commid="0" count="1036" tid="0" op="" dtype="" >2.3007e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000004000000000A" call="MPI_Irecv" bytes="1024" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0190e-03 0.0000e+00 1.7166e-05</hent>
<hent key="0380010000000000000004000000003A" call="MPI_Irecv" bytes="1024" orank="58" region="0" commid="0" count="3384" tid="0" op="" dtype="" >4.0007e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.4505e-03 1.1921e-06 1.4131e-03</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.3140e-05 3.3140e-05 3.3140e-05</hent>
<hent key="03800100000000000000070000000001" call="MPI_Irecv" bytes="1792" orank="1" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.0266e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000003" call="MPI_Irecv" bytes="1792" orank="3" region="0" commid="0" count="44" tid="0" op="" dtype="" >5.1498e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000007000000000A" call="MPI_Irecv" bytes="1792" orank="10" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.4544e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000003A" call="MPI_Irecv" bytes="1792" orank="58" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.6689e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000042" call="MPI_Irecv" bytes="1792" orank="66" region="0" commid="0" count="131" tid="0" op="" dtype="" >8.2970e-05 0.0000e+00 9.0599e-06</hent>
<hent key="038001000000000000000700000002C2" call="MPI_Irecv" bytes="1792" orank="706" region="0" commid="0" count="124" tid="0" op="" dtype="" >4.1246e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >5.1975e-05 0.0000e+00 2.5988e-05</hent>
<hent key="02400100000000000000040000000001" call="MPI_Isend" bytes="1024" orank="1" region="0" commid="0" count="668" tid="0" op="" dtype="" >4.1676e-04 0.0000e+00 2.0027e-05</hent>
<hent key="02400100000000000000040000000003" call="MPI_Isend" bytes="1024" orank="3" region="0" commid="0" count="1122" tid="0" op="" dtype="" >8.8167e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0240010000000000000004000000000A" call="MPI_Isend" bytes="1024" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.0540e-03 0.0000e+00 2.0027e-05</hent>
<hent key="0240010000000000000004000000003A" call="MPI_Isend" bytes="1024" orank="58" region="0" commid="0" count="3376" tid="0" op="" dtype="" >1.0998e-02 9.5367e-07 1.2779e-04</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5696e+00 0.0000e+00 2.2713e+00</hent>
<hent key="038001000000000000000A0000000001" call="MPI_Irecv" bytes="2560" orank="1" region="0" commid="0" count="6" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000003" call="MPI_Irecv" bytes="2560" orank="3" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000000A000000000A" call="MPI_Irecv" bytes="2560" orank="10" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000003A" call="MPI_Irecv" bytes="2560" orank="58" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000042" call="MPI_Irecv" bytes="2560" orank="66" region="0" commid="0" count="52" tid="0" op="" dtype="" >2.9564e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002C2" call="MPI_Irecv" bytes="2560" orank="706" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.2875e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000001" call="MPI_Isend" bytes="1792" orank="1" region="0" commid="0" count="43" tid="0" op="" dtype="" >8.9407e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000003" call="MPI_Isend" bytes="1792" orank="3" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.6856e-04 3.0994e-06 5.0068e-06</hent>
<hent key="0240010000000000000007000000000A" call="MPI_Isend" bytes="1792" orank="10" region="0" commid="0" count="37" tid="0" op="" dtype="" >9.5844e-05 9.5367e-07 1.4067e-05</hent>
<hent key="0240010000000000000007000000003A" call="MPI_Isend" bytes="1792" orank="58" region="0" commid="0" count="30" tid="0" op="" dtype="" >1.6141e-04 5.0068e-06 9.0599e-06</hent>
<hent key="02400100000000000000070000000042" call="MPI_Isend" bytes="1792" orank="66" region="0" commid="0" count="137" tid="0" op="" dtype="" >8.1539e-04 2.8610e-06 1.2875e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1386e-01 1.1386e-01 1.1386e-01</hent>
<hent key="024001000000000000000700000002C2" call="MPI_Isend" bytes="1792" orank="706" region="0" commid="0" count="155" tid="0" op="" dtype="" >7.9060e-04 3.8147e-06 1.6212e-05</hent>
<hent key="024001000000000000000A0000000001" call="MPI_Isend" bytes="2560" orank="1" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.6212e-05 9.5367e-07 3.0994e-06</hent>
<hent key="024001000000000000000A0000000003" call="MPI_Isend" bytes="2560" orank="3" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.1921e-05 3.8147e-06 4.0531e-06</hent>
<hent key="024001000000000000000A000000000A" call="MPI_Isend" bytes="2560" orank="10" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.6928e-05 1.9073e-06 1.2875e-05</hent>
<hent key="024001000000000000000A000000003A" call="MPI_Isend" bytes="2560" orank="58" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.5974e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000042" call="MPI_Isend" bytes="2560" orank="66" region="0" commid="0" count="47" tid="0" op="" dtype="" >2.9159e-04 3.8147e-06 1.2159e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >8.3923e-05 1.5974e-05 3.5048e-05</hent>
<hent key="024001000000000000000A00000002C2" call="MPI_Isend" bytes="2560" orank="706" region="0" commid="0" count="35" tid="0" op="" dtype="" >1.9169e-04 3.8147e-06 1.0967e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >2.2030e-04 8.5115e-05 1.3518e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >6.9141e-06 2.8610e-06 4.0531e-06</hent>
<hent key="03800100000000000000000400000001" call="MPI_Irecv" bytes="4" orank="1" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0879e-03 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000000400000003" call="MPI_Irecv" bytes="4" orank="3" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.7139e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0380010000000000000000040000000A" call="MPI_Irecv" bytes="4" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.3447e-04 0.0000e+00 1.4067e-05</hent>
<hent key="0380010000000000000000040000003A" call="MPI_Irecv" bytes="4" orank="58" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.5970e-04 0.0000e+00 1.6212e-05</hent>
<hent key="03800100000000000000000400000042" call="MPI_Irecv" bytes="4" orank="66" region="0" commid="0" count="3398" tid="0" op="" dtype="" >9.3603e-04 0.0000e+00 5.3167e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.8600e-04 5.9843e-05 1.2803e-04</hent>
<hent key="038001000000000000001C0000000001" call="MPI_Irecv" bytes="7168" orank="1" region="0" commid="0" count="9084" tid="0" op="" dtype="" >3.7127e-03 0.0000e+00 1.0967e-05</hent>
<hent key="038001000000000000001C0000000003" call="MPI_Irecv" bytes="7168" orank="3" region="0" commid="0" count="8743" tid="0" op="" dtype="" >2.0897e-03 0.0000e+00 1.4067e-05</hent>
<hent key="038001000000000000000004000002C2" call="MPI_Irecv" bytes="4" orank="706" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0660e-03 0.0000e+00 2.5034e-05</hent>
<hent key="02400100000000000000000400000001" call="MPI_Isend" bytes="4" orank="1" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3559e-03 0.0000e+00 4.2200e-05</hent>
<hent key="02400100000000000000000400000003" call="MPI_Isend" bytes="4" orank="3" region="0" commid="0" count="3398" tid="0" op="" dtype="" >3.2310e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000000040000000A" call="MPI_Isend" bytes="4" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6007e-03 0.0000e+00 2.2173e-05</hent>
<hent key="0240010000000000000000040000003A" call="MPI_Isend" bytes="4" orank="58" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.2576e-02 3.8147e-06 1.3995e-04</hent>
<hent key="02400100000000000000000400000042" call="MPI_Isend" bytes="4" orank="66" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.2889e-02 3.8147e-06 7.8201e-05</hent>
<hent key="024001000000000000001C0000000001" call="MPI_Isend" bytes="7168" orank="1" region="0" commid="0" count="10226" tid="0" op="" dtype="" >2.2187e-02 9.5367e-07 2.6941e-05</hent>
<hent key="024001000000000000001C0000000003" call="MPI_Isend" bytes="7168" orank="3" region="0" commid="0" count="8554" tid="0" op="" dtype="" >2.5568e-02 9.5367e-07 2.6941e-05</hent>
<hent key="024001000000000000000004000002C2" call="MPI_Isend" bytes="4" orank="706" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.2612e-02 3.8147e-06 6.1035e-05</hent>
<hent key="03800100000000000000050000000001" call="MPI_Irecv" bytes="1280" orank="1" region="0" commid="0" count="178" tid="0" op="" dtype="" >7.4625e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000003" call="MPI_Irecv" bytes="1280" orank="3" region="0" commid="0" count="187" tid="0" op="" dtype="" >1.6832e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000005000000000A" call="MPI_Irecv" bytes="1280" orank="10" region="0" commid="0" count="178" tid="0" op="" dtype="" >4.7684e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000003A" call="MPI_Irecv" bytes="1280" orank="58" region="0" commid="0" count="198" tid="0" op="" dtype="" >1.0800e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000050000000042" call="MPI_Irecv" bytes="1280" orank="66" region="0" commid="0" count="278" tid="0" op="" dtype="" >1.7905e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0419e+01 6.9141e-06 1.5588e-01</hent>
<hent key="0380010000000000000028000000003A" call="MPI_Irecv" bytes="10240" orank="58" region="0" commid="0" count="55" tid="0" op="" dtype="" >1.3828e-05 0.0000e+00 8.8215e-06</hent>
<hent key="038001000000000000000500000002C2" call="MPI_Irecv" bytes="1280" orank="706" region="0" commid="0" count="319" tid="0" op="" dtype="" >1.2326e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000080000000001" call="MPI_Irecv" bytes="2048" orank="1" region="0" commid="0" count="14" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000003" call="MPI_Irecv" bytes="2048" orank="3" region="0" commid="0" count="14" tid="0" op="" dtype="" >1.6689e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000008000000000A" call="MPI_Irecv" bytes="2048" orank="10" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000003A" call="MPI_Irecv" bytes="2048" orank="58" region="0" commid="0" count="9" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000042" call="MPI_Irecv" bytes="2048" orank="66" region="0" commid="0" count="3478" tid="0" op="" dtype="" >9.5582e-04 0.0000e+00 4.1962e-05</hent>
<hent key="038001000000000000000800000002C2" call="MPI_Irecv" bytes="2048" orank="706" region="0" commid="0" count="3467" tid="0" op="" dtype="" >1.0297e-03 0.0000e+00 3.8862e-05</hent>
<hent key="02400100000000000000050000000001" call="MPI_Isend" bytes="1280" orank="1" region="0" commid="0" count="168" tid="0" op="" dtype="" >3.0470e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000003" call="MPI_Isend" bytes="1280" orank="3" region="0" commid="0" count="189" tid="0" op="" dtype="" >7.1478e-04 2.8610e-06 9.0599e-06</hent>
<hent key="0240010000000000000005000000000A" call="MPI_Isend" bytes="1280" orank="10" region="0" commid="0" count="207" tid="0" op="" dtype="" >3.5954e-04 9.5367e-07 8.8215e-06</hent>
<hent key="0240010000000000000005000000003A" call="MPI_Isend" bytes="1280" orank="58" region="0" commid="0" count="242" tid="0" op="" dtype="" >1.1978e-03 9.5367e-07 2.0027e-05</hent>
<hent key="02400100000000000000050000000042" call="MPI_Isend" bytes="1280" orank="66" region="0" commid="0" count="299" tid="0" op="" dtype="" >1.7040e-03 3.8147e-06 1.5974e-05</hent>
<hent key="0240010000000000000028000000003A" call="MPI_Isend" bytes="10240" orank="58" region="0" commid="0" count="57" tid="0" op="" dtype="" >4.1699e-04 4.0531e-06 1.8835e-05</hent>
<hent key="024001000000000000000500000002C2" call="MPI_Isend" bytes="1280" orank="706" region="0" commid="0" count="295" tid="0" op="" dtype="" >1.4424e-03 2.8610e-06 1.4067e-05</hent>
</hash>
<internal rank="2" log_i="1723712895.689684" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="3" mpi_size="768" stamp_init="1723712829.548174" stamp_final="1723712895.684171" username="apac4" allocationname="unknown" flags="0" pid="684271" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61360e+01" utime="5.10561e+01" stime="7.38080e+00" mtime="3.03709e+01" gflop="0.00000e+00" gbyte="3.75904e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.03709e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c714e014f214be55f214ed147a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60022e+01" utime="5.10202e+01" stime="7.37372e+00" mtime="3.03709e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.03709e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4936e+08" > 4.5998e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4967e+08" > 3.0118e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8002e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5506e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.3699e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5911e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1356e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0434e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1363e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8814e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="192" >
<hent key="02400100000000000000080000000002" call="MPI_Isend" bytes="2048" orank="2" region="0" commid="0" count="14" tid="0" op="" dtype="" >3.4809e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000004" call="MPI_Isend" bytes="2048" orank="4" region="0" commid="0" count="5" tid="0" op="" dtype="" >3.5048e-05 3.0994e-06 2.0981e-05</hent>
<hent key="0240010000000000000008000000000B" call="MPI_Isend" bytes="2048" orank="11" region="0" commid="0" count="17" tid="0" op="" dtype="" >4.1008e-05 9.5367e-07 1.0967e-05</hent>
<hent key="0240010000000000000008000000003B" call="MPI_Isend" bytes="2048" orank="59" region="0" commid="0" count="16" tid="0" op="" dtype="" >9.1791e-05 5.0068e-06 6.9141e-06</hent>
<hent key="02400100000000000000080000000043" call="MPI_Isend" bytes="2048" orank="67" region="0" commid="0" count="3465" tid="0" op="" dtype="" >1.1424e-02 9.5367e-07 1.9073e-05</hent>
<hent key="024001000000000000000800000002C3" call="MPI_Isend" bytes="2048" orank="707" region="0" commid="0" count="3466" tid="0" op="" dtype="" >1.0982e-02 9.5367e-07 1.0014e-05</hent>
<hent key="024001000000000000000E0000000043" call="MPI_Isend" bytes="3584" orank="67" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000002" call="MPI_Irecv" bytes="640" orank="2" region="0" commid="0" count="428" tid="0" op="" dtype="" >1.4496e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000028000000004" call="MPI_Irecv" bytes="640" orank="4" region="0" commid="0" count="391" tid="0" op="" dtype="" >6.2704e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000B" call="MPI_Irecv" bytes="640" orank="11" region="0" commid="0" count="402" tid="0" op="" dtype="" >9.9182e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000003B" call="MPI_Irecv" bytes="640" orank="59" region="0" commid="0" count="393" tid="0" op="" dtype="" >1.8167e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000028000000043" call="MPI_Irecv" bytes="640" orank="67" region="0" commid="0" count="292" tid="0" op="" dtype="" >7.4625e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002C3" call="MPI_Irecv" bytes="640" orank="707" region="0" commid="0" count="251" tid="0" op="" dtype="" >7.8440e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000002" call="MPI_Isend" bytes="640" orank="2" region="0" commid="0" count="429" tid="0" op="" dtype="" >6.4421e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000028000000004" call="MPI_Isend" bytes="640" orank="4" region="0" commid="0" count="415" tid="0" op="" dtype="" >1.1396e-03 1.9073e-06 6.9141e-06</hent>
<hent key="0240010000000000000002800000000B" call="MPI_Isend" bytes="640" orank="11" region="0" commid="0" count="396" tid="0" op="" dtype="" >5.0783e-04 9.5367e-07 6.9141e-06</hent>
<hent key="0240010000000000000002800000003B" call="MPI_Isend" bytes="640" orank="59" region="0" commid="0" count="415" tid="0" op="" dtype="" >1.9784e-03 3.8147e-06 1.0967e-05</hent>
<hent key="02400100000000000000028000000043" call="MPI_Isend" bytes="640" orank="67" region="0" commid="0" count="277" tid="0" op="" dtype="" >1.2202e-03 2.8610e-06 5.9605e-06</hent>
<hent key="024001000000000000000280000002C3" call="MPI_Isend" bytes="640" orank="707" region="0" commid="0" count="292" tid="0" op="" dtype="" >1.2279e-03 2.8610e-06 1.0014e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-05 3.0994e-05 3.0994e-05</hent>
<hent key="03800100000000000000014000000002" call="MPI_Irecv" bytes="320" orank="2" region="0" commid="0" count="364" tid="0" op="" dtype="" >1.3351e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000004" call="MPI_Irecv" bytes="320" orank="4" region="0" commid="0" count="388" tid="0" op="" dtype="" >7.1526e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000B" call="MPI_Irecv" bytes="320" orank="11" region="0" commid="0" count="363" tid="0" op="" dtype="" >6.9141e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000003B" call="MPI_Irecv" bytes="320" orank="59" region="0" commid="0" count="369" tid="0" op="" dtype="" >1.6928e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000043" call="MPI_Irecv" bytes="320" orank="67" region="0" commid="0" count="178" tid="0" op="" dtype="" >3.4571e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002C3" call="MPI_Irecv" bytes="320" orank="707" region="0" commid="0" count="198" tid="0" op="" dtype="" >4.6730e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.8002e+00 0.0000e+00 1.5448e-01</hent>
<hent key="03800100000000000000400000000043" call="MPI_Irecv" bytes="16384" orank="67" region="0" commid="0" count="12699" tid="0" op="" dtype="" >3.1018e-03 0.0000e+00 6.9141e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6444e-02 9.5367e-07 1.6427e-02</hent>
<hent key="038001000000000000004000000002C3" call="MPI_Irecv" bytes="16384" orank="707" region="0" commid="0" count="12678" tid="0" op="" dtype="" >2.0471e-03 0.0000e+00 3.0994e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >5.4836e-05 0.0000e+00 5.3883e-05</hent>
<hent key="02400100000000000000014000000002" call="MPI_Isend" bytes="320" orank="2" region="0" commid="0" count="382" tid="0" op="" dtype="" >4.5538e-04 0.0000e+00 2.8610e-06</hent>
<hent key="02400100000000000000014000000004" call="MPI_Isend" bytes="320" orank="4" region="0" commid="0" count="361" tid="0" op="" dtype="" >8.9121e-04 9.5367e-07 1.1206e-05</hent>
<hent key="0240010000000000000001400000000B" call="MPI_Isend" bytes="320" orank="11" region="0" commid="0" count="348" tid="0" op="" dtype="" >3.7313e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000001400000003B" call="MPI_Isend" bytes="320" orank="59" region="0" commid="0" count="369" tid="0" op="" dtype="" >1.6899e-03 3.8147e-06 1.0967e-05</hent>
<hent key="02400100000000000000014000000043" call="MPI_Isend" bytes="320" orank="67" region="0" commid="0" count="166" tid="0" op="" dtype="" >7.1502e-04 2.8610e-06 1.0014e-05</hent>
<hent key="03800100000000000000200000000002" call="MPI_Irecv" bytes="8192" orank="2" region="0" commid="0" count="4145" tid="0" op="" dtype="" >1.2937e-03 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000200000000004" call="MPI_Irecv" bytes="8192" orank="4" region="0" commid="0" count="3669" tid="0" op="" dtype="" >5.2810e-04 0.0000e+00 5.1975e-05</hent>
<hent key="0380010000000000000020000000000B" call="MPI_Irecv" bytes="8192" orank="11" region="0" commid="0" count="12699" tid="0" op="" dtype="" >8.0211e-03 0.0000e+00 2.6941e-05</hent>
<hent key="0380010000000000000020000000003B" call="MPI_Irecv" bytes="8192" orank="59" region="0" commid="0" count="12676" tid="0" op="" dtype="" >1.2841e-03 0.0000e+00 4.0531e-06</hent>
<hent key="024001000000000000000140000002C3" call="MPI_Isend" bytes="320" orank="707" region="0" commid="0" count="178" tid="0" op="" dtype="" >7.3171e-04 2.8610e-06 5.0068e-06</hent>
<hent key="03800100000000000000000000000002" call="MPI_Irecv" bytes="0" orank="2" region="0" commid="0" count="262" tid="0" op="" dtype="" >8.0824e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000004" call="MPI_Irecv" bytes="0" orank="4" region="0" commid="0" count="276" tid="0" op="" dtype="" >6.4850e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000B" call="MPI_Irecv" bytes="0" orank="11" region="0" commid="0" count="269" tid="0" op="" dtype="" >6.5327e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000003B" call="MPI_Irecv" bytes="0" orank="59" region="0" commid="0" count="263" tid="0" op="" dtype="" >7.1287e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000043" call="MPI_Irecv" bytes="0" orank="67" region="0" commid="0" count="135" tid="0" op="" dtype="" >2.1935e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000043" call="MPI_Isend" bytes="16384" orank="67" region="0" commid="0" count="12693" tid="0" op="" dtype="" >9.0037e-02 3.8147e-06 7.6056e-05</hent>
<hent key="038001000000000000000000000002C3" call="MPI_Irecv" bytes="0" orank="707" region="0" commid="0" count="155" tid="0" op="" dtype="" >4.1723e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002C3" call="MPI_Isend" bytes="16384" orank="707" region="0" commid="0" count="12692" tid="0" op="" dtype="" >8.7613e-02 3.8147e-06 2.3127e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.4656e-04 0.0000e+00 2.7680e-04</hent>
<hent key="02400100000000000000200000000002" call="MPI_Isend" bytes="8192" orank="2" region="0" commid="0" count="3956" tid="0" op="" dtype="" >5.1169e-03 0.0000e+00 4.6015e-05</hent>
<hent key="02400100000000000000200000000004" call="MPI_Isend" bytes="8192" orank="4" region="0" commid="0" count="3535" tid="0" op="" dtype="" >6.8901e-03 0.0000e+00 7.4863e-05</hent>
<hent key="0240010000000000000020000000000B" call="MPI_Isend" bytes="8192" orank="11" region="0" commid="0" count="12683" tid="0" op="" dtype="" >8.1313e-03 0.0000e+00 8.2970e-05</hent>
<hent key="0240010000000000000020000000003B" call="MPI_Isend" bytes="8192" orank="59" region="0" commid="0" count="12691" tid="0" op="" dtype="" >8.0881e-02 3.8147e-06 3.7909e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.0081e-05 6.0081e-05 6.0081e-05</hent>
<hent key="02400100000000000000000000000002" call="MPI_Isend" bytes="0" orank="2" region="0" commid="0" count="260" tid="0" op="" dtype="" >1.8907e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000000000004" call="MPI_Isend" bytes="0" orank="4" region="0" commid="0" count="291" tid="0" op="" dtype="" >5.9128e-04 9.5367e-07 8.6069e-05</hent>
<hent key="0240010000000000000000000000000B" call="MPI_Isend" bytes="0" orank="11" region="0" commid="0" count="265" tid="0" op="" dtype="" >2.2531e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000000000000003B" call="MPI_Isend" bytes="0" orank="59" region="0" commid="0" count="256" tid="0" op="" dtype="" >1.0262e-03 1.9073e-06 9.0599e-06</hent>
<hent key="02400100000000000000000000000043" call="MPI_Isend" bytes="0" orank="67" region="0" commid="0" count="147" tid="0" op="" dtype="" >5.5194e-04 9.5367e-07 1.2875e-05</hent>
<hent key="024001000000000000000000000002C3" call="MPI_Isend" bytes="0" orank="707" region="0" commid="0" count="139" tid="0" op="" dtype="" >5.4049e-04 1.9073e-06 9.0599e-06</hent>
<hent key="03800100000000000000060000000002" call="MPI_Irecv" bytes="1536" orank="2" region="0" commid="0" count="87" tid="0" op="" dtype="" >2.5749e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000060000000004" call="MPI_Irecv" bytes="1536" orank="4" region="0" commid="0" count="80" tid="0" op="" dtype="" >2.0742e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000006000000000B" call="MPI_Irecv" bytes="1536" orank="11" region="0" commid="0" count="90" tid="0" op="" dtype="" >1.9312e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000003B" call="MPI_Irecv" bytes="1536" orank="59" region="0" commid="0" count="94" tid="0" op="" dtype="" >5.8889e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000043" call="MPI_Irecv" bytes="1536" orank="67" region="0" commid="0" count="210" tid="0" op="" dtype="" >6.0797e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002C3" call="MPI_Irecv" bytes="1536" orank="707" region="0" commid="0" count="231" tid="0" op="" dtype="" >8.8453e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000002" call="MPI_Isend" bytes="1536" orank="2" region="0" commid="0" count="101" tid="0" op="" dtype="" >2.1458e-04 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000060000000004" call="MPI_Isend" bytes="1536" orank="4" region="0" commid="0" count="83" tid="0" op="" dtype="" >2.8706e-04 2.1458e-06 7.1526e-06</hent>
<hent key="0240010000000000000006000000000B" call="MPI_Isend" bytes="1536" orank="11" region="0" commid="0" count="91" tid="0" op="" dtype="" >1.6809e-04 9.5367e-07 1.4067e-05</hent>
<hent key="0240010000000000000006000000003B" call="MPI_Isend" bytes="1536" orank="59" region="0" commid="0" count="81" tid="0" op="" dtype="" >4.1819e-04 4.0531e-06 6.1989e-06</hent>
<hent key="02400100000000000000060000000043" call="MPI_Isend" bytes="1536" orank="67" region="0" commid="0" count="205" tid="0" op="" dtype="" >9.6631e-04 2.8610e-06 1.0967e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C3" call="MPI_Isend" bytes="1536" orank="707" region="0" commid="0" count="213" tid="0" op="" dtype="" >9.6726e-04 3.8147e-06 8.8215e-06</hent>
<hent key="038001000000000000000C0000000043" call="MPI_Irecv" bytes="3072" orank="67" region="0" commid="0" count="8" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C3" call="MPI_Irecv" bytes="3072" orank="707" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C000000000B" call="MPI_Isend" bytes="3072" orank="11" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="024001000000000000000C0000000043" call="MPI_Isend" bytes="3072" orank="67" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.2888e-05 5.0068e-06 6.9141e-06</hent>
<hent key="024001000000000000000C00000002C3" call="MPI_Isend" bytes="3072" orank="707" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.8133e-05 5.0068e-06 6.1989e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.2016e-05 8.2016e-05 8.2016e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.8406e-04 5.8889e-05 6.3181e-05</hent>
<hent key="03800100000000000000038000000002" call="MPI_Irecv" bytes="896" orank="2" region="0" commid="0" count="2588" tid="0" op="" dtype="" >2.9230e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000038000000004" call="MPI_Irecv" bytes="896" orank="4" region="0" commid="0" count="2735" tid="0" op="" dtype="" >4.9376e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000003800000000B" call="MPI_Irecv" bytes="896" orank="11" region="0" commid="0" count="323" tid="0" op="" dtype="" >5.8413e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000003B" call="MPI_Irecv" bytes="896" orank="59" region="0" commid="0" count="339" tid="0" op="" dtype="" >1.7285e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000043" call="MPI_Irecv" bytes="896" orank="67" region="0" commid="0" count="331" tid="0" op="" dtype="" >8.0109e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002C3" call="MPI_Irecv" bytes="896" orank="707" region="0" commid="0" count="328" tid="0" op="" dtype="" >1.1277e-04 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.5974e-05 1.5974e-05 1.5974e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000003800000002C3" call="MPI_Irecv" bytes="14336" orank="707" region="0" commid="0" count="21" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="02400100000000000000038000000002" call="MPI_Isend" bytes="896" orank="2" region="0" commid="0" count="2641" tid="0" op="" dtype="" >2.0826e-03 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000038000000004" call="MPI_Isend" bytes="896" orank="4" region="0" commid="0" count="2741" tid="0" op="" dtype="" >2.7640e-03 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000003800000000B" call="MPI_Isend" bytes="896" orank="11" region="0" commid="0" count="341" tid="0" op="" dtype="" >5.1212e-04 9.5367e-07 9.7752e-06</hent>
<hent key="0240010000000000000003800000003B" call="MPI_Isend" bytes="896" orank="59" region="0" commid="0" count="323" tid="0" op="" dtype="" >1.6015e-03 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000038000000043" call="MPI_Isend" bytes="896" orank="67" region="0" commid="0" count="342" tid="0" op="" dtype="" >1.5342e-03 2.8610e-06 1.1921e-05</hent>
<hent key="024001000000000000000380000002C3" call="MPI_Isend" bytes="896" orank="707" region="0" commid="0" count="335" tid="0" op="" dtype="" >1.4324e-03 2.8610e-06 1.0014e-05</hent>
<hent key="02400100000000000000380000000043" call="MPI_Isend" bytes="14336" orank="67" region="0" commid="0" count="6" tid="0" op="" dtype="" >4.4107e-05 5.9605e-06 1.0014e-05</hent>
<hent key="024001000000000000003800000002C3" call="MPI_Isend" bytes="14336" orank="707" region="0" commid="0" count="7" tid="0" op="" dtype="" >5.3406e-05 6.9141e-06 8.8215e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.8615e+00 1.5974e-05 2.0311e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.2201e-03 3.2201e-03 3.2201e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0434e-02 1.0434e-02 1.0434e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.6353e-02 2.6353e-02 2.6353e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.0450e-01 1.9410e-03 1.8610e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >2.7919e-04 2.7919e-04 2.7919e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5911e+00 3.7098e-04 2.9955e-01</hent>
<hent key="03800100000000000000040000000002" call="MPI_Irecv" bytes="1024" orank="2" region="0" commid="0" count="1122" tid="0" op="" dtype="" >7.8917e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000004" call="MPI_Irecv" bytes="1024" orank="4" region="0" commid="0" count="992" tid="0" op="" dtype="" >1.7476e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000004000000000B" call="MPI_Irecv" bytes="1024" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.2265e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000003B" call="MPI_Irecv" bytes="1024" orank="59" region="0" commid="0" count="3392" tid="0" op="" dtype="" >2.7442e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.4067e-05 0.0000e+00 6.1989e-06</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="03800100000000000000070000000002" call="MPI_Irecv" bytes="1792" orank="2" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.5259e-05 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000004" call="MPI_Irecv" bytes="1792" orank="4" region="0" commid="0" count="38" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000B" call="MPI_Irecv" bytes="1792" orank="11" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.1683e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000003B" call="MPI_Irecv" bytes="1792" orank="59" region="0" commid="0" count="35" tid="0" op="" dtype="" >2.1219e-05 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000070000000043" call="MPI_Irecv" bytes="1792" orank="67" region="0" commid="0" count="124" tid="0" op="" dtype="" >2.5749e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002C3" call="MPI_Irecv" bytes="1792" orank="707" region="0" commid="0" count="151" tid="0" op="" dtype="" >5.9843e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >4.5776e-05 0.0000e+00 2.6941e-05</hent>
<hent key="02400100000000000000040000000002" call="MPI_Isend" bytes="1024" orank="2" region="0" commid="0" count="1036" tid="0" op="" dtype="" >6.9332e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000040000000004" call="MPI_Isend" bytes="1024" orank="4" region="0" commid="0" count="968" tid="0" op="" dtype="" >7.0953e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000004000000000B" call="MPI_Isend" bytes="1024" orank="11" region="0" commid="0" count="3394" tid="0" op="" dtype="" >1.9057e-03 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000004000000003B" call="MPI_Isend" bytes="1024" orank="59" region="0" commid="0" count="3396" tid="0" op="" dtype="" >8.7714e-03 9.5367e-07 1.3113e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5335e+00 0.0000e+00 2.2725e+00</hent>
<hent key="038001000000000000000A0000000002" call="MPI_Irecv" bytes="2560" orank="2" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000004" call="MPI_Irecv" bytes="2560" orank="4" region="0" commid="0" count="5" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A000000000B" call="MPI_Irecv" bytes="2560" orank="11" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000003B" call="MPI_Irecv" bytes="2560" orank="59" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000043" call="MPI_Irecv" bytes="2560" orank="67" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.4782e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002C3" call="MPI_Irecv" bytes="2560" orank="707" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.5974e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000002" call="MPI_Isend" bytes="1792" orank="2" region="0" commid="0" count="44" tid="0" op="" dtype="" >1.0085e-04 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000004" call="MPI_Isend" bytes="1792" orank="4" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.1587e-04 2.1458e-06 4.0531e-06</hent>
<hent key="0240010000000000000007000000000B" call="MPI_Isend" bytes="1792" orank="11" region="0" commid="0" count="42" tid="0" op="" dtype="" >9.7990e-05 9.5367e-07 1.5020e-05</hent>
<hent key="0240010000000000000007000000003B" call="MPI_Isend" bytes="1792" orank="59" region="0" commid="0" count="36" tid="0" op="" dtype="" >1.9479e-04 4.7684e-06 6.1989e-06</hent>
<hent key="02400100000000000000070000000043" call="MPI_Isend" bytes="1792" orank="67" region="0" commid="0" count="150" tid="0" op="" dtype="" >7.2575e-04 2.1458e-06 1.0967e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1363e-01 1.1363e-01 1.1363e-01</hent>
<hent key="024001000000000000000700000002C3" call="MPI_Isend" bytes="1792" orank="707" region="0" commid="0" count="137" tid="0" op="" dtype="" >6.2251e-04 1.9073e-06 1.0014e-05</hent>
<hent key="024001000000000000000A0000000002" call="MPI_Isend" bytes="2560" orank="2" region="0" commid="0" count="3" tid="0" op="" dtype="" >8.1062e-06 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000004" call="MPI_Isend" bytes="2560" orank="4" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.6689e-05 2.8610e-06 4.0531e-06</hent>
<hent key="024001000000000000000A000000000B" call="MPI_Isend" bytes="2560" orank="11" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.4557e-05 1.9073e-06 1.5974e-05</hent>
<hent key="024001000000000000000A000000003B" call="MPI_Isend" bytes="2560" orank="59" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.7166e-05 5.0068e-06 6.1989e-06</hent>
<hent key="024001000000000000000A0000000043" call="MPI_Isend" bytes="2560" orank="67" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.2364e-04 3.8147e-06 6.9141e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.1230e-04 1.8120e-05 5.0068e-05</hent>
<hent key="024001000000000000000A00000002C3" call="MPI_Isend" bytes="2560" orank="707" region="0" commid="0" count="39" tid="0" op="" dtype="" >2.0242e-04 4.0531e-06 6.1989e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >3.0899e-04 1.1706e-04 1.9193e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.7909e-05 3.7909e-05 3.7909e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.7220e-06 1.9073e-06 3.8147e-06</hent>
<hent key="03800100000000000000000400000002" call="MPI_Irecv" bytes="4" orank="2" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.8511e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000400000004" call="MPI_Irecv" bytes="4" orank="4" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.3910e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000000040000000B" call="MPI_Irecv" bytes="4" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.8998e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000003B" call="MPI_Irecv" bytes="4" orank="59" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.6291e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000400000043" call="MPI_Irecv" bytes="4" orank="67" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.6103e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >4.4727e-04 2.5988e-05 1.9813e-04</hent>
<hent key="038001000000000000001C0000000002" call="MPI_Irecv" bytes="7168" orank="2" region="0" commid="0" count="8554" tid="0" op="" dtype="" >2.6193e-03 0.0000e+00 3.0994e-06</hent>
<hent key="038001000000000000001C0000000004" call="MPI_Irecv" bytes="7168" orank="4" region="0" commid="0" count="9030" tid="0" op="" dtype="" >1.2221e-03 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000004000002C3" call="MPI_Irecv" bytes="4" orank="707" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.0678e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000400000002" call="MPI_Isend" bytes="4" orank="2" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3695e-03 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000000400000004" call="MPI_Isend" bytes="4" orank="4" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.5299e-03 0.0000e+00 7.1049e-05</hent>
<hent key="0240010000000000000000040000000B" call="MPI_Isend" bytes="4" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3616e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000000040000003B" call="MPI_Isend" bytes="4" orank="59" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.9995e-02 3.0994e-06 1.2422e-04</hent>
<hent key="02400100000000000000000400000043" call="MPI_Isend" bytes="4" orank="67" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.9035e-02 3.8147e-06 1.3113e-05</hent>
<hent key="024001000000000000001C0000000002" call="MPI_Isend" bytes="7168" orank="2" region="0" commid="0" count="8743" tid="0" op="" dtype="" >1.9276e-02 9.5367e-07 3.6001e-05</hent>
<hent key="024001000000000000001C0000000004" call="MPI_Isend" bytes="7168" orank="4" region="0" commid="0" count="9164" tid="0" op="" dtype="" >3.0031e-02 9.5367e-07 2.5034e-05</hent>
<hent key="024001000000000000000004000002C3" call="MPI_Isend" bytes="4" orank="707" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7710e-02 2.8610e-06 6.0081e-05</hent>
<hent key="03800100000000000000050000000002" call="MPI_Irecv" bytes="1280" orank="2" region="0" commid="0" count="189" tid="0" op="" dtype="" >5.9843e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000004" call="MPI_Irecv" bytes="1280" orank="4" region="0" commid="0" count="185" tid="0" op="" dtype="" >3.2663e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000005000000000B" call="MPI_Irecv" bytes="1280" orank="11" region="0" commid="0" count="190" tid="0" op="" dtype="" >3.6716e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000003B" call="MPI_Irecv" bytes="1280" orank="59" region="0" commid="0" count="197" tid="0" op="" dtype="" >1.0228e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000043" call="MPI_Irecv" bytes="1280" orank="67" region="0" commid="0" count="298" tid="0" op="" dtype="" >7.9870e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0718e+01 7.1526e-06 1.5601e-01</hent>
<hent key="0380010000000000000028000000003B" call="MPI_Irecv" bytes="10240" orank="59" region="0" commid="0" count="23" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000500000002C3" call="MPI_Irecv" bytes="1280" orank="707" region="0" commid="0" count="265" tid="0" op="" dtype="" >1.0562e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000002" call="MPI_Irecv" bytes="2048" orank="2" region="0" commid="0" count="13" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000004" call="MPI_Irecv" bytes="2048" orank="4" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000B" call="MPI_Irecv" bytes="2048" orank="11" region="0" commid="0" count="10" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000003B" call="MPI_Irecv" bytes="2048" orank="59" region="0" commid="0" count="12" tid="0" op="" dtype="" >6.6757e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000043" call="MPI_Irecv" bytes="2048" orank="67" region="0" commid="0" count="3480" tid="0" op="" dtype="" >4.9853e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002C3" call="MPI_Irecv" bytes="2048" orank="707" region="0" commid="0" count="3472" tid="0" op="" dtype="" >4.9543e-04 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000050000000002" call="MPI_Isend" bytes="1280" orank="2" region="0" commid="0" count="187" tid="0" op="" dtype="" >3.7718e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000050000000004" call="MPI_Isend" bytes="1280" orank="4" region="0" commid="0" count="196" tid="0" op="" dtype="" >6.4754e-04 1.9073e-06 7.1526e-06</hent>
<hent key="0240010000000000000005000000000B" call="MPI_Isend" bytes="1280" orank="11" region="0" commid="0" count="197" tid="0" op="" dtype="" >3.5858e-04 0.0000e+00 1.5974e-05</hent>
<hent key="0240010000000000000005000000003B" call="MPI_Isend" bytes="1280" orank="59" region="0" commid="0" count="202" tid="0" op="" dtype="" >1.0309e-03 1.9073e-06 1.0014e-05</hent>
<hent key="02400100000000000000050000000043" call="MPI_Isend" bytes="1280" orank="67" region="0" commid="0" count="297" tid="0" op="" dtype="" >1.3933e-03 3.0994e-06 3.1948e-05</hent>
<hent key="0240010000000000000028000000000B" call="MPI_Isend" bytes="10240" orank="11" region="0" commid="0" count="16" tid="0" op="" dtype="" >1.5736e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0240010000000000000028000000003B" call="MPI_Isend" bytes="10240" orank="59" region="0" commid="0" count="8" tid="0" op="" dtype="" >5.0783e-05 5.0068e-06 7.8678e-06</hent>
<hent key="024001000000000000000500000002C3" call="MPI_Isend" bytes="1280" orank="707" region="0" commid="0" count="293" tid="0" op="" dtype="" >1.2865e-03 3.8147e-06 1.0967e-05</hent>
</hash>
<internal rank="3" log_i="1723712895.684171" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="4" mpi_size="768" stamp_init="1723712829.548137" stamp_final="1723712895.684447" username="apac4" allocationname="unknown" flags="0" pid="684272" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61363e+01" utime="4.94148e+01" stime="8.16454e+00" mtime="3.01249e+01" gflop="0.00000e+00" gbyte="3.74454e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.01249e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4681569156a158f556a156a1507" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60032e+01" utime="4.93899e+01" stime="8.14723e+00" mtime="3.01249e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.01249e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4906e+08" > 5.8983e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4894e+08" > 3.6587e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2212e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5450e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0390e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9073e-06 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5902e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5574e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0368e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1348e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8015e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="192" >
<hent key="02400100000000000000080000000003" call="MPI_Isend" bytes="2048" orank="3" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.4305e-05 1.9073e-06 2.1458e-06</hent>
<hent key="02400100000000000000080000000005" call="MPI_Isend" bytes="2048" orank="5" region="0" commid="0" count="14" tid="0" op="" dtype="" >5.4598e-05 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000008000000000C" call="MPI_Isend" bytes="2048" orank="12" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.2663e-05 1.9073e-06 1.0967e-05</hent>
<hent key="0240010000000000000008000000003C" call="MPI_Isend" bytes="2048" orank="60" region="0" commid="0" count="13" tid="0" op="" dtype="" >6.9141e-05 5.0068e-06 6.1989e-06</hent>
<hent key="02400100000000000000080000000044" call="MPI_Isend" bytes="2048" orank="68" region="0" commid="0" count="3469" tid="0" op="" dtype="" >1.6281e-02 9.5367e-07 5.8889e-05</hent>
<hent key="024001000000000000000800000002C4" call="MPI_Isend" bytes="2048" orank="708" region="0" commid="0" count="3464" tid="0" op="" dtype="" >1.4471e-02 9.5367e-07 5.6982e-05</hent>
<hent key="038001000000000000000E00000002C4" call="MPI_Irecv" bytes="3584" orank="708" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="024001000000000000000E0000000044" call="MPI_Isend" bytes="3584" orank="68" region="0" commid="0" count="1" tid="0" op="" dtype="" >6.1989e-06 6.1989e-06 6.1989e-06</hent>
<hent key="03800100000000000000028000000003" call="MPI_Irecv" bytes="640" orank="3" region="0" commid="0" count="415" tid="0" op="" dtype="" >1.7142e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000005" call="MPI_Irecv" bytes="640" orank="5" region="0" commid="0" count="409" tid="0" op="" dtype="" >1.0777e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000002800000000C" call="MPI_Irecv" bytes="640" orank="12" region="0" commid="0" count="409" tid="0" op="" dtype="" >1.0991e-04 0.0000e+00 5.0068e-06</hent>
<hent key="024001000000000000000E00000002C4" call="MPI_Isend" bytes="3584" orank="708" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.2159e-05 5.9605e-06 6.1989e-06</hent>
<hent key="0380010000000000000002800000003C" call="MPI_Irecv" bytes="640" orank="60" region="0" commid="0" count="395" tid="0" op="" dtype="" >1.8811e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000028000000044" call="MPI_Irecv" bytes="640" orank="68" region="0" commid="0" count="273" tid="0" op="" dtype="" >1.1778e-04 0.0000e+00 6.1989e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002C4" call="MPI_Irecv" bytes="640" orank="708" region="0" commid="0" count="281" tid="0" op="" dtype="" >1.3709e-04 0.0000e+00 1.2159e-05</hent>
<hent key="02400100000000000000028000000003" call="MPI_Isend" bytes="640" orank="3" region="0" commid="0" count="391" tid="0" op="" dtype="" >5.2500e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000028000000005" call="MPI_Isend" bytes="640" orank="5" region="0" commid="0" count="425" tid="0" op="" dtype="" >1.3130e-03 1.9073e-06 9.0599e-06</hent>
<hent key="0240010000000000000002800000000C" call="MPI_Isend" bytes="640" orank="12" region="0" commid="0" count="410" tid="0" op="" dtype="" >5.6362e-04 9.5367e-07 1.0014e-05</hent>
<hent key="0240010000000000000002800000003C" call="MPI_Isend" bytes="640" orank="60" region="0" commid="0" count="395" tid="0" op="" dtype="" >1.9338e-03 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000028000000044" call="MPI_Isend" bytes="640" orank="68" region="0" commid="0" count="268" tid="0" op="" dtype="" >1.2732e-03 2.8610e-06 1.3828e-05</hent>
<hent key="024001000000000000000280000002C4" call="MPI_Isend" bytes="640" orank="708" region="0" commid="0" count="307" tid="0" op="" dtype="" >1.4956e-03 3.0994e-06 1.7166e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.1948e-05 3.1948e-05 3.1948e-05</hent>
<hent key="03800100000000000000014000000003" call="MPI_Irecv" bytes="320" orank="3" region="0" commid="0" count="361" tid="0" op="" dtype="" >1.3065e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000005" call="MPI_Irecv" bytes="320" orank="5" region="0" commid="0" count="341" tid="0" op="" dtype="" >9.3937e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000C" call="MPI_Irecv" bytes="320" orank="12" region="0" commid="0" count="343" tid="0" op="" dtype="" >8.5592e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000003C" call="MPI_Irecv" bytes="320" orank="60" region="0" commid="0" count="371" tid="0" op="" dtype="" >1.3256e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000014000000044" call="MPI_Irecv" bytes="320" orank="68" region="0" commid="0" count="171" tid="0" op="" dtype="" >4.7684e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002C4" call="MPI_Irecv" bytes="320" orank="708" region="0" commid="0" count="192" tid="0" op="" dtype="" >8.3208e-05 0.0000e+00 8.1062e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 9.5367e-07 2.1458e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.2212e+00 0.0000e+00 1.5446e-01</hent>
<hent key="03800100000000000000400000000044" call="MPI_Irecv" bytes="16384" orank="68" region="0" commid="0" count="12683" tid="0" op="" dtype="" >4.7379e-03 0.0000e+00 3.2902e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6490e-02 9.5367e-07 1.6473e-02</hent>
<hent key="038001000000000000004000000002C4" call="MPI_Irecv" bytes="16384" orank="708" region="0" commid="0" count="12699" tid="0" op="" dtype="" >4.8063e-03 0.0000e+00 2.8133e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="02400100000000000000014000000003" call="MPI_Isend" bytes="320" orank="3" region="0" commid="0" count="388" tid="0" op="" dtype="" >4.0269e-04 0.0000e+00 3.8147e-06</hent>
<hent key="02400100000000000000014000000005" call="MPI_Isend" bytes="320" orank="5" region="0" commid="0" count="350" tid="0" op="" dtype="" >9.5963e-04 1.9073e-06 4.0531e-06</hent>
<hent key="0240010000000000000001400000000C" call="MPI_Isend" bytes="320" orank="12" region="0" commid="0" count="361" tid="0" op="" dtype="" >3.8457e-04 0.0000e+00 8.8215e-06</hent>
<hent key="0240010000000000000001400000003C" call="MPI_Isend" bytes="320" orank="60" region="0" commid="0" count="351" tid="0" op="" dtype="" >1.6491e-03 2.8610e-06 1.1921e-05</hent>
<hent key="02400100000000000000014000000044" call="MPI_Isend" bytes="320" orank="68" region="0" commid="0" count="202" tid="0" op="" dtype="" >8.8501e-04 2.8610e-06 1.0967e-05</hent>
<hent key="03800100000000000000200000000003" call="MPI_Irecv" bytes="8192" orank="3" region="0" commid="0" count="3535" tid="0" op="" dtype="" >1.7211e-03 0.0000e+00 1.1921e-05</hent>
<hent key="03800100000000000000200000000005" call="MPI_Irecv" bytes="8192" orank="5" region="0" commid="0" count="3139" tid="0" op="" dtype="" >5.3406e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0380010000000000000020000000000C" call="MPI_Irecv" bytes="8192" orank="12" region="0" commid="0" count="12639" tid="0" op="" dtype="" >5.3070e-03 0.0000e+00 2.4080e-05</hent>
<hent key="0380010000000000000020000000003C" call="MPI_Irecv" bytes="8192" orank="60" region="0" commid="0" count="12523" tid="0" op="" dtype="" >1.2949e-03 0.0000e+00 2.1935e-05</hent>
<hent key="024001000000000000000140000002C4" call="MPI_Isend" bytes="320" orank="708" region="0" commid="0" count="184" tid="0" op="" dtype="" >9.1767e-04 3.0994e-06 1.9073e-05</hent>
<hent key="03800100000000000000000000000003" call="MPI_Irecv" bytes="0" orank="3" region="0" commid="0" count="291" tid="0" op="" dtype="" >1.0681e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000000000000005" call="MPI_Irecv" bytes="0" orank="5" region="0" commid="0" count="265" tid="0" op="" dtype="" >6.0558e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000C" call="MPI_Irecv" bytes="0" orank="12" region="0" commid="0" count="276" tid="0" op="" dtype="" >7.7963e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000000000000003C" call="MPI_Irecv" bytes="0" orank="60" region="0" commid="0" count="255" tid="0" op="" dtype="" >7.9870e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000044" call="MPI_Irecv" bytes="0" orank="68" region="0" commid="0" count="148" tid="0" op="" dtype="" >5.1498e-05 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000400000000044" call="MPI_Isend" bytes="16384" orank="68" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.4648e-01 4.0531e-06 6.7949e-05</hent>
<hent key="038001000000000000000000000002C4" call="MPI_Irecv" bytes="0" orank="708" region="0" commid="0" count="153" tid="0" op="" dtype="" >4.4584e-05 0.0000e+00 4.0531e-06</hent>
<hent key="024001000000000000004000000002C4" call="MPI_Isend" bytes="16384" orank="708" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.2021e-01 3.0994e-06 7.4911e-04</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.6062e-04 0.0000e+00 2.9206e-04</hent>
<hent key="02400100000000000000200000000003" call="MPI_Isend" bytes="8192" orank="3" region="0" commid="0" count="3669" tid="0" op="" dtype="" >4.8273e-03 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000200000000005" call="MPI_Isend" bytes="8192" orank="5" region="0" commid="0" count="3199" tid="0" op="" dtype="" >6.5987e-03 0.0000e+00 1.7166e-05</hent>
<hent key="0240010000000000000020000000000C" call="MPI_Isend" bytes="8192" orank="12" region="0" commid="0" count="12659" tid="0" op="" dtype="" >8.8758e-03 0.0000e+00 5.2929e-05</hent>
<hent key="0240010000000000000020000000003C" call="MPI_Isend" bytes="8192" orank="60" region="0" commid="0" count="12557" tid="0" op="" dtype="" >1.0361e-01 3.8147e-06 4.2200e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.0981e-05 2.0981e-05 2.0981e-05</hent>
<hent key="02400100000000000000000000000003" call="MPI_Isend" bytes="0" orank="3" region="0" commid="0" count="276" tid="0" op="" dtype="" >1.9026e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000005" call="MPI_Isend" bytes="0" orank="5" region="0" commid="0" count="234" tid="0" op="" dtype="" >4.6301e-04 9.5367e-07 3.4094e-05</hent>
<hent key="0240010000000000000000000000000C" call="MPI_Isend" bytes="0" orank="12" region="0" commid="0" count="266" tid="0" op="" dtype="" >2.0003e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0240010000000000000000000000003C" call="MPI_Isend" bytes="0" orank="60" region="0" commid="0" count="274" tid="0" op="" dtype="" >1.0834e-03 1.9073e-06 1.3113e-05</hent>
<hent key="02400100000000000000000000000044" call="MPI_Isend" bytes="0" orank="68" region="0" commid="0" count="155" tid="0" op="" dtype="" >6.4158e-04 9.5367e-07 1.2159e-05</hent>
<hent key="024001000000000000000000000002C4" call="MPI_Isend" bytes="0" orank="708" region="0" commid="0" count="144" tid="0" op="" dtype="" >6.5565e-04 1.9073e-06 1.3113e-05</hent>
<hent key="03800100000000000000060000000003" call="MPI_Irecv" bytes="1536" orank="3" region="0" commid="0" count="83" tid="0" op="" dtype="" >3.7670e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000005" call="MPI_Irecv" bytes="1536" orank="5" region="0" commid="0" count="87" tid="0" op="" dtype="" >2.8133e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000C" call="MPI_Irecv" bytes="1536" orank="12" region="0" commid="0" count="105" tid="0" op="" dtype="" >2.9087e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000003C" call="MPI_Irecv" bytes="1536" orank="60" region="0" commid="0" count="104" tid="0" op="" dtype="" >5.7936e-05 0.0000e+00 4.7684e-06</hent>
<hent key="03800100000000000000060000000044" call="MPI_Irecv" bytes="1536" orank="68" region="0" commid="0" count="238" tid="0" op="" dtype="" >9.5129e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002C4" call="MPI_Irecv" bytes="1536" orank="708" region="0" commid="0" count="194" tid="0" op="" dtype="" >1.2374e-04 0.0000e+00 8.1062e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000003" call="MPI_Isend" bytes="1536" orank="3" region="0" commid="0" count="80" tid="0" op="" dtype="" >1.6475e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000060000000005" call="MPI_Isend" bytes="1536" orank="5" region="0" commid="0" count="90" tid="0" op="" dtype="" >3.2878e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000006000000000C" call="MPI_Isend" bytes="1536" orank="12" region="0" commid="0" count="103" tid="0" op="" dtype="" >2.2054e-04 9.5367e-07 1.5020e-05</hent>
<hent key="0240010000000000000006000000003C" call="MPI_Isend" bytes="1536" orank="60" region="0" commid="0" count="96" tid="0" op="" dtype="" >5.1212e-04 4.0531e-06 9.0599e-06</hent>
<hent key="02400100000000000000060000000044" call="MPI_Isend" bytes="1536" orank="68" region="0" commid="0" count="205" tid="0" op="" dtype="" >1.0498e-03 3.8147e-06 1.6928e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C4" call="MPI_Isend" bytes="1536" orank="708" region="0" commid="0" count="221" tid="0" op="" dtype="" >1.1592e-03 3.8147e-06 1.1206e-05</hent>
<hent key="038001000000000000000C0000000044" call="MPI_Irecv" bytes="3072" orank="68" region="0" commid="0" count="10" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C4" call="MPI_Irecv" bytes="3072" orank="708" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C0000000044" call="MPI_Isend" bytes="3072" orank="68" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.2173e-05 5.0068e-06 6.1989e-06</hent>
<hent key="024001000000000000000C00000002C4" call="MPI_Isend" bytes="3072" orank="708" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.7895e-05 5.0068e-06 1.0967e-05</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.8944e-05 9.8944e-05 9.8944e-05</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.2769e-04 7.2956e-05 8.0824e-05</hent>
<hent key="03800100000000000000038000000003" call="MPI_Irecv" bytes="896" orank="3" region="0" commid="0" count="2741" tid="0" op="" dtype="" >6.3801e-04 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000038000000005" call="MPI_Irecv" bytes="896" orank="5" region="0" commid="0" count="2896" tid="0" op="" dtype="" >5.4598e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0380010000000000000003800000000C" call="MPI_Irecv" bytes="896" orank="12" region="0" commid="0" count="307" tid="0" op="" dtype="" >8.2016e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000003C" call="MPI_Irecv" bytes="896" orank="60" region="0" commid="0" count="321" tid="0" op="" dtype="" >1.6665e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000044" call="MPI_Irecv" bytes="896" orank="68" region="0" commid="0" count="315" tid="0" op="" dtype="" >1.1611e-04 0.0000e+00 1.0014e-05</hent>
<hent key="038001000000000000000380000002C4" call="MPI_Irecv" bytes="896" orank="708" region="0" commid="0" count="374" tid="0" op="" dtype="" >2.1577e-04 0.0000e+00 8.1062e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.2875e-05 1.2875e-05 1.2875e-05</hent>
<hent key="03800100000000000000380000000044" call="MPI_Irecv" bytes="14336" orank="68" region="0" commid="0" count="16" tid="0" op="" dtype="" >1.7881e-05 0.0000e+00 2.1458e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="02400100000000000000038000000003" call="MPI_Isend" bytes="896" orank="3" region="0" commid="0" count="2735" tid="0" op="" dtype="" >1.9844e-03 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000038000000005" call="MPI_Isend" bytes="896" orank="5" region="0" commid="0" count="2885" tid="0" op="" dtype="" >2.9931e-03 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000003800000000C" call="MPI_Isend" bytes="896" orank="12" region="0" commid="0" count="320" tid="0" op="" dtype="" >5.0664e-04 9.5367e-07 9.0599e-06</hent>
<hent key="0240010000000000000003800000003C" call="MPI_Isend" bytes="896" orank="60" region="0" commid="0" count="315" tid="0" op="" dtype="" >1.6108e-03 3.8147e-06 1.0967e-05</hent>
<hent key="02400100000000000000038000000044" call="MPI_Isend" bytes="896" orank="68" region="0" commid="0" count="322" tid="0" op="" dtype="" >1.5349e-03 2.8610e-06 1.5020e-05</hent>
<hent key="024001000000000000000380000002C4" call="MPI_Isend" bytes="896" orank="708" region="0" commid="0" count="330" tid="0" op="" dtype="" >1.6356e-03 2.8610e-06 1.1206e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.0803e+00 1.2159e-05 2.0311e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.1810e-03 3.1810e-03 3.1810e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0368e-02 1.0368e-02 1.0368e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.6948e-02 2.6948e-02 2.6948e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.9867e-01 3.3429e-03 1.7910e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.5501e-04 3.5501e-04 3.5501e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 9.5367e-07 2.1458e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5902e+00 4.4584e-04 2.5130e-01</hent>
<hent key="03800100000000000000040000000003" call="MPI_Irecv" bytes="1024" orank="3" region="0" commid="0" count="968" tid="0" op="" dtype="" >2.4796e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000040000000005" call="MPI_Irecv" bytes="1024" orank="5" region="0" commid="0" count="832" tid="0" op="" dtype="" >1.5473e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000000C" call="MPI_Irecv" bytes="1024" orank="12" region="0" commid="0" count="3382" tid="0" op="" dtype="" >7.6795e-04 0.0000e+00 1.7881e-05</hent>
<hent key="0380010000000000000004000000003C" call="MPI_Irecv" bytes="1024" orank="60" region="0" commid="0" count="3350" tid="0" op="" dtype="" >2.9731e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.0390e-03 1.9073e-06 1.0002e-03</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.1008e-05 4.1008e-05 4.1008e-05</hent>
<hent key="03800100000000000000070000000003" call="MPI_Irecv" bytes="1792" orank="3" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.1921e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000005" call="MPI_Irecv" bytes="1792" orank="5" region="0" commid="0" count="33" tid="0" op="" dtype="" >9.7752e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000C" call="MPI_Irecv" bytes="1792" orank="12" region="0" commid="0" count="43" tid="0" op="" dtype="" >9.7752e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000003C" call="MPI_Irecv" bytes="1792" orank="60" region="0" commid="0" count="46" tid="0" op="" dtype="" >3.0518e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000070000000044" call="MPI_Irecv" bytes="1792" orank="68" region="0" commid="0" count="138" tid="0" op="" dtype="" >5.0306e-05 0.0000e+00 3.0994e-06</hent>
<hent key="038001000000000000000700000002C4" call="MPI_Irecv" bytes="1792" orank="708" region="0" commid="0" count="107" tid="0" op="" dtype="" >6.0797e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >6.0320e-05 0.0000e+00 2.7895e-05</hent>
<hent key="02400100000000000000040000000003" call="MPI_Isend" bytes="1024" orank="3" region="0" commid="0" count="992" tid="0" op="" dtype="" >6.1846e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000040000000005" call="MPI_Isend" bytes="1024" orank="5" region="0" commid="0" count="844" tid="0" op="" dtype="" >6.5994e-04 0.0000e+00 1.9073e-05</hent>
<hent key="0240010000000000000004000000000C" call="MPI_Isend" bytes="1024" orank="12" region="0" commid="0" count="3388" tid="0" op="" dtype="" >1.9734e-03 0.0000e+00 1.6928e-05</hent>
<hent key="0240010000000000000004000000003C" call="MPI_Isend" bytes="1024" orank="60" region="0" commid="0" count="3358" tid="0" op="" dtype="" >1.0051e-02 9.5367e-07 2.5034e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >5.0068e-06 2.1458e-06 2.8610e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5279e+00 0.0000e+00 2.2674e+00</hent>
<hent key="038001000000000000000A0000000003" call="MPI_Irecv" bytes="2560" orank="3" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000005" call="MPI_Irecv" bytes="2560" orank="5" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000000C" call="MPI_Irecv" bytes="2560" orank="12" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000003C" call="MPI_Irecv" bytes="2560" orank="60" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000044" call="MPI_Irecv" bytes="2560" orank="68" region="0" commid="0" count="45" tid="0" op="" dtype="" >1.4305e-05 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A00000002C4" call="MPI_Irecv" bytes="2560" orank="708" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.3842e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000003" call="MPI_Isend" bytes="1792" orank="3" region="0" commid="0" count="38" tid="0" op="" dtype="" >8.2016e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000005" call="MPI_Isend" bytes="1792" orank="5" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.4615e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000007000000000C" call="MPI_Isend" bytes="1792" orank="12" region="0" commid="0" count="46" tid="0" op="" dtype="" >1.0204e-04 9.5367e-07 9.0599e-06</hent>
<hent key="0240010000000000000007000000003C" call="MPI_Isend" bytes="1792" orank="60" region="0" commid="0" count="44" tid="0" op="" dtype="" >2.5511e-04 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000070000000044" call="MPI_Isend" bytes="1792" orank="68" region="0" commid="0" count="152" tid="0" op="" dtype="" >7.9131e-04 3.8147e-06 1.3113e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1348e-01 1.1348e-01 1.1348e-01</hent>
<hent key="024001000000000000000700000002C4" call="MPI_Isend" bytes="1792" orank="708" region="0" commid="0" count="123" tid="0" op="" dtype="" >6.6018e-04 3.8147e-06 1.3113e-05</hent>
<hent key="024001000000000000000A0000000003" call="MPI_Isend" bytes="2560" orank="3" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.3590e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000005" call="MPI_Isend" bytes="2560" orank="5" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.5259e-05 3.0994e-06 4.0531e-06</hent>
<hent key="024001000000000000000A000000000C" call="MPI_Isend" bytes="2560" orank="12" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.2187e-05 1.9073e-06 1.1921e-05</hent>
<hent key="024001000000000000000A000000003C" call="MPI_Isend" bytes="2560" orank="60" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.1710e-05 4.7684e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000044" call="MPI_Isend" bytes="2560" orank="68" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.2697e-04 3.8147e-06 1.1921e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.3089e-04 2.3127e-05 5.4836e-05</hent>
<hent key="024001000000000000000A00000002C4" call="MPI_Isend" bytes="2560" orank="708" region="0" commid="0" count="38" tid="0" op="" dtype="" >2.2340e-04 4.7684e-06 1.0967e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >3.8624e-04 1.4210e-04 2.4414e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.7220e-06 1.9073e-06 3.8147e-06</hent>
<hent key="03800100000000000000000400000003" call="MPI_Irecv" bytes="4" orank="3" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0741e-03 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000000400000005" call="MPI_Irecv" bytes="4" orank="5" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.9973e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000000040000000C" call="MPI_Irecv" bytes="4" orank="12" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.5926e-04 0.0000e+00 2.6941e-05</hent>
<hent key="0380010000000000000000040000003C" call="MPI_Irecv" bytes="4" orank="60" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.2261e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000044" call="MPI_Irecv" bytes="4" orank="68" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.5115e-04 0.0000e+00 1.1206e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >7.0548e-04 1.0419e-04 2.4819e-04</hent>
<hent key="038001000000000000001C0000000003" call="MPI_Irecv" bytes="7168" orank="3" region="0" commid="0" count="9164" tid="0" op="" dtype="" >4.1440e-03 0.0000e+00 8.1062e-06</hent>
<hent key="038001000000000000001C0000000005" call="MPI_Irecv" bytes="7168" orank="5" region="0" commid="0" count="9560" tid="0" op="" dtype="" >1.7140e-03 0.0000e+00 2.3127e-05</hent>
<hent key="038001000000000000000004000002C4" call="MPI_Irecv" bytes="4" orank="708" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.9179e-04 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000000400000003" call="MPI_Isend" bytes="4" orank="3" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3416e-03 0.0000e+00 4.6968e-05</hent>
<hent key="02400100000000000000000400000005" call="MPI_Isend" bytes="4" orank="5" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3587e-03 0.0000e+00 1.8120e-05</hent>
<hent key="0240010000000000000000040000000C" call="MPI_Isend" bytes="4" orank="12" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6263e-03 0.0000e+00 1.4067e-05</hent>
<hent key="0240010000000000000000040000003C" call="MPI_Isend" bytes="4" orank="60" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.0364e-02 3.8147e-06 1.0681e-04</hent>
<hent key="02400100000000000000000400000044" call="MPI_Isend" bytes="4" orank="68" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.1817e-02 3.8147e-06 3.3140e-05</hent>
<hent key="024001000000000000001C0000000003" call="MPI_Isend" bytes="7168" orank="3" region="0" commid="0" count="9030" tid="0" op="" dtype="" >1.9829e-02 9.5367e-07 1.7881e-05</hent>
<hent key="024001000000000000001C0000000005" call="MPI_Isend" bytes="7168" orank="5" region="0" commid="0" count="9500" tid="0" op="" dtype="" >2.7876e-02 9.5367e-07 1.8120e-05</hent>
<hent key="024001000000000000000004000002C4" call="MPI_Isend" bytes="4" orank="708" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.1193e-02 3.0994e-06 8.3923e-05</hent>
<hent key="03800100000000000000050000000003" call="MPI_Irecv" bytes="1280" orank="3" region="0" commid="0" count="196" tid="0" op="" dtype="" >7.7009e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000005" call="MPI_Irecv" bytes="1280" orank="5" region="0" commid="0" count="209" tid="0" op="" dtype="" >5.2691e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000C" call="MPI_Irecv" bytes="1280" orank="12" region="0" commid="0" count="211" tid="0" op="" dtype="" >4.8637e-05 0.0000e+00 2.8610e-06</hent>
<hent key="0380010000000000000005000000003C" call="MPI_Irecv" bytes="1280" orank="60" region="0" commid="0" count="233" tid="0" op="" dtype="" >9.6321e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000044" call="MPI_Irecv" bytes="1280" orank="68" region="0" commid="0" count="306" tid="0" op="" dtype="" >1.2231e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0705e+01 8.1062e-06 1.5557e-01</hent>
<hent key="0380010000000000000028000000000C" call="MPI_Irecv" bytes="10240" orank="12" region="0" commid="0" count="60" tid="0" op="" dtype="" >1.3113e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000028000000003C" call="MPI_Irecv" bytes="10240" orank="60" region="0" commid="0" count="176" tid="0" op="" dtype="" >1.6689e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000500000002C4" call="MPI_Irecv" bytes="1280" orank="708" region="0" commid="0" count="297" tid="0" op="" dtype="" >1.7071e-04 0.0000e+00 4.7684e-06</hent>
<hent key="03800100000000000000080000000003" call="MPI_Irecv" bytes="2048" orank="3" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000005" call="MPI_Irecv" bytes="2048" orank="5" region="0" commid="0" count="16" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000C" call="MPI_Irecv" bytes="2048" orank="12" region="0" commid="0" count="12" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000003C" call="MPI_Irecv" bytes="2048" orank="60" region="0" commid="0" count="15" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000044" call="MPI_Irecv" bytes="2048" orank="68" region="0" commid="0" count="3453" tid="0" op="" dtype="" >6.4588e-04 0.0000e+00 1.3113e-05</hent>
<hent key="038001000000000000000800000002C4" call="MPI_Irecv" bytes="2048" orank="708" region="0" commid="0" count="3455" tid="0" op="" dtype="" >8.0013e-04 0.0000e+00 2.0027e-05</hent>
<hent key="02400100000000000000050000000003" call="MPI_Isend" bytes="1280" orank="3" region="0" commid="0" count="185" tid="0" op="" dtype="" >3.4070e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000005" call="MPI_Isend" bytes="1280" orank="5" region="0" commid="0" count="212" tid="0" op="" dtype="" >7.8082e-04 2.8610e-06 2.0027e-05</hent>
<hent key="0240010000000000000005000000000C" call="MPI_Isend" bytes="1280" orank="12" region="0" commid="0" count="188" tid="0" op="" dtype="" >3.0661e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000005000000003C" call="MPI_Isend" bytes="1280" orank="60" region="0" commid="0" count="245" tid="0" op="" dtype="" >1.1501e-03 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000050000000044" call="MPI_Isend" bytes="1280" orank="68" region="0" commid="0" count="279" tid="0" op="" dtype="" >1.3878e-03 2.8610e-06 1.2159e-05</hent>
<hent key="0240010000000000000028000000000C" call="MPI_Isend" bytes="10240" orank="12" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.9550e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0240010000000000000028000000003C" call="MPI_Isend" bytes="10240" orank="60" region="0" commid="0" count="142" tid="0" op="" dtype="" >1.2965e-03 3.8147e-06 2.5034e-05</hent>
<hent key="024001000000000000000500000002C4" call="MPI_Isend" bytes="1280" orank="708" region="0" commid="0" count="280" tid="0" op="" dtype="" >1.4412e-03 3.8147e-06 1.2875e-05</hent>
</hash>
<internal rank="4" log_i="1723712895.684447" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="5" mpi_size="768" stamp_init="1723712829.548185" stamp_final="1723712895.693828" username="apac4" allocationname="unknown" flags="0" pid="684273" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61456e+01" utime="5.13214e+01" stime="7.33399e+00" mtime="2.99720e+01" gflop="0.00000e+00" gbyte="3.77514e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.99720e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60129e+01" utime="5.12874e+01" stime="7.32561e+00" mtime="2.99720e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.99720e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4847e+08" > 4.5739e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4926e+08" > 2.9045e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.2444e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5763e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.2915e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5902e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8492e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0520e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1357e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8949e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="188" >
<hent key="02400100000000000000080000000004" call="MPI_Isend" bytes="2048" orank="4" region="0" commid="0" count="16" tid="0" op="" dtype="" >3.4094e-05 1.1921e-06 2.8610e-06</hent>
<hent key="02400100000000000000080000000006" call="MPI_Isend" bytes="2048" orank="6" region="0" commid="0" count="10" tid="0" op="" dtype="" >3.7670e-05 3.0994e-06 4.0531e-06</hent>
<hent key="0240010000000000000008000000000D" call="MPI_Isend" bytes="2048" orank="13" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.7193e-05 9.5367e-07 1.5020e-05</hent>
<hent key="0240010000000000000008000000003D" call="MPI_Isend" bytes="2048" orank="61" region="0" commid="0" count="18" tid="0" op="" dtype="" >9.4652e-05 4.7684e-06 5.9605e-06</hent>
<hent key="02400100000000000000080000000045" call="MPI_Isend" bytes="2048" orank="69" region="0" commid="0" count="3471" tid="0" op="" dtype="" >1.1641e-02 9.5367e-07 2.9087e-05</hent>
<hent key="024001000000000000000800000002C5" call="MPI_Isend" bytes="2048" orank="709" region="0" commid="0" count="3472" tid="0" op="" dtype="" >1.1121e-02 9.5367e-07 1.6928e-05</hent>
<hent key="03800100000000000000028000000004" call="MPI_Irecv" bytes="640" orank="4" region="0" commid="0" count="425" tid="0" op="" dtype="" >1.6046e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000028000000006" call="MPI_Irecv" bytes="640" orank="6" region="0" commid="0" count="406" tid="0" op="" dtype="" >2.4700e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000002800000000D" call="MPI_Irecv" bytes="640" orank="13" region="0" commid="0" count="426" tid="0" op="" dtype="" >8.7976e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000003D" call="MPI_Irecv" bytes="640" orank="61" region="0" commid="0" count="419" tid="0" op="" dtype="" >1.5330e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000045" call="MPI_Irecv" bytes="640" orank="69" region="0" commid="0" count="273" tid="0" op="" dtype="" >6.0320e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="038001000000000000000280000002C5" call="MPI_Irecv" bytes="640" orank="709" region="0" commid="0" count="297" tid="0" op="" dtype="" >8.7261e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000004" call="MPI_Isend" bytes="640" orank="4" region="0" commid="0" count="409" tid="0" op="" dtype="" >5.9175e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000028000000006" call="MPI_Isend" bytes="640" orank="6" region="0" commid="0" count="409" tid="0" op="" dtype="" >1.1916e-03 1.9073e-06 5.0068e-06</hent>
<hent key="0240010000000000000002800000000D" call="MPI_Isend" bytes="640" orank="13" region="0" commid="0" count="447" tid="0" op="" dtype="" >5.4097e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000002800000003D" call="MPI_Isend" bytes="640" orank="61" region="0" commid="0" count="387" tid="0" op="" dtype="" >1.8363e-03 3.8147e-06 5.9843e-05</hent>
<hent key="02400100000000000000028000000045" call="MPI_Isend" bytes="640" orank="69" region="0" commid="0" count="269" tid="0" op="" dtype="" >1.1404e-03 2.8610e-06 5.7936e-05</hent>
<hent key="024001000000000000000280000002C5" call="MPI_Isend" bytes="640" orank="709" region="0" commid="0" count="284" tid="0" op="" dtype="" >1.1628e-03 2.8610e-06 5.0068e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.2187e-05 3.2187e-05 3.2187e-05</hent>
<hent key="03800100000000000000014000000004" call="MPI_Irecv" bytes="320" orank="4" region="0" commid="0" count="350" tid="0" op="" dtype="" >1.4353e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000006" call="MPI_Irecv" bytes="320" orank="6" region="0" commid="0" count="350" tid="0" op="" dtype="" >1.7428e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000001400000000D" call="MPI_Irecv" bytes="320" orank="13" region="0" commid="0" count="324" tid="0" op="" dtype="" >7.1049e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000003D" call="MPI_Irecv" bytes="320" orank="61" region="0" commid="0" count="327" tid="0" op="" dtype="" >1.1945e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000045" call="MPI_Irecv" bytes="320" orank="69" region="0" commid="0" count="187" tid="0" op="" dtype="" >4.6730e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002C5" call="MPI_Irecv" bytes="320" orank="709" region="0" commid="0" count="182" tid="0" op="" dtype="" >6.6757e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.2444e+00 0.0000e+00 1.8300e-01</hent>
<hent key="03800100000000000000400000000045" call="MPI_Irecv" bytes="16384" orank="69" region="0" commid="0" count="12699" tid="0" op="" dtype="" >3.3348e-03 0.0000e+00 8.1062e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6445e-02 9.5367e-07 1.6428e-02</hent>
<hent key="038001000000000000004000000002C5" call="MPI_Irecv" bytes="16384" orank="709" region="0" commid="0" count="12699" tid="0" op="" dtype="" >2.3787e-03 0.0000e+00 3.0994e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000004" call="MPI_Isend" bytes="320" orank="4" region="0" commid="0" count="341" tid="0" op="" dtype="" >3.7241e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000006" call="MPI_Isend" bytes="320" orank="6" region="0" commid="0" count="357" tid="0" op="" dtype="" >9.4366e-04 9.5367e-07 6.9141e-06</hent>
<hent key="0240010000000000000001400000000D" call="MPI_Isend" bytes="320" orank="13" region="0" commid="0" count="377" tid="0" op="" dtype="" >3.9744e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000001400000003D" call="MPI_Isend" bytes="320" orank="61" region="0" commid="0" count="366" tid="0" op="" dtype="" >1.6282e-03 3.8147e-06 1.0967e-05</hent>
<hent key="02400100000000000000014000000045" call="MPI_Isend" bytes="320" orank="69" region="0" commid="0" count="182" tid="0" op="" dtype="" >7.1812e-04 2.8610e-06 5.0068e-06</hent>
<hent key="03800100000000000000200000000004" call="MPI_Irecv" bytes="8192" orank="4" region="0" commid="0" count="3199" tid="0" op="" dtype="" >1.0056e-03 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000200000000006" call="MPI_Irecv" bytes="8192" orank="6" region="0" commid="0" count="3515" tid="0" op="" dtype="" >4.3797e-04 0.0000e+00 3.0994e-06</hent>
<hent key="0380010000000000000020000000000D" call="MPI_Irecv" bytes="8192" orank="13" region="0" commid="0" count="12597" tid="0" op="" dtype="" >5.8551e-03 0.0000e+00 1.0014e-05</hent>
<hent key="0380010000000000000020000000003D" call="MPI_Irecv" bytes="8192" orank="61" region="0" commid="0" count="12480" tid="0" op="" dtype="" >1.3807e-03 0.0000e+00 5.9605e-06</hent>
<hent key="024001000000000000000140000002C5" call="MPI_Isend" bytes="320" orank="709" region="0" commid="0" count="186" tid="0" op="" dtype="" >7.4673e-04 2.8610e-06 9.7752e-06</hent>
<hent key="03800100000000000000000000000004" call="MPI_Irecv" bytes="0" orank="4" region="0" commid="0" count="234" tid="0" op="" dtype="" >7.8678e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000006" call="MPI_Irecv" bytes="0" orank="6" region="0" commid="0" count="265" tid="0" op="" dtype="" >1.0180e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000D" call="MPI_Irecv" bytes="0" orank="13" region="0" commid="0" count="279" tid="0" op="" dtype="" >6.1512e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000003D" call="MPI_Irecv" bytes="0" orank="61" region="0" commid="0" count="259" tid="0" op="" dtype="" >7.0810e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000045" call="MPI_Irecv" bytes="0" orank="69" region="0" commid="0" count="158" tid="0" op="" dtype="" >2.7180e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000045" call="MPI_Isend" bytes="16384" orank="69" region="0" commid="0" count="12699" tid="0" op="" dtype="" >8.9345e-02 4.0531e-06 8.7976e-05</hent>
<hent key="038001000000000000000000000002C5" call="MPI_Irecv" bytes="0" orank="709" region="0" commid="0" count="156" tid="0" op="" dtype="" >4.0293e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002C5" call="MPI_Isend" bytes="16384" orank="709" region="0" commid="0" count="12699" tid="0" op="" dtype="" >9.0019e-02 3.8147e-06 6.3896e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.2439e-04 0.0000e+00 2.7585e-04</hent>
<hent key="02400100000000000000200000000004" call="MPI_Isend" bytes="8192" orank="4" region="0" commid="0" count="3139" tid="0" op="" dtype="" >3.9947e-03 0.0000e+00 4.9114e-05</hent>
<hent key="02400100000000000000200000000006" call="MPI_Isend" bytes="8192" orank="6" region="0" commid="0" count="3027" tid="0" op="" dtype="" >6.0339e-03 0.0000e+00 7.2002e-05</hent>
<hent key="0240010000000000000020000000000D" call="MPI_Isend" bytes="8192" orank="13" region="0" commid="0" count="12672" tid="0" op="" dtype="" >7.5884e-03 0.0000e+00 6.1989e-05</hent>
<hent key="0240010000000000000020000000003D" call="MPI_Isend" bytes="8192" orank="61" region="0" commid="0" count="12510" tid="0" op="" dtype="" >7.8974e-02 3.8147e-06 2.2888e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.4877e-05 8.4877e-05 8.4877e-05</hent>
<hent key="02400100000000000000000000000004" call="MPI_Isend" bytes="0" orank="4" region="0" commid="0" count="265" tid="0" op="" dtype="" >1.9288e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000006" call="MPI_Isend" bytes="0" orank="6" region="0" commid="0" count="245" tid="0" op="" dtype="" >4.9710e-04 0.0000e+00 9.2030e-05</hent>
<hent key="0240010000000000000000000000000D" call="MPI_Isend" bytes="0" orank="13" region="0" commid="0" count="245" tid="0" op="" dtype="" >1.6499e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000000000000003D" call="MPI_Isend" bytes="0" orank="61" region="0" commid="0" count="246" tid="0" op="" dtype="" >9.4724e-04 1.9073e-06 1.0014e-05</hent>
<hent key="02400100000000000000000000000045" call="MPI_Isend" bytes="0" orank="69" region="0" commid="0" count="163" tid="0" op="" dtype="" >5.5695e-04 9.5367e-07 5.0068e-06</hent>
<hent key="024001000000000000000000000002C5" call="MPI_Isend" bytes="0" orank="709" region="0" commid="0" count="149" tid="0" op="" dtype="" >5.6243e-04 1.9073e-06 1.0014e-05</hent>
<hent key="03800100000000000000060000000004" call="MPI_Irecv" bytes="1536" orank="4" region="0" commid="0" count="90" tid="0" op="" dtype="" >3.6955e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000006" call="MPI_Irecv" bytes="1536" orank="6" region="0" commid="0" count="109" tid="0" op="" dtype="" >8.7023e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000006000000000D" call="MPI_Irecv" bytes="1536" orank="13" region="0" commid="0" count="76" tid="0" op="" dtype="" >1.6212e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000003D" call="MPI_Irecv" bytes="1536" orank="61" region="0" commid="0" count="97" tid="0" op="" dtype="" >4.9353e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000045" call="MPI_Irecv" bytes="1536" orank="69" region="0" commid="0" count="209" tid="0" op="" dtype="" >5.8174e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002C5" call="MPI_Irecv" bytes="1536" orank="709" region="0" commid="0" count="219" tid="0" op="" dtype="" >8.1062e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000004" call="MPI_Isend" bytes="1536" orank="4" region="0" commid="0" count="87" tid="0" op="" dtype="" >1.7881e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000006" call="MPI_Isend" bytes="1536" orank="6" region="0" commid="0" count="108" tid="0" op="" dtype="" >4.1747e-04 1.9073e-06 1.7881e-05</hent>
<hent key="0240010000000000000006000000000D" call="MPI_Isend" bytes="1536" orank="13" region="0" commid="0" count="87" tid="0" op="" dtype="" >1.6546e-04 9.5367e-07 7.1526e-06</hent>
<hent key="0240010000000000000006000000003D" call="MPI_Isend" bytes="1536" orank="61" region="0" commid="0" count="96" tid="0" op="" dtype="" >4.9520e-04 4.0531e-06 6.1989e-06</hent>
<hent key="02400100000000000000060000000045" call="MPI_Isend" bytes="1536" orank="69" region="0" commid="0" count="211" tid="0" op="" dtype="" >9.4676e-04 2.8610e-06 1.2159e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C5" call="MPI_Isend" bytes="1536" orank="709" region="0" commid="0" count="214" tid="0" op="" dtype="" >9.6273e-04 3.8147e-06 1.0014e-05</hent>
<hent key="038001000000000000000C0000000045" call="MPI_Irecv" bytes="3072" orank="69" region="0" commid="0" count="11" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C5" call="MPI_Irecv" bytes="3072" orank="709" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000045" call="MPI_Isend" bytes="3072" orank="69" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.7166e-05 5.0068e-06 6.1989e-06</hent>
<hent key="024001000000000000000C00000002C5" call="MPI_Isend" bytes="3072" orank="709" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.0054e-05 4.0531e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.2112e-04 1.2112e-04 1.2112e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.7394e-04 8.7976e-05 9.3937e-05</hent>
<hent key="03800100000000000000038000000004" call="MPI_Irecv" bytes="896" orank="4" region="0" commid="0" count="2885" tid="0" op="" dtype="" >3.7885e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000038000000006" call="MPI_Irecv" bytes="896" orank="6" region="0" commid="0" count="2761" tid="0" op="" dtype="" >7.4077e-04 0.0000e+00 4.7684e-06</hent>
<hent key="0380010000000000000003800000000D" call="MPI_Irecv" bytes="896" orank="13" region="0" commid="0" count="317" tid="0" op="" dtype="" >6.8188e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000003D" call="MPI_Irecv" bytes="896" orank="61" region="0" commid="0" count="328" tid="0" op="" dtype="" >1.6522e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000045" call="MPI_Irecv" bytes="896" orank="69" region="0" commid="0" count="350" tid="0" op="" dtype="" >9.8228e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002C5" call="MPI_Irecv" bytes="896" orank="709" region="0" commid="0" count="313" tid="0" op="" dtype="" >1.2898e-04 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.2983e-05 9.2983e-05 9.2983e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="02400100000000000000038000000004" call="MPI_Isend" bytes="896" orank="4" region="0" commid="0" count="2896" tid="0" op="" dtype="" >2.1813e-03 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000038000000006" call="MPI_Isend" bytes="896" orank="6" region="0" commid="0" count="2876" tid="0" op="" dtype="" >2.7587e-03 0.0000e+00 1.7166e-05</hent>
<hent key="0240010000000000000003800000000D" call="MPI_Isend" bytes="896" orank="13" region="0" commid="0" count="297" tid="0" op="" dtype="" >4.2367e-04 9.5367e-07 7.8678e-06</hent>
<hent key="0240010000000000000003800000003D" call="MPI_Isend" bytes="896" orank="61" region="0" commid="0" count="312" tid="0" op="" dtype="" >1.4935e-03 3.8147e-06 1.4067e-05</hent>
<hent key="02400100000000000000038000000045" call="MPI_Isend" bytes="896" orank="69" region="0" commid="0" count="328" tid="0" op="" dtype="" >1.3874e-03 2.8610e-06 1.0014e-05</hent>
<hent key="024001000000000000000380000002C5" call="MPI_Isend" bytes="896" orank="709" region="0" commid="0" count="341" tid="0" op="" dtype="" >1.4133e-03 2.8610e-06 9.0599e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.7764e+00 1.2875e-05 2.0312e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.2029e-03 3.2029e-03 3.2029e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0520e-02 1.0520e-02 1.0520e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.6617e-02 2.6617e-02 2.6617e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.8948e-01 3.0129e-03 1.6999e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >2.8110e-04 2.8110e-04 2.8110e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5902e+00 3.6812e-04 2.5123e-01</hent>
<hent key="03800100000000000000040000000004" call="MPI_Irecv" bytes="1024" orank="4" region="0" commid="0" count="844" tid="0" op="" dtype="" >8.6546e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000006" call="MPI_Irecv" bytes="1024" orank="6" region="0" commid="0" count="950" tid="0" op="" dtype="" >1.8167e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000004000000000D" call="MPI_Irecv" bytes="1024" orank="13" region="0" commid="0" count="3372" tid="0" op="" dtype="" >5.9748e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000004000000003D" call="MPI_Irecv" bytes="1024" orank="61" region="0" commid="0" count="3338" tid="0" op="" dtype="" >3.1257e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.6212e-05 0.0000e+00 9.0599e-06</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6955e-05 3.6955e-05 3.6955e-05</hent>
<hent key="03800100000000000000070000000004" call="MPI_Irecv" bytes="1792" orank="4" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.7405e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000006" call="MPI_Irecv" bytes="1792" orank="6" region="0" commid="0" count="34" tid="0" op="" dtype="" >3.1710e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000007000000000D" call="MPI_Irecv" bytes="1792" orank="13" region="0" commid="0" count="32" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000007000000003D" call="MPI_Irecv" bytes="1792" orank="61" region="0" commid="0" count="42" tid="0" op="" dtype="" >2.3603e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000045" call="MPI_Irecv" bytes="1792" orank="69" region="0" commid="0" count="127" tid="0" op="" dtype="" >4.5776e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002C5" call="MPI_Irecv" bytes="1792" orank="709" region="0" commid="0" count="137" tid="0" op="" dtype="" >7.0572e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >4.5776e-05 0.0000e+00 2.7895e-05</hent>
<hent key="02400100000000000000040000000004" call="MPI_Isend" bytes="1024" orank="4" region="0" commid="0" count="832" tid="0" op="" dtype="" >5.4336e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000040000000006" call="MPI_Isend" bytes="1024" orank="6" region="0" commid="0" count="826" tid="0" op="" dtype="" >6.1965e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0240010000000000000004000000000D" call="MPI_Isend" bytes="1024" orank="13" region="0" commid="0" count="3392" tid="0" op="" dtype="" >2.0106e-03 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000004000000003D" call="MPI_Isend" bytes="1024" orank="61" region="0" commid="0" count="3338" tid="0" op="" dtype="" >8.4651e-03 9.5367e-07 8.1062e-06</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >5.0068e-06 2.1458e-06 2.8610e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5592e+00 0.0000e+00 2.2731e+00</hent>
<hent key="038001000000000000000A0000000004" call="MPI_Irecv" bytes="2560" orank="4" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000006" call="MPI_Irecv" bytes="2560" orank="6" region="0" commid="0" count="4" tid="0" op="" dtype="" >4.0531e-06 9.5367e-07 1.1921e-06</hent>
<hent key="038001000000000000000A000000000D" call="MPI_Irecv" bytes="2560" orank="13" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000003D" call="MPI_Irecv" bytes="2560" orank="61" region="0" commid="0" count="9" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000045" call="MPI_Irecv" bytes="2560" orank="69" region="0" commid="0" count="46" tid="0" op="" dtype="" >1.4544e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002C5" call="MPI_Irecv" bytes="2560" orank="709" region="0" commid="0" count="34" tid="0" op="" dtype="" >2.0981e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000004" call="MPI_Isend" bytes="1792" orank="4" region="0" commid="0" count="33" tid="0" op="" dtype="" >7.1049e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000006" call="MPI_Isend" bytes="1792" orank="6" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.4997e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000007000000000D" call="MPI_Isend" bytes="1792" orank="13" region="0" commid="0" count="29" tid="0" op="" dtype="" >8.5831e-05 9.5367e-07 1.5020e-05</hent>
<hent key="0240010000000000000007000000003D" call="MPI_Isend" bytes="1792" orank="61" region="0" commid="0" count="55" tid="0" op="" dtype="" >2.8586e-04 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000070000000045" call="MPI_Isend" bytes="1792" orank="69" region="0" commid="0" count="129" tid="0" op="" dtype="" >5.7864e-04 3.8147e-06 1.0967e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1357e-01 1.1357e-01 1.1357e-01</hent>
<hent key="024001000000000000000700000002C5" call="MPI_Isend" bytes="1792" orank="709" region="0" commid="0" count="125" tid="0" op="" dtype="" >5.6052e-04 3.8147e-06 9.0599e-06</hent>
<hent key="024001000000000000000A0000000004" call="MPI_Isend" bytes="2560" orank="4" region="0" commid="0" count="9" tid="0" op="" dtype="" >2.0742e-05 9.5367e-07 3.0994e-06</hent>
<hent key="024001000000000000000A0000000006" call="MPI_Isend" bytes="2560" orank="6" region="0" commid="0" count="8" tid="0" op="" dtype="" >3.2902e-05 3.0994e-06 5.0068e-06</hent>
<hent key="024001000000000000000A000000000D" call="MPI_Isend" bytes="2560" orank="13" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.1008e-05 9.5367e-07 1.5020e-05</hent>
<hent key="024001000000000000000A000000003D" call="MPI_Isend" bytes="2560" orank="61" region="0" commid="0" count="13" tid="0" op="" dtype="" >7.0810e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000045" call="MPI_Isend" bytes="2560" orank="69" region="0" commid="0" count="31" tid="0" op="" dtype="" >1.5235e-04 4.0531e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.5426e-04 2.5988e-05 6.5088e-05</hent>
<hent key="024001000000000000000A00000002C5" call="MPI_Isend" bytes="2560" orank="709" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.5068e-04 4.0531e-06 1.1206e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >4.6992e-04 1.7309e-04 2.9683e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.8147e-05 3.8147e-05 3.8147e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.7220e-06 1.9073e-06 3.8147e-06</hent>
<hent key="03800100000000000000000400000004" call="MPI_Irecv" bytes="4" orank="4" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.9737e-04 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000000400000006" call="MPI_Irecv" bytes="4" orank="6" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.3893e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000000040000000D" call="MPI_Irecv" bytes="4" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.1836e-04 0.0000e+00 2.8610e-06</hent>
<hent key="0380010000000000000000040000003D" call="MPI_Irecv" bytes="4" orank="61" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.8184e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000045" call="MPI_Irecv" bytes="4" orank="69" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.4135e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >8.2898e-04 1.2398e-04 2.6703e-04</hent>
<hent key="038001000000000000001C0000000004" call="MPI_Irecv" bytes="7168" orank="4" region="0" commid="0" count="9500" tid="0" op="" dtype="" >2.9967e-03 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000001C0000000006" call="MPI_Irecv" bytes="7168" orank="6" region="0" commid="0" count="9184" tid="0" op="" dtype="" >1.2989e-03 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000004000002C5" call="MPI_Irecv" bytes="4" orank="709" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.9019e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000400000004" call="MPI_Isend" bytes="4" orank="4" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2145e-03 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000000400000006" call="MPI_Isend" bytes="4" orank="6" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.4159e-03 0.0000e+00 7.8917e-05</hent>
<hent key="0240010000000000000000040000000D" call="MPI_Isend" bytes="4" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2980e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000000040000003D" call="MPI_Isend" bytes="4" orank="61" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.9905e-02 3.8147e-06 1.1015e-04</hent>
<hent key="02400100000000000000000400000045" call="MPI_Isend" bytes="4" orank="69" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.8337e-02 3.8147e-06 1.3828e-05</hent>
<hent key="024001000000000000001C0000000004" call="MPI_Isend" bytes="7168" orank="4" region="0" commid="0" count="9560" tid="0" op="" dtype="" >2.1205e-02 9.5367e-07 9.0599e-06</hent>
<hent key="024001000000000000001C0000000006" call="MPI_Isend" bytes="7168" orank="6" region="0" commid="0" count="9672" tid="0" op="" dtype="" >2.9080e-02 9.5367e-07 1.5020e-05</hent>
<hent key="024001000000000000000004000002C5" call="MPI_Isend" bytes="4" orank="709" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7613e-02 3.0994e-06 6.8903e-05</hent>
<hent key="03800100000000000000050000000004" call="MPI_Irecv" bytes="1280" orank="4" region="0" commid="0" count="212" tid="0" op="" dtype="" >7.2956e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000006" call="MPI_Irecv" bytes="1280" orank="6" region="0" commid="0" count="207" tid="0" op="" dtype="" >1.6308e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000005000000000D" call="MPI_Irecv" bytes="1280" orank="13" region="0" commid="0" count="250" tid="0" op="" dtype="" >4.3154e-05 0.0000e+00 2.8610e-06</hent>
<hent key="0380010000000000000005000000003D" call="MPI_Irecv" bytes="1280" orank="61" region="0" commid="0" count="257" tid="0" op="" dtype="" >9.7275e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000045" call="MPI_Irecv" bytes="1280" orank="69" region="0" commid="0" count="281" tid="0" op="" dtype="" >7.8678e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0953e+01 7.8678e-06 1.5562e-01</hent>
<hent key="0380010000000000000028000000000D" call="MPI_Irecv" bytes="10240" orank="13" region="0" commid="0" count="102" tid="0" op="" dtype="" >2.5511e-05 0.0000e+00 3.0994e-06</hent>
<hent key="0380010000000000000028000000003D" call="MPI_Irecv" bytes="10240" orank="61" region="0" commid="0" count="219" tid="0" op="" dtype="" >1.8835e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002C5" call="MPI_Irecv" bytes="1280" orank="709" region="0" commid="0" count="292" tid="0" op="" dtype="" >1.3757e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000080000000004" call="MPI_Irecv" bytes="2048" orank="4" region="0" commid="0" count="14" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000006" call="MPI_Irecv" bytes="2048" orank="6" region="0" commid="0" count="11" tid="0" op="" dtype="" >9.7752e-06 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000008000000000D" call="MPI_Irecv" bytes="2048" orank="13" region="0" commid="0" count="14" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000003D" call="MPI_Irecv" bytes="2048" orank="61" region="0" commid="0" count="21" tid="0" op="" dtype="" >7.6294e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000045" call="MPI_Irecv" bytes="2048" orank="69" region="0" commid="0" count="3455" tid="0" op="" dtype="" >5.4264e-04 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000800000002C5" call="MPI_Irecv" bytes="2048" orank="709" region="0" commid="0" count="3461" tid="0" op="" dtype="" >5.5003e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000050000000004" call="MPI_Isend" bytes="1280" orank="4" region="0" commid="0" count="209" tid="0" op="" dtype="" >4.0293e-04 9.5367e-07 6.1989e-06</hent>
<hent key="02400100000000000000050000000006" call="MPI_Isend" bytes="1280" orank="6" region="0" commid="0" count="219" tid="0" op="" dtype="" >7.8869e-04 1.9073e-06 8.1062e-06</hent>
<hent key="0240010000000000000005000000000D" call="MPI_Isend" bytes="1280" orank="13" region="0" commid="0" count="206" tid="0" op="" dtype="" >3.1757e-04 0.0000e+00 6.1989e-06</hent>
<hent key="0240010000000000000005000000003D" call="MPI_Isend" bytes="1280" orank="61" region="0" commid="0" count="266" tid="0" op="" dtype="" >1.1837e-03 9.5367e-07 1.0014e-05</hent>
<hent key="02400100000000000000050000000045" call="MPI_Isend" bytes="1280" orank="69" region="0" commid="0" count="310" tid="0" op="" dtype="" >1.3337e-03 2.8610e-06 2.0027e-05</hent>
<hent key="0240010000000000000028000000000D" call="MPI_Isend" bytes="10240" orank="13" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.5020e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000028000000003D" call="MPI_Isend" bytes="10240" orank="61" region="0" commid="0" count="189" tid="0" op="" dtype="" >1.2045e-03 4.0531e-06 1.0967e-05</hent>
<hent key="024001000000000000000500000002C5" call="MPI_Isend" bytes="1280" orank="709" region="0" commid="0" count="289" tid="0" op="" dtype="" >1.2414e-03 2.8610e-06 9.0599e-06</hent>
</hash>
<internal rank="5" log_i="1723712895.693828" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="6" mpi_size="768" stamp_init="1723712829.548115" stamp_final="1723712895.683207" username="apac4" allocationname="unknown" flags="0" pid="684274" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61351e+01" utime="4.75249e+01" stime="8.76412e+00" mtime="2.95308e+01" gflop="0.00000e+00" gbyte="3.77136e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.95308e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009c149c14f9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.59998e+01" utime="4.74922e+01" stime="8.75463e+00" mtime="2.95308e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.95308e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4946e+08" > 8.0515e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4833e+08" > 6.0853e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8458e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5648e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.8248e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4022e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5899e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0616e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0432e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1380e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7532e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="194" >
<hent key="02400100000000000000080000000005" call="MPI_Isend" bytes="2048" orank="5" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.3842e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000007" call="MPI_Isend" bytes="2048" orank="7" region="0" commid="0" count="12" tid="0" op="" dtype="" >5.1498e-05 3.0994e-06 5.0068e-06</hent>
<hent key="0240010000000000000008000000000E" call="MPI_Isend" bytes="2048" orank="14" region="0" commid="0" count="9" tid="0" op="" dtype="" >4.3869e-05 1.9073e-06 1.5974e-05</hent>
<hent key="0240010000000000000008000000003E" call="MPI_Isend" bytes="2048" orank="62" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.3208e-04 5.0068e-06 9.0599e-06</hent>
<hent key="02400100000000000000080000000046" call="MPI_Isend" bytes="2048" orank="70" region="0" commid="0" count="3469" tid="0" op="" dtype="" >2.2950e-02 9.5367e-07 1.6189e-04</hent>
<hent key="024001000000000000000800000002C6" call="MPI_Isend" bytes="2048" orank="710" region="0" commid="0" count="3447" tid="0" op="" dtype="" >1.6964e-02 9.5367e-07 1.1611e-04</hent>
<hent key="038001000000000000000E00000002C6" call="MPI_Irecv" bytes="3584" orank="710" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000E0000000046" call="MPI_Isend" bytes="3584" orank="70" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000005" call="MPI_Irecv" bytes="640" orank="5" region="0" commid="0" count="409" tid="0" op="" dtype="" >1.6832e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000007" call="MPI_Irecv" bytes="640" orank="7" region="0" commid="0" count="415" tid="0" op="" dtype="" >1.1516e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000E" call="MPI_Irecv" bytes="640" orank="14" region="0" commid="0" count="406" tid="0" op="" dtype="" >1.1492e-04 0.0000e+00 5.0068e-06</hent>
<hent key="024001000000000000000E00000002C6" call="MPI_Isend" bytes="3584" orank="710" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0380010000000000000002800000003E" call="MPI_Irecv" bytes="640" orank="62" region="0" commid="0" count="385" tid="0" op="" dtype="" >1.7595e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000028000000046" call="MPI_Irecv" bytes="640" orank="70" region="0" commid="0" count="285" tid="0" op="" dtype="" >1.1611e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="038001000000000000000280000002C6" call="MPI_Irecv" bytes="640" orank="710" region="0" commid="0" count="279" tid="0" op="" dtype="" >9.9897e-05 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000028000000005" call="MPI_Isend" bytes="640" orank="5" region="0" commid="0" count="406" tid="0" op="" dtype="" >5.7554e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000028000000007" call="MPI_Isend" bytes="640" orank="7" region="0" commid="0" count="400" tid="0" op="" dtype="" >1.2882e-03 1.9073e-06 8.1062e-06</hent>
<hent key="0240010000000000000002800000000E" call="MPI_Isend" bytes="640" orank="14" region="0" commid="0" count="412" tid="0" op="" dtype="" >5.7602e-04 9.5367e-07 7.8678e-06</hent>
<hent key="0240010000000000000002800000003E" call="MPI_Isend" bytes="640" orank="62" region="0" commid="0" count="394" tid="0" op="" dtype="" >2.0373e-03 3.8147e-06 1.4782e-05</hent>
<hent key="02400100000000000000028000000046" call="MPI_Isend" bytes="640" orank="70" region="0" commid="0" count="292" tid="0" op="" dtype="" >1.5633e-03 2.8610e-06 2.8849e-05</hent>
<hent key="024001000000000000000280000002C6" call="MPI_Isend" bytes="640" orank="710" region="0" commid="0" count="267" tid="0" op="" dtype="" >1.3070e-03 2.8610e-06 1.5974e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.3140e-05 3.3140e-05 3.3140e-05</hent>
<hent key="03800100000000000000014000000005" call="MPI_Irecv" bytes="320" orank="5" region="0" commid="0" count="357" tid="0" op="" dtype="" >1.3828e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000007" call="MPI_Irecv" bytes="320" orank="7" region="0" commid="0" count="329" tid="0" op="" dtype="" >9.3937e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000E" call="MPI_Irecv" bytes="320" orank="14" region="0" commid="0" count="375" tid="0" op="" dtype="" >9.4891e-05 0.0000e+00 7.1526e-06</hent>
<hent key="0380010000000000000001400000003E" call="MPI_Irecv" bytes="320" orank="62" region="0" commid="0" count="373" tid="0" op="" dtype="" >1.8430e-04 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000014000000046" call="MPI_Irecv" bytes="320" orank="70" region="0" commid="0" count="184" tid="0" op="" dtype="" >7.0572e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002C6" call="MPI_Irecv" bytes="320" orank="710" region="0" commid="0" count="178" tid="0" op="" dtype="" >6.0797e-05 0.0000e+00 7.1526e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >4.7684e-06 0.0000e+00 2.8610e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.8458e+00 0.0000e+00 1.8260e-01</hent>
<hent key="03800100000000000000400000000046" call="MPI_Irecv" bytes="16384" orank="70" region="0" commid="0" count="12699" tid="0" op="" dtype="" >6.7437e-03 0.0000e+00 4.6968e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6472e-02 9.5367e-07 1.6455e-02</hent>
<hent key="038001000000000000004000000002C6" call="MPI_Irecv" bytes="16384" orank="710" region="0" commid="0" count="12639" tid="0" op="" dtype="" >8.1537e-03 0.0000e+00 1.4400e-04</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.7704e-04 0.0000e+00 2.7704e-04</hent>
<hent key="02400100000000000000014000000005" call="MPI_Isend" bytes="320" orank="5" region="0" commid="0" count="350" tid="0" op="" dtype="" >3.9506e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000007" call="MPI_Isend" bytes="320" orank="7" region="0" commid="0" count="339" tid="0" op="" dtype="" >9.8324e-04 1.9073e-06 8.1062e-06</hent>
<hent key="0240010000000000000001400000000E" call="MPI_Isend" bytes="320" orank="14" region="0" commid="0" count="364" tid="0" op="" dtype="" >4.2224e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000001400000003E" call="MPI_Isend" bytes="320" orank="62" region="0" commid="0" count="351" tid="0" op="" dtype="" >1.6906e-03 3.8147e-06 1.1921e-05</hent>
<hent key="02400100000000000000014000000046" call="MPI_Isend" bytes="320" orank="70" region="0" commid="0" count="198" tid="0" op="" dtype="" >1.0958e-03 2.8610e-06 2.5034e-05</hent>
<hent key="03800100000000000000200000000005" call="MPI_Irecv" bytes="8192" orank="5" region="0" commid="0" count="3027" tid="0" op="" dtype="" >1.2171e-03 0.0000e+00 1.7881e-05</hent>
<hent key="03800100000000000000200000000007" call="MPI_Irecv" bytes="8192" orank="7" region="0" commid="0" count="3061" tid="0" op="" dtype="" >8.5044e-04 0.0000e+00 6.8903e-05</hent>
<hent key="0380010000000000000020000000000E" call="MPI_Irecv" bytes="8192" orank="14" region="0" commid="0" count="12501" tid="0" op="" dtype="" >1.9751e-02 0.0000e+00 1.6093e-04</hent>
<hent key="0380010000000000000020000000003E" call="MPI_Irecv" bytes="8192" orank="62" region="0" commid="0" count="12670" tid="0" op="" dtype="" >2.2316e-03 0.0000e+00 2.0981e-05</hent>
<hent key="024001000000000000000140000002C6" call="MPI_Isend" bytes="320" orank="710" region="0" commid="0" count="178" tid="0" op="" dtype="" >8.7738e-04 2.8610e-06 2.5034e-05</hent>
<hent key="03800100000000000000000000000005" call="MPI_Irecv" bytes="0" orank="5" region="0" commid="0" count="245" tid="0" op="" dtype="" >8.1301e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000000000000007" call="MPI_Irecv" bytes="0" orank="7" region="0" commid="0" count="259" tid="0" op="" dtype="" >5.7220e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000E" call="MPI_Irecv" bytes="0" orank="14" region="0" commid="0" count="267" tid="0" op="" dtype="" >7.2956e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000000000003E" call="MPI_Irecv" bytes="0" orank="62" region="0" commid="0" count="271" tid="0" op="" dtype="" >9.9182e-05 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000000000000046" call="MPI_Irecv" bytes="0" orank="70" region="0" commid="0" count="142" tid="0" op="" dtype="" >3.9816e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000046" call="MPI_Isend" bytes="16384" orank="70" region="0" commid="0" count="12699" tid="0" op="" dtype="" >2.1035e-01 3.8147e-06 1.8716e-04</hent>
<hent key="038001000000000000000000000002C6" call="MPI_Irecv" bytes="0" orank="710" region="0" commid="0" count="151" tid="0" op="" dtype="" >4.3631e-05 0.0000e+00 5.0068e-06</hent>
<hent key="024001000000000000004000000002C6" call="MPI_Isend" bytes="16384" orank="710" region="0" commid="0" count="12576" tid="0" op="" dtype="" >1.3060e-01 2.8610e-06 1.2493e-04</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.2915e-04 0.0000e+00 2.7490e-04</hent>
<hent key="02400100000000000000200000000005" call="MPI_Isend" bytes="8192" orank="5" region="0" commid="0" count="3515" tid="0" op="" dtype="" >4.6980e-03 0.0000e+00 2.0027e-05</hent>
<hent key="02400100000000000000200000000007" call="MPI_Isend" bytes="8192" orank="7" region="0" commid="0" count="4202" tid="0" op="" dtype="" >9.0411e-03 0.0000e+00 6.8188e-05</hent>
<hent key="0240010000000000000020000000000E" call="MPI_Isend" bytes="8192" orank="14" region="0" commid="0" count="12691" tid="0" op="" dtype="" >1.1237e-02 0.0000e+00 1.1802e-04</hent>
<hent key="0240010000000000000020000000003E" call="MPI_Isend" bytes="8192" orank="62" region="0" commid="0" count="12699" tid="0" op="" dtype="" >2.0246e-01 3.8147e-06 1.9717e-04</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.6069e-05 8.6069e-05 8.6069e-05</hent>
<hent key="02400100000000000000000000000005" call="MPI_Isend" bytes="0" orank="5" region="0" commid="0" count="265" tid="0" op="" dtype="" >1.9002e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000000000007" call="MPI_Isend" bytes="0" orank="7" region="0" commid="0" count="263" tid="0" op="" dtype="" >5.2333e-04 9.5367e-07 3.6001e-05</hent>
<hent key="0240010000000000000000000000000E" call="MPI_Isend" bytes="0" orank="14" region="0" commid="0" count="254" tid="0" op="" dtype="" >2.0790e-04 0.0000e+00 1.1206e-05</hent>
<hent key="0240010000000000000000000000003E" call="MPI_Isend" bytes="0" orank="62" region="0" commid="0" count="275" tid="0" op="" dtype="" >1.1842e-03 1.9073e-06 1.7881e-05</hent>
<hent key="02400100000000000000000000000046" call="MPI_Isend" bytes="0" orank="70" region="0" commid="0" count="148" tid="0" op="" dtype="" >7.0119e-04 9.5367e-07 1.7166e-05</hent>
<hent key="024001000000000000000000000002C6" call="MPI_Isend" bytes="0" orank="710" region="0" commid="0" count="152" tid="0" op="" dtype="" >6.6209e-04 9.5367e-07 2.2888e-05</hent>
<hent key="03800100000000000000060000000005" call="MPI_Irecv" bytes="1536" orank="5" region="0" commid="0" count="108" tid="0" op="" dtype="" >3.2187e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000007" call="MPI_Irecv" bytes="1536" orank="7" region="0" commid="0" count="87" tid="0" op="" dtype="" >2.9087e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000006000000000E" call="MPI_Irecv" bytes="1536" orank="14" region="0" commid="0" count="97" tid="0" op="" dtype="" >4.1485e-05 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000006000000003E" call="MPI_Irecv" bytes="1536" orank="62" region="0" commid="0" count="89" tid="0" op="" dtype="" >4.1008e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000046" call="MPI_Irecv" bytes="1536" orank="70" region="0" commid="0" count="213" tid="0" op="" dtype="" >1.1420e-04 0.0000e+00 6.9141e-06</hent>
<hent key="038001000000000000000600000002C6" call="MPI_Irecv" bytes="1536" orank="710" region="0" commid="0" count="207" tid="0" op="" dtype="" >7.6532e-05 0.0000e+00 8.1062e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000005" call="MPI_Isend" bytes="1536" orank="5" region="0" commid="0" count="109" tid="0" op="" dtype="" >2.1958e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000007" call="MPI_Isend" bytes="1536" orank="7" region="0" commid="0" count="89" tid="0" op="" dtype="" >3.4595e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000006000000000E" call="MPI_Isend" bytes="1536" orank="14" region="0" commid="0" count="93" tid="0" op="" dtype="" >1.9598e-04 9.5367e-07 2.3127e-05</hent>
<hent key="0240010000000000000006000000003E" call="MPI_Isend" bytes="1536" orank="62" region="0" commid="0" count="89" tid="0" op="" dtype="" >5.0855e-04 3.8147e-06 1.5974e-05</hent>
<hent key="02400100000000000000060000000046" call="MPI_Isend" bytes="1536" orank="70" region="0" commid="0" count="222" tid="0" op="" dtype="" >1.3189e-03 3.8147e-06 2.1935e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C6" call="MPI_Isend" bytes="1536" orank="710" region="0" commid="0" count="214" tid="0" op="" dtype="" >1.1201e-03 3.0994e-06 1.6212e-05</hent>
<hent key="038001000000000000000C0000000007" call="MPI_Irecv" bytes="3072" orank="7" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000C0000000046" call="MPI_Irecv" bytes="3072" orank="70" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C6" call="MPI_Irecv" bytes="3072" orank="710" region="0" commid="0" count="9" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000C0000000046" call="MPI_Isend" bytes="3072" orank="70" region="0" commid="0" count="6" tid="0" op="" dtype="" >6.0797e-05 5.0068e-06 2.2173e-05</hent>
<hent key="024001000000000000000C00000002C6" call="MPI_Isend" bytes="3072" orank="710" region="0" commid="0" count="9" tid="0" op="" dtype="" >4.9829e-05 4.7684e-06 9.0599e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.3590e-04 1.3590e-04 1.3590e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.9397e-04 9.6083e-05 1.0085e-04</hent>
<hent key="03800100000000000000038000000005" call="MPI_Irecv" bytes="896" orank="5" region="0" commid="0" count="2876" tid="0" op="" dtype="" >3.9721e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000038000000007" call="MPI_Irecv" bytes="896" orank="7" region="0" commid="0" count="2916" tid="0" op="" dtype="" >7.7820e-04 0.0000e+00 1.5974e-05</hent>
<hent key="0380010000000000000003800000000E" call="MPI_Irecv" bytes="896" orank="14" region="0" commid="0" count="281" tid="0" op="" dtype="" >6.5565e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000003800000003E" call="MPI_Irecv" bytes="896" orank="62" region="0" commid="0" count="318" tid="0" op="" dtype="" >1.7285e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000038000000046" call="MPI_Irecv" bytes="896" orank="70" region="0" commid="0" count="326" tid="0" op="" dtype="" >1.5283e-04 0.0000e+00 1.1921e-05</hent>
<hent key="038001000000000000000380000002C6" call="MPI_Irecv" bytes="896" orank="710" region="0" commid="0" count="339" tid="0" op="" dtype="" >1.3828e-04 0.0000e+00 8.8215e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >7.6056e-05 7.6056e-05 7.6056e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="038001000000000000003800000002C6" call="MPI_Irecv" bytes="14336" orank="710" region="0" commid="0" count="60" tid="0" op="" dtype="" >3.6955e-05 0.0000e+00 1.2159e-05</hent>
<hent key="02400100000000000000038000000005" call="MPI_Isend" bytes="896" orank="5" region="0" commid="0" count="2761" tid="0" op="" dtype="" >2.1455e-03 0.0000e+00 1.8835e-05</hent>
<hent key="02400100000000000000038000000007" call="MPI_Isend" bytes="896" orank="7" region="0" commid="0" count="2598" tid="0" op="" dtype="" >2.8429e-03 0.0000e+00 1.5020e-05</hent>
<hent key="0240010000000000000003800000000E" call="MPI_Isend" bytes="896" orank="14" region="0" commid="0" count="336" tid="0" op="" dtype="" >5.5456e-04 9.5367e-07 8.8215e-06</hent>
<hent key="0240010000000000000003800000003E" call="MPI_Isend" bytes="896" orank="62" region="0" commid="0" count="328" tid="0" op="" dtype="" >1.7247e-03 3.8147e-06 1.2159e-05</hent>
<hent key="02400100000000000000038000000046" call="MPI_Isend" bytes="896" orank="70" region="0" commid="0" count="302" tid="0" op="" dtype="" >1.7662e-03 2.8610e-06 3.7909e-05</hent>
<hent key="024001000000000000000380000002C6" call="MPI_Isend" bytes="896" orank="710" region="0" commid="0" count="319" tid="0" op="" dtype="" >1.5645e-03 3.0994e-06 1.5974e-05</hent>
<hent key="024001000000000000003800000002C6" call="MPI_Isend" bytes="14336" orank="710" region="0" commid="0" count="123" tid="0" op="" dtype="" >1.1275e-03 4.0531e-06 3.7909e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.0848e+00 2.5988e-05 2.0308e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.1970e-03 3.1970e-03 3.1970e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0432e-02 1.0432e-02 1.0432e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.6120e-02 2.6120e-02 2.6120e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.9841e-01 1.7631e-03 1.8017e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.7313e-04 3.7313e-04 3.7313e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5899e+00 4.1604e-04 2.5120e-01</hent>
<hent key="03800100000000000000040000000005" call="MPI_Irecv" bytes="1024" orank="5" region="0" commid="0" count="826" tid="0" op="" dtype="" >8.8692e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000007" call="MPI_Irecv" bytes="1024" orank="7" region="0" commid="0" count="806" tid="0" op="" dtype="" >1.7691e-04 0.0000e+00 1.3113e-05</hent>
<hent key="0380010000000000000004000000000E" call="MPI_Irecv" bytes="1024" orank="14" region="0" commid="0" count="3344" tid="0" op="" dtype="" >1.1816e-03 0.0000e+00 5.7936e-05</hent>
<hent key="0380010000000000000004000000003E" call="MPI_Irecv" bytes="1024" orank="62" region="0" commid="0" count="3392" tid="0" op="" dtype="" >4.6778e-04 0.0000e+00 2.0981e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >5.8248e-03 0.0000e+00 4.8399e-03</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000070000000005" call="MPI_Irecv" bytes="1792" orank="5" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.7166e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000007" call="MPI_Irecv" bytes="1792" orank="7" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.4305e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000007000000000E" call="MPI_Irecv" bytes="1792" orank="14" region="0" commid="0" count="50" tid="0" op="" dtype="" >1.2875e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000003E" call="MPI_Irecv" bytes="1792" orank="62" region="0" commid="0" count="58" tid="0" op="" dtype="" >3.2187e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000046" call="MPI_Irecv" bytes="1792" orank="70" region="0" commid="0" count="132" tid="0" op="" dtype="" >6.3419e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000700000002C6" call="MPI_Irecv" bytes="1792" orank="710" region="0" commid="0" count="133" tid="0" op="" dtype="" >3.2902e-05 0.0000e+00 2.8610e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >3.9816e-05 0.0000e+00 1.7166e-05</hent>
<hent key="02400100000000000000040000000005" call="MPI_Isend" bytes="1024" orank="5" region="0" commid="0" count="950" tid="0" op="" dtype="" >6.3872e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000040000000007" call="MPI_Isend" bytes="1024" orank="7" region="0" commid="0" count="1134" tid="0" op="" dtype="" >9.2101e-04 0.0000e+00 1.4067e-05</hent>
<hent key="0240010000000000000004000000000E" call="MPI_Isend" bytes="1024" orank="14" region="0" commid="0" count="3396" tid="0" op="" dtype="" >2.6944e-03 0.0000e+00 7.7009e-05</hent>
<hent key="0240010000000000000004000000003E" call="MPI_Isend" bytes="1024" orank="62" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6430e-02 9.5367e-07 1.4210e-04</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5476e+00 0.0000e+00 2.2673e+00</hent>
<hent key="038001000000000000000A0000000005" call="MPI_Irecv" bytes="2560" orank="5" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000007" call="MPI_Irecv" bytes="2560" orank="7" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000000E" call="MPI_Irecv" bytes="2560" orank="14" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A000000003E" call="MPI_Irecv" bytes="2560" orank="62" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000046" call="MPI_Irecv" bytes="2560" orank="70" region="0" commid="0" count="45" tid="0" op="" dtype="" >3.8147e-05 0.0000e+00 7.8678e-06</hent>
<hent key="038001000000000000000A00000002C6" call="MPI_Irecv" bytes="2560" orank="710" region="0" commid="0" count="48" tid="0" op="" dtype="" >2.8133e-05 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000070000000005" call="MPI_Isend" bytes="1792" orank="5" region="0" commid="0" count="34" tid="0" op="" dtype="" >7.5817e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000007" call="MPI_Isend" bytes="1792" orank="7" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.8501e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000007000000000E" call="MPI_Isend" bytes="1792" orank="14" region="0" commid="0" count="42" tid="0" op="" dtype="" >1.4043e-04 9.5367e-07 1.3113e-05</hent>
<hent key="0240010000000000000007000000003E" call="MPI_Isend" bytes="1792" orank="62" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.5940e-04 3.8147e-06 1.7166e-05</hent>
<hent key="02400100000000000000070000000046" call="MPI_Isend" bytes="1792" orank="70" region="0" commid="0" count="135" tid="0" op="" dtype="" >7.7629e-04 3.8147e-06 2.9802e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1380e-01 1.1380e-01 1.1380e-01</hent>
<hent key="024001000000000000000700000002C6" call="MPI_Isend" bytes="1792" orank="710" region="0" commid="0" count="158" tid="0" op="" dtype="" >8.0729e-04 9.5367e-07 3.0994e-05</hent>
<hent key="024001000000000000000A0000000005" call="MPI_Isend" bytes="2560" orank="5" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.0599e-06 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000007" call="MPI_Isend" bytes="2560" orank="7" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.9339e-05 4.0531e-06 5.0068e-06</hent>
<hent key="024001000000000000000A000000000E" call="MPI_Isend" bytes="2560" orank="14" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.2411e-05 1.9073e-06 1.0967e-05</hent>
<hent key="024001000000000000000A000000003E" call="MPI_Isend" bytes="2560" orank="62" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.8876e-05 5.0068e-06 1.0014e-05</hent>
<hent key="024001000000000000000A0000000046" call="MPI_Isend" bytes="2560" orank="70" region="0" commid="0" count="44" tid="0" op="" dtype="" >2.7561e-04 4.0531e-06 1.0967e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.5521e-04 2.1935e-05 7.5102e-05</hent>
<hent key="024001000000000000000A00000002C6" call="MPI_Isend" bytes="2560" orank="710" region="0" commid="0" count="52" tid="0" op="" dtype="" >2.8920e-04 4.0531e-06 1.3113e-05</hent>
<hent key="02400100000000000000100000000046" call="MPI_Isend" bytes="4096" orank="70" region="0" commid="0" count="1" tid="0" op="" dtype="" >7.1526e-06 7.1526e-06 7.1526e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.2500e-04 1.8787e-04 3.3712e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.1035e-05 6.1035e-05 6.1035e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.0068e-06 2.1458e-06 2.8610e-06</hent>
<hent key="03800100000000000000000400000005" call="MPI_Irecv" bytes="4" orank="5" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1754e-03 0.0000e+00 1.6928e-05</hent>
<hent key="03800100000000000000000400000007" call="MPI_Irecv" bytes="4" orank="7" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.7411e-04 0.0000e+00 2.0027e-05</hent>
<hent key="0380010000000000000000040000000E" call="MPI_Irecv" bytes="4" orank="14" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2274e-03 0.0000e+00 2.0981e-05</hent>
<hent key="0380010000000000000000040000003E" call="MPI_Irecv" bytes="4" orank="62" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.0419e-04 0.0000e+00 1.8120e-05</hent>
<hent key="03800100000000000000000400000046" call="MPI_Irecv" bytes="4" orank="70" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2577e-03 0.0000e+00 3.0994e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.4676e-04 1.2898e-04 3.1996e-04</hent>
<hent key="038001000000000000001C0000000005" call="MPI_Irecv" bytes="7168" orank="5" region="0" commid="0" count="9672" tid="0" op="" dtype="" >3.8071e-03 0.0000e+00 1.4067e-05</hent>
<hent key="038001000000000000001C0000000007" call="MPI_Irecv" bytes="7168" orank="7" region="0" commid="0" count="9638" tid="0" op="" dtype="" >2.4507e-03 0.0000e+00 7.2002e-05</hent>
<hent key="038001000000000000000004000002C6" call="MPI_Irecv" bytes="4" orank="710" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3759e-03 0.0000e+00 5.8889e-05</hent>
<hent key="02400100000000000000000400000005" call="MPI_Isend" bytes="4" orank="5" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4105e-03 0.0000e+00 6.1989e-05</hent>
<hent key="02400100000000000000000400000007" call="MPI_Isend" bytes="4" orank="7" region="0" commid="0" count="3398" tid="0" op="" dtype="" >3.0262e-03 0.0000e+00 1.1921e-05</hent>
<hent key="0240010000000000000000040000000E" call="MPI_Isend" bytes="4" orank="14" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.5202e-03 0.0000e+00 1.4067e-05</hent>
<hent key="0240010000000000000000040000003E" call="MPI_Isend" bytes="4" orank="62" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.6445e-02 3.8147e-06 1.1992e-04</hent>
<hent key="02400100000000000000000400000046" call="MPI_Isend" bytes="4" orank="70" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.8738e-02 3.8147e-06 2.0695e-04</hent>
<hent key="024001000000000000001C0000000005" call="MPI_Isend" bytes="7168" orank="5" region="0" commid="0" count="9184" tid="0" op="" dtype="" >2.1551e-02 9.5367e-07 6.2943e-05</hent>
<hent key="024001000000000000001C0000000007" call="MPI_Isend" bytes="7168" orank="7" region="0" commid="0" count="8497" tid="0" op="" dtype="" >2.8648e-02 9.5367e-07 1.6928e-05</hent>
<hent key="024001000000000000000004000002C6" call="MPI_Isend" bytes="4" orank="710" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.4041e-02 3.8147e-06 8.2016e-05</hent>
<hent key="03800100000000000000050000000005" call="MPI_Irecv" bytes="1280" orank="5" region="0" commid="0" count="219" tid="0" op="" dtype="" >7.5579e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000007" call="MPI_Irecv" bytes="1280" orank="7" region="0" commid="0" count="221" tid="0" op="" dtype="" >6.8188e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000005000000000E" call="MPI_Irecv" bytes="1280" orank="14" region="0" commid="0" count="262" tid="0" op="" dtype="" >8.4639e-05 0.0000e+00 7.8678e-06</hent>
<hent key="0380010000000000000005000000003E" call="MPI_Irecv" bytes="1280" orank="62" region="0" commid="0" count="183" tid="0" op="" dtype="" >8.7500e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000046" call="MPI_Irecv" bytes="1280" orank="70" region="0" commid="0" count="300" tid="0" op="" dtype="" >1.3423e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0219e+01 6.9141e-06 1.5549e-01</hent>
<hent key="0380010000000000000028000000000E" call="MPI_Irecv" bytes="10240" orank="14" region="0" commid="0" count="198" tid="0" op="" dtype="" >7.7963e-05 0.0000e+00 1.8835e-05</hent>
<hent key="0380010000000000000028000000003E" call="MPI_Irecv" bytes="10240" orank="62" region="0" commid="0" count="29" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000500000002C6" call="MPI_Irecv" bytes="1280" orank="710" region="0" commid="0" count="298" tid="0" op="" dtype="" >9.7752e-05 0.0000e+00 8.8215e-06</hent>
<hent key="03800100000000000000080000000005" call="MPI_Irecv" bytes="2048" orank="5" region="0" commid="0" count="10" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000007" call="MPI_Irecv" bytes="2048" orank="7" region="0" commid="0" count="11" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000008000000000E" call="MPI_Irecv" bytes="2048" orank="14" region="0" commid="0" count="12" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000003E" call="MPI_Irecv" bytes="2048" orank="62" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.1206e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000046" call="MPI_Irecv" bytes="2048" orank="70" region="0" commid="0" count="3464" tid="0" op="" dtype="" >1.0576e-03 0.0000e+00 3.0994e-05</hent>
<hent key="038001000000000000000800000002C6" call="MPI_Irecv" bytes="2048" orank="710" region="0" commid="0" count="3451" tid="0" op="" dtype="" >1.4522e-03 0.0000e+00 9.1076e-05</hent>
<hent key="02400100000000000000050000000005" call="MPI_Isend" bytes="1280" orank="5" region="0" commid="0" count="207" tid="0" op="" dtype="" >3.9649e-04 9.5367e-07 7.1526e-06</hent>
<hent key="02400100000000000000050000000007" call="MPI_Isend" bytes="1280" orank="7" region="0" commid="0" count="206" tid="0" op="" dtype="" >7.8583e-04 2.8610e-06 2.5034e-05</hent>
<hent key="0240010000000000000005000000000E" call="MPI_Isend" bytes="1280" orank="14" region="0" commid="0" count="184" tid="0" op="" dtype="" >3.9744e-04 0.0000e+00 2.4080e-05</hent>
<hent key="0240010000000000000005000000003E" call="MPI_Isend" bytes="1280" orank="62" region="0" commid="0" count="189" tid="0" op="" dtype="" >1.0464e-03 3.8147e-06 1.9073e-05</hent>
<hent key="02400100000000000000050000000046" call="MPI_Isend" bytes="1280" orank="70" region="0" commid="0" count="279" tid="0" op="" dtype="" >1.5366e-03 2.8610e-06 3.1233e-05</hent>
<hent key="0240010000000000000028000000000E" call="MPI_Isend" bytes="10240" orank="14" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000500000002C6" call="MPI_Isend" bytes="1280" orank="710" region="0" commid="0" count="300" tid="0" op="" dtype="" >1.5936e-03 3.8147e-06 2.0027e-05</hent>
</hash>
<internal rank="6" log_i="1723712895.683207" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="7" mpi_size="768" stamp_init="1723712829.548160" stamp_final="1723712895.684138" username="apac4" allocationname="unknown" flags="0" pid="684275" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61360e+01" utime="5.12755e+01" stime="7.39827e+00" mtime="3.02475e+01" gflop="0.00000e+00" gbyte="3.77785e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.02475e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001215111535" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60031e+01" utime="5.12415e+01" stime="7.38961e+00" mtime="3.02475e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.02475e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4709e+08" > 4.7516e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4964e+08" > 2.9104e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.6387e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5710e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.9870e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5898e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1064e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0488e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1356e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8817e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="200" >
<hent key="02400100000000000000080000000000" call="MPI_Isend" bytes="2048" orank="0" region="0" commid="0" count="10" tid="0" op="" dtype="" >3.8147e-05 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000080000000006" call="MPI_Isend" bytes="2048" orank="6" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.5749e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000008000000000F" call="MPI_Isend" bytes="2048" orank="15" region="0" commid="0" count="15" tid="0" op="" dtype="" >5.7697e-05 9.5367e-07 1.5974e-05</hent>
<hent key="0240010000000000000008000000003F" call="MPI_Isend" bytes="2048" orank="63" region="0" commid="0" count="14" tid="0" op="" dtype="" >7.7724e-05 5.0068e-06 6.9141e-06</hent>
<hent key="02400100000000000000080000000047" call="MPI_Isend" bytes="2048" orank="71" region="0" commid="0" count="3422" tid="0" op="" dtype="" >1.1631e-02 9.5367e-07 2.9087e-05</hent>
<hent key="024001000000000000000800000002C7" call="MPI_Isend" bytes="2048" orank="711" region="0" commid="0" count="3249" tid="0" op="" dtype="" >1.0433e-02 9.5367e-07 1.0967e-05</hent>
<hent key="038001000000000000000E0000000047" call="MPI_Irecv" bytes="3584" orank="71" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="024001000000000000000E0000000047" call="MPI_Isend" bytes="3584" orank="71" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000000" call="MPI_Irecv" bytes="640" orank="0" region="0" commid="0" count="406" tid="0" op="" dtype="" >9.1314e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000006" call="MPI_Irecv" bytes="640" orank="6" region="0" commid="0" count="400" tid="0" op="" dtype="" >1.7452e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000002800000000F" call="MPI_Irecv" bytes="640" orank="15" region="0" commid="0" count="396" tid="0" op="" dtype="" >1.2493e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000003F" call="MPI_Irecv" bytes="640" orank="63" region="0" commid="0" count="387" tid="0" op="" dtype="" >1.5116e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000047" call="MPI_Irecv" bytes="640" orank="71" region="0" commid="0" count="289" tid="0" op="" dtype="" >8.1778e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="038001000000000000000280000002C7" call="MPI_Irecv" bytes="640" orank="711" region="0" commid="0" count="283" tid="0" op="" dtype="" >8.9169e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000000" call="MPI_Isend" bytes="640" orank="0" region="0" commid="0" count="412" tid="0" op="" dtype="" >1.1821e-03 1.9073e-06 7.1526e-06</hent>
<hent key="02400100000000000000028000000006" call="MPI_Isend" bytes="640" orank="6" region="0" commid="0" count="415" tid="0" op="" dtype="" >5.3382e-04 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000002800000000F" call="MPI_Isend" bytes="640" orank="15" region="0" commid="0" count="396" tid="0" op="" dtype="" >4.9329e-04 9.5367e-07 8.1062e-06</hent>
<hent key="0240010000000000000002800000003F" call="MPI_Isend" bytes="640" orank="63" region="0" commid="0" count="396" tid="0" op="" dtype="" >1.8790e-03 3.8147e-06 9.0599e-06</hent>
<hent key="02400100000000000000028000000047" call="MPI_Isend" bytes="640" orank="71" region="0" commid="0" count="278" tid="0" op="" dtype="" >1.1907e-03 2.8610e-06 5.0068e-06</hent>
<hent key="024001000000000000000280000002C7" call="MPI_Isend" bytes="640" orank="711" region="0" commid="0" count="274" tid="0" op="" dtype="" >1.1356e-03 2.8610e-06 5.0068e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.2902e-05 3.2902e-05 3.2902e-05</hent>
<hent key="03800100000000000000014000000000" call="MPI_Irecv" bytes="320" orank="0" region="0" commid="0" count="399" tid="0" op="" dtype="" >1.1015e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000006" call="MPI_Irecv" bytes="320" orank="6" region="0" commid="0" count="339" tid="0" op="" dtype="" >1.4663e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000F" call="MPI_Irecv" bytes="320" orank="15" region="0" commid="0" count="357" tid="0" op="" dtype="" >8.8692e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000003F" call="MPI_Irecv" bytes="320" orank="63" region="0" commid="0" count="404" tid="0" op="" dtype="" >1.4901e-04 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000014000000047" call="MPI_Irecv" bytes="320" orank="71" region="0" commid="0" count="161" tid="0" op="" dtype="" >4.6730e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002C7" call="MPI_Irecv" bytes="320" orank="711" region="0" commid="0" count="191" tid="0" op="" dtype="" >6.5804e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.6387e+00 0.0000e+00 1.8346e-01</hent>
<hent key="03800100000000000000400000000047" call="MPI_Irecv" bytes="16384" orank="71" region="0" commid="0" count="12589" tid="0" op="" dtype="" >3.1741e-03 0.0000e+00 9.0599e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6443e-02 0.0000e+00 1.6426e-02</hent>
<hent key="038001000000000000004000000002C7" call="MPI_Irecv" bytes="16384" orank="711" region="0" commid="0" count="11973" tid="0" op="" dtype="" >1.7419e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.9816e-05 0.0000e+00 3.8862e-05</hent>
<hent key="02400100000000000000014000000000" call="MPI_Isend" bytes="320" orank="0" region="0" commid="0" count="405" tid="0" op="" dtype="" >1.0409e-03 9.5367e-07 4.0531e-06</hent>
<hent key="02400100000000000000014000000006" call="MPI_Isend" bytes="320" orank="6" region="0" commid="0" count="329" tid="0" op="" dtype="" >3.3426e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000001400000000F" call="MPI_Isend" bytes="320" orank="15" region="0" commid="0" count="360" tid="0" op="" dtype="" >3.8028e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000001400000003F" call="MPI_Isend" bytes="320" orank="63" region="0" commid="0" count="340" tid="0" op="" dtype="" >1.5409e-03 3.8147e-06 9.0599e-06</hent>
<hent key="02400100000000000000014000000047" call="MPI_Isend" bytes="320" orank="71" region="0" commid="0" count="172" tid="0" op="" dtype="" >7.1883e-04 2.8610e-06 1.0967e-05</hent>
<hent key="03800100000000000000200000000000" call="MPI_Irecv" bytes="8192" orank="0" region="0" commid="0" count="4139" tid="0" op="" dtype="" >5.6386e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000200000000006" call="MPI_Irecv" bytes="8192" orank="6" region="0" commid="0" count="4202" tid="0" op="" dtype="" >1.3018e-03 0.0000e+00 2.8610e-06</hent>
<hent key="0380010000000000000020000000000F" call="MPI_Irecv" bytes="8192" orank="15" region="0" commid="0" count="12321" tid="0" op="" dtype="" >6.9437e-03 0.0000e+00 1.0014e-05</hent>
<hent key="0380010000000000000020000000003F" call="MPI_Irecv" bytes="8192" orank="63" region="0" commid="0" count="12547" tid="0" op="" dtype="" >1.5578e-03 0.0000e+00 5.9605e-06</hent>
<hent key="024001000000000000000140000002C7" call="MPI_Isend" bytes="320" orank="711" region="0" commid="0" count="179" tid="0" op="" dtype="" >7.2098e-04 2.8610e-06 5.9605e-06</hent>
<hent key="03800100000000000000000000000000" call="MPI_Irecv" bytes="0" orank="0" region="0" commid="0" count="248" tid="0" op="" dtype="" >5.5552e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000006" call="MPI_Irecv" bytes="0" orank="6" region="0" commid="0" count="263" tid="0" op="" dtype="" >9.1553e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000000000000000F" call="MPI_Irecv" bytes="0" orank="15" region="0" commid="0" count="268" tid="0" op="" dtype="" >8.0109e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000000000000003F" call="MPI_Irecv" bytes="0" orank="63" region="0" commid="0" count="278" tid="0" op="" dtype="" >7.2479e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000047" call="MPI_Irecv" bytes="0" orank="71" region="0" commid="0" count="146" tid="0" op="" dtype="" >4.2200e-05 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000400000000047" call="MPI_Isend" bytes="16384" orank="71" region="0" commid="0" count="12523" tid="0" op="" dtype="" >9.7255e-02 4.7684e-06 4.4823e-05</hent>
<hent key="038001000000000000000000000002C7" call="MPI_Irecv" bytes="0" orank="711" region="0" commid="0" count="150" tid="0" op="" dtype="" >2.8133e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002C7" call="MPI_Isend" bytes="16384" orank="711" region="0" commid="0" count="11898" tid="0" op="" dtype="" >8.8018e-02 3.8147e-06 1.5974e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.2391e-04 0.0000e+00 2.7394e-04</hent>
<hent key="02400100000000000000200000000000" call="MPI_Isend" bytes="8192" orank="0" region="0" commid="0" count="3470" tid="0" op="" dtype="" >5.7263e-03 0.0000e+00 7.2956e-05</hent>
<hent key="02400100000000000000200000000006" call="MPI_Isend" bytes="8192" orank="6" region="0" commid="0" count="3061" tid="0" op="" dtype="" >3.5422e-03 0.0000e+00 1.5974e-05</hent>
<hent key="0240010000000000000020000000000F" call="MPI_Isend" bytes="8192" orank="15" region="0" commid="0" count="12373" tid="0" op="" dtype="" >7.9665e-03 0.0000e+00 5.1975e-05</hent>
<hent key="0240010000000000000020000000003F" call="MPI_Isend" bytes="8192" orank="63" region="0" commid="0" count="12644" tid="0" op="" dtype="" >8.2656e-02 3.8147e-06 3.8862e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >7.6056e-05 7.6056e-05 7.6056e-05</hent>
<hent key="02400100000000000000000000000000" call="MPI_Isend" bytes="0" orank="0" region="0" commid="0" count="281" tid="0" op="" dtype="" >5.4812e-04 0.0000e+00 9.7990e-05</hent>
<hent key="02400100000000000000000000000006" call="MPI_Isend" bytes="0" orank="6" region="0" commid="0" count="259" tid="0" op="" dtype="" >1.6117e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000000000000000F" call="MPI_Isend" bytes="0" orank="15" region="0" commid="0" count="262" tid="0" op="" dtype="" >2.2721e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000000000000003F" call="MPI_Isend" bytes="0" orank="63" region="0" commid="0" count="279" tid="0" op="" dtype="" >1.1208e-03 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000000000000047" call="MPI_Isend" bytes="0" orank="71" region="0" commid="0" count="143" tid="0" op="" dtype="" >5.4646e-04 9.5367e-07 9.0599e-06</hent>
<hent key="024001000000000000000000000002C7" call="MPI_Isend" bytes="0" orank="711" region="0" commid="0" count="148" tid="0" op="" dtype="" >5.5075e-04 9.5367e-07 5.0068e-06</hent>
<hent key="03800100000000000000060000000000" call="MPI_Irecv" bytes="1536" orank="0" region="0" commid="0" count="95" tid="0" op="" dtype="" >2.5749e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000006" call="MPI_Irecv" bytes="1536" orank="6" region="0" commid="0" count="89" tid="0" op="" dtype="" >4.2915e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000F" call="MPI_Irecv" bytes="1536" orank="15" region="0" commid="0" count="99" tid="0" op="" dtype="" >3.1710e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000003F" call="MPI_Irecv" bytes="1536" orank="63" region="0" commid="0" count="103" tid="0" op="" dtype="" >3.6716e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000047" call="MPI_Irecv" bytes="1536" orank="71" region="0" commid="0" count="226" tid="0" op="" dtype="" >8.4162e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002C7" call="MPI_Irecv" bytes="1536" orank="711" region="0" commid="0" count="228" tid="0" op="" dtype="" >7.7724e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000030000000000" call="MPI_Isend" bytes="768" orank="0" region="0" commid="0" count="6" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="02400100000000000000060000000000" call="MPI_Isend" bytes="1536" orank="0" region="0" commid="0" count="88" tid="0" op="" dtype="" >3.3522e-04 2.8610e-06 2.1935e-05</hent>
<hent key="02400100000000000000060000000006" call="MPI_Isend" bytes="1536" orank="6" region="0" commid="0" count="87" tid="0" op="" dtype="" >1.8787e-04 9.5367e-07 7.1526e-06</hent>
<hent key="0240010000000000000006000000000F" call="MPI_Isend" bytes="1536" orank="15" region="0" commid="0" count="107" tid="0" op="" dtype="" >2.3222e-04 9.5367e-07 1.5020e-05</hent>
<hent key="0240010000000000000006000000003F" call="MPI_Isend" bytes="1536" orank="63" region="0" commid="0" count="104" tid="0" op="" dtype="" >5.5313e-04 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000060000000047" call="MPI_Isend" bytes="1536" orank="71" region="0" commid="0" count="232" tid="0" op="" dtype="" >1.0622e-03 3.8147e-06 1.0014e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C7" call="MPI_Isend" bytes="1536" orank="711" region="0" commid="0" count="227" tid="0" op="" dtype="" >1.0164e-03 3.8147e-06 1.0014e-05</hent>
<hent key="038001000000000000000C000000003F" call="MPI_Irecv" bytes="3072" orank="63" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000C0000000047" call="MPI_Irecv" bytes="3072" orank="71" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C7" call="MPI_Irecv" bytes="3072" orank="711" region="0" commid="0" count="4" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000000" call="MPI_Isend" bytes="3072" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="024001000000000000000C0000000006" call="MPI_Isend" bytes="3072" orank="6" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="024001000000000000000C0000000047" call="MPI_Isend" bytes="3072" orank="71" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.2173e-05 5.0068e-06 6.1989e-06</hent>
<hent key="024001000000000000000C00000002C7" call="MPI_Isend" bytes="3072" orank="711" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.0981e-05 5.0068e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.5402e-04 1.5402e-04 1.5402e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.5381e-04 1.1396e-04 1.2088e-04</hent>
<hent key="03800100000000000000038000000000" call="MPI_Irecv" bytes="896" orank="0" region="0" commid="0" count="2584" tid="0" op="" dtype="" >4.3297e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000006" call="MPI_Irecv" bytes="896" orank="6" region="0" commid="0" count="2598" tid="0" op="" dtype="" >3.4165e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000F" call="MPI_Irecv" bytes="896" orank="15" region="0" commid="0" count="314" tid="0" op="" dtype="" >1.0371e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000003F" call="MPI_Irecv" bytes="896" orank="63" region="0" commid="0" count="288" tid="0" op="" dtype="" >1.1635e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000047" call="MPI_Irecv" bytes="896" orank="71" region="0" commid="0" count="334" tid="0" op="" dtype="" >1.1683e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002C7" call="MPI_Irecv" bytes="896" orank="711" region="0" commid="0" count="322" tid="0" op="" dtype="" >9.2983e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.7936e-05 5.7936e-05 5.7936e-05</hent>
<hent key="03800100000000000000380000000047" call="MPI_Irecv" bytes="14336" orank="71" region="0" commid="0" count="110" tid="0" op="" dtype="" >2.5749e-05 0.0000e+00 2.8610e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000003800000002C7" call="MPI_Irecv" bytes="14336" orank="711" region="0" commid="0" count="726" tid="0" op="" dtype="" >1.0371e-04 0.0000e+00 2.8610e-06</hent>
<hent key="02400100000000000000038000000000" call="MPI_Isend" bytes="896" orank="0" region="0" commid="0" count="2747" tid="0" op="" dtype="" >2.4881e-03 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000038000000006" call="MPI_Isend" bytes="896" orank="6" region="0" commid="0" count="2916" tid="0" op="" dtype="" >2.0401e-03 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000003800000000F" call="MPI_Isend" bytes="896" orank="15" region="0" commid="0" count="334" tid="0" op="" dtype="" >5.0688e-04 9.5367e-07 8.8215e-06</hent>
<hent key="0240010000000000000003800000003F" call="MPI_Isend" bytes="896" orank="63" region="0" commid="0" count="310" tid="0" op="" dtype="" >1.6203e-03 3.8147e-06 1.0395e-04</hent>
<hent key="02400100000000000000038000000047" call="MPI_Isend" bytes="896" orank="71" region="0" commid="0" count="345" tid="0" op="" dtype="" >1.5087e-03 2.8610e-06 1.1206e-05</hent>
<hent key="024001000000000000000380000002C7" call="MPI_Isend" bytes="896" orank="711" region="0" commid="0" count="344" tid="0" op="" dtype="" >1.4422e-03 2.8610e-06 5.9605e-06</hent>
<hent key="02400100000000000000380000000047" call="MPI_Isend" bytes="14336" orank="71" region="0" commid="0" count="176" tid="0" op="" dtype="" >1.3077e-03 5.9605e-06 1.3113e-05</hent>
<hent key="024001000000000000003800000002C7" call="MPI_Isend" bytes="14336" orank="711" region="0" commid="0" count="801" tid="0" op="" dtype="" >5.8224e-03 3.8147e-06 1.3828e-05</hent>
<hent key="02400100000000000000180000000000" call="MPI_Isend" bytes="6144" orank="0" region="0" commid="0" count="22" tid="0" op="" dtype="" >5.4121e-05 1.9073e-06 5.0068e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.9785e+00 5.8889e-05 2.0311e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.2141e-03 3.2141e-03 3.2141e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0488e-02 1.0488e-02 1.0488e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.6551e-02 2.6551e-02 2.6551e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.7808e-01 1.3471e-03 1.6027e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >2.9707e-04 2.9707e-04 2.9707e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5898e+00 3.8481e-04 2.5115e-01</hent>
<hent key="03800100000000000000040000000000" call="MPI_Irecv" bytes="1024" orank="0" region="0" commid="0" count="1114" tid="0" op="" dtype="" >1.7953e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000006" call="MPI_Irecv" bytes="1024" orank="6" region="0" commid="0" count="1134" tid="0" op="" dtype="" >1.0037e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000000F" call="MPI_Irecv" bytes="1024" orank="15" region="0" commid="0" count="3292" tid="0" op="" dtype="" >6.9213e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000004000000003F" call="MPI_Irecv" bytes="1024" orank="63" region="0" commid="0" count="3352" tid="0" op="" dtype="" >3.1471e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.5020e-05 0.0000e+00 6.9141e-06</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.7909e-05 3.7909e-05 3.7909e-05</hent>
<hent key="03800100000000000000070000000000" call="MPI_Irecv" bytes="1792" orank="0" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000006" call="MPI_Irecv" bytes="1792" orank="6" region="0" commid="0" count="47" tid="0" op="" dtype="" >2.0504e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000F" call="MPI_Irecv" bytes="1792" orank="15" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000003F" call="MPI_Irecv" bytes="1792" orank="63" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.5497e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000047" call="MPI_Irecv" bytes="1792" orank="71" region="0" commid="0" count="165" tid="0" op="" dtype="" >5.7459e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002C7" call="MPI_Irecv" bytes="1792" orank="711" region="0" commid="0" count="314" tid="0" op="" dtype="" >5.1260e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >6.0320e-05 0.0000e+00 2.6226e-05</hent>
<hent key="02400100000000000000040000000000" call="MPI_Isend" bytes="1024" orank="0" region="0" commid="0" count="934" tid="0" op="" dtype="" >7.0071e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000040000000006" call="MPI_Isend" bytes="1024" orank="6" region="0" commid="0" count="806" tid="0" op="" dtype="" >5.1761e-04 0.0000e+00 2.8610e-06</hent>
<hent key="0240010000000000000004000000000F" call="MPI_Isend" bytes="1024" orank="15" region="0" commid="0" count="3296" tid="0" op="" dtype="" >1.8883e-03 0.0000e+00 1.5020e-05</hent>
<hent key="0240010000000000000004000000003F" call="MPI_Isend" bytes="1024" orank="63" region="0" commid="0" count="3384" tid="0" op="" dtype="" >8.9228e-03 9.5367e-07 2.1935e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >1.0967e-05 1.9073e-06 9.0599e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5539e+00 0.0000e+00 2.2674e+00</hent>
<hent key="038001000000000000000A0000000000" call="MPI_Irecv" bytes="2560" orank="0" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000006" call="MPI_Irecv" bytes="2560" orank="6" region="0" commid="0" count="9" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000000F" call="MPI_Irecv" bytes="2560" orank="15" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000003F" call="MPI_Irecv" bytes="2560" orank="63" region="0" commid="0" count="11" tid="0" op="" dtype="" >6.6757e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000047" call="MPI_Irecv" bytes="2560" orank="71" region="0" commid="0" count="48" tid="0" op="" dtype="" >1.7405e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002C7" call="MPI_Irecv" bytes="2560" orank="711" region="0" commid="0" count="43" tid="0" op="" dtype="" >1.3828e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000000" call="MPI_Isend" bytes="1792" orank="0" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.1802e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000070000000006" call="MPI_Isend" bytes="1792" orank="6" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.0061e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000007000000000F" call="MPI_Isend" bytes="1792" orank="15" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.0300e-04 9.5367e-07 1.3828e-05</hent>
<hent key="0240010000000000000007000000003F" call="MPI_Isend" bytes="1792" orank="63" region="0" commid="0" count="56" tid="0" op="" dtype="" >3.0065e-04 4.7684e-06 9.0599e-06</hent>
<hent key="02400100000000000000070000000047" call="MPI_Isend" bytes="1792" orank="71" region="0" commid="0" count="153" tid="0" op="" dtype="" >6.4898e-04 9.5367e-07 1.0014e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1356e-01 1.1356e-01 1.1356e-01</hent>
<hent key="024001000000000000000700000002C7" call="MPI_Isend" bytes="1792" orank="711" region="0" commid="0" count="335" tid="0" op="" dtype="" >1.1814e-03 9.5367e-07 6.9141e-06</hent>
<hent key="024001000000000000000A0000000000" call="MPI_Isend" bytes="2560" orank="0" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.5020e-05 2.8610e-06 4.0531e-06</hent>
<hent key="024001000000000000000A0000000006" call="MPI_Isend" bytes="2560" orank="6" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.7405e-05 2.1458e-06 1.2159e-05</hent>
<hent key="024001000000000000000A000000000F" call="MPI_Isend" bytes="2560" orank="15" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.1935e-05 1.9073e-06 1.5020e-05</hent>
<hent key="024001000000000000000A000000003F" call="MPI_Isend" bytes="2560" orank="63" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.8133e-05 5.0068e-06 6.1989e-06</hent>
<hent key="024001000000000000000A0000000047" call="MPI_Isend" bytes="2560" orank="71" region="0" commid="0" count="52" tid="0" op="" dtype="" >2.5773e-04 3.8147e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.7595e-04 2.4080e-05 7.8917e-05</hent>
<hent key="024001000000000000000A00000002C7" call="MPI_Isend" bytes="2560" orank="711" region="0" commid="0" count="50" tid="0" op="" dtype="" >2.4581e-04 3.8147e-06 1.0967e-05</hent>
<hent key="03800100000000000000100000000047" call="MPI_Irecv" bytes="4096" orank="71" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >6.1083e-04 2.1577e-04 3.9506e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6955e-05 3.6955e-05 3.6955e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >3.8147e-06 9.5367e-07 2.8610e-06</hent>
<hent key="03800100000000000000000400000000" call="MPI_Irecv" bytes="4" orank="0" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.3559e-04 0.0000e+00 4.7684e-06</hent>
<hent key="03800100000000000000000400000006" call="MPI_Irecv" bytes="4" orank="6" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.5197e-04 0.0000e+00 3.0994e-06</hent>
<hent key="0380010000000000000000040000000F" call="MPI_Irecv" bytes="4" orank="15" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.6338e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000003F" call="MPI_Irecv" bytes="4" orank="63" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.4741e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000400000047" call="MPI_Irecv" bytes="4" orank="71" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.6219e-04 0.0000e+00 3.0994e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >8.0991e-04 7.7963e-05 2.9111e-04</hent>
<hent key="038001000000000000001C0000000000" call="MPI_Irecv" bytes="7168" orank="0" region="0" commid="0" count="8560" tid="0" op="" dtype="" >1.1916e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000001C0000000006" call="MPI_Irecv" bytes="7168" orank="6" region="0" commid="0" count="8497" tid="0" op="" dtype="" >2.7752e-03 0.0000e+00 6.1989e-06</hent>
<hent key="038001000000000000000004000002C7" call="MPI_Irecv" bytes="4" orank="711" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.1785e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000400000000" call="MPI_Isend" bytes="4" orank="0" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.9531e-03 0.0000e+00 2.3127e-05</hent>
<hent key="02400100000000000000000400000006" call="MPI_Isend" bytes="4" orank="6" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3847e-03 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000040000000F" call="MPI_Isend" bytes="4" orank="15" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1425e-03 0.0000e+00 6.1989e-06</hent>
<hent key="0240010000000000000000040000003F" call="MPI_Isend" bytes="4" orank="63" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.0423e-02 3.8147e-06 1.0896e-04</hent>
<hent key="02400100000000000000000400000047" call="MPI_Isend" bytes="4" orank="71" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.0408e-02 3.8147e-06 1.4067e-05</hent>
<hent key="024001000000000000001C0000000000" call="MPI_Isend" bytes="7168" orank="0" region="0" commid="0" count="9207" tid="0" op="" dtype="" >2.6907e-02 9.5367e-07 2.1935e-05</hent>
<hent key="024001000000000000001C0000000006" call="MPI_Isend" bytes="7168" orank="6" region="0" commid="0" count="9638" tid="0" op="" dtype="" >2.1922e-02 9.5367e-07 1.7166e-05</hent>
<hent key="024001000000000000000004000002C7" call="MPI_Isend" bytes="4" orank="711" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.8490e-02 3.8147e-06 6.0797e-05</hent>
<hent key="03800100000000000000050000000000" call="MPI_Irecv" bytes="1280" orank="0" region="0" commid="0" count="198" tid="0" op="" dtype="" >5.6505e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000006" call="MPI_Irecv" bytes="1280" orank="6" region="0" commid="0" count="206" tid="0" op="" dtype="" >8.8215e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000F" call="MPI_Irecv" bytes="1280" orank="15" region="0" commid="0" count="309" tid="0" op="" dtype="" >8.5115e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000003F" call="MPI_Irecv" bytes="1280" orank="63" region="0" commid="0" count="211" tid="0" op="" dtype="" >7.1526e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000047" call="MPI_Irecv" bytes="1280" orank="71" region="0" commid="0" count="294" tid="0" op="" dtype="" >8.4162e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0631e+01 8.8215e-06 1.5579e-01</hent>
<hent key="0380010000000000000028000000000F" call="MPI_Irecv" bytes="10240" orank="15" region="0" commid="0" count="378" tid="0" op="" dtype="" >8.4639e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000028000000003F" call="MPI_Irecv" bytes="10240" orank="63" region="0" commid="0" count="152" tid="0" op="" dtype="" >3.2187e-05 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000000500000002C7" call="MPI_Irecv" bytes="1280" orank="711" region="0" commid="0" count="296" tid="0" op="" dtype="" >9.2268e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000000" call="MPI_Irecv" bytes="2048" orank="0" region="0" commid="0" count="13" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000080000000006" call="MPI_Irecv" bytes="2048" orank="6" region="0" commid="0" count="12" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000F" call="MPI_Irecv" bytes="2048" orank="15" region="0" commid="0" count="16" tid="0" op="" dtype="" >6.6757e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000003F" call="MPI_Irecv" bytes="2048" orank="63" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.1921e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000047" call="MPI_Irecv" bytes="2048" orank="71" region="0" commid="0" count="3426" tid="0" op="" dtype="" >5.6887e-04 0.0000e+00 3.0994e-06</hent>
<hent key="038001000000000000000800000002C7" call="MPI_Irecv" bytes="2048" orank="711" region="0" commid="0" count="3266" tid="0" op="" dtype="" >4.5681e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000050000000000" call="MPI_Isend" bytes="1280" orank="0" region="0" commid="0" count="177" tid="0" op="" dtype="" >6.0129e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000050000000006" call="MPI_Isend" bytes="1280" orank="6" region="0" commid="0" count="221" tid="0" op="" dtype="" >3.8457e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000005000000000F" call="MPI_Isend" bytes="1280" orank="15" region="0" commid="0" count="291" tid="0" op="" dtype="" >3.9077e-04 0.0000e+00 1.1921e-05</hent>
<hent key="0240010000000000000005000000003F" call="MPI_Isend" bytes="1280" orank="63" region="0" commid="0" count="209" tid="0" op="" dtype="" >1.0362e-03 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000050000000047" call="MPI_Isend" bytes="1280" orank="71" region="0" commid="0" count="295" tid="0" op="" dtype="" >1.3397e-03 3.0994e-06 9.0599e-06</hent>
<hent key="0240010000000000000028000000000F" call="MPI_Isend" bytes="10240" orank="15" region="0" commid="0" count="326" tid="0" op="" dtype="" >1.4925e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000028000000003F" call="MPI_Isend" bytes="10240" orank="63" region="0" commid="0" count="55" tid="0" op="" dtype="" >3.6526e-04 5.0068e-06 9.0599e-06</hent>
<hent key="024001000000000000000500000002C7" call="MPI_Isend" bytes="1280" orank="711" region="0" commid="0" count="287" tid="0" op="" dtype="" >1.2221e-03 2.8610e-06 8.1062e-06</hent>
</hash>
<internal rank="7" log_i="1723712895.684138" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="8" mpi_size="768" stamp_init="1723712829.548080" stamp_final="1723712895.692241" username="apac4" allocationname="unknown" flags="0" pid="684276" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61442e+01" utime="4.93061e+01" stime="8.22571e+00" mtime="3.01867e+01" gflop="0.00000e+00" gbyte="3.77052e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.01867e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60107e+01" utime="4.92703e+01" stime="8.21930e+00" mtime="3.01867e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.01867e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4882e+08" > 4.2905e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4921e+08" > 4.3544e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8071e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5833e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0300e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.0994e-06 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5834e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.8601e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0387e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1380e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7603e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="197" >
<hent key="02400100000000000000080000000000" call="MPI_Isend" bytes="2048" orank="0" region="0" commid="0" count="10" tid="0" op="" dtype="" >2.6226e-05 9.5367e-07 1.2159e-05</hent>
<hent key="02400100000000000000080000000009" call="MPI_Isend" bytes="2048" orank="9" region="0" commid="0" count="12" tid="0" op="" dtype="" >4.9829e-05 2.8610e-06 7.8678e-06</hent>
<hent key="0240010000000000000008000000000F" call="MPI_Isend" bytes="2048" orank="15" region="0" commid="0" count="13" tid="0" op="" dtype="" >2.9325e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000010" call="MPI_Isend" bytes="2048" orank="16" region="0" commid="0" count="15" tid="0" op="" dtype="" >4.4823e-05 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000080000000048" call="MPI_Isend" bytes="2048" orank="72" region="0" commid="0" count="3437" tid="0" op="" dtype="" >1.5919e-02 9.5367e-07 3.7909e-05</hent>
<hent key="024001000000000000000800000002C8" call="MPI_Isend" bytes="2048" orank="712" region="0" commid="0" count="3408" tid="0" op="" dtype="" >1.3112e-02 9.5367e-07 3.0994e-05</hent>
<hent key="038001000000000000000E0000000048" call="MPI_Irecv" bytes="3584" orank="72" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000E0000000000" call="MPI_Isend" bytes="3584" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >8.1062e-06 8.1062e-06 8.1062e-06</hent>
<hent key="03800100000000000000028000000000" call="MPI_Irecv" bytes="640" orank="0" region="0" commid="0" count="404" tid="0" op="" dtype="" >1.0514e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000028000000009" call="MPI_Irecv" bytes="640" orank="9" region="0" commid="0" count="406" tid="0" op="" dtype="" >1.4162e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000002800000000F" call="MPI_Irecv" bytes="640" orank="15" region="0" commid="0" count="411" tid="0" op="" dtype="" >1.7476e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000010" call="MPI_Irecv" bytes="640" orank="16" region="0" commid="0" count="422" tid="0" op="" dtype="" >1.1587e-04 0.0000e+00 3.0994e-06</hent>
<hent key="024001000000000000000E00000002C8" call="MPI_Isend" bytes="3584" orank="712" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000048" call="MPI_Irecv" bytes="640" orank="72" region="0" commid="0" count="280" tid="0" op="" dtype="" >9.4175e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002C8" call="MPI_Irecv" bytes="640" orank="712" region="0" commid="0" count="277" tid="0" op="" dtype="" >1.5879e-04 0.0000e+00 7.0810e-05</hent>
<hent key="02400100000000000000028000000000" call="MPI_Isend" bytes="640" orank="0" region="0" commid="0" count="414" tid="0" op="" dtype="" >5.0735e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000028000000009" call="MPI_Isend" bytes="640" orank="9" region="0" commid="0" count="395" tid="0" op="" dtype="" >1.2014e-03 1.9073e-06 1.0014e-05</hent>
<hent key="0240010000000000000002800000000F" call="MPI_Isend" bytes="640" orank="15" region="0" commid="0" count="425" tid="0" op="" dtype="" >5.8150e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000028000000010" call="MPI_Isend" bytes="640" orank="16" region="0" commid="0" count="382" tid="0" op="" dtype="" >5.1618e-04 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000028000000048" call="MPI_Isend" bytes="640" orank="72" region="0" commid="0" count="261" tid="0" op="" dtype="" >1.4029e-03 3.8147e-06 1.2875e-05</hent>
<hent key="024001000000000000000280000002C8" call="MPI_Isend" bytes="640" orank="712" region="0" commid="0" count="254" tid="0" op="" dtype="" >1.1630e-03 2.8610e-06 1.1921e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.1948e-05 3.1948e-05 3.1948e-05</hent>
<hent key="03800100000000000000014000000000" call="MPI_Irecv" bytes="320" orank="0" region="0" commid="0" count="349" tid="0" op="" dtype="" >8.4639e-05 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000014000000009" call="MPI_Irecv" bytes="320" orank="9" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.0157e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000001400000000F" call="MPI_Irecv" bytes="320" orank="15" region="0" commid="0" count="372" tid="0" op="" dtype="" >1.6189e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000010" call="MPI_Irecv" bytes="320" orank="16" region="0" commid="0" count="352" tid="0" op="" dtype="" >9.4652e-05 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000014000000048" call="MPI_Irecv" bytes="320" orank="72" region="0" commid="0" count="165" tid="0" op="" dtype="" >5.8174e-05 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000140000002C8" call="MPI_Irecv" bytes="320" orank="712" region="0" commid="0" count="180" tid="0" op="" dtype="" >6.4611e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >6.9141e-06 9.5367e-07 2.1458e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.8071e+00 0.0000e+00 1.8677e-01</hent>
<hent key="03800100000000000000400000000048" call="MPI_Irecv" bytes="16384" orank="72" region="0" commid="0" count="12474" tid="0" op="" dtype="" >1.3299e-02 0.0000e+00 2.0981e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6482e-02 9.5367e-07 1.6466e-02</hent>
<hent key="038001000000000000004000000002C8" call="MPI_Irecv" bytes="16384" orank="712" region="0" commid="0" count="12672" tid="0" op="" dtype="" >4.3955e-03 0.0000e+00 2.0981e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000014000000000" call="MPI_Isend" bytes="320" orank="0" region="0" commid="0" count="373" tid="0" op="" dtype="" >3.7336e-04 0.0000e+00 1.4782e-05</hent>
<hent key="02400100000000000000014000000009" call="MPI_Isend" bytes="320" orank="9" region="0" commid="0" count="382" tid="0" op="" dtype="" >1.0209e-03 9.5367e-07 7.1526e-06</hent>
<hent key="0240010000000000000001400000000F" call="MPI_Isend" bytes="320" orank="15" region="0" commid="0" count="362" tid="0" op="" dtype="" >3.8600e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000010" call="MPI_Isend" bytes="320" orank="16" region="0" commid="0" count="339" tid="0" op="" dtype="" >3.4332e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000014000000048" call="MPI_Isend" bytes="320" orank="72" region="0" commid="0" count="178" tid="0" op="" dtype="" >9.3770e-04 3.8147e-06 1.1921e-05</hent>
<hent key="03800100000000000000200000000000" call="MPI_Irecv" bytes="8192" orank="0" region="0" commid="0" count="12411" tid="0" op="" dtype="" >2.8379e-03 0.0000e+00 2.5988e-05</hent>
<hent key="03800100000000000000200000000009" call="MPI_Irecv" bytes="8192" orank="9" region="0" commid="0" count="3848" tid="0" op="" dtype="" >7.5769e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000020000000000F" call="MPI_Irecv" bytes="8192" orank="15" region="0" commid="0" count="3126" tid="0" op="" dtype="" >1.2321e-03 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000200000000010" call="MPI_Irecv" bytes="8192" orank="16" region="0" commid="0" count="12630" tid="0" op="" dtype="" >2.2819e-03 0.0000e+00 2.2173e-05</hent>
<hent key="024001000000000000000140000002C8" call="MPI_Isend" bytes="320" orank="712" region="0" commid="0" count="187" tid="0" op="" dtype="" >8.4352e-04 2.8610e-06 1.0967e-05</hent>
<hent key="03800100000000000000000000000000" call="MPI_Irecv" bytes="0" orank="0" region="0" commid="0" count="257" tid="0" op="" dtype="" >5.9366e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000000000000009" call="MPI_Irecv" bytes="0" orank="9" region="0" commid="0" count="258" tid="0" op="" dtype="" >8.6784e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000000000000000F" call="MPI_Irecv" bytes="0" orank="15" region="0" commid="0" count="253" tid="0" op="" dtype="" >8.5115e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000010" call="MPI_Irecv" bytes="0" orank="16" region="0" commid="0" count="228" tid="0" op="" dtype="" >7.1764e-05 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000000000000048" call="MPI_Irecv" bytes="0" orank="72" region="0" commid="0" count="147" tid="0" op="" dtype="" >4.4107e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000048" call="MPI_Isend" bytes="16384" orank="72" region="0" commid="0" count="12586" tid="0" op="" dtype="" >1.4141e-01 3.8147e-06 9.8944e-05</hent>
<hent key="038001000000000000000000000002C8" call="MPI_Irecv" bytes="0" orank="712" region="0" commid="0" count="145" tid="0" op="" dtype="" >4.5538e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002C8" call="MPI_Isend" bytes="16384" orank="712" region="0" commid="0" count="12462" tid="0" op="" dtype="" >8.3837e-02 2.8610e-06 9.1076e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.3511e-04 0.0000e+00 2.8586e-04</hent>
<hent key="02400100000000000000200000000000" call="MPI_Isend" bytes="8192" orank="0" region="0" commid="0" count="12550" tid="0" op="" dtype="" >9.0992e-03 0.0000e+00 2.5034e-05</hent>
<hent key="02400100000000000000200000000009" call="MPI_Isend" bytes="8192" orank="9" region="0" commid="0" count="3353" tid="0" op="" dtype="" >6.7370e-03 0.0000e+00 4.9829e-05</hent>
<hent key="0240010000000000000020000000000F" call="MPI_Isend" bytes="8192" orank="15" region="0" commid="0" count="3714" tid="0" op="" dtype="" >3.9783e-03 0.0000e+00 5.1975e-05</hent>
<hent key="02400100000000000000200000000010" call="MPI_Isend" bytes="8192" orank="16" region="0" commid="0" count="12624" tid="0" op="" dtype="" >9.2874e-03 0.0000e+00 4.4107e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.4796e-05 2.4796e-05 2.4796e-05</hent>
<hent key="02400100000000000000000000000000" call="MPI_Isend" bytes="0" orank="0" region="0" commid="0" count="254" tid="0" op="" dtype="" >2.0528e-04 0.0000e+00 1.4782e-05</hent>
<hent key="02400100000000000000000000000009" call="MPI_Isend" bytes="0" orank="9" region="0" commid="0" count="247" tid="0" op="" dtype="" >4.8232e-04 9.5367e-07 3.3140e-05</hent>
<hent key="0240010000000000000000000000000F" call="MPI_Isend" bytes="0" orank="15" region="0" commid="0" count="250" tid="0" op="" dtype="" >1.6999e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000010" call="MPI_Isend" bytes="0" orank="16" region="0" commid="0" count="234" tid="0" op="" dtype="" >1.9670e-04 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000000000000048" call="MPI_Isend" bytes="0" orank="72" region="0" commid="0" count="153" tid="0" op="" dtype="" >6.9761e-04 1.9073e-06 1.0967e-05</hent>
<hent key="024001000000000000000000000002C8" call="MPI_Isend" bytes="0" orank="712" region="0" commid="0" count="145" tid="0" op="" dtype="" >6.1107e-04 1.9073e-06 1.3828e-05</hent>
<hent key="03800100000000000000060000000000" call="MPI_Irecv" bytes="1536" orank="0" region="0" commid="0" count="105" tid="0" op="" dtype="" >2.6464e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000009" call="MPI_Irecv" bytes="1536" orank="9" region="0" commid="0" count="94" tid="0" op="" dtype="" >2.9087e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000006000000000F" call="MPI_Irecv" bytes="1536" orank="15" region="0" commid="0" count="82" tid="0" op="" dtype="" >3.3379e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000010" call="MPI_Irecv" bytes="1536" orank="16" region="0" commid="0" count="111" tid="0" op="" dtype="" >2.6226e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000048" call="MPI_Irecv" bytes="1536" orank="72" region="0" commid="0" count="216" tid="0" op="" dtype="" >8.5592e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000600000002C8" call="MPI_Irecv" bytes="1536" orank="712" region="0" commid="0" count="206" tid="0" op="" dtype="" >9.0837e-05 0.0000e+00 1.4067e-05</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000000" call="MPI_Isend" bytes="1536" orank="0" region="0" commid="0" count="101" tid="0" op="" dtype="" >2.2674e-04 9.5367e-07 1.7881e-05</hent>
<hent key="02400100000000000000060000000009" call="MPI_Isend" bytes="1536" orank="9" region="0" commid="0" count="92" tid="0" op="" dtype="" >3.6669e-04 2.8610e-06 4.1962e-05</hent>
<hent key="0240010000000000000006000000000F" call="MPI_Isend" bytes="1536" orank="15" region="0" commid="0" count="94" tid="0" op="" dtype="" >1.9979e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000060000000010" call="MPI_Isend" bytes="1536" orank="16" region="0" commid="0" count="110" tid="0" op="" dtype="" >2.9755e-04 9.5367e-07 1.3113e-05</hent>
<hent key="02400100000000000000060000000048" call="MPI_Isend" bytes="1536" orank="72" region="0" commid="0" count="222" tid="0" op="" dtype="" >1.3118e-03 4.0531e-06 1.1921e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C8" call="MPI_Isend" bytes="1536" orank="712" region="0" commid="0" count="246" tid="0" op="" dtype="" >1.2240e-03 2.8610e-06 1.2875e-05</hent>
<hent key="038001000000000000000C0000000009" call="MPI_Irecv" bytes="3072" orank="9" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000C0000000048" call="MPI_Irecv" bytes="3072" orank="72" region="0" commid="0" count="8" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C8" call="MPI_Irecv" bytes="3072" orank="712" region="0" commid="0" count="16" tid="0" op="" dtype="" >9.2983e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000000" call="MPI_Isend" bytes="3072" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.0967e-05 1.0967e-05 1.0967e-05</hent>
<hent key="024001000000000000000C0000000048" call="MPI_Isend" bytes="3072" orank="72" region="0" commid="0" count="4" tid="0" op="" dtype="" >3.3855e-05 5.9605e-06 1.0967e-05</hent>
<hent key="024001000000000000000C00000002C8" call="MPI_Isend" bytes="3072" orank="712" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.0770e-05 4.7684e-06 6.9141e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9312e-04 1.9312e-04 1.9312e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.3297e-04 1.3781e-04 1.4806e-04</hent>
<hent key="03800100000000000000038000000000" call="MPI_Irecv" bytes="896" orank="0" region="0" commid="0" count="346" tid="0" op="" dtype="" >1.1873e-04 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000038000000009" call="MPI_Irecv" bytes="896" orank="9" region="0" commid="0" count="2662" tid="0" op="" dtype="" >5.6863e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000003800000000F" call="MPI_Irecv" bytes="896" orank="15" region="0" commid="0" count="2896" tid="0" op="" dtype="" >7.5436e-04 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000038000000010" call="MPI_Irecv" bytes="896" orank="16" region="0" commid="0" count="311" tid="0" op="" dtype="" >6.8665e-05 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000038000000048" call="MPI_Irecv" bytes="896" orank="72" region="0" commid="0" count="342" tid="0" op="" dtype="" >1.2445e-04 0.0000e+00 6.1989e-06</hent>
<hent key="038001000000000000000380000002C8" call="MPI_Irecv" bytes="896" orank="712" region="0" commid="0" count="316" tid="0" op="" dtype="" >1.1039e-04 0.0000e+00 8.1062e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.5690e-03 5.5690e-03 5.5690e-03</hent>
<hent key="03800100000000000000380000000048" call="MPI_Irecv" bytes="14336" orank="72" region="0" commid="0" count="225" tid="0" op="" dtype="" >2.4509e-04 0.0000e+00 5.9605e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000003800000002C8" call="MPI_Irecv" bytes="14336" orank="712" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.5020e-05 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000038000000000" call="MPI_Isend" bytes="896" orank="0" region="0" commid="0" count="311" tid="0" op="" dtype="" >4.7684e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000038000000009" call="MPI_Isend" bytes="896" orank="9" region="0" commid="0" count="2833" tid="0" op="" dtype="" >2.9120e-03 0.0000e+00 1.3113e-05</hent>
<hent key="0240010000000000000003800000000F" call="MPI_Isend" bytes="896" orank="15" region="0" commid="0" count="2695" tid="0" op="" dtype="" >2.0015e-03 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000038000000010" call="MPI_Isend" bytes="896" orank="16" region="0" commid="0" count="365" tid="0" op="" dtype="" >5.5790e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000038000000048" call="MPI_Isend" bytes="896" orank="72" region="0" commid="0" count="358" tid="0" op="" dtype="" >2.0356e-03 3.8147e-06 1.6928e-05</hent>
<hent key="024001000000000000000380000002C8" call="MPI_Isend" bytes="896" orank="712" region="0" commid="0" count="302" tid="0" op="" dtype="" >1.4300e-03 2.8610e-06 9.0599e-06</hent>
<hent key="02400100000000000000380000000048" call="MPI_Isend" bytes="14336" orank="72" region="0" commid="0" count="113" tid="0" op="" dtype="" >1.2770e-03 5.9605e-06 2.2888e-05</hent>
<hent key="024001000000000000003800000002C8" call="MPI_Isend" bytes="14336" orank="712" region="0" commid="0" count="237" tid="0" op="" dtype="" >1.7159e-03 4.0531e-06 2.5034e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.0172e+00 1.0014e-05 2.0307e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.2029e-03 3.2029e-03 3.2029e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0387e-02 1.0387e-02 1.0387e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.7011e-02 2.7011e-02 2.7011e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.0537e-01 1.7319e-03 1.8754e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.6907e-04 3.6907e-04 3.6907e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5834e+00 3.8505e-04 2.5109e-01</hent>
<hent key="03800100000000000000040000000000" call="MPI_Irecv" bytes="1024" orank="0" region="0" commid="0" count="3314" tid="0" op="" dtype="" >5.0998e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000040000000009" call="MPI_Irecv" bytes="1024" orank="9" region="0" commid="0" count="1026" tid="0" op="" dtype="" >1.8859e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000004000000000F" call="MPI_Irecv" bytes="1024" orank="15" region="0" commid="0" count="846" tid="0" op="" dtype="" >1.7071e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000040000000010" call="MPI_Irecv" bytes="1024" orank="16" region="0" commid="0" count="3380" tid="0" op="" dtype="" >6.0153e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.0300e-02 2.8610e-06 8.4829e-03</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.1962e-05 4.1962e-05 4.1962e-05</hent>
<hent key="03800100000000000000070000000000" call="MPI_Irecv" bytes="1792" orank="0" region="0" commid="0" count="37" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000009" call="MPI_Irecv" bytes="1792" orank="9" region="0" commid="0" count="43" tid="0" op="" dtype="" >1.4782e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000007000000000F" call="MPI_Irecv" bytes="1792" orank="15" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.0490e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000010" call="MPI_Irecv" bytes="1792" orank="16" region="0" commid="0" count="27" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000048" call="MPI_Irecv" bytes="1792" orank="72" region="0" commid="0" count="193" tid="0" op="" dtype="" >5.4598e-05 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000700000002C8" call="MPI_Irecv" bytes="1792" orank="712" region="0" commid="0" count="153" tid="0" op="" dtype="" >5.1022e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >4.6492e-05 0.0000e+00 2.5988e-05</hent>
<hent key="02400100000000000000040000000000" call="MPI_Isend" bytes="1024" orank="0" region="0" commid="0" count="3358" tid="0" op="" dtype="" >2.2624e-03 0.0000e+00 2.0981e-05</hent>
<hent key="02400100000000000000040000000009" call="MPI_Isend" bytes="1024" orank="9" region="0" commid="0" count="886" tid="0" op="" dtype="" >6.9809e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0240010000000000000004000000000F" call="MPI_Isend" bytes="1024" orank="15" region="0" commid="0" count="1008" tid="0" op="" dtype="" >6.2537e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000040000000010" call="MPI_Isend" bytes="1024" orank="16" region="0" commid="0" count="3378" tid="0" op="" dtype="" >1.9939e-03 0.0000e+00 1.1921e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >3.8147e-06 1.9073e-06 1.9073e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5606e+00 0.0000e+00 2.2674e+00</hent>
<hent key="038001000000000000000A0000000000" call="MPI_Irecv" bytes="2560" orank="0" region="0" commid="0" count="4" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000009" call="MPI_Irecv" bytes="2560" orank="9" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000000A000000000F" call="MPI_Irecv" bytes="2560" orank="15" region="0" commid="0" count="6" tid="0" op="" dtype="" >6.4373e-06 9.5367e-07 1.1921e-06</hent>
<hent key="038001000000000000000A0000000010" call="MPI_Irecv" bytes="2560" orank="16" region="0" commid="0" count="7" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000048" call="MPI_Irecv" bytes="2560" orank="72" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.7166e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002C8" call="MPI_Irecv" bytes="2560" orank="712" region="0" commid="0" count="67" tid="0" op="" dtype="" >2.2173e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000000" call="MPI_Isend" bytes="1792" orank="0" region="0" commid="0" count="36" tid="0" op="" dtype="" >1.2279e-04 9.5367e-07 1.2159e-05</hent>
<hent key="02400100000000000000070000000009" call="MPI_Isend" bytes="1792" orank="9" region="0" commid="0" count="43" tid="0" op="" dtype="" >1.5950e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000007000000000F" call="MPI_Isend" bytes="1792" orank="15" region="0" commid="0" count="42" tid="0" op="" dtype="" >8.8215e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000010" call="MPI_Isend" bytes="1792" orank="16" region="0" commid="0" count="28" tid="0" op="" dtype="" >7.7486e-05 9.5367e-07 1.6212e-05</hent>
<hent key="02400100000000000000070000000048" call="MPI_Isend" bytes="1792" orank="72" region="0" commid="0" count="146" tid="0" op="" dtype="" >8.4472e-04 1.1921e-06 1.0967e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1380e-01 1.1380e-01 1.1380e-01</hent>
<hent key="024001000000000000000700000002C8" call="MPI_Isend" bytes="1792" orank="712" region="0" commid="0" count="221" tid="0" op="" dtype="" >9.8372e-04 9.5367e-07 1.5974e-05</hent>
<hent key="024001000000000000000A0000000000" call="MPI_Isend" bytes="2560" orank="0" region="0" commid="0" count="8" tid="0" op="" dtype="" >3.9101e-05 9.5367e-07 1.0014e-05</hent>
<hent key="024001000000000000000A0000000009" call="MPI_Isend" bytes="2560" orank="9" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.6464e-05 3.8147e-06 5.0068e-06</hent>
<hent key="024001000000000000000A000000000F" call="MPI_Isend" bytes="2560" orank="15" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.7166e-05 2.1458e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000010" call="MPI_Isend" bytes="2560" orank="16" region="0" commid="0" count="10" tid="0" op="" dtype="" >5.8651e-05 9.5367e-07 1.5020e-05</hent>
<hent key="024001000000000000000A0000000048" call="MPI_Isend" bytes="2560" orank="72" region="0" commid="0" count="45" tid="0" op="" dtype="" >2.6631e-04 5.0068e-06 7.1526e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.2697e-04 3.2902e-05 9.7990e-05</hent>
<hent key="024001000000000000000A00000002C8" call="MPI_Isend" bytes="2560" orank="712" region="0" commid="0" count="58" tid="0" op="" dtype="" >3.0994e-04 3.8147e-06 9.0599e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >7.1120e-04 2.5606e-04 4.5514e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >7.8678e-06 1.9073e-06 5.9605e-06</hent>
<hent key="03800100000000000000000400000000" call="MPI_Irecv" bytes="4" orank="0" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.7960e-04 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000000400000009" call="MPI_Irecv" bytes="4" orank="9" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.7755e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000000F" call="MPI_Irecv" bytes="4" orank="15" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0371e-03 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000000400000010" call="MPI_Irecv" bytes="4" orank="16" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.7057e-04 0.0000e+00 2.4796e-05</hent>
<hent key="03800100000000000000000400000048" call="MPI_Irecv" bytes="4" orank="72" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2000e-03 0.0000e+00 1.8120e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.2848e-03 1.8096e-04 4.3082e-04</hent>
<hent key="038001000000000000001C0000000009" call="MPI_Irecv" bytes="7168" orank="9" region="0" commid="0" count="8851" tid="0" op="" dtype="" >1.8845e-03 0.0000e+00 1.2159e-05</hent>
<hent key="038001000000000000001C000000000F" call="MPI_Irecv" bytes="7168" orank="15" region="0" commid="0" count="9573" tid="0" op="" dtype="" >3.8683e-03 0.0000e+00 1.5020e-05</hent>
<hent key="038001000000000000000004000002C8" call="MPI_Irecv" bytes="4" orank="712" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0526e-03 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000000400000000" call="MPI_Isend" bytes="4" orank="0" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4987e-03 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000000400000009" call="MPI_Isend" bytes="4" orank="9" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.8224e-03 0.0000e+00 1.3113e-05</hent>
<hent key="0240010000000000000000040000000F" call="MPI_Isend" bytes="4" orank="15" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2462e-03 0.0000e+00 4.1008e-05</hent>
<hent key="02400100000000000000000400000010" call="MPI_Isend" bytes="4" orank="16" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1227e-03 0.0000e+00 2.5034e-05</hent>
<hent key="02400100000000000000000400000048" call="MPI_Isend" bytes="4" orank="72" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.8999e-02 4.7684e-06 9.1696e-04</hent>
<hent key="024001000000000000001C0000000009" call="MPI_Isend" bytes="7168" orank="9" region="0" commid="0" count="9346" tid="0" op="" dtype="" >2.8178e-02 9.5367e-07 1.5974e-05</hent>
<hent key="024001000000000000001C000000000F" call="MPI_Isend" bytes="7168" orank="15" region="0" commid="0" count="8985" tid="0" op="" dtype="" >2.0163e-02 9.5367e-07 2.0027e-05</hent>
<hent key="024001000000000000000004000002C8" call="MPI_Isend" bytes="4" orank="712" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.0177e-02 3.8147e-06 1.1992e-04</hent>
<hent key="03800100000000000000050000000000" call="MPI_Irecv" bytes="1280" orank="0" region="0" commid="0" count="269" tid="0" op="" dtype="" >7.0333e-05 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000050000000009" call="MPI_Irecv" bytes="1280" orank="9" region="0" commid="0" count="213" tid="0" op="" dtype="" >8.1062e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000005000000000F" call="MPI_Irecv" bytes="1280" orank="15" region="0" commid="0" count="171" tid="0" op="" dtype="" >6.7472e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000010" call="MPI_Irecv" bytes="1280" orank="16" region="0" commid="0" count="238" tid="0" op="" dtype="" >6.3658e-05 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000050000000048" call="MPI_Irecv" bytes="1280" orank="72" region="0" commid="0" count="303" tid="0" op="" dtype="" >1.1277e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0350e+01 7.8678e-06 1.5558e-01</hent>
<hent key="03800100000000000000280000000000" call="MPI_Irecv" bytes="10240" orank="0" region="0" commid="0" count="288" tid="0" op="" dtype="" >9.1791e-05 0.0000e+00 1.3113e-05</hent>
<hent key="03800100000000000000280000000010" call="MPI_Irecv" bytes="10240" orank="16" region="0" commid="0" count="69" tid="0" op="" dtype="" >1.2398e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002C8" call="MPI_Irecv" bytes="1280" orank="712" region="0" commid="0" count="282" tid="0" op="" dtype="" >1.0562e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000080000000000" call="MPI_Irecv" bytes="2048" orank="0" region="0" commid="0" count="12" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000009" call="MPI_Irecv" bytes="2048" orank="9" region="0" commid="0" count="21" tid="0" op="" dtype="" >8.8215e-06 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000008000000000F" call="MPI_Irecv" bytes="2048" orank="15" region="0" commid="0" count="20" tid="0" op="" dtype="" >9.7752e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000010" call="MPI_Irecv" bytes="2048" orank="16" region="0" commid="0" count="21" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000048" call="MPI_Irecv" bytes="2048" orank="72" region="0" commid="0" count="3405" tid="0" op="" dtype="" >6.7854e-04 0.0000e+00 1.5974e-05</hent>
<hent key="038001000000000000000800000002C8" call="MPI_Irecv" bytes="2048" orank="712" region="0" commid="0" count="3455" tid="0" op="" dtype="" >8.2994e-04 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000050000000000" call="MPI_Isend" bytes="1280" orank="0" region="0" commid="0" count="230" tid="0" op="" dtype="" >3.4928e-04 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000050000000009" call="MPI_Isend" bytes="1280" orank="9" region="0" commid="0" count="201" tid="0" op="" dtype="" >7.2289e-04 1.9073e-06 1.0967e-05</hent>
<hent key="0240010000000000000005000000000F" call="MPI_Isend" bytes="1280" orank="15" region="0" commid="0" count="202" tid="0" op="" dtype="" >3.8600e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000050000000010" call="MPI_Isend" bytes="1280" orank="16" region="0" commid="0" count="236" tid="0" op="" dtype="" >3.6502e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000050000000048" call="MPI_Isend" bytes="1280" orank="72" region="0" commid="0" count="293" tid="0" op="" dtype="" >1.7331e-03 3.8147e-06 1.6212e-05</hent>
<hent key="02400100000000000000280000000000" call="MPI_Isend" bytes="10240" orank="0" region="0" commid="0" count="149" tid="0" op="" dtype="" >9.4652e-05 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000280000000010" call="MPI_Isend" bytes="10240" orank="16" region="0" commid="0" count="75" tid="0" op="" dtype="" >4.6968e-05 0.0000e+00 4.0531e-06</hent>
<hent key="024001000000000000000500000002C8" call="MPI_Isend" bytes="1280" orank="712" region="0" commid="0" count="268" tid="0" op="" dtype="" >1.2710e-03 2.8610e-06 1.2159e-05</hent>
</hash>
<internal rank="8" log_i="1723712895.692241" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="9" mpi_size="768" stamp_init="1723712829.548165" stamp_final="1723712895.680139" username="apac4" allocationname="unknown" flags="0" pid="684277" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61320e+01" utime="5.11113e+01" stime="7.48138e+00" mtime="3.03944e+01" gflop="0.00000e+00" gbyte="3.76339e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.03944e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f9154d55f915f9151e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60004e+01" utime="5.10815e+01" stime="7.46905e+00" mtime="3.03944e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.03944e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4856e+08" > 3.6330e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4867e+08" > 2.5127e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7828e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5678e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6689e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.4162e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5887e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.2136e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0448e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1362e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8939e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="194" >
<hent key="02400100000000000000080000000001" call="MPI_Isend" bytes="2048" orank="1" region="0" commid="0" count="12" tid="0" op="" dtype="" >1.8835e-05 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000080000000008" call="MPI_Isend" bytes="2048" orank="8" region="0" commid="0" count="21" tid="0" op="" dtype="" >4.8161e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000008000000000A" call="MPI_Isend" bytes="2048" orank="10" region="0" commid="0" count="14" tid="0" op="" dtype="" >5.2214e-05 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000080000000011" call="MPI_Isend" bytes="2048" orank="17" region="0" commid="0" count="12" tid="0" op="" dtype="" >4.0054e-05 9.5367e-07 1.3113e-05</hent>
<hent key="02400100000000000000080000000049" call="MPI_Isend" bytes="2048" orank="73" region="0" commid="0" count="3465" tid="0" op="" dtype="" >1.2004e-02 9.5367e-07 2.6941e-05</hent>
<hent key="024001000000000000000800000002C9" call="MPI_Isend" bytes="2048" orank="713" region="0" commid="0" count="3473" tid="0" op="" dtype="" >1.1143e-02 9.5367e-07 1.3113e-05</hent>
<hent key="03800100000000000000028000000001" call="MPI_Irecv" bytes="640" orank="1" region="0" commid="0" count="390" tid="0" op="" dtype="" >8.0585e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000008" call="MPI_Irecv" bytes="640" orank="8" region="0" commid="0" count="395" tid="0" op="" dtype="" >1.4329e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000A" call="MPI_Irecv" bytes="640" orank="10" region="0" commid="0" count="401" tid="0" op="" dtype="" >7.6056e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000011" call="MPI_Irecv" bytes="640" orank="17" region="0" commid="0" count="432" tid="0" op="" dtype="" >9.7275e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000049" call="MPI_Irecv" bytes="640" orank="73" region="0" commid="0" count="265" tid="0" op="" dtype="" >6.3658e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="038001000000000000000280000002C9" call="MPI_Irecv" bytes="640" orank="713" region="0" commid="0" count="301" tid="0" op="" dtype="" >9.0122e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000001" call="MPI_Isend" bytes="640" orank="1" region="0" commid="0" count="392" tid="0" op="" dtype="" >4.4417e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000028000000008" call="MPI_Isend" bytes="640" orank="8" region="0" commid="0" count="406" tid="0" op="" dtype="" >5.4312e-04 9.5367e-07 5.0068e-06</hent>
<hent key="0240010000000000000002800000000A" call="MPI_Isend" bytes="640" orank="10" region="0" commid="0" count="412" tid="0" op="" dtype="" >1.2348e-03 1.9073e-06 1.3113e-05</hent>
<hent key="02400100000000000000028000000011" call="MPI_Isend" bytes="640" orank="17" region="0" commid="0" count="417" tid="0" op="" dtype="" >4.8900e-04 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000028000000049" call="MPI_Isend" bytes="640" orank="73" region="0" commid="0" count="284" tid="0" op="" dtype="" >1.3618e-03 3.8147e-06 1.0014e-05</hent>
<hent key="024001000000000000000280000002C9" call="MPI_Isend" bytes="640" orank="713" region="0" commid="0" count="258" tid="0" op="" dtype="" >1.1070e-03 2.8610e-06 1.0014e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.1948e-05 3.1948e-05 3.1948e-05</hent>
<hent key="03800100000000000000014000000001" call="MPI_Irecv" bytes="320" orank="1" region="0" commid="0" count="355" tid="0" op="" dtype="" >8.6069e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000008" call="MPI_Irecv" bytes="320" orank="8" region="0" commid="0" count="382" tid="0" op="" dtype="" >1.2445e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000001400000000A" call="MPI_Irecv" bytes="320" orank="10" region="0" commid="0" count="342" tid="0" op="" dtype="" >5.2929e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000011" call="MPI_Irecv" bytes="320" orank="17" region="0" commid="0" count="359" tid="0" op="" dtype="" >5.8174e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000049" call="MPI_Irecv" bytes="320" orank="73" region="0" commid="0" count="171" tid="0" op="" dtype="" >3.4809e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000140000002C9" call="MPI_Irecv" bytes="320" orank="713" region="0" commid="0" count="193" tid="0" op="" dtype="" >4.9114e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >6.1989e-06 0.0000e+00 2.1458e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.7828e+00 0.0000e+00 1.8519e-01</hent>
<hent key="03800100000000000000400000000049" call="MPI_Irecv" bytes="16384" orank="73" region="0" commid="0" count="12676" tid="0" op="" dtype="" >4.4611e-03 0.0000e+00 7.1526e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6441e-02 0.0000e+00 1.6426e-02</hent>
<hent key="038001000000000000004000000002C9" call="MPI_Irecv" bytes="16384" orank="713" region="0" commid="0" count="12699" tid="0" op="" dtype="" >2.3649e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.6968e-05 0.0000e+00 4.6968e-05</hent>
<hent key="02400100000000000000014000000001" call="MPI_Isend" bytes="320" orank="1" region="0" commid="0" count="370" tid="0" op="" dtype="" >3.6144e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000014000000008" call="MPI_Isend" bytes="320" orank="8" region="0" commid="0" count="365" tid="0" op="" dtype="" >3.9482e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000001400000000A" call="MPI_Isend" bytes="320" orank="10" region="0" commid="0" count="344" tid="0" op="" dtype="" >9.2912e-04 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000014000000011" call="MPI_Isend" bytes="320" orank="17" region="0" commid="0" count="368" tid="0" op="" dtype="" >3.3855e-04 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000014000000049" call="MPI_Isend" bytes="320" orank="73" region="0" commid="0" count="166" tid="0" op="" dtype="" >7.5674e-04 3.8147e-06 6.1989e-06</hent>
<hent key="03800100000000000000200000000001" call="MPI_Irecv" bytes="8192" orank="1" region="0" commid="0" count="12632" tid="0" op="" dtype="" >2.6400e-03 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000200000000008" call="MPI_Irecv" bytes="8192" orank="8" region="0" commid="0" count="3353" tid="0" op="" dtype="" >7.9751e-04 0.0000e+00 2.8610e-06</hent>
<hent key="0380010000000000000020000000000A" call="MPI_Irecv" bytes="8192" orank="10" region="0" commid="0" count="3181" tid="0" op="" dtype="" >4.2439e-04 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000200000000011" call="MPI_Irecv" bytes="8192" orank="17" region="0" commid="0" count="12626" tid="0" op="" dtype="" >1.4787e-03 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000140000002C9" call="MPI_Isend" bytes="320" orank="713" region="0" commid="0" count="191" tid="0" op="" dtype="" >7.9846e-04 2.8610e-06 1.0014e-05</hent>
<hent key="03800100000000000000000000000001" call="MPI_Irecv" bytes="0" orank="1" region="0" commid="0" count="265" tid="0" op="" dtype="" >5.7697e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000008" call="MPI_Irecv" bytes="0" orank="8" region="0" commid="0" count="247" tid="0" op="" dtype="" >8.9169e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000000000000000A" call="MPI_Irecv" bytes="0" orank="10" region="0" commid="0" count="242" tid="0" op="" dtype="" >4.6253e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000011" call="MPI_Irecv" bytes="0" orank="17" region="0" commid="0" count="254" tid="0" op="" dtype="" >4.6253e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000049" call="MPI_Irecv" bytes="0" orank="73" region="0" commid="0" count="165" tid="0" op="" dtype="" >4.0293e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000049" call="MPI_Isend" bytes="16384" orank="73" region="0" commid="0" count="12693" tid="0" op="" dtype="" >1.0806e-01 3.8147e-06 5.5075e-05</hent>
<hent key="038001000000000000000000000002C9" call="MPI_Irecv" bytes="0" orank="713" region="0" commid="0" count="140" tid="0" op="" dtype="" >3.7193e-05 0.0000e+00 4.0531e-06</hent>
<hent key="024001000000000000004000000002C9" call="MPI_Isend" bytes="16384" orank="713" region="0" commid="0" count="12699" tid="0" op="" dtype="" >8.1685e-02 3.0994e-06 6.5088e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.3249e-04 0.0000e+00 2.7680e-04</hent>
<hent key="02400100000000000000200000000001" call="MPI_Isend" bytes="8192" orank="1" region="0" commid="0" count="12684" tid="0" op="" dtype="" >3.9599e-03 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000200000000008" call="MPI_Isend" bytes="8192" orank="8" region="0" commid="0" count="3848" tid="0" op="" dtype="" >4.3402e-03 0.0000e+00 1.5020e-05</hent>
<hent key="0240010000000000000020000000000A" call="MPI_Isend" bytes="8192" orank="10" region="0" commid="0" count="2784" tid="0" op="" dtype="" >4.8590e-03 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000200000000011" call="MPI_Isend" bytes="8192" orank="17" region="0" commid="0" count="12691" tid="0" op="" dtype="" >6.6037e-03 0.0000e+00 5.7220e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.7909e-05 3.7909e-05 3.7909e-05</hent>
<hent key="02400100000000000000000000000001" call="MPI_Isend" bytes="0" orank="1" region="0" commid="0" count="289" tid="0" op="" dtype="" >1.6856e-04 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000000000000008" call="MPI_Isend" bytes="0" orank="8" region="0" commid="0" count="258" tid="0" op="" dtype="" >1.8024e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000000000000A" call="MPI_Isend" bytes="0" orank="10" region="0" commid="0" count="259" tid="0" op="" dtype="" >5.4717e-04 0.0000e+00 9.2983e-05</hent>
<hent key="02400100000000000000000000000011" call="MPI_Isend" bytes="0" orank="17" region="0" commid="0" count="242" tid="0" op="" dtype="" >1.7381e-04 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000000000000049" call="MPI_Isend" bytes="0" orank="73" region="0" commid="0" count="148" tid="0" op="" dtype="" >5.7554e-04 9.5367e-07 8.8215e-06</hent>
<hent key="024001000000000000000000000002C9" call="MPI_Isend" bytes="0" orank="713" region="0" commid="0" count="149" tid="0" op="" dtype="" >5.8079e-04 1.1921e-06 9.0599e-06</hent>
<hent key="03800100000000000000060000000001" call="MPI_Irecv" bytes="1536" orank="1" region="0" commid="0" count="104" tid="0" op="" dtype="" >2.8133e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000008" call="MPI_Irecv" bytes="1536" orank="8" region="0" commid="0" count="92" tid="0" op="" dtype="" >3.5763e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000A" call="MPI_Irecv" bytes="1536" orank="10" region="0" commid="0" count="104" tid="0" op="" dtype="" >1.9550e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000011" call="MPI_Irecv" bytes="1536" orank="17" region="0" commid="0" count="94" tid="0" op="" dtype="" >2.3842e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000049" call="MPI_Irecv" bytes="1536" orank="73" region="0" commid="0" count="225" tid="0" op="" dtype="" >5.2691e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002C9" call="MPI_Irecv" bytes="1536" orank="713" region="0" commid="0" count="218" tid="0" op="" dtype="" >7.3910e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000001" call="MPI_Isend" bytes="1536" orank="1" region="0" commid="0" count="113" tid="0" op="" dtype="" >2.1935e-04 9.5367e-07 1.3828e-05</hent>
<hent key="02400100000000000000060000000008" call="MPI_Isend" bytes="1536" orank="8" region="0" commid="0" count="94" tid="0" op="" dtype="" >1.8668e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000006000000000A" call="MPI_Isend" bytes="1536" orank="10" region="0" commid="0" count="95" tid="0" op="" dtype="" >3.5000e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000060000000011" call="MPI_Isend" bytes="1536" orank="17" region="0" commid="0" count="99" tid="0" op="" dtype="" >1.9526e-04 9.5367e-07 1.5974e-05</hent>
<hent key="02400100000000000000060000000049" call="MPI_Isend" bytes="1536" orank="73" region="0" commid="0" count="214" tid="0" op="" dtype="" >1.1497e-03 4.7684e-06 1.0967e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002C9" call="MPI_Isend" bytes="1536" orank="713" region="0" commid="0" count="214" tid="0" op="" dtype="" >9.8419e-04 2.8610e-06 6.1989e-06</hent>
<hent key="038001000000000000000C000000000A" call="MPI_Irecv" bytes="3072" orank="10" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C0000000011" call="MPI_Irecv" bytes="3072" orank="17" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C0000000049" call="MPI_Irecv" bytes="3072" orank="73" region="0" commid="0" count="5" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002C9" call="MPI_Irecv" bytes="3072" orank="713" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C0000000008" call="MPI_Isend" bytes="3072" orank="8" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="024001000000000000000C0000000049" call="MPI_Isend" bytes="3072" orank="73" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.6955e-05 5.9605e-06 7.1526e-06</hent>
<hent key="024001000000000000000C00000002C9" call="MPI_Isend" bytes="3072" orank="713" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.8133e-05 5.0068e-06 6.1989e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.1005e-04 2.1005e-04 2.1005e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.7708e-04 1.5497e-04 1.6308e-04</hent>
<hent key="03800100000000000000038000000001" call="MPI_Irecv" bytes="896" orank="1" region="0" commid="0" count="353" tid="0" op="" dtype="" >8.0585e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000008" call="MPI_Irecv" bytes="896" orank="8" region="0" commid="0" count="2833" tid="0" op="" dtype="" >5.2643e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000A" call="MPI_Irecv" bytes="896" orank="10" region="0" commid="0" count="2913" tid="0" op="" dtype="" >5.9772e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000038000000011" call="MPI_Irecv" bytes="896" orank="17" region="0" commid="0" count="315" tid="0" op="" dtype="" >5.7459e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000049" call="MPI_Irecv" bytes="896" orank="73" region="0" commid="0" count="332" tid="0" op="" dtype="" >7.3671e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002C9" call="MPI_Irecv" bytes="896" orank="713" region="0" commid="0" count="326" tid="0" op="" dtype="" >9.1076e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.9829e-05 4.9829e-05 4.9829e-05</hent>
<hent key="03800100000000000000380000000049" call="MPI_Irecv" bytes="14336" orank="73" region="0" commid="0" count="23" tid="0" op="" dtype="" >1.1921e-05 0.0000e+00 2.8610e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="02400100000000000000038000000001" call="MPI_Isend" bytes="896" orank="1" region="0" commid="0" count="311" tid="0" op="" dtype="" >4.2105e-04 9.5367e-07 6.1989e-06</hent>
<hent key="02400100000000000000038000000008" call="MPI_Isend" bytes="896" orank="8" region="0" commid="0" count="2662" tid="0" op="" dtype="" >1.8675e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000003800000000A" call="MPI_Isend" bytes="896" orank="10" region="0" commid="0" count="2965" tid="0" op="" dtype="" >2.9311e-03 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000038000000011" call="MPI_Isend" bytes="896" orank="17" region="0" commid="0" count="349" tid="0" op="" dtype="" >4.6921e-04 9.5367e-07 1.0014e-05</hent>
<hent key="02400100000000000000038000000049" call="MPI_Isend" bytes="896" orank="73" region="0" commid="0" count="324" tid="0" op="" dtype="" >1.6487e-03 3.8147e-06 1.0967e-05</hent>
<hent key="024001000000000000000380000002C9" call="MPI_Isend" bytes="896" orank="713" region="0" commid="0" count="330" tid="0" op="" dtype="" >1.4486e-03 3.8147e-06 9.0599e-06</hent>
<hent key="02400100000000000000380000000049" call="MPI_Isend" bytes="14336" orank="73" region="0" commid="0" count="6" tid="0" op="" dtype="" >5.1498e-05 6.1989e-06 1.3113e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.9794e+00 5.1022e-05 2.0304e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.2170e-03 3.2170e-03 3.2170e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0448e-02 1.0448e-02 1.0448e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.6621e-02 2.6621e-02 2.6621e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.9526e-01 2.2271e-03 1.7657e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.2210e-04 3.2210e-04 3.2210e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5887e+00 3.6883e-04 2.5105e-01</hent>
<hent key="03800100000000000000040000000001" call="MPI_Irecv" bytes="1024" orank="1" region="0" commid="0" count="3378" tid="0" op="" dtype="" >4.5300e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000040000000008" call="MPI_Irecv" bytes="1024" orank="8" region="0" commid="0" count="886" tid="0" op="" dtype="" >1.4925e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000000A" call="MPI_Irecv" bytes="1024" orank="10" region="0" commid="0" count="830" tid="0" op="" dtype="" >1.6856e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000040000000011" call="MPI_Irecv" bytes="1024" orank="17" region="0" commid="0" count="3380" tid="0" op="" dtype="" >4.4703e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.6689e-05 0.0000e+00 8.8215e-06</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.8147e-05 3.8147e-05 3.8147e-05</hent>
<hent key="03800100000000000000070000000001" call="MPI_Irecv" bytes="1792" orank="1" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.3828e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000008" call="MPI_Irecv" bytes="1792" orank="8" region="0" commid="0" count="43" tid="0" op="" dtype="" >1.8835e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000A" call="MPI_Irecv" bytes="1792" orank="10" region="0" commid="0" count="42" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000011" call="MPI_Irecv" bytes="1792" orank="17" region="0" commid="0" count="44" tid="0" op="" dtype="" >8.3447e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000049" call="MPI_Irecv" bytes="1792" orank="73" region="0" commid="0" count="139" tid="0" op="" dtype="" >2.9087e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002C9" call="MPI_Irecv" bytes="1792" orank="713" region="0" commid="0" count="116" tid="0" op="" dtype="" >3.9816e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >5.6982e-05 0.0000e+00 2.8849e-05</hent>
<hent key="02400100000000000000040000000001" call="MPI_Isend" bytes="1024" orank="1" region="0" commid="0" count="3394" tid="0" op="" dtype="" >1.5857e-03 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000040000000008" call="MPI_Isend" bytes="1024" orank="8" region="0" commid="0" count="1026" tid="0" op="" dtype="" >6.4683e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000004000000000A" call="MPI_Isend" bytes="1024" orank="10" region="0" commid="0" count="758" tid="0" op="" dtype="" >5.6195e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000040000000011" call="MPI_Isend" bytes="1024" orank="17" region="0" commid="0" count="3396" tid="0" op="" dtype="" >1.6911e-03 0.0000e+00 1.2159e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >3.8147e-06 1.9073e-06 1.9073e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5506e+00 0.0000e+00 2.2674e+00</hent>
<hent key="038001000000000000000A0000000001" call="MPI_Irecv" bytes="2560" orank="1" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000008" call="MPI_Irecv" bytes="2560" orank="8" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000000A" call="MPI_Irecv" bytes="2560" orank="10" region="0" commid="0" count="8" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000011" call="MPI_Irecv" bytes="2560" orank="17" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000049" call="MPI_Irecv" bytes="2560" orank="73" region="0" commid="0" count="44" tid="0" op="" dtype="" >1.0252e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002C9" call="MPI_Irecv" bytes="2560" orank="713" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.6212e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000001" call="MPI_Isend" bytes="1792" orank="1" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.3256e-04 9.5367e-07 1.5974e-05</hent>
<hent key="02400100000000000000070000000008" call="MPI_Isend" bytes="1792" orank="8" region="0" commid="0" count="43" tid="0" op="" dtype="" >9.3699e-05 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000007000000000A" call="MPI_Isend" bytes="1792" orank="10" region="0" commid="0" count="45" tid="0" op="" dtype="" >1.7357e-04 2.8610e-06 7.1526e-06</hent>
<hent key="02400100000000000000070000000011" call="MPI_Isend" bytes="1792" orank="17" region="0" commid="0" count="34" tid="0" op="" dtype="" >1.2088e-04 9.5367e-07 1.5974e-05</hent>
<hent key="02400100000000000000070000000049" call="MPI_Isend" bytes="1792" orank="73" region="0" commid="0" count="137" tid="0" op="" dtype="" >7.4363e-04 9.5367e-07 1.4067e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1362e-01 1.1362e-01 1.1362e-01</hent>
<hent key="024001000000000000000700000002C9" call="MPI_Isend" bytes="1792" orank="713" region="0" commid="0" count="133" tid="0" op="" dtype="" >6.4445e-04 3.8147e-06 1.0967e-05</hent>
<hent key="024001000000000000000A0000000001" call="MPI_Isend" bytes="2560" orank="1" region="0" commid="0" count="3" tid="0" op="" dtype="" >5.0068e-06 9.5367e-07 2.1458e-06</hent>
<hent key="024001000000000000000A0000000008" call="MPI_Isend" bytes="2560" orank="8" region="0" commid="0" count="8" tid="0" op="" dtype="" >2.1219e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A000000000A" call="MPI_Isend" bytes="2560" orank="10" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.7193e-05 2.8610e-06 5.0068e-06</hent>
<hent key="024001000000000000000A0000000011" call="MPI_Isend" bytes="2560" orank="17" region="0" commid="0" count="2" tid="0" op="" dtype="" >2.8610e-06 9.5367e-07 1.9073e-06</hent>
<hent key="024001000000000000000A0000000049" call="MPI_Isend" bytes="2560" orank="73" region="0" commid="0" count="46" tid="0" op="" dtype="" >2.5988e-04 5.0068e-06 6.9141e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.5010e-04 3.5048e-05 1.0800e-04</hent>
<hent key="024001000000000000000A00000002C9" call="MPI_Isend" bytes="2560" orank="713" region="0" commid="0" count="41" tid="0" op="" dtype="" >2.0528e-04 3.8147e-06 6.1989e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >7.9226e-04 2.8515e-04 5.0712e-04</hent>
<hent key="024001000000000000001000000002C9" call="MPI_Isend" bytes="4096" orank="713" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.5048e-05 3.5048e-05 3.5048e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >7.8678e-06 1.9073e-06 5.9605e-06</hent>
<hent key="03800100000000000000000400000001" call="MPI_Irecv" bytes="4" orank="1" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.4012e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000008" call="MPI_Irecv" bytes="4" orank="8" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.6781e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000000040000000A" call="MPI_Irecv" bytes="4" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.7755e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000011" call="MPI_Irecv" bytes="4" orank="17" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.5361e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000049" call="MPI_Irecv" bytes="4" orank="73" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.7214e-04 0.0000e+00 3.5048e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.4780e-03 2.0695e-04 5.1308e-04</hent>
<hent key="038001000000000000001C0000000008" call="MPI_Irecv" bytes="7168" orank="8" region="0" commid="0" count="9346" tid="0" op="" dtype="" >2.2678e-03 0.0000e+00 9.0599e-06</hent>
<hent key="038001000000000000001C000000000A" call="MPI_Irecv" bytes="7168" orank="10" region="0" commid="0" count="9518" tid="0" op="" dtype="" >1.2250e-03 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000004000002C9" call="MPI_Irecv" bytes="4" orank="713" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.0201e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000400000001" call="MPI_Isend" bytes="4" orank="1" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3759e-03 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000400000008" call="MPI_Isend" bytes="4" orank="8" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1110e-03 0.0000e+00 3.8147e-06</hent>
<hent key="0240010000000000000000040000000A" call="MPI_Isend" bytes="4" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.1105e-03 0.0000e+00 7.8917e-05</hent>
<hent key="02400100000000000000000400000011" call="MPI_Isend" bytes="4" orank="17" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1587e-03 0.0000e+00 2.9087e-05</hent>
<hent key="02400100000000000000000400000049" call="MPI_Isend" bytes="4" orank="73" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3652e-02 4.7684e-06 1.6928e-05</hent>
<hent key="024001000000000000001C0000000008" call="MPI_Isend" bytes="7168" orank="8" region="0" commid="0" count="8851" tid="0" op="" dtype="" >2.0010e-02 9.5367e-07 1.5974e-05</hent>
<hent key="024001000000000000001C000000000A" call="MPI_Isend" bytes="7168" orank="10" region="0" commid="0" count="9915" tid="0" op="" dtype="" >2.8531e-02 9.5367e-07 2.2888e-05</hent>
<hent key="024001000000000000000004000002C9" call="MPI_Isend" bytes="4" orank="713" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7962e-02 3.8147e-06 1.0490e-04</hent>
<hent key="03800100000000000000050000000001" call="MPI_Irecv" bytes="1280" orank="1" region="0" commid="0" count="194" tid="0" op="" dtype="" >4.6015e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000008" call="MPI_Irecv" bytes="1280" orank="8" region="0" commid="0" count="201" tid="0" op="" dtype="" >7.1764e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000A" call="MPI_Irecv" bytes="1280" orank="10" region="0" commid="0" count="196" tid="0" op="" dtype="" >4.1962e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000011" call="MPI_Irecv" bytes="1280" orank="17" region="0" commid="0" count="201" tid="0" op="" dtype="" >3.7193e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000049" call="MPI_Irecv" bytes="1280" orank="73" region="0" commid="0" count="291" tid="0" op="" dtype="" >5.7697e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0735e+01 7.8678e-06 1.5589e-01</hent>
<hent key="03800100000000000000280000000001" call="MPI_Irecv" bytes="10240" orank="1" region="0" commid="0" count="67" tid="0" op="" dtype="" >1.7166e-05 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000280000000011" call="MPI_Irecv" bytes="10240" orank="17" region="0" commid="0" count="73" tid="0" op="" dtype="" >1.1921e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002C9" call="MPI_Irecv" bytes="1280" orank="713" region="0" commid="0" count="272" tid="0" op="" dtype="" >9.7275e-05 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000080000000001" call="MPI_Irecv" bytes="2048" orank="1" region="0" commid="0" count="13" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000008" call="MPI_Irecv" bytes="2048" orank="8" region="0" commid="0" count="12" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000A" call="MPI_Irecv" bytes="2048" orank="10" region="0" commid="0" count="18" tid="0" op="" dtype="" >2.3842e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000011" call="MPI_Irecv" bytes="2048" orank="17" region="0" commid="0" count="15" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000049" call="MPI_Irecv" bytes="2048" orank="73" region="0" commid="0" count="3460" tid="0" op="" dtype="" >5.0974e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002C9" call="MPI_Irecv" bytes="2048" orank="713" region="0" commid="0" count="3484" tid="0" op="" dtype="" >6.4015e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000050000000001" call="MPI_Isend" bytes="1280" orank="1" region="0" commid="0" count="164" tid="0" op="" dtype="" >2.5225e-04 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000050000000008" call="MPI_Isend" bytes="1280" orank="8" region="0" commid="0" count="213" tid="0" op="" dtype="" >3.9530e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000005000000000A" call="MPI_Isend" bytes="1280" orank="10" region="0" commid="0" count="196" tid="0" op="" dtype="" >6.9857e-04 2.1458e-06 9.0599e-06</hent>
<hent key="02400100000000000000050000000011" call="MPI_Isend" bytes="1280" orank="17" region="0" commid="0" count="178" tid="0" op="" dtype="" >3.0017e-04 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000050000000049" call="MPI_Isend" bytes="1280" orank="73" region="0" commid="0" count="307" tid="0" op="" dtype="" >1.6363e-03 3.8147e-06 2.0027e-05</hent>
<hent key="02400100000000000000280000000001" call="MPI_Isend" bytes="10240" orank="1" region="0" commid="0" count="15" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000280000000011" call="MPI_Isend" bytes="10240" orank="17" region="0" commid="0" count="8" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000500000002C9" call="MPI_Isend" bytes="1280" orank="713" region="0" commid="0" count="302" tid="0" op="" dtype="" >1.3723e-03 3.8147e-06 1.0014e-05</hent>
</hash>
<internal rank="9" log_i="1723712895.680139" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="10" mpi_size="768" stamp_init="1723712829.548118" stamp_final="1723712895.693686" username="apac4" allocationname="unknown" flags="0" pid="684278" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61456e+01" utime="4.94797e+01" stime="8.23156e+00" mtime="3.01305e+01" gflop="0.00000e+00" gbyte="3.77449e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.01305e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf475148e14a014be56a0149b14f0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60105e+01" utime="4.94507e+01" stime="8.21850e+00" mtime="3.01305e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.01305e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4883e+08" > 4.9227e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4877e+08" > 4.2875e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1986e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5790e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.7922e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.6955e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5832e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.4168e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0408e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1378e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8107e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="190" >
<hent key="02400100000000000000080000000002" call="MPI_Isend" bytes="2048" orank="2" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.6941e-05 9.5367e-07 7.1526e-06</hent>
<hent key="02400100000000000000080000000009" call="MPI_Isend" bytes="2048" orank="9" region="0" commid="0" count="18" tid="0" op="" dtype="" >4.0770e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000008000000000B" call="MPI_Isend" bytes="2048" orank="11" region="0" commid="0" count="15" tid="0" op="" dtype="" >5.4121e-05 2.8610e-06 4.7684e-06</hent>
<hent key="02400100000000000000080000000012" call="MPI_Isend" bytes="2048" orank="18" region="0" commid="0" count="17" tid="0" op="" dtype="" >4.9353e-05 9.5367e-07 1.1921e-05</hent>
<hent key="0240010000000000000008000000004A" call="MPI_Isend" bytes="2048" orank="74" region="0" commid="0" count="3460" tid="0" op="" dtype="" >1.8027e-02 9.5367e-07 3.3855e-05</hent>
<hent key="024001000000000000000800000002CA" call="MPI_Isend" bytes="2048" orank="714" region="0" commid="0" count="3467" tid="0" op="" dtype="" >1.6795e-02 9.5367e-07 3.7193e-05</hent>
<hent key="038001000000000000000E000000004A" call="MPI_Irecv" bytes="3584" orank="74" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000E00000002CA" call="MPI_Irecv" bytes="3584" orank="714" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="03800100000000000000028000000002" call="MPI_Irecv" bytes="640" orank="2" region="0" commid="0" count="407" tid="0" op="" dtype="" >1.2684e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000028000000009" call="MPI_Irecv" bytes="640" orank="9" region="0" commid="0" count="412" tid="0" op="" dtype="" >1.6236e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000B" call="MPI_Irecv" bytes="640" orank="11" region="0" commid="0" count="364" tid="0" op="" dtype="" >9.0599e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000028000000012" call="MPI_Irecv" bytes="640" orank="18" region="0" commid="0" count="424" tid="0" op="" dtype="" >9.3222e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000004A" call="MPI_Irecv" bytes="640" orank="74" region="0" commid="0" count="257" tid="0" op="" dtype="" >1.7953e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="038001000000000000000280000002CA" call="MPI_Irecv" bytes="640" orank="714" region="0" commid="0" count="271" tid="0" op="" dtype="" >2.6679e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000028000000002" call="MPI_Isend" bytes="640" orank="2" region="0" commid="0" count="412" tid="0" op="" dtype="" >5.0521e-04 9.5367e-07 7.1526e-06</hent>
<hent key="02400100000000000000028000000009" call="MPI_Isend" bytes="640" orank="9" region="0" commid="0" count="401" tid="0" op="" dtype="" >5.3930e-04 9.5367e-07 8.8215e-06</hent>
<hent key="0240010000000000000002800000000B" call="MPI_Isend" bytes="640" orank="11" region="0" commid="0" count="424" tid="0" op="" dtype="" >1.2150e-03 1.9073e-06 1.0014e-05</hent>
<hent key="02400100000000000000028000000012" call="MPI_Isend" bytes="640" orank="18" region="0" commid="0" count="392" tid="0" op="" dtype="" >4.3225e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000002800000004A" call="MPI_Isend" bytes="640" orank="74" region="0" commid="0" count="295" tid="0" op="" dtype="" >1.6916e-03 3.8147e-06 1.5020e-05</hent>
<hent key="024001000000000000000280000002CA" call="MPI_Isend" bytes="640" orank="714" region="0" commid="0" count="299" tid="0" op="" dtype="" >1.3778e-03 2.8610e-06 1.1921e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.2187e-05 3.2187e-05 3.2187e-05</hent>
<hent key="03800100000000000000014000000002" call="MPI_Irecv" bytes="320" orank="2" region="0" commid="0" count="358" tid="0" op="" dtype="" >1.0824e-04 0.0000e+00 3.8147e-06</hent>
<hent key="03800100000000000000014000000009" call="MPI_Irecv" bytes="320" orank="9" region="0" commid="0" count="344" tid="0" op="" dtype="" >1.3924e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000B" call="MPI_Irecv" bytes="320" orank="11" region="0" commid="0" count="391" tid="0" op="" dtype="" >1.0610e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000012" call="MPI_Irecv" bytes="320" orank="18" region="0" commid="0" count="345" tid="0" op="" dtype="" >8.5831e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000004A" call="MPI_Irecv" bytes="320" orank="74" region="0" commid="0" count="181" tid="0" op="" dtype="" >1.2541e-04 0.0000e+00 9.0599e-06</hent>
<hent key="038001000000000000000140000002CA" call="MPI_Irecv" bytes="320" orank="714" region="0" commid="0" count="180" tid="0" op="" dtype="" >2.0647e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >7.3910e-06 0.0000e+00 3.0994e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.1986e+00 0.0000e+00 1.5376e-01</hent>
<hent key="0380010000000000000040000000004A" call="MPI_Irecv" bytes="16384" orank="74" region="0" commid="0" count="12699" tid="0" op="" dtype="" >7.0114e-03 0.0000e+00 2.4080e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6436e-02 9.5367e-07 1.6419e-02</hent>
<hent key="038001000000000000004000000002CA" call="MPI_Irecv" bytes="16384" orank="714" region="0" commid="0" count="12691" tid="0" op="" dtype="" >5.0192e-03 0.0000e+00 2.9087e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000002" call="MPI_Isend" bytes="320" orank="2" region="0" commid="0" count="357" tid="0" op="" dtype="" >3.4952e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000014000000009" call="MPI_Isend" bytes="320" orank="9" region="0" commid="0" count="342" tid="0" op="" dtype="" >3.3832e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000001400000000B" call="MPI_Isend" bytes="320" orank="11" region="0" commid="0" count="382" tid="0" op="" dtype="" >9.5415e-04 1.9073e-06 4.0531e-06</hent>
<hent key="02400100000000000000014000000012" call="MPI_Isend" bytes="320" orank="18" region="0" commid="0" count="385" tid="0" op="" dtype="" >3.5858e-04 0.0000e+00 9.7752e-06</hent>
<hent key="0240010000000000000001400000004A" call="MPI_Isend" bytes="320" orank="74" region="0" commid="0" count="172" tid="0" op="" dtype="" >9.0170e-04 3.8147e-06 1.3113e-05</hent>
<hent key="03800100000000000000200000000002" call="MPI_Irecv" bytes="8192" orank="2" region="0" commid="0" count="12699" tid="0" op="" dtype="" >3.9475e-03 0.0000e+00 2.7180e-05</hent>
<hent key="03800100000000000000200000000009" call="MPI_Irecv" bytes="8192" orank="9" region="0" commid="0" count="2784" tid="0" op="" dtype="" >1.0631e-03 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000020000000000B" call="MPI_Irecv" bytes="8192" orank="11" region="0" commid="0" count="4063" tid="0" op="" dtype="" >1.0321e-03 0.0000e+00 1.2159e-05</hent>
<hent key="03800100000000000000200000000012" call="MPI_Irecv" bytes="8192" orank="18" region="0" commid="0" count="12699" tid="0" op="" dtype="" >2.0471e-03 0.0000e+00 1.7166e-05</hent>
<hent key="024001000000000000000140000002CA" call="MPI_Isend" bytes="320" orank="714" region="0" commid="0" count="163" tid="0" op="" dtype="" >7.0477e-04 2.8610e-06 1.0014e-05</hent>
<hent key="03800100000000000000000000000002" call="MPI_Irecv" bytes="0" orank="2" region="0" commid="0" count="252" tid="0" op="" dtype="" >9.9182e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000000000009" call="MPI_Irecv" bytes="0" orank="9" region="0" commid="0" count="259" tid="0" op="" dtype="" >6.6280e-05 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000000000000000B" call="MPI_Irecv" bytes="0" orank="11" region="0" commid="0" count="257" tid="0" op="" dtype="" >7.6771e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000012" call="MPI_Irecv" bytes="0" orank="18" region="0" commid="0" count="264" tid="0" op="" dtype="" >5.7936e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000004A" call="MPI_Irecv" bytes="0" orank="74" region="0" commid="0" count="143" tid="0" op="" dtype="" >1.1683e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000040000000004A" call="MPI_Isend" bytes="16384" orank="74" region="0" commid="0" count="12667" tid="0" op="" dtype="" >1.7510e-01 4.0531e-06 9.4080e-04</hent>
<hent key="038001000000000000000000000002CA" call="MPI_Irecv" bytes="0" orank="714" region="0" commid="0" count="140" tid="0" op="" dtype="" >9.1076e-05 0.0000e+00 6.9141e-06</hent>
<hent key="024001000000000000004000000002CA" call="MPI_Isend" bytes="16384" orank="714" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.1007e-01 3.8147e-06 4.1962e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.3726e-04 0.0000e+00 2.8396e-04</hent>
<hent key="02400100000000000000200000000002" call="MPI_Isend" bytes="8192" orank="2" region="0" commid="0" count="12699" tid="0" op="" dtype="" >5.0731e-03 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000200000000009" call="MPI_Isend" bytes="8192" orank="9" region="0" commid="0" count="3181" tid="0" op="" dtype="" >4.2522e-03 0.0000e+00 4.9114e-05</hent>
<hent key="0240010000000000000020000000000B" call="MPI_Isend" bytes="8192" orank="11" region="0" commid="0" count="3681" tid="0" op="" dtype="" >8.0509e-03 0.0000e+00 4.6015e-05</hent>
<hent key="02400100000000000000200000000012" call="MPI_Isend" bytes="8192" orank="18" region="0" commid="0" count="12647" tid="0" op="" dtype="" >9.2142e-03 0.0000e+00 5.1975e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.4836e-05 5.4836e-05 5.4836e-05</hent>
<hent key="02400100000000000000000000000002" call="MPI_Isend" bytes="0" orank="2" region="0" commid="0" count="260" tid="0" op="" dtype="" >1.8549e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000009" call="MPI_Isend" bytes="0" orank="9" region="0" commid="0" count="242" tid="0" op="" dtype="" >1.5736e-04 0.0000e+00 8.8215e-06</hent>
<hent key="0240010000000000000000000000000B" call="MPI_Isend" bytes="0" orank="11" region="0" commid="0" count="264" tid="0" op="" dtype="" >5.4121e-04 9.5367e-07 6.7949e-05</hent>
<hent key="02400100000000000000000000000012" call="MPI_Isend" bytes="0" orank="18" region="0" commid="0" count="242" tid="0" op="" dtype="" >1.5759e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000000000000004A" call="MPI_Isend" bytes="0" orank="74" region="0" commid="0" count="144" tid="0" op="" dtype="" >6.5494e-04 2.8610e-06 1.4067e-05</hent>
<hent key="024001000000000000000000000002CA" call="MPI_Isend" bytes="0" orank="714" region="0" commid="0" count="140" tid="0" op="" dtype="" >5.5337e-04 2.8610e-06 1.0014e-05</hent>
<hent key="03800100000000000000060000000002" call="MPI_Irecv" bytes="1536" orank="2" region="0" commid="0" count="99" tid="0" op="" dtype="" >3.1233e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000009" call="MPI_Irecv" bytes="1536" orank="9" region="0" commid="0" count="95" tid="0" op="" dtype="" >3.8147e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000B" call="MPI_Irecv" bytes="1536" orank="11" region="0" commid="0" count="85" tid="0" op="" dtype="" >2.5988e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000060000000012" call="MPI_Irecv" bytes="1536" orank="18" region="0" commid="0" count="115" tid="0" op="" dtype="" >2.5034e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000004A" call="MPI_Irecv" bytes="1536" orank="74" region="0" commid="0" count="215" tid="0" op="" dtype="" >1.6665e-04 0.0000e+00 1.0014e-05</hent>
<hent key="038001000000000000000600000002CA" call="MPI_Irecv" bytes="1536" orank="714" region="0" commid="0" count="230" tid="0" op="" dtype="" >2.6393e-04 0.0000e+00 5.9605e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000060000000002" call="MPI_Isend" bytes="1536" orank="2" region="0" commid="0" count="130" tid="0" op="" dtype="" >2.3913e-04 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000060000000009" call="MPI_Isend" bytes="1536" orank="9" region="0" commid="0" count="104" tid="0" op="" dtype="" >1.9503e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000006000000000B" call="MPI_Isend" bytes="1536" orank="11" region="0" commid="0" count="82" tid="0" op="" dtype="" >2.8801e-04 2.8610e-06 4.0531e-06</hent>
<hent key="02400100000000000000060000000012" call="MPI_Isend" bytes="1536" orank="18" region="0" commid="0" count="100" tid="0" op="" dtype="" >2.2411e-04 9.5367e-07 2.0981e-05</hent>
<hent key="0240010000000000000006000000004A" call="MPI_Isend" bytes="1536" orank="74" region="0" commid="0" count="232" tid="0" op="" dtype="" >1.4348e-03 4.7684e-06 1.5020e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002CA" call="MPI_Isend" bytes="1536" orank="714" region="0" commid="0" count="216" tid="0" op="" dtype="" >1.1828e-03 3.8147e-06 1.2875e-05</hent>
<hent key="038001000000000000000C000000004A" call="MPI_Irecv" bytes="3072" orank="74" region="0" commid="0" count="4" tid="0" op="" dtype="" >7.8678e-06 9.5367e-07 3.0994e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002CA" call="MPI_Irecv" bytes="3072" orank="714" region="0" commid="0" count="7" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 3.0994e-06</hent>
<hent key="024001000000000000000C0000000009" call="MPI_Isend" bytes="3072" orank="9" region="0" commid="0" count="1" tid="0" op="" dtype="" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="024001000000000000000C000000004A" call="MPI_Isend" bytes="3072" orank="74" region="0" commid="0" count="8" tid="0" op="" dtype="" >6.4611e-05 6.1989e-06 1.0967e-05</hent>
<hent key="024001000000000000000C00000002CA" call="MPI_Isend" bytes="3072" orank="714" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.7895e-05 5.0068e-06 1.0967e-05</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.2197e-04 2.2197e-04 2.2197e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >5.1117e-04 1.6212e-04 1.7905e-04</hent>
<hent key="03800100000000000000038000000002" call="MPI_Irecv" bytes="896" orank="2" region="0" commid="0" count="321" tid="0" op="" dtype="" >8.5115e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000009" call="MPI_Irecv" bytes="896" orank="9" region="0" commid="0" count="2965" tid="0" op="" dtype="" >5.2047e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000003800000000B" call="MPI_Irecv" bytes="896" orank="11" region="0" commid="0" count="2618" tid="0" op="" dtype="" >5.9748e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000038000000012" call="MPI_Irecv" bytes="896" orank="18" region="0" commid="0" count="294" tid="0" op="" dtype="" >7.3910e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000004A" call="MPI_Irecv" bytes="896" orank="74" region="0" commid="0" count="323" tid="0" op="" dtype="" >2.1505e-04 0.0000e+00 1.0014e-05</hent>
<hent key="038001000000000000000380000002CA" call="MPI_Irecv" bytes="896" orank="714" region="0" commid="0" count="347" tid="0" op="" dtype="" >3.2878e-04 0.0000e+00 1.8120e-05</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.5740e-03 5.5740e-03 5.5740e-03</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000003800000002CA" call="MPI_Irecv" bytes="14336" orank="714" region="0" commid="0" count="8" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="02400100000000000000038000000002" call="MPI_Isend" bytes="896" orank="2" region="0" commid="0" count="300" tid="0" op="" dtype="" >4.3082e-04 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000038000000009" call="MPI_Isend" bytes="896" orank="9" region="0" commid="0" count="2913" tid="0" op="" dtype="" >2.1248e-03 0.0000e+00 7.8678e-06</hent>
<hent key="0240010000000000000003800000000B" call="MPI_Isend" bytes="896" orank="11" region="0" commid="0" count="2746" tid="0" op="" dtype="" >2.8419e-03 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000038000000012" call="MPI_Isend" bytes="896" orank="18" region="0" commid="0" count="324" tid="0" op="" dtype="" >4.4513e-04 9.5367e-07 1.0014e-05</hent>
<hent key="0240010000000000000003800000004A" call="MPI_Isend" bytes="896" orank="74" region="0" commid="0" count="335" tid="0" op="" dtype="" >1.9457e-03 3.8147e-06 1.4067e-05</hent>
<hent key="024001000000000000000380000002CA" call="MPI_Isend" bytes="896" orank="714" region="0" commid="0" count="331" tid="0" op="" dtype="" >1.5986e-03 2.8610e-06 1.5974e-05</hent>
<hent key="0240010000000000000038000000004A" call="MPI_Isend" bytes="14336" orank="74" region="0" commid="0" count="32" tid="0" op="" dtype="" >6.8951e-04 5.9605e-06 4.5061e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.3453e+00 1.0014e-05 2.0312e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.2110e-03 3.2110e-03 3.2110e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0408e-02 1.0408e-02 1.0408e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.6020e-02 2.6020e-02 2.6020e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.9885e-01 2.2550e-03 1.8012e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >2.4509e-04 2.4509e-04 2.4509e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 9.5367e-07 9.5367e-07</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5832e+00 3.6597e-04 2.5098e-01</hent>
<hent key="03800100000000000000040000000002" call="MPI_Irecv" bytes="1024" orank="2" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.7936e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000040000000009" call="MPI_Irecv" bytes="1024" orank="9" region="0" commid="0" count="758" tid="0" op="" dtype="" >1.0633e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000004000000000B" call="MPI_Irecv" bytes="1024" orank="11" region="0" commid="0" count="1114" tid="0" op="" dtype="" >2.0933e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000012" call="MPI_Irecv" bytes="1024" orank="18" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.3072e-04 0.0000e+00 2.5034e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >4.7922e-05 1.9073e-06 2.0981e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.1008e-05 4.1008e-05 4.1008e-05</hent>
<hent key="03800100000000000000070000000002" call="MPI_Irecv" bytes="1792" orank="2" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000009" call="MPI_Irecv" bytes="1792" orank="9" region="0" commid="0" count="45" tid="0" op="" dtype="" >1.5259e-05 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000007000000000B" call="MPI_Irecv" bytes="1792" orank="11" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.3828e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000012" call="MPI_Irecv" bytes="1792" orank="18" region="0" commid="0" count="28" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000007000000004A" call="MPI_Irecv" bytes="1792" orank="74" region="0" commid="0" count="128" tid="0" op="" dtype="" >1.0777e-04 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000700000002CA" call="MPI_Irecv" bytes="1792" orank="714" region="0" commid="0" count="127" tid="0" op="" dtype="" >1.3351e-04 0.0000e+00 4.0531e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >5.2929e-05 0.0000e+00 2.6226e-05</hent>
<hent key="02400100000000000000040000000002" call="MPI_Isend" bytes="1024" orank="2" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.2581e-03 0.0000e+00 1.9073e-05</hent>
<hent key="02400100000000000000040000000009" call="MPI_Isend" bytes="1024" orank="9" region="0" commid="0" count="830" tid="0" op="" dtype="" >4.9400e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000004000000000B" call="MPI_Isend" bytes="1024" orank="11" region="0" commid="0" count="958" tid="0" op="" dtype="" >7.9226e-04 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000040000000012" call="MPI_Isend" bytes="1024" orank="18" region="0" commid="0" count="3380" tid="0" op="" dtype="" >2.0266e-03 0.0000e+00 1.0014e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5563e+00 0.0000e+00 2.2686e+00</hent>
<hent key="038001000000000000000A0000000002" call="MPI_Irecv" bytes="2560" orank="2" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000009" call="MPI_Irecv" bytes="2560" orank="9" region="0" commid="0" count="9" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000000B" call="MPI_Irecv" bytes="2560" orank="11" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000012" call="MPI_Irecv" bytes="2560" orank="18" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000004A" call="MPI_Irecv" bytes="2560" orank="74" region="0" commid="0" count="50" tid="0" op="" dtype="" >4.3631e-05 0.0000e+00 9.0599e-06</hent>
<hent key="038001000000000000000A00000002CA" call="MPI_Irecv" bytes="2560" orank="714" region="0" commid="0" count="46" tid="0" op="" dtype="" >7.1764e-05 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000070000000002" call="MPI_Isend" bytes="1792" orank="2" region="0" commid="0" count="47" tid="0" op="" dtype="" >9.5606e-05 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000070000000009" call="MPI_Isend" bytes="1792" orank="9" region="0" commid="0" count="42" tid="0" op="" dtype="" >9.2030e-05 1.1921e-06 3.0994e-06</hent>
<hent key="0240010000000000000007000000000B" call="MPI_Isend" bytes="1792" orank="11" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.1086e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000070000000012" call="MPI_Isend" bytes="1792" orank="18" region="0" commid="0" count="44" tid="0" op="" dtype="" >9.7275e-05 9.5367e-07 1.0014e-05</hent>
<hent key="0240010000000000000007000000004A" call="MPI_Isend" bytes="1792" orank="74" region="0" commid="0" count="114" tid="0" op="" dtype="" >7.1335e-04 1.9073e-06 1.5974e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1378e-01 1.1378e-01 1.1378e-01</hent>
<hent key="024001000000000000000700000002CA" call="MPI_Isend" bytes="1792" orank="714" region="0" commid="0" count="126" tid="0" op="" dtype="" >7.1287e-04 3.8147e-06 1.3828e-05</hent>
<hent key="024001000000000000000A0000000002" call="MPI_Isend" bytes="2560" orank="2" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.4080e-05 1.9073e-06 1.0967e-05</hent>
<hent key="024001000000000000000A0000000009" call="MPI_Isend" bytes="2560" orank="9" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.9550e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A000000000B" call="MPI_Isend" bytes="2560" orank="11" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.5259e-05 3.0994e-06 4.0531e-06</hent>
<hent key="024001000000000000000A0000000012" call="MPI_Isend" bytes="2560" orank="18" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.8120e-05 9.5367e-07 1.3113e-05</hent>
<hent key="024001000000000000000A000000004A" call="MPI_Isend" bytes="2560" orank="74" region="0" commid="0" count="51" tid="0" op="" dtype="" >3.4237e-04 5.0068e-06 1.1921e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.6393e-04 3.0994e-05 1.2088e-04</hent>
<hent key="024001000000000000000A00000002CA" call="MPI_Isend" bytes="2560" orank="714" region="0" commid="0" count="46" tid="0" op="" dtype="" >2.7204e-04 5.0068e-06 1.1206e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >8.3709e-04 3.0208e-04 5.3501e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.1948e-05 3.1948e-05 3.1948e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >6.9141e-06 2.1458e-06 4.7684e-06</hent>
<hent key="03800100000000000000000400000002" call="MPI_Irecv" bytes="4" orank="2" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.6410e-04 0.0000e+00 1.2875e-05</hent>
<hent key="03800100000000000000000400000009" call="MPI_Irecv" bytes="4" orank="9" region="0" commid="0" count="3398" tid="0" op="" dtype="" >9.4438e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000040000000B" call="MPI_Irecv" bytes="4" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.1011e-04 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000000400000012" call="MPI_Irecv" bytes="4" orank="18" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.0582e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000000040000004A" call="MPI_Irecv" bytes="4" orank="74" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.5570e-03 0.0000e+00 9.2983e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.5733e-03 2.0909e-04 5.5194e-04</hent>
<hent key="038001000000000000001C0000000009" call="MPI_Irecv" bytes="7168" orank="9" region="0" commid="0" count="9915" tid="0" op="" dtype="" >3.6321e-03 0.0000e+00 8.1062e-06</hent>
<hent key="038001000000000000001C000000000B" call="MPI_Irecv" bytes="7168" orank="11" region="0" commid="0" count="8636" tid="0" op="" dtype="" >2.0990e-03 0.0000e+00 7.1526e-06</hent>
<hent key="038001000000000000000004000002CA" call="MPI_Irecv" bytes="4" orank="714" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.0843e-03 0.0000e+00 3.1948e-05</hent>
<hent key="02400100000000000000000400000002" call="MPI_Isend" bytes="4" orank="2" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.5719e-03 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000000400000009" call="MPI_Isend" bytes="4" orank="9" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3132e-03 0.0000e+00 4.7207e-05</hent>
<hent key="0240010000000000000000040000000B" call="MPI_Isend" bytes="4" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.9902e-03 0.0000e+00 2.5034e-05</hent>
<hent key="02400100000000000000000400000012" call="MPI_Isend" bytes="4" orank="18" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3871e-03 0.0000e+00 2.4080e-05</hent>
<hent key="0240010000000000000000040000004A" call="MPI_Isend" bytes="4" orank="74" region="0" commid="0" count="3398" tid="0" op="" dtype="" >3.0092e-02 3.8147e-06 4.3869e-05</hent>
<hent key="024001000000000000001C0000000009" call="MPI_Isend" bytes="7168" orank="9" region="0" commid="0" count="9518" tid="0" op="" dtype="" >2.0757e-02 9.5367e-07 1.9789e-05</hent>
<hent key="024001000000000000001C000000000B" call="MPI_Isend" bytes="7168" orank="11" region="0" commid="0" count="9018" tid="0" op="" dtype="" >2.8949e-02 9.5367e-07 2.3127e-05</hent>
<hent key="024001000000000000000004000002CA" call="MPI_Isend" bytes="4" orank="714" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.0046e-02 2.8610e-06 1.2994e-04</hent>
<hent key="03800100000000000000050000000002" call="MPI_Irecv" bytes="1280" orank="2" region="0" commid="0" count="207" tid="0" op="" dtype="" >6.0081e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000009" call="MPI_Irecv" bytes="1280" orank="9" region="0" commid="0" count="196" tid="0" op="" dtype="" >7.7486e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000B" call="MPI_Irecv" bytes="1280" orank="11" region="0" commid="0" count="213" tid="0" op="" dtype="" >5.6744e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000050000000012" call="MPI_Irecv" bytes="1280" orank="18" region="0" commid="0" count="209" tid="0" op="" dtype="" >5.0068e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000004A" call="MPI_Irecv" bytes="1280" orank="74" region="0" commid="0" count="331" tid="0" op="" dtype="" >2.0933e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0533e+01 6.9141e-06 1.5588e-01</hent>
<hent key="038001000000000000000500000002CA" call="MPI_Irecv" bytes="1280" orank="714" region="0" commid="0" count="295" tid="0" op="" dtype="" >3.3689e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000080000000002" call="MPI_Irecv" bytes="2048" orank="2" region="0" commid="0" count="15" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000009" call="MPI_Irecv" bytes="2048" orank="9" region="0" commid="0" count="14" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000B" call="MPI_Irecv" bytes="2048" orank="11" region="0" commid="0" count="16" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000012" call="MPI_Irecv" bytes="2048" orank="18" region="0" commid="0" count="13" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000004A" call="MPI_Irecv" bytes="2048" orank="74" region="0" commid="0" count="3464" tid="0" op="" dtype="" >1.2946e-03 0.0000e+00 2.2888e-05</hent>
<hent key="038001000000000000000800000002CA" call="MPI_Irecv" bytes="2048" orank="714" region="0" commid="0" count="3453" tid="0" op="" dtype="" >1.1344e-03 0.0000e+00 1.5974e-05</hent>
<hent key="02400100000000000000050000000002" call="MPI_Isend" bytes="1280" orank="2" region="0" commid="0" count="178" tid="0" op="" dtype="" >2.7871e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000050000000009" call="MPI_Isend" bytes="1280" orank="9" region="0" commid="0" count="196" tid="0" op="" dtype="" >3.5214e-04 9.5367e-07 5.9605e-06</hent>
<hent key="0240010000000000000005000000000B" call="MPI_Isend" bytes="1280" orank="11" region="0" commid="0" count="193" tid="0" op="" dtype="" >6.6733e-04 1.9073e-06 1.9073e-05</hent>
<hent key="02400100000000000000050000000012" call="MPI_Isend" bytes="1280" orank="18" region="0" commid="0" count="209" tid="0" op="" dtype="" >3.0613e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000005000000004A" call="MPI_Isend" bytes="1280" orank="74" region="0" commid="0" count="286" tid="0" op="" dtype="" >1.7560e-03 4.7684e-06 1.3113e-05</hent>
<hent key="02400100000000000000280000000012" call="MPI_Isend" bytes="10240" orank="18" region="0" commid="0" count="52" tid="0" op="" dtype="" >3.4809e-05 0.0000e+00 1.9073e-06</hent>
<hent key="024001000000000000000500000002CA" call="MPI_Isend" bytes="1280" orank="714" region="0" commid="0" count="305" tid="0" op="" dtype="" >1.5700e-03 3.8147e-06 3.0994e-05</hent>
</hash>
<internal rank="10" log_i="1723712895.693686" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="11" mpi_size="768" stamp_init="1723712829.548148" stamp_final="1723712895.690365" username="apac4" allocationname="unknown" flags="0" pid="684279" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61422e+01" utime="5.11445e+01" stime="7.54226e+00" mtime="3.06879e+01" gflop="0.00000e+00" gbyte="3.78033e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.06879e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4321434143514f0553514351492" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60099e+01" utime="5.11160e+01" stime="7.52802e+00" mtime="3.06879e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.06879e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4919e+08" > 3.6979e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4846e+08" > 2.5878e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2069e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5753e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1962e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5884e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.7911e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0420e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1357e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8794e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="195" >
<hent key="02400100000000000000080000000003" call="MPI_Isend" bytes="2048" orank="3" region="0" commid="0" count="10" tid="0" op="" dtype="" >2.6464e-05 9.5367e-07 1.1921e-05</hent>
<hent key="0240010000000000000008000000000A" call="MPI_Isend" bytes="2048" orank="10" region="0" commid="0" count="16" tid="0" op="" dtype="" >3.6240e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000008000000000C" call="MPI_Isend" bytes="2048" orank="12" region="0" commid="0" count="13" tid="0" op="" dtype="" >5.2691e-05 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000080000000013" call="MPI_Isend" bytes="2048" orank="19" region="0" commid="0" count="11" tid="0" op="" dtype="" >3.3617e-05 1.9073e-06 1.3828e-05</hent>
<hent key="0240010000000000000008000000004B" call="MPI_Isend" bytes="2048" orank="75" region="0" commid="0" count="3454" tid="0" op="" dtype="" >1.1590e-02 9.5367e-07 2.1935e-05</hent>
<hent key="024001000000000000000800000002CB" call="MPI_Isend" bytes="2048" orank="715" region="0" commid="0" count="3473" tid="0" op="" dtype="" >1.1263e-02 9.5367e-07 1.3113e-05</hent>
<hent key="038001000000000000000E000000004B" call="MPI_Irecv" bytes="3584" orank="75" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000E00000002CB" call="MPI_Irecv" bytes="3584" orank="715" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000E000000004B" call="MPI_Isend" bytes="3584" orank="75" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000003" call="MPI_Irecv" bytes="640" orank="3" region="0" commid="0" count="396" tid="0" op="" dtype="" >8.4639e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000A" call="MPI_Irecv" bytes="640" orank="10" region="0" commid="0" count="424" tid="0" op="" dtype="" >1.5545e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000C" call="MPI_Irecv" bytes="640" orank="12" region="0" commid="0" count="419" tid="0" op="" dtype="" >1.0061e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000013" call="MPI_Irecv" bytes="640" orank="19" region="0" commid="0" count="394" tid="0" op="" dtype="" >7.3433e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000004B" call="MPI_Irecv" bytes="640" orank="75" region="0" commid="0" count="298" tid="0" op="" dtype="" >9.5129e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000000280000002CB" call="MPI_Irecv" bytes="640" orank="715" region="0" commid="0" count="278" tid="0" op="" dtype="" >7.3910e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000003" call="MPI_Isend" bytes="640" orank="3" region="0" commid="0" count="402" tid="0" op="" dtype="" >4.7421e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000002800000000A" call="MPI_Isend" bytes="640" orank="10" region="0" commid="0" count="364" tid="0" op="" dtype="" >5.3811e-04 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000002800000000C" call="MPI_Isend" bytes="640" orank="12" region="0" commid="0" count="433" tid="0" op="" dtype="" >1.3719e-03 1.9073e-06 1.7881e-05</hent>
<hent key="02400100000000000000028000000013" call="MPI_Isend" bytes="640" orank="19" region="0" commid="0" count="410" tid="0" op="" dtype="" >5.0545e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000002800000004B" call="MPI_Isend" bytes="640" orank="75" region="0" commid="0" count="281" tid="0" op="" dtype="" >1.3580e-03 3.8147e-06 1.0967e-05</hent>
<hent key="024001000000000000000280000002CB" call="MPI_Isend" bytes="640" orank="715" region="0" commid="0" count="282" tid="0" op="" dtype="" >1.2121e-03 2.8610e-06 8.8215e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.2187e-05 3.2187e-05 3.2187e-05</hent>
<hent key="03800100000000000000014000000003" call="MPI_Irecv" bytes="320" orank="3" region="0" commid="0" count="348" tid="0" op="" dtype="" >6.8665e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000A" call="MPI_Irecv" bytes="320" orank="10" region="0" commid="0" count="382" tid="0" op="" dtype="" >1.4925e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000C" call="MPI_Irecv" bytes="320" orank="12" region="0" commid="0" count="321" tid="0" op="" dtype="" >7.2718e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000013" call="MPI_Irecv" bytes="320" orank="19" region="0" commid="0" count="352" tid="0" op="" dtype="" >6.8903e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000004B" call="MPI_Irecv" bytes="320" orank="75" region="0" commid="0" count="182" tid="0" op="" dtype="" >6.1035e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002CB" call="MPI_Irecv" bytes="320" orank="715" region="0" commid="0" count="175" tid="0" op="" dtype="" >5.8413e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.2069e+00 0.0000e+00 1.5378e-01</hent>
<hent key="0380010000000000000040000000004B" call="MPI_Irecv" bytes="16384" orank="75" region="0" commid="0" count="12620" tid="0" op="" dtype="" >4.7388e-03 0.0000e+00 2.0027e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6440e-02 0.0000e+00 1.6424e-02</hent>
<hent key="038001000000000000004000000002CB" call="MPI_Irecv" bytes="16384" orank="715" region="0" commid="0" count="12624" tid="0" op="" dtype="" >2.5914e-03 0.0000e+00 6.9141e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000003" call="MPI_Isend" bytes="320" orank="3" region="0" commid="0" count="363" tid="0" op="" dtype="" >3.3927e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000001400000000A" call="MPI_Isend" bytes="320" orank="10" region="0" commid="0" count="391" tid="0" op="" dtype="" >4.5109e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000001400000000C" call="MPI_Isend" bytes="320" orank="12" region="0" commid="0" count="341" tid="0" op="" dtype="" >9.6488e-04 1.9073e-06 1.0014e-05</hent>
<hent key="02400100000000000000014000000013" call="MPI_Isend" bytes="320" orank="19" region="0" commid="0" count="341" tid="0" op="" dtype="" >3.2115e-04 0.0000e+00 8.8215e-06</hent>
<hent key="0240010000000000000001400000004B" call="MPI_Isend" bytes="320" orank="75" region="0" commid="0" count="178" tid="0" op="" dtype="" >8.0037e-04 3.8147e-06 5.9605e-06</hent>
<hent key="03800100000000000000200000000003" call="MPI_Irecv" bytes="8192" orank="3" region="0" commid="0" count="12683" tid="0" op="" dtype="" >2.5952e-03 0.0000e+00 1.5974e-05</hent>
<hent key="0380010000000000000020000000000A" call="MPI_Irecv" bytes="8192" orank="10" region="0" commid="0" count="3681" tid="0" op="" dtype="" >1.1454e-03 0.0000e+00 1.1921e-05</hent>
<hent key="0380010000000000000020000000000C" call="MPI_Irecv" bytes="8192" orank="12" region="0" commid="0" count="3108" tid="0" op="" dtype="" >3.9148e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000200000000013" call="MPI_Irecv" bytes="8192" orank="19" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.3614e-03 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000140000002CB" call="MPI_Isend" bytes="320" orank="715" region="0" commid="0" count="178" tid="0" op="" dtype="" >7.4863e-04 2.8610e-06 7.1526e-06</hent>
<hent key="03800100000000000000000000000003" call="MPI_Irecv" bytes="0" orank="3" region="0" commid="0" count="265" tid="0" op="" dtype="" >4.6968e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000A" call="MPI_Irecv" bytes="0" orank="10" region="0" commid="0" count="264" tid="0" op="" dtype="" >9.8705e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000C" call="MPI_Irecv" bytes="0" orank="12" region="0" commid="0" count="228" tid="0" op="" dtype="" >4.8161e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000013" call="MPI_Irecv" bytes="0" orank="19" region="0" commid="0" count="224" tid="0" op="" dtype="" >4.9353e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000004B" call="MPI_Irecv" bytes="0" orank="75" region="0" commid="0" count="145" tid="0" op="" dtype="" >3.8385e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000040000000004B" call="MPI_Isend" bytes="16384" orank="75" region="0" commid="0" count="12648" tid="0" op="" dtype="" >1.0837e-01 3.8147e-06 3.5048e-05</hent>
<hent key="038001000000000000000000000002CB" call="MPI_Irecv" bytes="0" orank="715" region="0" commid="0" count="145" tid="0" op="" dtype="" >3.1710e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002CB" call="MPI_Isend" bytes="16384" orank="715" region="0" commid="0" count="12699" tid="0" op="" dtype="" >8.5361e-02 3.0994e-06 3.8147e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.1819e-04 0.0000e+00 2.6679e-04</hent>
<hent key="02400100000000000000200000000003" call="MPI_Isend" bytes="8192" orank="3" region="0" commid="0" count="12699" tid="0" op="" dtype="" >3.9763e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000020000000000A" call="MPI_Isend" bytes="8192" orank="10" region="0" commid="0" count="4063" tid="0" op="" dtype="" >4.8432e-03 0.0000e+00 1.5020e-05</hent>
<hent key="0240010000000000000020000000000C" call="MPI_Isend" bytes="8192" orank="12" region="0" commid="0" count="3201" tid="0" op="" dtype="" >5.7342e-03 0.0000e+00 7.9870e-05</hent>
<hent key="02400100000000000000200000000013" call="MPI_Isend" bytes="8192" orank="19" region="0" commid="0" count="12674" tid="0" op="" dtype="" >7.1096e-03 0.0000e+00 6.1035e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.9141e-05 6.9141e-05 6.9141e-05</hent>
<hent key="02400100000000000000000000000003" call="MPI_Isend" bytes="0" orank="3" region="0" commid="0" count="269" tid="0" op="" dtype="" >1.9026e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000000000000A" call="MPI_Isend" bytes="0" orank="10" region="0" commid="0" count="257" tid="0" op="" dtype="" >1.8764e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000000000000000C" call="MPI_Isend" bytes="0" orank="12" region="0" commid="0" count="235" tid="0" op="" dtype="" >4.8566e-04 9.5367e-07 5.1975e-05</hent>
<hent key="02400100000000000000000000000013" call="MPI_Isend" bytes="0" orank="19" region="0" commid="0" count="237" tid="0" op="" dtype="" >1.5950e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0240010000000000000000000000004B" call="MPI_Isend" bytes="0" orank="75" region="0" commid="0" count="149" tid="0" op="" dtype="" >5.8031e-04 9.5367e-07 5.0068e-06</hent>
<hent key="024001000000000000000000000002CB" call="MPI_Isend" bytes="0" orank="715" region="0" commid="0" count="153" tid="0" op="" dtype="" >5.8651e-04 1.9073e-06 5.9605e-06</hent>
<hent key="03800100000000000000060000000003" call="MPI_Irecv" bytes="1536" orank="3" region="0" commid="0" count="91" tid="0" op="" dtype="" >1.5736e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000A" call="MPI_Irecv" bytes="1536" orank="10" region="0" commid="0" count="82" tid="0" op="" dtype="" >3.1710e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000C" call="MPI_Irecv" bytes="1536" orank="12" region="0" commid="0" count="101" tid="0" op="" dtype="" >2.5034e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000013" call="MPI_Irecv" bytes="1536" orank="19" region="0" commid="0" count="110" tid="0" op="" dtype="" >2.1935e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000004B" call="MPI_Irecv" bytes="1536" orank="75" region="0" commid="0" count="220" tid="0" op="" dtype="" >6.2227e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002CB" call="MPI_Irecv" bytes="1536" orank="715" region="0" commid="0" count="221" tid="0" op="" dtype="" >7.3910e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000003" call="MPI_Isend" bytes="1536" orank="3" region="0" commid="0" count="90" tid="0" op="" dtype="" >1.8811e-04 9.5367e-07 1.5020e-05</hent>
<hent key="0240010000000000000006000000000A" call="MPI_Isend" bytes="1536" orank="10" region="0" commid="0" count="85" tid="0" op="" dtype="" >1.7571e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000006000000000C" call="MPI_Isend" bytes="1536" orank="12" region="0" commid="0" count="127" tid="0" op="" dtype="" >4.9448e-04 2.8610e-06 6.9141e-06</hent>
<hent key="02400100000000000000060000000013" call="MPI_Isend" bytes="1536" orank="19" region="0" commid="0" count="110" tid="0" op="" dtype="" >2.2149e-04 9.5367e-07 1.5020e-05</hent>
<hent key="0240010000000000000006000000004B" call="MPI_Isend" bytes="1536" orank="75" region="0" commid="0" count="240" tid="0" op="" dtype="" >1.3247e-03 4.7684e-06 1.0967e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002CB" call="MPI_Isend" bytes="1536" orank="715" region="0" commid="0" count="203" tid="0" op="" dtype="" >9.7203e-04 3.8147e-06 1.0014e-05</hent>
<hent key="038001000000000000000C0000000003" call="MPI_Irecv" bytes="3072" orank="3" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C000000000C" call="MPI_Irecv" bytes="3072" orank="12" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C000000004B" call="MPI_Irecv" bytes="3072" orank="75" region="0" commid="0" count="12" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002CB" call="MPI_Irecv" bytes="3072" orank="715" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C0000000013" call="MPI_Isend" bytes="3072" orank="19" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="024001000000000000000C000000004B" call="MPI_Isend" bytes="3072" orank="75" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.8358e-05 5.9605e-06 6.1989e-06</hent>
<hent key="024001000000000000000C00000002CB" call="MPI_Isend" bytes="3072" orank="715" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.1008e-05 5.0068e-06 6.9141e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.4390e-04 2.4390e-04 2.4390e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >5.6100e-04 1.8191e-04 1.9407e-04</hent>
<hent key="03800100000000000000038000000003" call="MPI_Irecv" bytes="896" orank="3" region="0" commid="0" count="341" tid="0" op="" dtype="" >6.1989e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000A" call="MPI_Irecv" bytes="896" orank="10" region="0" commid="0" count="2746" tid="0" op="" dtype="" >3.8314e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000C" call="MPI_Irecv" bytes="896" orank="12" region="0" commid="0" count="2908" tid="0" op="" dtype="" >4.4179e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000038000000013" call="MPI_Irecv" bytes="896" orank="19" region="0" commid="0" count="319" tid="0" op="" dtype="" >5.6744e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000004B" call="MPI_Irecv" bytes="896" orank="75" region="0" commid="0" count="311" tid="0" op="" dtype="" >9.8944e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002CB" call="MPI_Irecv" bytes="896" orank="715" region="0" commid="0" count="349" tid="0" op="" dtype="" >9.7513e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.6042e-05 6.6042e-05 6.6042e-05</hent>
<hent key="0380010000000000000038000000004B" call="MPI_Irecv" bytes="14336" orank="75" region="0" commid="0" count="79" tid="0" op="" dtype="" >2.4080e-05 0.0000e+00 1.9073e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000003800000002CB" call="MPI_Irecv" bytes="14336" orank="715" region="0" commid="0" count="75" tid="0" op="" dtype="" >1.7881e-05 0.0000e+00 2.8610e-06</hent>
<hent key="02400100000000000000038000000003" call="MPI_Isend" bytes="896" orank="3" region="0" commid="0" count="323" tid="0" op="" dtype="" >4.3631e-04 9.5367e-07 5.9605e-06</hent>
<hent key="0240010000000000000003800000000A" call="MPI_Isend" bytes="896" orank="10" region="0" commid="0" count="2618" tid="0" op="" dtype="" >1.9705e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000003800000000C" call="MPI_Isend" bytes="896" orank="12" region="0" commid="0" count="2832" tid="0" op="" dtype="" >2.8725e-03 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000038000000013" call="MPI_Isend" bytes="896" orank="19" region="0" commid="0" count="325" tid="0" op="" dtype="" >4.4298e-04 9.5367e-07 1.0014e-05</hent>
<hent key="0240010000000000000003800000004B" call="MPI_Isend" bytes="896" orank="75" region="0" commid="0" count="329" tid="0" op="" dtype="" >2.2087e-03 3.8147e-06 5.7888e-04</hent>
<hent key="024001000000000000000380000002CB" call="MPI_Isend" bytes="896" orank="715" region="0" commid="0" count="345" tid="0" op="" dtype="" >1.5273e-03 2.8610e-06 7.8678e-06</hent>
<hent key="0240010000000000000038000000004B" call="MPI_Isend" bytes="14336" orank="75" region="0" commid="0" count="51" tid="0" op="" dtype="" >4.3821e-04 5.9605e-06 1.4067e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.7490e+00 7.8678e-06 2.0313e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.1769e-03 3.1769e-03 3.1769e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0420e-02 1.0420e-02 1.0420e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.6320e-02 2.6320e-02 2.6320e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.0953e-01 2.0730e-03 1.9096e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.8123e-04 3.8123e-04 3.8123e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5884e+00 3.7384e-04 2.5095e-01</hent>
<hent key="03800100000000000000040000000003" call="MPI_Irecv" bytes="1024" orank="3" region="0" commid="0" count="3394" tid="0" op="" dtype="" >5.3525e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000000A" call="MPI_Irecv" bytes="1024" orank="10" region="0" commid="0" count="958" tid="0" op="" dtype="" >1.0538e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000000C" call="MPI_Irecv" bytes="1024" orank="12" region="0" commid="0" count="848" tid="0" op="" dtype="" >1.3399e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000013" call="MPI_Irecv" bytes="1024" orank="19" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.3678e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.5020e-05 0.0000e+00 6.9141e-06</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.9101e-05 3.9101e-05 3.9101e-05</hent>
<hent key="03800100000000000000070000000003" call="MPI_Irecv" bytes="1792" orank="3" region="0" commid="0" count="42" tid="0" op="" dtype="" >1.0729e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000A" call="MPI_Irecv" bytes="1792" orank="10" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000C" call="MPI_Irecv" bytes="1792" orank="12" region="0" commid="0" count="38" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000013" call="MPI_Irecv" bytes="1792" orank="19" region="0" commid="0" count="47" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000004B" call="MPI_Irecv" bytes="1792" orank="75" region="0" commid="0" count="144" tid="0" op="" dtype="" >4.0293e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002CB" call="MPI_Irecv" bytes="1792" orank="715" region="0" commid="0" count="163" tid="0" op="" dtype="" >5.4121e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >3.9101e-05 0.0000e+00 2.2173e-05</hent>
<hent key="02400100000000000000040000000003" call="MPI_Isend" bytes="1024" orank="3" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7598e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000004000000000A" call="MPI_Isend" bytes="1024" orank="10" region="0" commid="0" count="1114" tid="0" op="" dtype="" >6.7735e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000004000000000C" call="MPI_Isend" bytes="1024" orank="12" region="0" commid="0" count="886" tid="0" op="" dtype="" >6.4206e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000040000000013" call="MPI_Isend" bytes="1024" orank="19" region="0" commid="0" count="3392" tid="0" op="" dtype="" >1.7283e-03 0.0000e+00 1.4067e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >4.2915e-06 2.1458e-06 2.1458e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5582e+00 0.0000e+00 2.2716e+00</hent>
<hent key="038001000000000000000A0000000003" call="MPI_Irecv" bytes="2560" orank="3" region="0" commid="0" count="5" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A000000000A" call="MPI_Irecv" bytes="2560" orank="10" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000000C" call="MPI_Irecv" bytes="2560" orank="12" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000013" call="MPI_Irecv" bytes="2560" orank="19" region="0" commid="0" count="7" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000004B" call="MPI_Irecv" bytes="2560" orank="75" region="0" commid="0" count="33" tid="0" op="" dtype="" >1.3590e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002CB" call="MPI_Irecv" bytes="2560" orank="715" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.3590e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000003" call="MPI_Isend" bytes="1792" orank="3" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.2803e-04 9.5367e-07 1.3828e-05</hent>
<hent key="0240010000000000000007000000000A" call="MPI_Isend" bytes="1792" orank="10" region="0" commid="0" count="37" tid="0" op="" dtype="" >8.4400e-05 1.1921e-06 6.9141e-06</hent>
<hent key="0240010000000000000007000000000C" call="MPI_Isend" bytes="1792" orank="12" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.6499e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000070000000013" call="MPI_Isend" bytes="1792" orank="19" region="0" commid="0" count="48" tid="0" op="" dtype="" >1.3781e-04 9.5367e-07 1.5974e-05</hent>
<hent key="0240010000000000000007000000004B" call="MPI_Isend" bytes="1792" orank="75" region="0" commid="0" count="142" tid="0" op="" dtype="" >7.4768e-04 9.5367e-07 1.0967e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1357e-01 1.1357e-01 1.1357e-01</hent>
<hent key="024001000000000000000700000002CB" call="MPI_Isend" bytes="1792" orank="715" region="0" commid="0" count="128" tid="0" op="" dtype="" >6.1727e-04 3.8147e-06 6.1989e-06</hent>
<hent key="024001000000000000000A0000000003" call="MPI_Isend" bytes="2560" orank="3" region="0" commid="0" count="3" tid="0" op="" dtype="" >3.8147e-06 9.5367e-07 1.9073e-06</hent>
<hent key="024001000000000000000A000000000A" call="MPI_Isend" bytes="2560" orank="10" region="0" commid="0" count="2" tid="0" op="" dtype="" >5.2452e-06 2.1458e-06 3.0994e-06</hent>
<hent key="024001000000000000000A000000000C" call="MPI_Isend" bytes="2560" orank="12" region="0" commid="0" count="8" tid="0" op="" dtype="" >3.2187e-05 2.8610e-06 5.0068e-06</hent>
<hent key="024001000000000000000A0000000013" call="MPI_Isend" bytes="2560" orank="19" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.5511e-05 9.5367e-07 1.5974e-05</hent>
<hent key="024001000000000000000A000000004B" call="MPI_Isend" bytes="2560" orank="75" region="0" commid="0" count="51" tid="0" op="" dtype="" >2.9755e-04 5.0068e-06 1.3113e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.8419e-04 3.1948e-05 1.2612e-04</hent>
<hent key="024001000000000000000A00000002CB" call="MPI_Isend" bytes="2560" orank="715" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.1458e-04 4.0531e-06 9.7752e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.3293e-04 3.3712e-04 5.9581e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6955e-05 3.6955e-05 3.6955e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >6.6757e-06 1.9073e-06 4.7684e-06</hent>
<hent key="03800100000000000000000400000003" call="MPI_Irecv" bytes="4" orank="3" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.1151e-04 0.0000e+00 2.8849e-05</hent>
<hent key="0380010000000000000000040000000A" call="MPI_Irecv" bytes="4" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.1955e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000040000000C" call="MPI_Irecv" bytes="4" orank="12" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.2057e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000013" call="MPI_Irecv" bytes="4" orank="19" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.0306e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000000040000004B" call="MPI_Irecv" bytes="4" orank="75" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.5814e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.7672e-03 2.4104e-04 6.1798e-04</hent>
<hent key="038001000000000000001C000000000A" call="MPI_Irecv" bytes="7168" orank="10" region="0" commid="0" count="9018" tid="0" op="" dtype="" >2.7726e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000001C000000000C" call="MPI_Irecv" bytes="7168" orank="12" region="0" commid="0" count="9591" tid="0" op="" dtype="" >1.3247e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000004000002CB" call="MPI_Irecv" bytes="4" orank="715" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.6495e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000400000003" call="MPI_Isend" bytes="4" orank="3" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2107e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000000040000000A" call="MPI_Isend" bytes="4" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4510e-03 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000040000000C" call="MPI_Isend" bytes="4" orank="12" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.1353e-03 0.0000e+00 1.0395e-04</hent>
<hent key="02400100000000000000000400000013" call="MPI_Isend" bytes="4" orank="19" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1950e-03 0.0000e+00 2.6941e-05</hent>
<hent key="0240010000000000000000040000004B" call="MPI_Isend" bytes="4" orank="75" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3507e-02 3.8147e-06 2.2888e-05</hent>
<hent key="024001000000000000001C000000000A" call="MPI_Isend" bytes="7168" orank="10" region="0" commid="0" count="8636" tid="0" op="" dtype="" >1.9841e-02 9.5367e-07 1.6928e-05</hent>
<hent key="024001000000000000001C000000000C" call="MPI_Isend" bytes="7168" orank="12" region="0" commid="0" count="9498" tid="0" op="" dtype="" >2.7706e-02 9.5367e-07 1.5974e-05</hent>
<hent key="024001000000000000000004000002CB" call="MPI_Isend" bytes="4" orank="715" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.8016e-02 3.8147e-06 9.2030e-05</hent>
<hent key="03800100000000000000050000000003" call="MPI_Irecv" bytes="1280" orank="3" region="0" commid="0" count="197" tid="0" op="" dtype="" >3.4809e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000A" call="MPI_Irecv" bytes="1280" orank="10" region="0" commid="0" count="193" tid="0" op="" dtype="" >7.5579e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000C" call="MPI_Irecv" bytes="1280" orank="12" region="0" commid="0" count="210" tid="0" op="" dtype="" >5.1975e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000013" call="MPI_Irecv" bytes="1280" orank="19" region="0" commid="0" count="239" tid="0" op="" dtype="" >5.1498e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000004B" call="MPI_Irecv" bytes="1280" orank="75" region="0" commid="0" count="306" tid="0" op="" dtype="" >9.0122e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0805e+01 7.8678e-06 1.5598e-01</hent>
<hent key="03800100000000000000280000000003" call="MPI_Irecv" bytes="10240" orank="3" region="0" commid="0" count="16" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002CB" call="MPI_Irecv" bytes="1280" orank="715" region="0" commid="0" count="289" tid="0" op="" dtype="" >8.4162e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000003" call="MPI_Irecv" bytes="2048" orank="3" region="0" commid="0" count="17" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000A" call="MPI_Irecv" bytes="2048" orank="10" region="0" commid="0" count="15" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000C" call="MPI_Irecv" bytes="2048" orank="12" region="0" commid="0" count="21" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000013" call="MPI_Irecv" bytes="2048" orank="19" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000004B" call="MPI_Irecv" bytes="2048" orank="75" region="0" commid="0" count="3443" tid="0" op="" dtype="" >4.6182e-04 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000800000002CB" call="MPI_Irecv" bytes="2048" orank="715" region="0" commid="0" count="3429" tid="0" op="" dtype="" >6.4683e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000050000000003" call="MPI_Isend" bytes="1280" orank="3" region="0" commid="0" count="190" tid="0" op="" dtype="" >2.7966e-04 9.5367e-07 8.1062e-06</hent>
<hent key="0240010000000000000005000000000A" call="MPI_Isend" bytes="1280" orank="10" region="0" commid="0" count="213" tid="0" op="" dtype="" >4.0960e-04 9.5367e-07 5.9605e-06</hent>
<hent key="0240010000000000000005000000000C" call="MPI_Isend" bytes="1280" orank="12" region="0" commid="0" count="181" tid="0" op="" dtype="" >6.6757e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000050000000013" call="MPI_Isend" bytes="1280" orank="19" region="0" commid="0" count="216" tid="0" op="" dtype="" >3.1495e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000005000000004B" call="MPI_Isend" bytes="1280" orank="75" region="0" commid="0" count="269" tid="0" op="" dtype="" >1.4307e-03 4.0531e-06 1.1921e-05</hent>
<hent key="02400100000000000000280000000013" call="MPI_Isend" bytes="10240" orank="19" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.3351e-05 0.0000e+00 2.1458e-06</hent>
<hent key="024001000000000000000500000002CB" call="MPI_Isend" bytes="1280" orank="715" region="0" commid="0" count="288" tid="0" op="" dtype="" >1.3299e-03 3.8147e-06 1.0014e-05</hent>
</hash>
<internal rank="11" log_i="1723712895.690365" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="12" mpi_size="768" stamp_init="1723712829.548129" stamp_final="1723712895.680555" username="apac4" allocationname="unknown" flags="0" pid="684280" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61324e+01" utime="4.95431e+01" stime="7.99533e+00" mtime="2.99919e+01" gflop="0.00000e+00" gbyte="3.76583e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.99919e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007d14781480" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.59999e+01" utime="4.95154e+01" stime="7.98031e+00" mtime="2.99919e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.99919e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4937e+08" > 4.9141e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4744e+08" > 3.6074e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0949e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5717e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9701e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9897e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5825e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.1409e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0431e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1378e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8085e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="197" >
<hent key="02400100000000000000080000000004" call="MPI_Isend" bytes="2048" orank="4" region="0" commid="0" count="12" tid="0" op="" dtype="" >3.2902e-05 9.5367e-07 1.0967e-05</hent>
<hent key="0240010000000000000008000000000B" call="MPI_Isend" bytes="2048" orank="11" region="0" commid="0" count="21" tid="0" op="" dtype="" >4.5300e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000008000000000D" call="MPI_Isend" bytes="2048" orank="13" region="0" commid="0" count="11" tid="0" op="" dtype="" >4.4107e-05 3.8147e-06 4.0531e-06</hent>
<hent key="02400100000000000000080000000014" call="MPI_Isend" bytes="2048" orank="20" region="0" commid="0" count="25" tid="0" op="" dtype="" >7.0572e-05 9.5367e-07 1.2875e-05</hent>
<hent key="0240010000000000000008000000004C" call="MPI_Isend" bytes="2048" orank="76" region="0" commid="0" count="3457" tid="0" op="" dtype="" >1.6676e-02 9.5367e-07 5.1022e-05</hent>
<hent key="024001000000000000000800000002CC" call="MPI_Isend" bytes="2048" orank="716" region="0" commid="0" count="3457" tid="0" op="" dtype="" >1.3782e-02 9.5367e-07 4.2200e-05</hent>
<hent key="038001000000000000000E000000004C" call="MPI_Irecv" bytes="3584" orank="76" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="024001000000000000000E000000004C" call="MPI_Isend" bytes="3584" orank="76" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000004" call="MPI_Irecv" bytes="640" orank="4" region="0" commid="0" count="410" tid="0" op="" dtype="" >9.6798e-05 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000002800000000B" call="MPI_Irecv" bytes="640" orank="11" region="0" commid="0" count="433" tid="0" op="" dtype="" >1.3638e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000002800000000D" call="MPI_Irecv" bytes="640" orank="13" region="0" commid="0" count="382" tid="0" op="" dtype="" >9.3937e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000028000000014" call="MPI_Irecv" bytes="640" orank="20" region="0" commid="0" count="389" tid="0" op="" dtype="" >1.2112e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000002800000004C" call="MPI_Irecv" bytes="640" orank="76" region="0" commid="0" count="275" tid="0" op="" dtype="" >9.4175e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002CC" call="MPI_Irecv" bytes="640" orank="716" region="0" commid="0" count="313" tid="0" op="" dtype="" >1.4973e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000028000000004" call="MPI_Isend" bytes="640" orank="4" region="0" commid="0" count="409" tid="0" op="" dtype="" >4.8280e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000002800000000B" call="MPI_Isend" bytes="640" orank="11" region="0" commid="0" count="419" tid="0" op="" dtype="" >6.0701e-04 9.5367e-07 5.9605e-06</hent>
<hent key="0240010000000000000002800000000D" call="MPI_Isend" bytes="640" orank="13" region="0" commid="0" count="422" tid="0" op="" dtype="" >1.2770e-03 1.9073e-06 6.9141e-06</hent>
<hent key="02400100000000000000028000000014" call="MPI_Isend" bytes="640" orank="20" region="0" commid="0" count="399" tid="0" op="" dtype="" >5.5623e-04 9.5367e-07 1.0014e-05</hent>
<hent key="0240010000000000000002800000004C" call="MPI_Isend" bytes="640" orank="76" region="0" commid="0" count="260" tid="0" op="" dtype="" >1.2348e-03 3.8147e-06 1.0967e-05</hent>
<hent key="024001000000000000000280000002CC" call="MPI_Isend" bytes="640" orank="716" region="0" commid="0" count="285" tid="0" op="" dtype="" >1.2596e-03 2.8610e-06 9.0599e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.3140e-05 3.3140e-05 3.3140e-05</hent>
<hent key="03800100000000000000014000000004" call="MPI_Irecv" bytes="320" orank="4" region="0" commid="0" count="361" tid="0" op="" dtype="" >9.1553e-05 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000001400000000B" call="MPI_Irecv" bytes="320" orank="11" region="0" commid="0" count="341" tid="0" op="" dtype="" >1.4567e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000D" call="MPI_Irecv" bytes="320" orank="13" region="0" commid="0" count="335" tid="0" op="" dtype="" >8.3447e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000014" call="MPI_Irecv" bytes="320" orank="20" region="0" commid="0" count="364" tid="0" op="" dtype="" >8.7976e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000001400000004C" call="MPI_Irecv" bytes="320" orank="76" region="0" commid="0" count="194" tid="0" op="" dtype="" >6.3658e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002CC" call="MPI_Irecv" bytes="320" orank="716" region="0" commid="0" count="165" tid="0" op="" dtype="" >7.6294e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >6.1989e-06 9.5367e-07 2.1458e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.0949e+00 0.0000e+00 1.5410e-01</hent>
<hent key="0380010000000000000040000000004C" call="MPI_Irecv" bytes="16384" orank="76" region="0" commid="0" count="12423" tid="0" op="" dtype="" >4.4317e-03 0.0000e+00 2.8133e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6480e-02 0.0000e+00 1.6464e-02</hent>
<hent key="038001000000000000004000000002CC" call="MPI_Irecv" bytes="16384" orank="716" region="0" commid="0" count="12524" tid="0" op="" dtype="" >4.7598e-03 0.0000e+00 2.6941e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >6.1989e-05 0.0000e+00 6.1035e-05</hent>
<hent key="02400100000000000000014000000004" call="MPI_Isend" bytes="320" orank="4" region="0" commid="0" count="343" tid="0" op="" dtype="" >3.3903e-04 0.0000e+00 1.1206e-05</hent>
<hent key="0240010000000000000001400000000B" call="MPI_Isend" bytes="320" orank="11" region="0" commid="0" count="321" tid="0" op="" dtype="" >3.8052e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000001400000000D" call="MPI_Isend" bytes="320" orank="13" region="0" commid="0" count="353" tid="0" op="" dtype="" >9.8014e-04 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000014000000014" call="MPI_Isend" bytes="320" orank="20" region="0" commid="0" count="390" tid="0" op="" dtype="" >4.2343e-04 0.0000e+00 1.1921e-05</hent>
<hent key="0240010000000000000001400000004C" call="MPI_Isend" bytes="320" orank="76" region="0" commid="0" count="160" tid="0" op="" dtype="" >7.1216e-04 2.8610e-06 1.2159e-05</hent>
<hent key="03800100000000000000200000000004" call="MPI_Irecv" bytes="8192" orank="4" region="0" commid="0" count="12659" tid="0" op="" dtype="" >4.0560e-03 0.0000e+00 2.7895e-05</hent>
<hent key="0380010000000000000020000000000B" call="MPI_Irecv" bytes="8192" orank="11" region="0" commid="0" count="3201" tid="0" op="" dtype="" >1.4899e-03 0.0000e+00 1.0014e-05</hent>
<hent key="0380010000000000000020000000000D" call="MPI_Irecv" bytes="8192" orank="13" region="0" commid="0" count="3149" tid="0" op="" dtype="" >5.1999e-04 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000200000000014" call="MPI_Irecv" bytes="8192" orank="20" region="0" commid="0" count="12676" tid="0" op="" dtype="" >2.1937e-03 0.0000e+00 3.1948e-05</hent>
<hent key="024001000000000000000140000002CC" call="MPI_Isend" bytes="320" orank="716" region="0" commid="0" count="181" tid="0" op="" dtype="" >8.3327e-04 2.8610e-06 1.5974e-05</hent>
<hent key="03800100000000000000000000000004" call="MPI_Irecv" bytes="0" orank="4" region="0" commid="0" count="266" tid="0" op="" dtype="" >5.5075e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000B" call="MPI_Irecv" bytes="0" orank="11" region="0" commid="0" count="235" tid="0" op="" dtype="" >8.8930e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000000000000D" call="MPI_Irecv" bytes="0" orank="13" region="0" commid="0" count="263" tid="0" op="" dtype="" >6.7234e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000014" call="MPI_Irecv" bytes="0" orank="20" region="0" commid="0" count="264" tid="0" op="" dtype="" >6.1989e-05 0.0000e+00 8.1062e-06</hent>
<hent key="0380010000000000000000000000004C" call="MPI_Irecv" bytes="0" orank="76" region="0" commid="0" count="150" tid="0" op="" dtype="" >4.1246e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000040000000004C" call="MPI_Isend" bytes="16384" orank="76" region="0" commid="0" count="12646" tid="0" op="" dtype="" >1.7537e-01 3.8147e-06 6.7949e-05</hent>
<hent key="038001000000000000000000000002CC" call="MPI_Irecv" bytes="0" orank="716" region="0" commid="0" count="144" tid="0" op="" dtype="" >4.8876e-05 0.0000e+00 5.0068e-06</hent>
<hent key="024001000000000000004000000002CC" call="MPI_Isend" bytes="16384" orank="716" region="0" commid="0" count="12692" tid="0" op="" dtype="" >1.1334e-01 2.8610e-06 4.2319e-04</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.4966e-04 0.0000e+00 2.8801e-04</hent>
<hent key="02400100000000000000200000000004" call="MPI_Isend" bytes="8192" orank="4" region="0" commid="0" count="12639" tid="0" op="" dtype="" >7.1278e-03 0.0000e+00 3.0994e-05</hent>
<hent key="0240010000000000000020000000000B" call="MPI_Isend" bytes="8192" orank="11" region="0" commid="0" count="3108" tid="0" op="" dtype="" >3.5994e-03 0.0000e+00 2.1935e-05</hent>
<hent key="0240010000000000000020000000000D" call="MPI_Isend" bytes="8192" orank="13" region="0" commid="0" count="4251" tid="0" op="" dtype="" >9.0623e-03 0.0000e+00 1.5974e-05</hent>
<hent key="02400100000000000000200000000014" call="MPI_Isend" bytes="8192" orank="20" region="0" commid="0" count="12682" tid="0" op="" dtype="" >1.1851e-02 0.0000e+00 5.6982e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.6982e-05 5.6982e-05 5.6982e-05</hent>
<hent key="02400100000000000000000000000004" call="MPI_Isend" bytes="0" orank="4" region="0" commid="0" count="276" tid="0" op="" dtype="" >1.9288e-04 0.0000e+00 1.7166e-05</hent>
<hent key="0240010000000000000000000000000B" call="MPI_Isend" bytes="0" orank="11" region="0" commid="0" count="228" tid="0" op="" dtype="" >1.7858e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000000000000000D" call="MPI_Isend" bytes="0" orank="13" region="0" commid="0" count="238" tid="0" op="" dtype="" >4.6706e-04 0.0000e+00 2.7895e-05</hent>
<hent key="02400100000000000000000000000014" call="MPI_Isend" bytes="0" orank="20" region="0" commid="0" count="252" tid="0" op="" dtype="" >1.8072e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000000000000004C" call="MPI_Isend" bytes="0" orank="76" region="0" commid="0" count="155" tid="0" op="" dtype="" >6.6257e-04 1.1921e-06 1.2875e-05</hent>
<hent key="024001000000000000000000000002CC" call="MPI_Isend" bytes="0" orank="716" region="0" commid="0" count="145" tid="0" op="" dtype="" >5.9676e-04 2.8610e-06 1.5020e-05</hent>
<hent key="03800100000000000000060000000004" call="MPI_Irecv" bytes="1536" orank="4" region="0" commid="0" count="103" tid="0" op="" dtype="" >2.6226e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000B" call="MPI_Irecv" bytes="1536" orank="11" region="0" commid="0" count="127" tid="0" op="" dtype="" >5.3167e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000D" call="MPI_Irecv" bytes="1536" orank="13" region="0" commid="0" count="95" tid="0" op="" dtype="" >3.4332e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000060000000014" call="MPI_Irecv" bytes="1536" orank="20" region="0" commid="0" count="89" tid="0" op="" dtype="" >2.0027e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000004C" call="MPI_Irecv" bytes="1536" orank="76" region="0" commid="0" count="206" tid="0" op="" dtype="" >7.4863e-05 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000600000002CC" call="MPI_Irecv" bytes="1536" orank="716" region="0" commid="0" count="219" tid="0" op="" dtype="" >1.2636e-04 0.0000e+00 4.0531e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000004" call="MPI_Isend" bytes="1536" orank="4" region="0" commid="0" count="105" tid="0" op="" dtype="" >2.1052e-04 9.5367e-07 1.4067e-05</hent>
<hent key="0240010000000000000006000000000B" call="MPI_Isend" bytes="1536" orank="11" region="0" commid="0" count="101" tid="0" op="" dtype="" >2.0790e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000006000000000D" call="MPI_Isend" bytes="1536" orank="13" region="0" commid="0" count="88" tid="0" op="" dtype="" >3.2854e-04 2.8610e-06 8.1062e-06</hent>
<hent key="02400100000000000000060000000014" call="MPI_Isend" bytes="1536" orank="20" region="0" commid="0" count="89" tid="0" op="" dtype="" >1.5974e-04 9.5367e-07 1.2159e-05</hent>
<hent key="0240010000000000000006000000004C" call="MPI_Isend" bytes="1536" orank="76" region="0" commid="0" count="221" tid="0" op="" dtype="" >1.2002e-03 3.8147e-06 1.6928e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002CC" call="MPI_Isend" bytes="1536" orank="716" region="0" commid="0" count="209" tid="0" op="" dtype="" >1.0171e-03 2.8610e-06 1.0014e-05</hent>
<hent key="038001000000000000000C0000000014" call="MPI_Irecv" bytes="3072" orank="20" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C000000004C" call="MPI_Irecv" bytes="3072" orank="76" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002CC" call="MPI_Irecv" bytes="3072" orank="716" region="0" commid="0" count="6" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C000000000B" call="MPI_Isend" bytes="3072" orank="11" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="024001000000000000000C000000004C" call="MPI_Isend" bytes="3072" orank="76" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.7166e-05 5.0068e-06 6.1989e-06</hent>
<hent key="024001000000000000000C00000002CC" call="MPI_Isend" bytes="3072" orank="716" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.6226e-05 4.0531e-06 7.1526e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.6917e-04 2.6917e-04 2.6917e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >6.0797e-04 2.0003e-04 2.0695e-04</hent>
<hent key="03800100000000000000038000000004" call="MPI_Irecv" bytes="896" orank="4" region="0" commid="0" count="320" tid="0" op="" dtype="" >9.5606e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000B" call="MPI_Irecv" bytes="896" orank="11" region="0" commid="0" count="2832" tid="0" op="" dtype="" >9.3007e-04 0.0000e+00 1.8120e-05</hent>
<hent key="0380010000000000000003800000000D" call="MPI_Irecv" bytes="896" orank="13" region="0" commid="0" count="2884" tid="0" op="" dtype="" >5.0020e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000038000000014" call="MPI_Irecv" bytes="896" orank="20" region="0" commid="0" count="338" tid="0" op="" dtype="" >8.2254e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000004C" call="MPI_Irecv" bytes="896" orank="76" region="0" commid="0" count="326" tid="0" op="" dtype="" >1.1659e-04 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000380000002CC" call="MPI_Irecv" bytes="896" orank="716" region="0" commid="0" count="340" tid="0" op="" dtype="" >1.6046e-04 0.0000e+00 9.0599e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.5730e-03 5.5730e-03 5.5730e-03</hent>
<hent key="0380010000000000000038000000004C" call="MPI_Irecv" bytes="14336" orank="76" region="0" commid="0" count="276" tid="0" op="" dtype="" >1.0228e-04 0.0000e+00 2.8610e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000003800000002CC" call="MPI_Irecv" bytes="14336" orank="716" region="0" commid="0" count="175" tid="0" op="" dtype="" >3.8385e-05 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000038000000004" call="MPI_Isend" bytes="896" orank="4" region="0" commid="0" count="307" tid="0" op="" dtype="" >4.1080e-04 9.5367e-07 6.9141e-06</hent>
<hent key="0240010000000000000003800000000B" call="MPI_Isend" bytes="896" orank="11" region="0" commid="0" count="2908" tid="0" op="" dtype="" >2.3031e-03 0.0000e+00 2.0981e-05</hent>
<hent key="0240010000000000000003800000000D" call="MPI_Isend" bytes="896" orank="13" region="0" commid="0" count="2580" tid="0" op="" dtype="" >2.7275e-03 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000038000000014" call="MPI_Isend" bytes="896" orank="20" region="0" commid="0" count="308" tid="0" op="" dtype="" >4.4298e-04 9.5367e-07 9.0599e-06</hent>
<hent key="0240010000000000000003800000004C" call="MPI_Isend" bytes="896" orank="76" region="0" commid="0" count="350" tid="0" op="" dtype="" >1.7250e-03 3.8147e-06 1.7881e-05</hent>
<hent key="024001000000000000000380000002CC" call="MPI_Isend" bytes="896" orank="716" region="0" commid="0" count="342" tid="0" op="" dtype="" >1.5888e-03 2.8610e-06 1.5020e-05</hent>
<hent key="0240010000000000000038000000004C" call="MPI_Isend" bytes="14336" orank="76" region="0" commid="0" count="53" tid="0" op="" dtype="" >8.1944e-04 5.9605e-06 3.7193e-05</hent>
<hent key="024001000000000000003800000002CC" call="MPI_Isend" bytes="14336" orank="716" region="0" commid="0" count="7" tid="0" op="" dtype="" >6.2943e-05 5.9605e-06 1.5020e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.2377e+00 1.5974e-05 2.0312e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.1631e-03 3.1631e-03 3.1631e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0431e-02 1.0431e-02 1.0431e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.6253e-02 2.6253e-02 2.6253e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.1808e-01 2.4409e-03 1.9920e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.8004e-04 3.8004e-04 3.8004e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 9.5367e-07 9.5367e-07</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5825e+00 3.9291e-04 2.5088e-01</hent>
<hent key="03800100000000000000040000000004" call="MPI_Irecv" bytes="1024" orank="4" region="0" commid="0" count="3388" tid="0" op="" dtype="" >5.4789e-04 0.0000e+00 2.0981e-05</hent>
<hent key="0380010000000000000004000000000B" call="MPI_Irecv" bytes="1024" orank="11" region="0" commid="0" count="886" tid="0" op="" dtype="" >2.9111e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0380010000000000000004000000000D" call="MPI_Irecv" bytes="1024" orank="13" region="0" commid="0" count="864" tid="0" op="" dtype="" >1.6809e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000040000000014" call="MPI_Irecv" bytes="1024" orank="20" region="0" commid="0" count="3392" tid="0" op="" dtype="" >4.9925e-04 0.0000e+00 1.8835e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.9701e-03 9.5367e-07 1.1530e-03</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.1008e-05 4.1008e-05 4.1008e-05</hent>
<hent key="03800100000000000000070000000004" call="MPI_Irecv" bytes="1792" orank="4" region="0" commid="0" count="46" tid="0" op="" dtype="" >1.0729e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000B" call="MPI_Irecv" bytes="1792" orank="11" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000D" call="MPI_Irecv" bytes="1792" orank="13" region="0" commid="0" count="34" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000014" call="MPI_Irecv" bytes="1792" orank="20" region="0" commid="0" count="47" tid="0" op="" dtype="" >8.5831e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000007000000004C" call="MPI_Irecv" bytes="1792" orank="76" region="0" commid="0" count="226" tid="0" op="" dtype="" >5.9128e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002CC" call="MPI_Irecv" bytes="1792" orank="716" region="0" commid="0" count="172" tid="0" op="" dtype="" >7.2718e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >3.6955e-05 0.0000e+00 1.6212e-05</hent>
<hent key="02400100000000000000040000000004" call="MPI_Isend" bytes="1024" orank="4" region="0" commid="0" count="3382" tid="0" op="" dtype="" >2.0854e-03 0.0000e+00 2.9087e-05</hent>
<hent key="0240010000000000000004000000000B" call="MPI_Isend" bytes="1024" orank="11" region="0" commid="0" count="848" tid="0" op="" dtype="" >5.5218e-04 0.0000e+00 6.1989e-06</hent>
<hent key="0240010000000000000004000000000D" call="MPI_Isend" bytes="1024" orank="13" region="0" commid="0" count="1174" tid="0" op="" dtype="" >9.3699e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000040000000014" call="MPI_Isend" bytes="1024" orank="20" region="0" commid="0" count="3394" tid="0" op="" dtype="" >2.0945e-03 0.0000e+00 2.0981e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5490e+00 0.0000e+00 2.2679e+00</hent>
<hent key="038001000000000000000A0000000004" call="MPI_Irecv" bytes="2560" orank="4" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000000B" call="MPI_Irecv" bytes="2560" orank="11" region="0" commid="0" count="8" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000000D" call="MPI_Irecv" bytes="2560" orank="13" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000014" call="MPI_Irecv" bytes="2560" orank="20" region="0" commid="0" count="4" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A000000004C" call="MPI_Irecv" bytes="2560" orank="76" region="0" commid="0" count="45" tid="0" op="" dtype="" >1.3351e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002CC" call="MPI_Irecv" bytes="2560" orank="716" region="0" commid="0" count="55" tid="0" op="" dtype="" >3.3379e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000004" call="MPI_Isend" bytes="1792" orank="4" region="0" commid="0" count="43" tid="0" op="" dtype="" >9.6321e-05 9.5367e-07 1.0014e-05</hent>
<hent key="0240010000000000000007000000000B" call="MPI_Isend" bytes="1792" orank="11" region="0" commid="0" count="38" tid="0" op="" dtype="" >8.7023e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000007000000000D" call="MPI_Isend" bytes="1792" orank="13" region="0" commid="0" count="34" tid="0" op="" dtype="" >1.2350e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000070000000014" call="MPI_Isend" bytes="1792" orank="20" region="0" commid="0" count="42" tid="0" op="" dtype="" >1.0037e-04 9.5367e-07 1.7881e-05</hent>
<hent key="0240010000000000000007000000004C" call="MPI_Isend" bytes="1792" orank="76" region="0" commid="0" count="133" tid="0" op="" dtype="" >7.0333e-04 9.5367e-07 2.0981e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1378e-01 1.1378e-01 1.1378e-01</hent>
<hent key="024001000000000000000700000002CC" call="MPI_Isend" bytes="1792" orank="716" region="0" commid="0" count="120" tid="0" op="" dtype="" >6.0606e-04 9.5367e-07 2.4080e-05</hent>
<hent key="024001000000000000000A0000000004" call="MPI_Isend" bytes="2560" orank="4" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.4094e-05 9.5367e-07 1.0967e-05</hent>
<hent key="024001000000000000000A000000000B" call="MPI_Isend" bytes="2560" orank="11" region="0" commid="0" count="2" tid="0" op="" dtype="" >4.7684e-06 1.9073e-06 2.8610e-06</hent>
<hent key="024001000000000000000A000000000D" call="MPI_Isend" bytes="2560" orank="13" region="0" commid="0" count="4" tid="0" op="" dtype="" >3.0279e-05 3.0994e-06 1.8120e-05</hent>
<hent key="024001000000000000000A0000000014" call="MPI_Isend" bytes="2560" orank="20" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.6226e-05 1.9073e-06 1.1206e-05</hent>
<hent key="024001000000000000000A000000004C" call="MPI_Isend" bytes="2560" orank="76" region="0" commid="0" count="35" tid="0" op="" dtype="" >2.0051e-04 5.0068e-06 1.0967e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.1805e-04 4.1962e-05 1.3995e-04</hent>
<hent key="024001000000000000000A00000002CC" call="MPI_Isend" bytes="2560" orank="716" region="0" commid="0" count="50" tid="0" op="" dtype="" >2.5821e-04 3.8147e-06 1.0014e-05</hent>
<hent key="0240010000000000000010000000004C" call="MPI_Isend" bytes="4096" orank="76" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.0290e-03 3.6812e-04 6.6090e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6955e-05 3.6955e-05 3.6955e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >8.1062e-06 2.1458e-06 5.9605e-06</hent>
<hent key="03800100000000000000000400000004" call="MPI_Irecv" bytes="4" orank="4" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.7101e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0380010000000000000000040000000B" call="MPI_Irecv" bytes="4" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1001e-03 0.0000e+00 1.0014e-05</hent>
<hent key="0380010000000000000000040000000D" call="MPI_Irecv" bytes="4" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.3740e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000000400000014" call="MPI_Irecv" bytes="4" orank="20" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.9618e-04 0.0000e+00 2.4080e-05</hent>
<hent key="0380010000000000000000040000004C" call="MPI_Irecv" bytes="4" orank="76" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.4829e-04 0.0000e+00 2.4080e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.9085e-03 2.6178e-04 6.5589e-04</hent>
<hent key="038001000000000000001C000000000B" call="MPI_Irecv" bytes="7168" orank="11" region="0" commid="0" count="9498" tid="0" op="" dtype="" >4.3428e-03 0.0000e+00 1.0967e-05</hent>
<hent key="038001000000000000001C000000000D" call="MPI_Irecv" bytes="7168" orank="13" region="0" commid="0" count="9550" tid="0" op="" dtype="" >1.7083e-03 0.0000e+00 1.5020e-05</hent>
<hent key="038001000000000000000004000002CC" call="MPI_Irecv" bytes="4" orank="716" region="0" commid="0" count="3398" tid="0" op="" dtype="" >9.1505e-04 0.0000e+00 2.3842e-05</hent>
<hent key="02400100000000000000000400000004" call="MPI_Isend" bytes="4" orank="4" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4017e-03 0.0000e+00 1.5020e-05</hent>
<hent key="0240010000000000000000040000000B" call="MPI_Isend" bytes="4" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2357e-03 0.0000e+00 4.1962e-05</hent>
<hent key="0240010000000000000000040000000D" call="MPI_Isend" bytes="4" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.7292e-03 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000000400000014" call="MPI_Isend" bytes="4" orank="20" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.5700e-03 0.0000e+00 2.1935e-05</hent>
<hent key="0240010000000000000000040000004C" call="MPI_Isend" bytes="4" orank="76" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.7459e-02 3.8147e-06 4.5776e-05</hent>
<hent key="024001000000000000001C000000000B" call="MPI_Isend" bytes="7168" orank="11" region="0" commid="0" count="9591" tid="0" op="" dtype="" >2.1064e-02 9.5367e-07 3.6955e-05</hent>
<hent key="024001000000000000001C000000000D" call="MPI_Isend" bytes="7168" orank="13" region="0" commid="0" count="8448" tid="0" op="" dtype="" >2.6308e-02 9.5367e-07 7.6056e-05</hent>
<hent key="024001000000000000000004000002CC" call="MPI_Isend" bytes="4" orank="716" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.1061e-02 2.8610e-06 1.2898e-04</hent>
<hent key="03800100000000000000050000000004" call="MPI_Irecv" bytes="1280" orank="4" region="0" commid="0" count="188" tid="0" op="" dtype="" >6.0558e-05 0.0000e+00 7.8678e-06</hent>
<hent key="0380010000000000000005000000000B" call="MPI_Irecv" bytes="1280" orank="11" region="0" commid="0" count="181" tid="0" op="" dtype="" >6.3896e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000D" call="MPI_Irecv" bytes="1280" orank="13" region="0" commid="0" count="218" tid="0" op="" dtype="" >5.6982e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000014" call="MPI_Irecv" bytes="1280" orank="20" region="0" commid="0" count="195" tid="0" op="" dtype="" >4.3869e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000004C" call="MPI_Irecv" bytes="1280" orank="76" region="0" commid="0" count="274" tid="0" op="" dtype="" >9.6083e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0599e+01 6.9141e-06 1.5555e-01</hent>
<hent key="03800100000000000000280000000004" call="MPI_Irecv" bytes="10240" orank="4" region="0" commid="0" count="40" tid="0" op="" dtype="" >9.0599e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000280000000014" call="MPI_Irecv" bytes="10240" orank="20" region="0" commid="0" count="23" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000500000002CC" call="MPI_Irecv" bytes="1280" orank="716" region="0" commid="0" count="275" tid="0" op="" dtype="" >1.5068e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000004" call="MPI_Irecv" bytes="2048" orank="4" region="0" commid="0" count="9" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000B" call="MPI_Irecv" bytes="2048" orank="11" region="0" commid="0" count="13" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000D" call="MPI_Irecv" bytes="2048" orank="13" region="0" commid="0" count="17" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000014" call="MPI_Irecv" bytes="2048" orank="20" region="0" commid="0" count="14" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000004C" call="MPI_Irecv" bytes="2048" orank="76" region="0" commid="0" count="3396" tid="0" op="" dtype="" >6.2943e-04 0.0000e+00 1.5020e-05</hent>
<hent key="038001000000000000000800000002CC" call="MPI_Irecv" bytes="2048" orank="716" region="0" commid="0" count="3408" tid="0" op="" dtype="" >1.0753e-03 0.0000e+00 3.7193e-05</hent>
<hent key="02400100000000000000050000000004" call="MPI_Isend" bytes="1280" orank="4" region="0" commid="0" count="211" tid="0" op="" dtype="" >3.0637e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000005000000000B" call="MPI_Isend" bytes="1280" orank="11" region="0" commid="0" count="210" tid="0" op="" dtype="" >4.0460e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000005000000000D" call="MPI_Isend" bytes="1280" orank="13" region="0" commid="0" count="193" tid="0" op="" dtype="" >6.9141e-04 1.9073e-06 7.1526e-06</hent>
<hent key="02400100000000000000050000000014" call="MPI_Isend" bytes="1280" orank="20" region="0" commid="0" count="194" tid="0" op="" dtype="" >3.3736e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000005000000004C" call="MPI_Isend" bytes="1280" orank="76" region="0" commid="0" count="321" tid="0" op="" dtype="" >1.8044e-03 3.8147e-06 1.3995e-04</hent>
<hent key="02400100000000000000280000000004" call="MPI_Isend" bytes="10240" orank="4" region="0" commid="0" count="60" tid="0" op="" dtype="" >2.3127e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000280000000014" call="MPI_Isend" bytes="10240" orank="20" region="0" commid="0" count="17" tid="0" op="" dtype="" >2.3365e-05 0.0000e+00 1.5020e-05</hent>
<hent key="024001000000000000000500000002CC" call="MPI_Isend" bytes="1280" orank="716" region="0" commid="0" count="303" tid="0" op="" dtype="" >1.4253e-03 3.0994e-06 1.6928e-05</hent>
</hash>
<internal rank="12" log_i="1723712895.680555" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="13" mpi_size="768" stamp_init="1723712829.548156" stamp_final="1723712895.689294" username="apac4" allocationname="unknown" flags="0" pid="684281" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61411e+01" utime="5.13048e+01" stime="7.40160e+00" mtime="3.08060e+01" gflop="0.00000e+00" gbyte="3.77033e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.08060e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d814df55d814d814d7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60083e+01" utime="5.12710e+01" stime="7.39298e+00" mtime="3.08060e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.08060e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4895e+08" > 3.7302e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4938e+08" > 2.6520e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9440e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5811e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0562e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5877e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.4868e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0437e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1355e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9165e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="195" >
<hent key="02400100000000000000080000000005" call="MPI_Isend" bytes="2048" orank="5" region="0" commid="0" count="14" tid="0" op="" dtype="" >5.5075e-05 9.5367e-07 1.3113e-05</hent>
<hent key="0240010000000000000008000000000C" call="MPI_Isend" bytes="2048" orank="12" region="0" commid="0" count="17" tid="0" op="" dtype="" >3.8385e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000008000000000E" call="MPI_Isend" bytes="2048" orank="14" region="0" commid="0" count="11" tid="0" op="" dtype="" >4.0293e-05 2.8610e-06 4.0531e-06</hent>
<hent key="02400100000000000000080000000015" call="MPI_Isend" bytes="2048" orank="21" region="0" commid="0" count="18" tid="0" op="" dtype="" >5.6028e-05 9.5367e-07 1.4067e-05</hent>
<hent key="0240010000000000000008000000004D" call="MPI_Isend" bytes="2048" orank="77" region="0" commid="0" count="3449" tid="0" op="" dtype="" >1.2153e-02 9.5367e-07 2.4796e-05</hent>
<hent key="024001000000000000000800000002CD" call="MPI_Isend" bytes="2048" orank="717" region="0" commid="0" count="3450" tid="0" op="" dtype="" >1.0974e-02 9.5367e-07 1.9073e-05</hent>
<hent key="038001000000000000000E000000004D" call="MPI_Irecv" bytes="3584" orank="77" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000E000000004D" call="MPI_Isend" bytes="3584" orank="77" region="0" commid="0" count="1" tid="0" op="" dtype="" >7.1526e-06 7.1526e-06 7.1526e-06</hent>
<hent key="03800100000000000000028000000005" call="MPI_Irecv" bytes="640" orank="5" region="0" commid="0" count="447" tid="0" op="" dtype="" >1.2660e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000C" call="MPI_Irecv" bytes="640" orank="12" region="0" commid="0" count="422" tid="0" op="" dtype="" >1.3590e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000E" call="MPI_Irecv" bytes="640" orank="14" region="0" commid="0" count="395" tid="0" op="" dtype="" >8.0824e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000015" call="MPI_Irecv" bytes="640" orank="21" region="0" commid="0" count="406" tid="0" op="" dtype="" >8.3685e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000004D" call="MPI_Irecv" bytes="640" orank="77" region="0" commid="0" count="267" tid="0" op="" dtype="" >7.5102e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000000280000002CD" call="MPI_Irecv" bytes="640" orank="717" region="0" commid="0" count="275" tid="0" op="" dtype="" >9.2268e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000005" call="MPI_Isend" bytes="640" orank="5" region="0" commid="0" count="426" tid="0" op="" dtype="" >4.7159e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0240010000000000000002800000000C" call="MPI_Isend" bytes="640" orank="12" region="0" commid="0" count="382" tid="0" op="" dtype="" >5.2667e-04 9.5367e-07 5.0068e-06</hent>
<hent key="0240010000000000000002800000000E" call="MPI_Isend" bytes="640" orank="14" region="0" commid="0" count="395" tid="0" op="" dtype="" >1.1506e-03 1.9073e-06 9.0599e-06</hent>
<hent key="02400100000000000000028000000015" call="MPI_Isend" bytes="640" orank="21" region="0" commid="0" count="374" tid="0" op="" dtype="" >3.9911e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000002800000004D" call="MPI_Isend" bytes="640" orank="77" region="0" commid="0" count="290" tid="0" op="" dtype="" >1.4236e-03 3.8147e-06 1.1921e-05</hent>
<hent key="024001000000000000000280000002CD" call="MPI_Isend" bytes="640" orank="717" region="0" commid="0" count="280" tid="0" op="" dtype="" >1.2035e-03 2.8610e-06 1.0014e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.2187e-05 3.2187e-05 3.2187e-05</hent>
<hent key="03800100000000000000014000000005" call="MPI_Irecv" bytes="320" orank="5" region="0" commid="0" count="377" tid="0" op="" dtype="" >9.3937e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000C" call="MPI_Irecv" bytes="320" orank="12" region="0" commid="0" count="353" tid="0" op="" dtype="" >1.1039e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000E" call="MPI_Irecv" bytes="320" orank="14" region="0" commid="0" count="368" tid="0" op="" dtype="" >7.4148e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000015" call="MPI_Irecv" bytes="320" orank="21" region="0" commid="0" count="346" tid="0" op="" dtype="" >5.9605e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000004D" call="MPI_Irecv" bytes="320" orank="77" region="0" commid="0" count="188" tid="0" op="" dtype="" >4.6730e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002CD" call="MPI_Irecv" bytes="320" orank="717" region="0" commid="0" count="181" tid="0" op="" dtype="" >5.6028e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.9440e+00 0.0000e+00 1.8241e-01</hent>
<hent key="0380010000000000000040000000004D" call="MPI_Irecv" bytes="16384" orank="77" region="0" commid="0" count="12516" tid="0" op="" dtype="" >4.8802e-03 0.0000e+00 6.9141e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6441e-02 0.0000e+00 1.6426e-02</hent>
<hent key="038001000000000000004000000002CD" call="MPI_Irecv" bytes="16384" orank="717" region="0" commid="0" count="12588" tid="0" op="" dtype="" >2.2652e-03 0.0000e+00 6.9141e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >6.5804e-05 0.0000e+00 6.4850e-05</hent>
<hent key="02400100000000000000014000000005" call="MPI_Isend" bytes="320" orank="5" region="0" commid="0" count="324" tid="0" op="" dtype="" >2.7728e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000001400000000C" call="MPI_Isend" bytes="320" orank="12" region="0" commid="0" count="335" tid="0" op="" dtype="" >3.3736e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000001400000000E" call="MPI_Isend" bytes="320" orank="14" region="0" commid="0" count="368" tid="0" op="" dtype="" >9.3126e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000014000000015" call="MPI_Isend" bytes="320" orank="21" region="0" commid="0" count="361" tid="0" op="" dtype="" >3.1495e-04 0.0000e+00 1.1921e-05</hent>
<hent key="0240010000000000000001400000004D" call="MPI_Isend" bytes="320" orank="77" region="0" commid="0" count="164" tid="0" op="" dtype="" >7.6032e-04 3.8147e-06 1.0967e-05</hent>
<hent key="03800100000000000000200000000005" call="MPI_Irecv" bytes="8192" orank="5" region="0" commid="0" count="12672" tid="0" op="" dtype="" >2.5840e-03 0.0000e+00 1.1921e-05</hent>
<hent key="0380010000000000000020000000000C" call="MPI_Irecv" bytes="8192" orank="12" region="0" commid="0" count="4251" tid="0" op="" dtype="" >1.3008e-03 0.0000e+00 3.0994e-06</hent>
<hent key="0380010000000000000020000000000E" call="MPI_Irecv" bytes="8192" orank="14" region="0" commid="0" count="3730" tid="0" op="" dtype="" >6.1655e-04 0.0000e+00 3.8147e-06</hent>
<hent key="03800100000000000000200000000015" call="MPI_Irecv" bytes="8192" orank="21" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.2450e-03 0.0000e+00 4.0531e-06</hent>
<hent key="024001000000000000000140000002CD" call="MPI_Isend" bytes="320" orank="717" region="0" commid="0" count="192" tid="0" op="" dtype="" >8.1444e-04 3.0994e-06 5.9605e-06</hent>
<hent key="03800100000000000000000000000005" call="MPI_Irecv" bytes="0" orank="5" region="0" commid="0" count="245" tid="0" op="" dtype="" >5.7936e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000C" call="MPI_Irecv" bytes="0" orank="12" region="0" commid="0" count="238" tid="0" op="" dtype="" >7.8678e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000000000000000E" call="MPI_Irecv" bytes="0" orank="14" region="0" commid="0" count="265" tid="0" op="" dtype="" >5.5552e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000015" call="MPI_Irecv" bytes="0" orank="21" region="0" commid="0" count="253" tid="0" op="" dtype="" >5.2214e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000004D" call="MPI_Irecv" bytes="0" orank="77" region="0" commid="0" count="144" tid="0" op="" dtype="" >3.4332e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000040000000004D" call="MPI_Isend" bytes="16384" orank="77" region="0" commid="0" count="12624" tid="0" op="" dtype="" >1.1053e-01 3.8147e-06 2.7895e-05</hent>
<hent key="038001000000000000000000000002CD" call="MPI_Irecv" bytes="0" orank="717" region="0" commid="0" count="157" tid="0" op="" dtype="" >3.4809e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002CD" call="MPI_Isend" bytes="16384" orank="717" region="0" commid="0" count="12660" tid="0" op="" dtype="" >8.4034e-02 3.0994e-06 5.0068e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.5109e-04 0.0000e+00 2.7680e-04</hent>
<hent key="02400100000000000000200000000005" call="MPI_Isend" bytes="8192" orank="5" region="0" commid="0" count="12597" tid="0" op="" dtype="" >4.0810e-03 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000020000000000C" call="MPI_Isend" bytes="8192" orank="12" region="0" commid="0" count="3149" tid="0" op="" dtype="" >3.7215e-03 0.0000e+00 4.9114e-05</hent>
<hent key="0240010000000000000020000000000E" call="MPI_Isend" bytes="8192" orank="14" region="0" commid="0" count="3881" tid="0" op="" dtype="" >7.5493e-03 0.0000e+00 7.5102e-05</hent>
<hent key="02400100000000000000200000000015" call="MPI_Isend" bytes="8192" orank="21" region="0" commid="0" count="12679" tid="0" op="" dtype="" >9.5735e-03 0.0000e+00 6.4135e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.2030e-05 9.2030e-05 9.2030e-05</hent>
<hent key="02400100000000000000000000000005" call="MPI_Isend" bytes="0" orank="5" region="0" commid="0" count="279" tid="0" op="" dtype="" >1.8835e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000000000000C" call="MPI_Isend" bytes="0" orank="12" region="0" commid="0" count="263" tid="0" op="" dtype="" >1.7023e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000000000000000E" call="MPI_Isend" bytes="0" orank="14" region="0" commid="0" count="272" tid="0" op="" dtype="" >5.4169e-04 0.0000e+00 8.7023e-05</hent>
<hent key="02400100000000000000000000000015" call="MPI_Isend" bytes="0" orank="21" region="0" commid="0" count="278" tid="0" op="" dtype="" >1.7619e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000000000000004D" call="MPI_Isend" bytes="0" orank="77" region="0" commid="0" count="140" tid="0" op="" dtype="" >5.3191e-04 9.5367e-07 7.1526e-06</hent>
<hent key="024001000000000000000000000002CD" call="MPI_Isend" bytes="0" orank="717" region="0" commid="0" count="155" tid="0" op="" dtype="" >6.0487e-04 9.5367e-07 8.1062e-06</hent>
<hent key="03800100000000000000060000000005" call="MPI_Irecv" bytes="1536" orank="5" region="0" commid="0" count="87" tid="0" op="" dtype="" >2.0504e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000C" call="MPI_Irecv" bytes="1536" orank="12" region="0" commid="0" count="88" tid="0" op="" dtype="" >2.4557e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000E" call="MPI_Irecv" bytes="1536" orank="14" region="0" commid="0" count="94" tid="0" op="" dtype="" >2.8849e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000015" call="MPI_Irecv" bytes="1536" orank="21" region="0" commid="0" count="107" tid="0" op="" dtype="" >1.7405e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000004D" call="MPI_Irecv" bytes="1536" orank="77" region="0" commid="0" count="213" tid="0" op="" dtype="" >5.5552e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002CD" call="MPI_Irecv" bytes="1536" orank="717" region="0" commid="0" count="200" tid="0" op="" dtype="" >8.5115e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000060000000005" call="MPI_Isend" bytes="1536" orank="5" region="0" commid="0" count="76" tid="0" op="" dtype="" >1.2851e-04 9.5367e-07 1.0967e-05</hent>
<hent key="0240010000000000000006000000000C" call="MPI_Isend" bytes="1536" orank="12" region="0" commid="0" count="95" tid="0" op="" dtype="" >1.9360e-04 9.5367e-07 7.1526e-06</hent>
<hent key="0240010000000000000006000000000E" call="MPI_Isend" bytes="1536" orank="14" region="0" commid="0" count="89" tid="0" op="" dtype="" >3.3450e-04 2.8610e-06 1.6928e-05</hent>
<hent key="02400100000000000000060000000015" call="MPI_Isend" bytes="1536" orank="21" region="0" commid="0" count="111" tid="0" op="" dtype="" >2.0742e-04 9.5367e-07 1.3113e-05</hent>
<hent key="0240010000000000000006000000004D" call="MPI_Isend" bytes="1536" orank="77" region="0" commid="0" count="207" tid="0" op="" dtype="" >1.1129e-03 3.8147e-06 1.1921e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002CD" call="MPI_Isend" bytes="1536" orank="717" region="0" commid="0" count="211" tid="0" op="" dtype="" >9.7060e-04 3.8147e-06 6.1989e-06</hent>
<hent key="038001000000000000000C0000000015" call="MPI_Irecv" bytes="3072" orank="21" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C000000004D" call="MPI_Irecv" bytes="3072" orank="77" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002CD" call="MPI_Irecv" bytes="3072" orank="717" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C000000000E" call="MPI_Isend" bytes="3072" orank="14" region="0" commid="0" count="1" tid="0" op="" dtype="" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="024001000000000000000C000000004D" call="MPI_Isend" bytes="3072" orank="77" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.9802e-05 5.0068e-06 6.9141e-06</hent>
<hent key="024001000000000000000C00000002CD" call="MPI_Isend" bytes="3072" orank="717" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.0027e-05 5.0068e-06 5.0068e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8801e-04 2.8801e-04 2.8801e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >6.5804e-04 2.1601e-04 2.2602e-04</hent>
<hent key="03800100000000000000038000000005" call="MPI_Irecv" bytes="896" orank="5" region="0" commid="0" count="297" tid="0" op="" dtype="" >9.2983e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000C" call="MPI_Irecv" bytes="896" orank="12" region="0" commid="0" count="2580" tid="0" op="" dtype="" >4.4823e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000003800000000E" call="MPI_Irecv" bytes="896" orank="14" region="0" commid="0" count="2714" tid="0" op="" dtype="" >5.9295e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000015" call="MPI_Irecv" bytes="896" orank="21" region="0" commid="0" count="314" tid="0" op="" dtype="" >6.3658e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000004D" call="MPI_Irecv" bytes="896" orank="77" region="0" commid="0" count="327" tid="0" op="" dtype="" >8.5592e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002CD" call="MPI_Irecv" bytes="896" orank="717" region="0" commid="0" count="338" tid="0" op="" dtype="" >1.1992e-04 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.5020e-05 1.5020e-05 1.5020e-05</hent>
<hent key="0380010000000000000038000000004D" call="MPI_Irecv" bytes="14336" orank="77" region="0" commid="0" count="183" tid="0" op="" dtype="" >5.2214e-05 0.0000e+00 3.0994e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000003800000002CD" call="MPI_Irecv" bytes="14336" orank="717" region="0" commid="0" count="111" tid="0" op="" dtype="" >1.6928e-05 0.0000e+00 3.0994e-06</hent>
<hent key="02400100000000000000038000000005" call="MPI_Isend" bytes="896" orank="5" region="0" commid="0" count="317" tid="0" op="" dtype="" >4.2510e-04 9.5367e-07 6.9141e-06</hent>
<hent key="0240010000000000000003800000000C" call="MPI_Isend" bytes="896" orank="12" region="0" commid="0" count="2884" tid="0" op="" dtype="" >2.1567e-03 0.0000e+00 6.1989e-06</hent>
<hent key="0240010000000000000003800000000E" call="MPI_Isend" bytes="896" orank="14" region="0" commid="0" count="2666" tid="0" op="" dtype="" >2.5795e-03 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000038000000015" call="MPI_Isend" bytes="896" orank="21" region="0" commid="0" count="313" tid="0" op="" dtype="" >4.0770e-04 9.5367e-07 1.0014e-05</hent>
<hent key="0240010000000000000003800000004D" call="MPI_Isend" bytes="896" orank="77" region="0" commid="0" count="319" tid="0" op="" dtype="" >1.6258e-03 3.8147e-06 1.2159e-05</hent>
<hent key="024001000000000000000380000002CD" call="MPI_Isend" bytes="896" orank="717" region="0" commid="0" count="339" tid="0" op="" dtype="" >1.4944e-03 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000038000000004D" call="MPI_Isend" bytes="14336" orank="77" region="0" commid="0" count="75" tid="0" op="" dtype="" >6.6447e-04 5.0068e-06 1.5020e-05</hent>
<hent key="024001000000000000003800000002CD" call="MPI_Isend" bytes="14336" orank="717" region="0" commid="0" count="39" tid="0" op="" dtype="" >2.4247e-04 4.0531e-06 9.0599e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.9448e+00 2.1935e-05 2.0318e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.2070e-03 3.2070e-03 3.2070e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0437e-02 1.0437e-02 1.0437e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.6609e-02 2.6609e-02 2.6609e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.9998e-01 2.9850e-03 1.8050e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >2.9802e-04 2.9802e-04 2.9802e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5877e+00 3.7789e-04 2.5084e-01</hent>
<hent key="03800100000000000000040000000005" call="MPI_Irecv" bytes="1024" orank="5" region="0" commid="0" count="3392" tid="0" op="" dtype="" >5.5671e-04 0.0000e+00 3.0994e-06</hent>
<hent key="0380010000000000000004000000000C" call="MPI_Irecv" bytes="1024" orank="12" region="0" commid="0" count="1174" tid="0" op="" dtype="" >1.7524e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000000E" call="MPI_Irecv" bytes="1024" orank="14" region="0" commid="0" count="1008" tid="0" op="" dtype="" >2.4581e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000015" call="MPI_Irecv" bytes="1024" orank="21" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.4966e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.5974e-05 0.0000e+00 8.8215e-06</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.9101e-05 3.9101e-05 3.9101e-05</hent>
<hent key="03800100000000000000070000000005" call="MPI_Irecv" bytes="1792" orank="5" region="0" commid="0" count="29" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000C" call="MPI_Irecv" bytes="1792" orank="12" region="0" commid="0" count="34" tid="0" op="" dtype="" >1.4544e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000E" call="MPI_Irecv" bytes="1792" orank="14" region="0" commid="0" count="26" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000015" call="MPI_Irecv" bytes="1792" orank="21" region="0" commid="0" count="36" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000004D" call="MPI_Irecv" bytes="1792" orank="77" region="0" commid="0" count="187" tid="0" op="" dtype="" >4.5300e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002CD" call="MPI_Irecv" bytes="1792" orank="717" region="0" commid="0" count="165" tid="0" op="" dtype="" >5.8174e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >6.2943e-05 0.0000e+00 2.5034e-05</hent>
<hent key="02400100000000000000040000000005" call="MPI_Isend" bytes="1024" orank="5" region="0" commid="0" count="3372" tid="0" op="" dtype="" >1.5454e-03 0.0000e+00 6.1989e-06</hent>
<hent key="0240010000000000000004000000000C" call="MPI_Isend" bytes="1024" orank="12" region="0" commid="0" count="864" tid="0" op="" dtype="" >5.1904e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000004000000000E" call="MPI_Isend" bytes="1024" orank="14" region="0" commid="0" count="1050" tid="0" op="" dtype="" >7.8940e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000040000000015" call="MPI_Isend" bytes="1024" orank="21" region="0" commid="0" count="3392" tid="0" op="" dtype="" >1.7865e-03 0.0000e+00 9.0599e-06</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >3.8147e-06 1.9073e-06 1.9073e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5640e+00 0.0000e+00 2.2679e+00</hent>
<hent key="038001000000000000000A0000000005" call="MPI_Irecv" bytes="2560" orank="5" region="0" commid="0" count="8" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000000C" call="MPI_Irecv" bytes="2560" orank="12" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000000E" call="MPI_Irecv" bytes="2560" orank="14" region="0" commid="0" count="5" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000015" call="MPI_Irecv" bytes="2560" orank="21" region="0" commid="0" count="10" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000004D" call="MPI_Irecv" bytes="2560" orank="77" region="0" commid="0" count="52" tid="0" op="" dtype="" >1.5497e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002CD" call="MPI_Irecv" bytes="2560" orank="717" region="0" commid="0" count="34" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000005" call="MPI_Isend" bytes="1792" orank="5" region="0" commid="0" count="32" tid="0" op="" dtype="" >9.2745e-05 9.5367e-07 1.5974e-05</hent>
<hent key="0240010000000000000007000000000C" call="MPI_Isend" bytes="1792" orank="12" region="0" commid="0" count="34" tid="0" op="" dtype="" >7.2718e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000007000000000E" call="MPI_Isend" bytes="1792" orank="14" region="0" commid="0" count="35" tid="0" op="" dtype="" >1.3566e-04 2.8610e-06 8.1062e-06</hent>
<hent key="02400100000000000000070000000015" call="MPI_Isend" bytes="1792" orank="21" region="0" commid="0" count="37" tid="0" op="" dtype="" >8.6546e-05 9.5367e-07 1.5974e-05</hent>
<hent key="0240010000000000000007000000004D" call="MPI_Isend" bytes="1792" orank="77" region="0" commid="0" count="158" tid="0" op="" dtype="" >8.2493e-04 9.5367e-07 1.3113e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1355e-01 1.1355e-01 1.1355e-01</hent>
<hent key="024001000000000000000700000002CD" call="MPI_Isend" bytes="1792" orank="717" region="0" commid="0" count="143" tid="0" op="" dtype="" >6.6161e-04 1.9073e-06 1.0967e-05</hent>
<hent key="024001000000000000000A0000000005" call="MPI_Isend" bytes="2560" orank="5" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.8385e-05 1.1921e-06 1.5020e-05</hent>
<hent key="024001000000000000000A000000000C" call="MPI_Isend" bytes="2560" orank="12" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.1921e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A000000000E" call="MPI_Isend" bytes="2560" orank="14" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.3842e-05 3.8147e-06 4.0531e-06</hent>
<hent key="024001000000000000000A0000000015" call="MPI_Isend" bytes="2560" orank="21" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.5988e-05 1.9073e-06 1.5020e-05</hent>
<hent key="024001000000000000000A000000004D" call="MPI_Isend" bytes="2560" orank="77" region="0" commid="0" count="52" tid="0" op="" dtype="" >2.9778e-04 5.0068e-06 6.9141e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.3307e-04 4.5061e-05 1.4496e-04</hent>
<hent key="024001000000000000000A00000002CD" call="MPI_Isend" bytes="2560" orank="717" region="0" commid="0" count="47" tid="0" op="" dtype="" >2.3890e-04 3.8147e-06 6.1989e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.1091e-03 3.9411e-04 7.1502e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.7909e-05 3.7909e-05 3.7909e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >6.9141e-06 2.1458e-06 4.7684e-06</hent>
<hent key="03800100000000000000000400000005" call="MPI_Irecv" bytes="4" orank="5" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.3631e-04 0.0000e+00 6.1989e-06</hent>
<hent key="0380010000000000000000040000000C" call="MPI_Irecv" bytes="4" orank="12" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.2660e-04 0.0000e+00 3.8147e-06</hent>
<hent key="0380010000000000000000040000000E" call="MPI_Irecv" bytes="4" orank="14" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.4346e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000015" call="MPI_Irecv" bytes="4" orank="21" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.7970e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000000040000004D" call="MPI_Irecv" bytes="4" orank="77" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.5313e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.0986e-03 2.8181e-04 7.3791e-04</hent>
<hent key="038001000000000000001C000000000C" call="MPI_Irecv" bytes="7168" orank="12" region="0" commid="0" count="8448" tid="0" op="" dtype="" >2.5232e-03 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000001C000000000E" call="MPI_Irecv" bytes="7168" orank="14" region="0" commid="0" count="8969" tid="0" op="" dtype="" >1.6482e-03 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000004000002CD" call="MPI_Irecv" bytes="4" orank="717" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.7864e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000400000005" call="MPI_Isend" bytes="4" orank="5" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2851e-03 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000000040000000C" call="MPI_Isend" bytes="4" orank="12" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1628e-03 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000000040000000E" call="MPI_Isend" bytes="4" orank="14" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3270e-03 0.0000e+00 7.9155e-05</hent>
<hent key="02400100000000000000000400000015" call="MPI_Isend" bytes="4" orank="21" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6296e-03 0.0000e+00 2.7895e-05</hent>
<hent key="0240010000000000000000040000004D" call="MPI_Isend" bytes="4" orank="77" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.4041e-02 4.7684e-06 4.3869e-05</hent>
<hent key="024001000000000000001C000000000C" call="MPI_Isend" bytes="7168" orank="12" region="0" commid="0" count="9550" tid="0" op="" dtype="" >2.0885e-02 9.5367e-07 3.6001e-05</hent>
<hent key="024001000000000000001C000000000E" call="MPI_Isend" bytes="7168" orank="14" region="0" commid="0" count="8818" tid="0" op="" dtype="" >2.5853e-02 9.5367e-07 2.2173e-05</hent>
<hent key="024001000000000000000004000002CD" call="MPI_Isend" bytes="4" orank="717" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7757e-02 3.0994e-06 1.0514e-04</hent>
<hent key="03800100000000000000050000000005" call="MPI_Irecv" bytes="1280" orank="5" region="0" commid="0" count="206" tid="0" op="" dtype="" >5.7936e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000C" call="MPI_Irecv" bytes="1280" orank="12" region="0" commid="0" count="193" tid="0" op="" dtype="" >6.9857e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000E" call="MPI_Irecv" bytes="1280" orank="14" region="0" commid="0" count="205" tid="0" op="" dtype="" >4.3154e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000015" call="MPI_Irecv" bytes="1280" orank="21" region="0" commid="0" count="207" tid="0" op="" dtype="" >3.4332e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000004D" call="MPI_Irecv" bytes="1280" orank="77" region="0" commid="0" count="283" tid="0" op="" dtype="" >8.8930e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0990e+01 8.8215e-06 1.5570e-01</hent>
<hent key="03800100000000000000280000000005" call="MPI_Irecv" bytes="10240" orank="5" region="0" commid="0" count="27" tid="0" op="" dtype="" >8.5831e-06 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000000500000002CD" call="MPI_Irecv" bytes="1280" orank="717" region="0" commid="0" count="307" tid="0" op="" dtype="" >1.0204e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000080000000005" call="MPI_Irecv" bytes="2048" orank="5" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000C" call="MPI_Irecv" bytes="2048" orank="12" region="0" commid="0" count="11" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000E" call="MPI_Irecv" bytes="2048" orank="14" region="0" commid="0" count="17" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000015" call="MPI_Irecv" bytes="2048" orank="21" region="0" commid="0" count="19" tid="0" op="" dtype="" >7.3910e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000004D" call="MPI_Irecv" bytes="2048" orank="77" region="0" commid="0" count="3428" tid="0" op="" dtype="" >5.0044e-04 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000800000002CD" call="MPI_Irecv" bytes="2048" orank="717" region="0" commid="0" count="3432" tid="0" op="" dtype="" >5.6410e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000050000000005" call="MPI_Isend" bytes="1280" orank="5" region="0" commid="0" count="250" tid="0" op="" dtype="" >3.4738e-04 0.0000e+00 1.1921e-05</hent>
<hent key="0240010000000000000005000000000C" call="MPI_Isend" bytes="1280" orank="12" region="0" commid="0" count="218" tid="0" op="" dtype="" >4.1127e-04 9.5367e-07 5.0068e-06</hent>
<hent key="0240010000000000000005000000000E" call="MPI_Isend" bytes="1280" orank="14" region="0" commid="0" count="204" tid="0" op="" dtype="" >7.4911e-04 1.9073e-06 5.0068e-05</hent>
<hent key="02400100000000000000050000000015" call="MPI_Isend" bytes="1280" orank="21" region="0" commid="0" count="209" tid="0" op="" dtype="" >3.0065e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0240010000000000000005000000004D" call="MPI_Isend" bytes="1280" orank="77" region="0" commid="0" count="312" tid="0" op="" dtype="" >1.6034e-03 3.8147e-06 6.1989e-06</hent>
<hent key="02400100000000000000280000000005" call="MPI_Isend" bytes="10240" orank="5" region="0" commid="0" count="102" tid="0" op="" dtype="" >2.8610e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000280000000015" call="MPI_Isend" bytes="10240" orank="21" region="0" commid="0" count="20" tid="0" op="" dtype="" >1.6928e-05 0.0000e+00 1.9073e-06</hent>
<hent key="024001000000000000000500000002CD" call="MPI_Isend" bytes="1280" orank="717" region="0" commid="0" count="276" tid="0" op="" dtype="" >1.2357e-03 3.8147e-06 1.0014e-05</hent>
</hash>
<internal rank="13" log_i="1723712895.689294" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="14" mpi_size="768" stamp_init="1723712829.548148" stamp_final="1723712895.689867" username="apac4" allocationname="unknown" flags="0" pid="684282" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61417e+01" utime="4.95296e+01" stime="8.09513e+00" mtime="3.01264e+01" gflop="0.00000e+00" gbyte="3.77529e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.01264e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000070146f14bc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60078e+01" utime="4.94954e+01" stime="8.08708e+00" mtime="3.01264e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.01264e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4941e+08" > 4.6472e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4857e+08" > 4.0747e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3657e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5695e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2206e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.6955e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5819e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.7259e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0426e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1373e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7972e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="196" >
<hent key="02400100000000000000080000000006" call="MPI_Isend" bytes="2048" orank="6" region="0" commid="0" count="12" tid="0" op="" dtype="" >6.1274e-05 9.5367e-07 1.4067e-05</hent>
<hent key="0240010000000000000008000000000D" call="MPI_Isend" bytes="2048" orank="13" region="0" commid="0" count="17" tid="0" op="" dtype="" >4.0770e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000008000000000F" call="MPI_Isend" bytes="2048" orank="15" region="0" commid="0" count="13" tid="0" op="" dtype="" >5.4598e-05 3.8147e-06 5.0068e-06</hent>
<hent key="02400100000000000000080000000016" call="MPI_Isend" bytes="2048" orank="22" region="0" commid="0" count="14" tid="0" op="" dtype="" >7.8917e-05 9.5367e-07 1.3113e-05</hent>
<hent key="0240010000000000000008000000004E" call="MPI_Isend" bytes="2048" orank="78" region="0" commid="0" count="3443" tid="0" op="" dtype="" >1.6546e-02 9.5367e-07 5.4121e-05</hent>
<hent key="024001000000000000000800000002CE" call="MPI_Isend" bytes="2048" orank="718" region="0" commid="0" count="3403" tid="0" op="" dtype="" >1.3723e-02 9.5367e-07 3.9101e-05</hent>
<hent key="024001000000000000000E000000004E" call="MPI_Isend" bytes="3584" orank="78" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000006" call="MPI_Irecv" bytes="640" orank="6" region="0" commid="0" count="412" tid="0" op="" dtype="" >7.7248e-05 0.0000e+00 8.1062e-06</hent>
<hent key="0380010000000000000002800000000D" call="MPI_Irecv" bytes="640" orank="13" region="0" commid="0" count="395" tid="0" op="" dtype="" >1.5187e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000F" call="MPI_Irecv" bytes="640" orank="15" region="0" commid="0" count="421" tid="0" op="" dtype="" >8.8215e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000016" call="MPI_Irecv" bytes="640" orank="22" region="0" commid="0" count="390" tid="0" op="" dtype="" >1.1420e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000002800000004E" call="MPI_Irecv" bytes="640" orank="78" region="0" commid="0" count="306" tid="0" op="" dtype="" >1.3924e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002CE" call="MPI_Irecv" bytes="640" orank="718" region="0" commid="0" count="305" tid="0" op="" dtype="" >1.1539e-04 0.0000e+00 9.7752e-06</hent>
<hent key="02400100000000000000028000000006" call="MPI_Isend" bytes="640" orank="6" region="0" commid="0" count="406" tid="0" op="" dtype="" >5.0640e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000002800000000D" call="MPI_Isend" bytes="640" orank="13" region="0" commid="0" count="395" tid="0" op="" dtype="" >5.6553e-04 9.5367e-07 5.9605e-06</hent>
<hent key="0240010000000000000002800000000F" call="MPI_Isend" bytes="640" orank="15" region="0" commid="0" count="386" tid="0" op="" dtype="" >1.3034e-03 1.9073e-06 1.0014e-05</hent>
<hent key="02400100000000000000028000000016" call="MPI_Isend" bytes="640" orank="22" region="0" commid="0" count="386" tid="0" op="" dtype="" >4.7064e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000002800000004E" call="MPI_Isend" bytes="640" orank="78" region="0" commid="0" count="301" tid="0" op="" dtype="" >1.5681e-03 3.8147e-06 2.0981e-05</hent>
<hent key="024001000000000000000280000002CE" call="MPI_Isend" bytes="640" orank="718" region="0" commid="0" count="276" tid="0" op="" dtype="" >1.2429e-03 2.8610e-06 1.1921e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-05 3.0994e-05 3.0994e-05</hent>
<hent key="03800100000000000000014000000006" call="MPI_Irecv" bytes="320" orank="6" region="0" commid="0" count="364" tid="0" op="" dtype="" >6.9618e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000001400000000D" call="MPI_Irecv" bytes="320" orank="13" region="0" commid="0" count="368" tid="0" op="" dtype="" >1.3709e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000F" call="MPI_Irecv" bytes="320" orank="15" region="0" commid="0" count="342" tid="0" op="" dtype="" >6.9857e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000014000000016" call="MPI_Irecv" bytes="320" orank="22" region="0" commid="0" count="370" tid="0" op="" dtype="" >7.1287e-05 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000001400000004E" call="MPI_Irecv" bytes="320" orank="78" region="0" commid="0" count="203" tid="0" op="" dtype="" >6.7711e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002CE" call="MPI_Irecv" bytes="320" orank="718" region="0" commid="0" count="156" tid="0" op="" dtype="" >6.2943e-05 0.0000e+00 1.0967e-05</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.3657e+00 0.0000e+00 1.8218e-01</hent>
<hent key="0380010000000000000040000000004E" call="MPI_Irecv" bytes="16384" orank="78" region="0" commid="0" count="12550" tid="0" op="" dtype="" >7.4432e-03 0.0000e+00 2.6941e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6460e-02 2.1458e-06 1.6443e-02</hent>
<hent key="038001000000000000004000000002CE" call="MPI_Irecv" bytes="16384" orank="718" region="0" commid="0" count="12396" tid="0" op="" dtype="" >7.4139e-03 0.0000e+00 3.8147e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000006" call="MPI_Isend" bytes="320" orank="6" region="0" commid="0" count="375" tid="0" op="" dtype="" >3.5691e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000001400000000D" call="MPI_Isend" bytes="320" orank="13" region="0" commid="0" count="368" tid="0" op="" dtype="" >4.0674e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000001400000000F" call="MPI_Isend" bytes="320" orank="15" region="0" commid="0" count="341" tid="0" op="" dtype="" >1.0452e-03 1.9073e-06 4.0531e-06</hent>
<hent key="02400100000000000000014000000016" call="MPI_Isend" bytes="320" orank="22" region="0" commid="0" count="396" tid="0" op="" dtype="" >4.0174e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000001400000004E" call="MPI_Isend" bytes="320" orank="78" region="0" commid="0" count="174" tid="0" op="" dtype="" >8.6737e-04 3.8147e-06 1.5974e-05</hent>
<hent key="03800100000000000000200000000006" call="MPI_Irecv" bytes="8192" orank="6" region="0" commid="0" count="12691" tid="0" op="" dtype="" >3.0394e-03 0.0000e+00 2.5988e-05</hent>
<hent key="0380010000000000000020000000000D" call="MPI_Irecv" bytes="8192" orank="13" region="0" commid="0" count="3881" tid="0" op="" dtype="" >1.1973e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000020000000000F" call="MPI_Irecv" bytes="8192" orank="15" region="0" commid="0" count="3557" tid="0" op="" dtype="" >8.8835e-04 0.0000e+00 1.5020e-05</hent>
<hent key="03800100000000000000200000000016" call="MPI_Irecv" bytes="8192" orank="22" region="0" commid="0" count="12611" tid="0" op="" dtype="" >3.4933e-03 0.0000e+00 2.6941e-05</hent>
<hent key="024001000000000000000140000002CE" call="MPI_Isend" bytes="320" orank="718" region="0" commid="0" count="171" tid="0" op="" dtype="" >7.6580e-04 2.8610e-06 1.2875e-05</hent>
<hent key="03800100000000000000000000000006" call="MPI_Irecv" bytes="0" orank="6" region="0" commid="0" count="254" tid="0" op="" dtype="" >5.2691e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000D" call="MPI_Irecv" bytes="0" orank="13" region="0" commid="0" count="272" tid="0" op="" dtype="" >1.0920e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000F" call="MPI_Irecv" bytes="0" orank="15" region="0" commid="0" count="228" tid="0" op="" dtype="" >4.9591e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000016" call="MPI_Irecv" bytes="0" orank="22" region="0" commid="0" count="280" tid="0" op="" dtype="" >8.6069e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000000000004E" call="MPI_Irecv" bytes="0" orank="78" region="0" commid="0" count="152" tid="0" op="" dtype="" >5.0783e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000040000000004E" call="MPI_Isend" bytes="16384" orank="78" region="0" commid="0" count="12673" tid="0" op="" dtype="" >1.5806e-01 4.0531e-06 9.0837e-05</hent>
<hent key="038001000000000000000000000002CE" call="MPI_Irecv" bytes="0" orank="718" region="0" commid="0" count="146" tid="0" op="" dtype="" >3.7432e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002CE" call="MPI_Isend" bytes="16384" orank="718" region="0" commid="0" count="12509" tid="0" op="" dtype="" >1.0023e-01 2.8610e-06 5.5790e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.1461e-04 0.0000e+00 2.6798e-04</hent>
<hent key="02400100000000000000200000000006" call="MPI_Isend" bytes="8192" orank="6" region="0" commid="0" count="12501" tid="0" op="" dtype="" >5.9059e-03 0.0000e+00 5.4121e-05</hent>
<hent key="0240010000000000000020000000000D" call="MPI_Isend" bytes="8192" orank="13" region="0" commid="0" count="3730" tid="0" op="" dtype="" >5.0581e-03 0.0000e+00 1.6212e-05</hent>
<hent key="0240010000000000000020000000000F" call="MPI_Isend" bytes="8192" orank="15" region="0" commid="0" count="3759" tid="0" op="" dtype="" >7.5104e-03 0.0000e+00 6.9141e-05</hent>
<hent key="02400100000000000000200000000016" call="MPI_Isend" bytes="8192" orank="22" region="0" commid="0" count="12668" tid="0" op="" dtype="" >1.2615e-02 0.0000e+00 6.3896e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.2016e-05 8.2016e-05 8.2016e-05</hent>
<hent key="02400100000000000000000000000006" call="MPI_Isend" bytes="0" orank="6" region="0" commid="0" count="267" tid="0" op="" dtype="" >2.0194e-04 0.0000e+00 9.0599e-06</hent>
<hent key="0240010000000000000000000000000D" call="MPI_Isend" bytes="0" orank="13" region="0" commid="0" count="265" tid="0" op="" dtype="" >1.6260e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000000000000000F" call="MPI_Isend" bytes="0" orank="15" region="0" commid="0" count="246" tid="0" op="" dtype="" >5.3167e-04 9.5367e-07 3.2902e-05</hent>
<hent key="02400100000000000000000000000016" call="MPI_Isend" bytes="0" orank="22" region="0" commid="0" count="274" tid="0" op="" dtype="" >1.9431e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000000000000004E" call="MPI_Isend" bytes="0" orank="78" region="0" commid="0" count="150" tid="0" op="" dtype="" >6.3705e-04 9.5367e-07 1.2875e-05</hent>
<hent key="024001000000000000000000000002CE" call="MPI_Isend" bytes="0" orank="718" region="0" commid="0" count="151" tid="0" op="" dtype="" >6.1798e-04 1.9073e-06 1.5974e-05</hent>
<hent key="03800100000000000000060000000006" call="MPI_Irecv" bytes="1536" orank="6" region="0" commid="0" count="93" tid="0" op="" dtype="" >1.9073e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000D" call="MPI_Irecv" bytes="1536" orank="13" region="0" commid="0" count="89" tid="0" op="" dtype="" >3.2663e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000000F" call="MPI_Irecv" bytes="1536" orank="15" region="0" commid="0" count="90" tid="0" op="" dtype="" >2.0742e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000016" call="MPI_Irecv" bytes="1536" orank="22" region="0" commid="0" count="98" tid="0" op="" dtype="" >2.9802e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000004E" call="MPI_Irecv" bytes="1536" orank="78" region="0" commid="0" count="204" tid="0" op="" dtype="" >8.2016e-05 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000600000002CE" call="MPI_Irecv" bytes="1536" orank="718" region="0" commid="0" count="202" tid="0" op="" dtype="" >7.7009e-05 0.0000e+00 4.0531e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >8.8215e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >5.4836e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000006" call="MPI_Isend" bytes="1536" orank="6" region="0" commid="0" count="97" tid="0" op="" dtype="" >1.6904e-04 9.5367e-07 1.1921e-05</hent>
<hent key="0240010000000000000006000000000D" call="MPI_Isend" bytes="1536" orank="13" region="0" commid="0" count="94" tid="0" op="" dtype="" >1.8668e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000006000000000F" call="MPI_Isend" bytes="1536" orank="15" region="0" commid="0" count="105" tid="0" op="" dtype="" >4.4036e-04 2.8610e-06 1.9073e-05</hent>
<hent key="02400100000000000000060000000016" call="MPI_Isend" bytes="1536" orank="22" region="0" commid="0" count="93" tid="0" op="" dtype="" >1.9050e-04 9.5367e-07 1.0967e-05</hent>
<hent key="0240010000000000000006000000004E" call="MPI_Isend" bytes="1536" orank="78" region="0" commid="0" count="219" tid="0" op="" dtype="" >1.2443e-03 3.8147e-06 1.7881e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002CE" call="MPI_Isend" bytes="1536" orank="718" region="0" commid="0" count="230" tid="0" op="" dtype="" >1.1730e-03 3.0994e-06 1.3113e-05</hent>
<hent key="038001000000000000000C000000000D" call="MPI_Irecv" bytes="3072" orank="13" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C000000000F" call="MPI_Irecv" bytes="3072" orank="15" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C000000004E" call="MPI_Irecv" bytes="3072" orank="78" region="0" commid="0" count="8" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002CE" call="MPI_Irecv" bytes="3072" orank="718" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C000000000F" call="MPI_Isend" bytes="3072" orank="15" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="024001000000000000000C000000004E" call="MPI_Isend" bytes="3072" orank="78" region="0" commid="0" count="9" tid="0" op="" dtype="" >5.3167e-05 5.0068e-06 7.1526e-06</hent>
<hent key="024001000000000000000C00000002CE" call="MPI_Isend" bytes="3072" orank="718" region="0" commid="0" count="9" tid="0" op="" dtype="" >5.1975e-05 5.0068e-06 9.0599e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.9802e-04 2.9802e-04 2.9802e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >6.9404e-04 2.2507e-04 2.4080e-04</hent>
<hent key="03800100000000000000038000000006" call="MPI_Irecv" bytes="896" orank="6" region="0" commid="0" count="336" tid="0" op="" dtype="" >6.7472e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000D" call="MPI_Irecv" bytes="896" orank="13" region="0" commid="0" count="2666" tid="0" op="" dtype="" >4.1819e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000003800000000F" call="MPI_Irecv" bytes="896" orank="15" region="0" commid="0" count="2773" tid="0" op="" dtype="" >6.7496e-04 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000038000000016" call="MPI_Irecv" bytes="896" orank="22" region="0" commid="0" count="319" tid="0" op="" dtype="" >8.3685e-05 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000003800000004E" call="MPI_Irecv" bytes="896" orank="78" region="0" commid="0" count="327" tid="0" op="" dtype="" >1.1539e-04 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000380000002CE" call="MPI_Irecv" bytes="896" orank="718" region="0" commid="0" count="332" tid="0" op="" dtype="" >1.2064e-04 0.0000e+00 6.9141e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.5711e-03 5.5711e-03 5.5711e-03</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="0380010000000000000038000000004E" call="MPI_Irecv" bytes="14336" orank="78" region="0" commid="0" count="149" tid="0" op="" dtype="" >1.0419e-04 0.0000e+00 7.1526e-06</hent>
<hent key="038001000000000000003800000002CE" call="MPI_Irecv" bytes="14336" orank="718" region="0" commid="0" count="303" tid="0" op="" dtype="" >1.7357e-04 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000038000000006" call="MPI_Isend" bytes="896" orank="6" region="0" commid="0" count="281" tid="0" op="" dtype="" >4.2057e-04 9.5367e-07 1.0014e-05</hent>
<hent key="0240010000000000000003800000000D" call="MPI_Isend" bytes="896" orank="13" region="0" commid="0" count="2714" tid="0" op="" dtype="" >1.9264e-03 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000003800000000F" call="MPI_Isend" bytes="896" orank="15" region="0" commid="0" count="2755" tid="0" op="" dtype="" >3.1395e-03 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000038000000016" call="MPI_Isend" bytes="896" orank="22" region="0" commid="0" count="308" tid="0" op="" dtype="" >5.0187e-04 9.5367e-07 1.2875e-05</hent>
<hent key="0240010000000000000003800000004E" call="MPI_Isend" bytes="896" orank="78" region="0" commid="0" count="332" tid="0" op="" dtype="" >1.8039e-03 3.8147e-06 2.2173e-05</hent>
<hent key="024001000000000000000380000002CE" call="MPI_Isend" bytes="896" orank="718" region="0" commid="0" count="332" tid="0" op="" dtype="" >1.6196e-03 2.8610e-06 2.4080e-05</hent>
<hent key="0240010000000000000038000000004E" call="MPI_Isend" bytes="14336" orank="78" region="0" commid="0" count="26" tid="0" op="" dtype="" >2.8181e-04 5.0068e-06 2.2173e-05</hent>
<hent key="024001000000000000003800000002CE" call="MPI_Isend" bytes="14336" orank="718" region="0" commid="0" count="190" tid="0" op="" dtype="" >1.6277e-03 3.8147e-06 3.0994e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.3125e+00 1.5020e-05 2.0314e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.1900e-03 3.1900e-03 3.1900e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0426e-02 1.0426e-02 1.0426e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.5783e-02 2.5783e-02 2.5783e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.1105e-01 2.1091e-03 1.9249e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.7813e-04 3.7813e-04 3.7813e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5819e+00 3.9005e-04 2.5078e-01</hent>
<hent key="03800100000000000000040000000006" call="MPI_Irecv" bytes="1024" orank="6" region="0" commid="0" count="3396" tid="0" op="" dtype="" >5.2571e-04 0.0000e+00 1.1921e-05</hent>
<hent key="0380010000000000000004000000000D" call="MPI_Irecv" bytes="1024" orank="13" region="0" commid="0" count="1050" tid="0" op="" dtype="" >1.2779e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000004000000000F" call="MPI_Irecv" bytes="1024" orank="15" region="0" commid="0" count="966" tid="0" op="" dtype="" >3.1734e-04 0.0000e+00 2.0027e-05</hent>
<hent key="03800100000000000000040000000016" call="MPI_Irecv" bytes="1024" orank="22" region="0" commid="0" count="3376" tid="0" op="" dtype="" >7.8845e-04 0.0000e+00 2.5034e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >3.2206e-03 2.1458e-06 3.1662e-03</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.0981e-05 2.0981e-05 2.0981e-05</hent>
<hent key="03800100000000000000070000000006" call="MPI_Irecv" bytes="1792" orank="6" region="0" commid="0" count="42" tid="0" op="" dtype="" >8.1062e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000D" call="MPI_Irecv" bytes="1792" orank="13" region="0" commid="0" count="35" tid="0" op="" dtype="" >1.0967e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000F" call="MPI_Irecv" bytes="1792" orank="15" region="0" commid="0" count="36" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000016" call="MPI_Irecv" bytes="1792" orank="22" region="0" commid="0" count="29" tid="0" op="" dtype="" >9.7752e-06 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000007000000004E" call="MPI_Irecv" bytes="1792" orank="78" region="0" commid="0" count="173" tid="0" op="" dtype="" >5.5075e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002CE" call="MPI_Irecv" bytes="1792" orank="718" region="0" commid="0" count="226" tid="0" op="" dtype="" >9.0837e-05 0.0000e+00 1.0014e-05</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >7.9155e-05 0.0000e+00 4.6968e-05</hent>
<hent key="02400100000000000000040000000006" call="MPI_Isend" bytes="1024" orank="6" region="0" commid="0" count="3344" tid="0" op="" dtype="" >2.4121e-03 0.0000e+00 4.6968e-05</hent>
<hent key="0240010000000000000004000000000D" call="MPI_Isend" bytes="1024" orank="13" region="0" commid="0" count="1008" tid="0" op="" dtype="" >6.1321e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000004000000000F" call="MPI_Isend" bytes="1024" orank="15" region="0" commid="0" count="1014" tid="0" op="" dtype="" >7.8797e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000040000000016" call="MPI_Isend" bytes="1024" orank="22" region="0" commid="0" count="3390" tid="0" op="" dtype="" >2.1126e-03 0.0000e+00 1.1921e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >3.0994e-06 9.5367e-07 2.1458e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5468e+00 0.0000e+00 2.2687e+00</hent>
<hent key="038001000000000000000A0000000006" call="MPI_Irecv" bytes="2560" orank="6" region="0" commid="0" count="7" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000000D" call="MPI_Irecv" bytes="2560" orank="13" region="0" commid="0" count="6" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000000F" call="MPI_Irecv" bytes="2560" orank="15" region="0" commid="0" count="11" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000016" call="MPI_Irecv" bytes="2560" orank="22" region="0" commid="0" count="14" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000004E" call="MPI_Irecv" bytes="2560" orank="78" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.5020e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002CE" call="MPI_Irecv" bytes="2560" orank="718" region="0" commid="0" count="53" tid="0" op="" dtype="" >1.9789e-05 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000070000000006" call="MPI_Isend" bytes="1792" orank="6" region="0" commid="0" count="50" tid="0" op="" dtype="" >1.4520e-04 9.5367e-07 1.4067e-05</hent>
<hent key="0240010000000000000007000000000D" call="MPI_Isend" bytes="1792" orank="13" region="0" commid="0" count="26" tid="0" op="" dtype="" >5.3167e-05 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000007000000000F" call="MPI_Isend" bytes="1792" orank="15" region="0" commid="0" count="45" tid="0" op="" dtype="" >1.9574e-04 3.0994e-06 5.9605e-06</hent>
<hent key="02400100000000000000070000000016" call="MPI_Isend" bytes="1792" orank="22" region="0" commid="0" count="28" tid="0" op="" dtype="" >6.5804e-05 9.5367e-07 1.1921e-05</hent>
<hent key="0240010000000000000007000000004E" call="MPI_Isend" bytes="1792" orank="78" region="0" commid="0" count="131" tid="0" op="" dtype="" >7.6938e-04 9.5367e-07 1.3113e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1373e-01 1.1373e-01 1.1373e-01</hent>
<hent key="024001000000000000000700000002CE" call="MPI_Isend" bytes="1792" orank="718" region="0" commid="0" count="193" tid="0" op="" dtype="" >9.3532e-04 9.5367e-07 1.6212e-05</hent>
<hent key="024001000000000000000A0000000006" call="MPI_Isend" bytes="2560" orank="6" region="0" commid="0" count="3" tid="0" op="" dtype="" >5.0068e-06 9.5367e-07 2.1458e-06</hent>
<hent key="024001000000000000000A000000000D" call="MPI_Isend" bytes="2560" orank="13" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.0014e-05 1.9073e-06 2.1458e-06</hent>
<hent key="024001000000000000000A000000000F" call="MPI_Isend" bytes="2560" orank="15" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.5974e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000016" call="MPI_Isend" bytes="2560" orank="22" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.1206e-05 1.1206e-05 1.1206e-05</hent>
<hent key="024001000000000000000A000000004E" call="MPI_Isend" bytes="2560" orank="78" region="0" commid="0" count="49" tid="0" op="" dtype="" >3.1328e-04 4.7684e-06 1.3828e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.3808e-04 4.1962e-05 1.4901e-04</hent>
<hent key="024001000000000000000A00000002CE" call="MPI_Isend" bytes="2560" orank="718" region="0" commid="0" count="47" tid="0" op="" dtype="" >2.9540e-04 4.0531e-06 1.4067e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.1740e-03 4.1795e-04 7.5603e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.5048e-05 3.5048e-05 3.5048e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >7.8678e-06 1.9073e-06 5.9605e-06</hent>
<hent key="03800100000000000000000400000006" call="MPI_Irecv" bytes="4" orank="6" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.0892e-04 0.0000e+00 1.1206e-05</hent>
<hent key="0380010000000000000000040000000D" call="MPI_Irecv" bytes="4" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0774e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0380010000000000000000040000000F" call="MPI_Irecv" bytes="4" orank="15" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.5933e-04 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000000400000016" call="MPI_Irecv" bytes="4" orank="22" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.4278e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0380010000000000000000040000004E" call="MPI_Irecv" bytes="4" orank="78" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.8167e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.2197e-03 2.9588e-04 7.8583e-04</hent>
<hent key="038001000000000000001C000000000D" call="MPI_Irecv" bytes="7168" orank="13" region="0" commid="0" count="8818" tid="0" op="" dtype="" >2.9776e-03 0.0000e+00 6.1989e-06</hent>
<hent key="038001000000000000001C000000000F" call="MPI_Irecv" bytes="7168" orank="15" region="0" commid="0" count="9142" tid="0" op="" dtype="" >1.9524e-03 0.0000e+00 1.0014e-05</hent>
<hent key="038001000000000000000004000002CE" call="MPI_Irecv" bytes="4" orank="718" region="0" commid="0" count="3398" tid="0" op="" dtype="" >9.4819e-04 0.0000e+00 2.0027e-05</hent>
<hent key="02400100000000000000000400000006" call="MPI_Isend" bytes="4" orank="6" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.5125e-03 0.0000e+00 2.0981e-05</hent>
<hent key="0240010000000000000000040000000D" call="MPI_Isend" bytes="4" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6792e-03 0.0000e+00 4.6015e-05</hent>
<hent key="0240010000000000000000040000000F" call="MPI_Isend" bytes="4" orank="15" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.5995e-03 0.0000e+00 8.8215e-06</hent>
<hent key="02400100000000000000000400000016" call="MPI_Isend" bytes="4" orank="22" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.9093e-03 0.0000e+00 2.1935e-05</hent>
<hent key="0240010000000000000000040000004E" call="MPI_Isend" bytes="4" orank="78" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.7710e-02 4.0531e-06 8.5115e-05</hent>
<hent key="024001000000000000001C000000000D" call="MPI_Isend" bytes="7168" orank="13" region="0" commid="0" count="8969" tid="0" op="" dtype="" >2.0628e-02 9.5367e-07 2.1935e-05</hent>
<hent key="024001000000000000001C000000000F" call="MPI_Isend" bytes="7168" orank="15" region="0" commid="0" count="8940" tid="0" op="" dtype="" >2.7624e-02 9.5367e-07 1.5020e-05</hent>
<hent key="024001000000000000000004000002CE" call="MPI_Isend" bytes="4" orank="718" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.0705e-02 3.0994e-06 1.3113e-04</hent>
<hent key="03800100000000000000050000000006" call="MPI_Irecv" bytes="1280" orank="6" region="0" commid="0" count="184" tid="0" op="" dtype="" >3.2902e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000D" call="MPI_Irecv" bytes="1280" orank="13" region="0" commid="0" count="204" tid="0" op="" dtype="" >6.9857e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000F" call="MPI_Irecv" bytes="1280" orank="15" region="0" commid="0" count="213" tid="0" op="" dtype="" >3.3617e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000016" call="MPI_Irecv" bytes="1280" orank="22" region="0" commid="0" count="209" tid="0" op="" dtype="" >4.4823e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000004E" call="MPI_Irecv" bytes="1280" orank="78" region="0" commid="0" count="264" tid="0" op="" dtype="" >1.1063e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0419e+01 9.0599e-06 1.5559e-01</hent>
<hent key="03800100000000000000280000000006" call="MPI_Irecv" bytes="10240" orank="6" region="0" commid="0" count="8" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000280000000016" call="MPI_Irecv" bytes="10240" orank="22" region="0" commid="0" count="88" tid="0" op="" dtype="" >1.6212e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002CE" call="MPI_Irecv" bytes="1280" orank="718" region="0" commid="0" count="290" tid="0" op="" dtype="" >8.1778e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000080000000006" call="MPI_Irecv" bytes="2048" orank="6" region="0" commid="0" count="9" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000000D" call="MPI_Irecv" bytes="2048" orank="13" region="0" commid="0" count="11" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000F" call="MPI_Irecv" bytes="2048" orank="15" region="0" commid="0" count="15" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000016" call="MPI_Irecv" bytes="2048" orank="22" region="0" commid="0" count="12" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000004E" call="MPI_Irecv" bytes="2048" orank="78" region="0" commid="0" count="3420" tid="0" op="" dtype="" >6.7258e-04 0.0000e+00 1.5974e-05</hent>
<hent key="038001000000000000000800000002CE" call="MPI_Irecv" bytes="2048" orank="718" region="0" commid="0" count="3383" tid="0" op="" dtype="" >9.8729e-04 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000050000000006" call="MPI_Isend" bytes="1280" orank="6" region="0" commid="0" count="262" tid="0" op="" dtype="" >4.0722e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000005000000000D" call="MPI_Isend" bytes="1280" orank="13" region="0" commid="0" count="205" tid="0" op="" dtype="" >3.9625e-04 9.5367e-07 5.9605e-06</hent>
<hent key="0240010000000000000005000000000F" call="MPI_Isend" bytes="1280" orank="15" region="0" commid="0" count="188" tid="0" op="" dtype="" >7.4077e-04 2.8610e-06 1.0967e-05</hent>
<hent key="02400100000000000000050000000016" call="MPI_Isend" bytes="1280" orank="22" region="0" commid="0" count="207" tid="0" op="" dtype="" >3.4189e-04 0.0000e+00 1.1921e-05</hent>
<hent key="0240010000000000000005000000004E" call="MPI_Isend" bytes="1280" orank="78" region="0" commid="0" count="288" tid="0" op="" dtype="" >1.6217e-03 3.8147e-06 1.4782e-05</hent>
<hent key="02400100000000000000280000000006" call="MPI_Isend" bytes="10240" orank="6" region="0" commid="0" count="198" tid="0" op="" dtype="" >9.1553e-05 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000280000000016" call="MPI_Isend" bytes="10240" orank="22" region="0" commid="0" count="31" tid="0" op="" dtype="" >4.0531e-05 0.0000e+00 1.5020e-05</hent>
<hent key="024001000000000000000500000002CE" call="MPI_Isend" bytes="1280" orank="718" region="0" commid="0" count="285" tid="0" op="" dtype="" >1.4024e-03 2.8610e-06 1.5020e-05</hent>
</hash>
<internal rank="14" log_i="1723712895.689867" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="15" mpi_size="768" stamp_init="1723712829.548190" stamp_final="1723712895.693953" username="apac4" allocationname="unknown" flags="0" pid="684283" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61458e+01" utime="5.13269e+01" stime="7.32468e+00" mtime="3.05103e+01" gflop="0.00000e+00" gbyte="3.76228e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.05103e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000045152d56451545150a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60146e+01" utime="5.12922e+01" stime="7.31742e+00" mtime="3.05103e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.05103e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4796e+08" > 3.5983e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4912e+08" > 3.0663e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9228e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5723e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.7909e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5871e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.0838e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0432e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1355e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8908e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="199" >
<hent key="02400100000000000000080000000007" call="MPI_Isend" bytes="2048" orank="7" region="0" commid="0" count="16" tid="0" op="" dtype="" >4.3631e-05 9.5367e-07 1.5020e-05</hent>
<hent key="02400100000000000000080000000008" call="MPI_Isend" bytes="2048" orank="8" region="0" commid="0" count="20" tid="0" op="" dtype="" >8.1062e-05 3.0994e-06 5.0068e-06</hent>
<hent key="0240010000000000000008000000000E" call="MPI_Isend" bytes="2048" orank="14" region="0" commid="0" count="15" tid="0" op="" dtype="" >3.6001e-05 1.9073e-06 6.1989e-06</hent>
<hent key="02400100000000000000080000000017" call="MPI_Isend" bytes="2048" orank="23" region="0" commid="0" count="15" tid="0" op="" dtype="" >5.5552e-05 1.1921e-06 1.5974e-05</hent>
<hent key="0240010000000000000008000000004F" call="MPI_Isend" bytes="2048" orank="79" region="0" commid="0" count="3392" tid="0" op="" dtype="" >1.1530e-02 9.5367e-07 2.2888e-05</hent>
<hent key="024001000000000000000800000002CF" call="MPI_Isend" bytes="2048" orank="719" region="0" commid="0" count="3348" tid="0" op="" dtype="" >1.0680e-02 9.5367e-07 1.0014e-05</hent>
<hent key="038001000000000000000E000000004F" call="MPI_Irecv" bytes="3584" orank="79" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="03800100000000000000028000000007" call="MPI_Irecv" bytes="640" orank="7" region="0" commid="0" count="396" tid="0" op="" dtype="" >9.7752e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000008" call="MPI_Irecv" bytes="640" orank="8" region="0" commid="0" count="425" tid="0" op="" dtype="" >9.2983e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000000E" call="MPI_Irecv" bytes="640" orank="14" region="0" commid="0" count="386" tid="0" op="" dtype="" >1.4567e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000028000000017" call="MPI_Irecv" bytes="640" orank="23" region="0" commid="0" count="410" tid="0" op="" dtype="" >8.6308e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000004F" call="MPI_Irecv" bytes="640" orank="79" region="0" commid="0" count="271" tid="0" op="" dtype="" >6.2466e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000014000000004F" call="MPI_Irecv" bytes="5120" orank="79" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002CF" call="MPI_Irecv" bytes="640" orank="719" region="0" commid="0" count="269" tid="0" op="" dtype="" >1.1182e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000007" call="MPI_Isend" bytes="640" orank="7" region="0" commid="0" count="396" tid="0" op="" dtype="" >4.4918e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000028000000008" call="MPI_Isend" bytes="640" orank="8" region="0" commid="0" count="411" tid="0" op="" dtype="" >1.2276e-03 1.9073e-06 7.8678e-06</hent>
<hent key="0240010000000000000002800000000E" call="MPI_Isend" bytes="640" orank="14" region="0" commid="0" count="421" tid="0" op="" dtype="" >5.8389e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000028000000017" call="MPI_Isend" bytes="640" orank="23" region="0" commid="0" count="386" tid="0" op="" dtype="" >4.1938e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000002800000004F" call="MPI_Isend" bytes="640" orank="79" region="0" commid="0" count="264" tid="0" op="" dtype="" >1.2183e-03 3.8147e-06 1.0967e-05</hent>
<hent key="024001000000000000000280000002CF" call="MPI_Isend" bytes="640" orank="719" region="0" commid="0" count="279" tid="0" op="" dtype="" >1.1556e-03 2.8610e-06 9.7752e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.3140e-05 3.3140e-05 3.3140e-05</hent>
<hent key="03800100000000000000014000000007" call="MPI_Irecv" bytes="320" orank="7" region="0" commid="0" count="360" tid="0" op="" dtype="" >6.2943e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000008" call="MPI_Irecv" bytes="320" orank="8" region="0" commid="0" count="362" tid="0" op="" dtype="" >7.7724e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000000E" call="MPI_Irecv" bytes="320" orank="14" region="0" commid="0" count="341" tid="0" op="" dtype="" >1.4567e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000017" call="MPI_Irecv" bytes="320" orank="23" region="0" commid="0" count="349" tid="0" op="" dtype="" >8.5831e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000004F" call="MPI_Irecv" bytes="320" orank="79" region="0" commid="0" count="193" tid="0" op="" dtype="" >4.1246e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002CF" call="MPI_Irecv" bytes="320" orank="719" region="0" commid="0" count="168" tid="0" op="" dtype="" >6.2227e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.9228e+00 0.0000e+00 1.8667e-01</hent>
<hent key="0380010000000000000040000000004F" call="MPI_Irecv" bytes="16384" orank="79" region="0" commid="0" count="12670" tid="0" op="" dtype="" >7.2970e-03 0.0000e+00 8.1062e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6442e-02 0.0000e+00 1.6426e-02</hent>
<hent key="038001000000000000004000000002CF" call="MPI_Irecv" bytes="16384" orank="719" region="0" commid="0" count="12187" tid="0" op="" dtype="" >3.0777e-03 0.0000e+00 7.1526e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="02400100000000000000014000000007" call="MPI_Isend" bytes="320" orank="7" region="0" commid="0" count="357" tid="0" op="" dtype="" >3.3164e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000014000000008" call="MPI_Isend" bytes="320" orank="8" region="0" commid="0" count="372" tid="0" op="" dtype="" >9.7299e-04 9.5367e-07 4.0531e-06</hent>
<hent key="0240010000000000000001400000000E" call="MPI_Isend" bytes="320" orank="14" region="0" commid="0" count="342" tid="0" op="" dtype="" >3.5477e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000014000000017" call="MPI_Isend" bytes="320" orank="23" region="0" commid="0" count="346" tid="0" op="" dtype="" >2.9683e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000001400000004F" call="MPI_Isend" bytes="320" orank="79" region="0" commid="0" count="187" tid="0" op="" dtype="" >8.1706e-04 3.0994e-06 1.0014e-05</hent>
<hent key="03800100000000000000200000000007" call="MPI_Irecv" bytes="8192" orank="7" region="0" commid="0" count="12373" tid="0" op="" dtype="" >2.5322e-03 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000200000000008" call="MPI_Irecv" bytes="8192" orank="8" region="0" commid="0" count="3714" tid="0" op="" dtype="" >5.1999e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000020000000000E" call="MPI_Irecv" bytes="8192" orank="14" region="0" commid="0" count="3759" tid="0" op="" dtype="" >1.3216e-03 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000200000000017" call="MPI_Irecv" bytes="8192" orank="23" region="0" commid="0" count="12674" tid="0" op="" dtype="" >1.7767e-03 0.0000e+00 2.8610e-06</hent>
<hent key="024001000000000000000140000002CF" call="MPI_Isend" bytes="320" orank="719" region="0" commid="0" count="168" tid="0" op="" dtype="" >6.7663e-04 2.8610e-06 6.9141e-06</hent>
<hent key="03800100000000000000000000000007" call="MPI_Irecv" bytes="0" orank="7" region="0" commid="0" count="262" tid="0" op="" dtype="" >6.0081e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000008" call="MPI_Irecv" bytes="0" orank="8" region="0" commid="0" count="250" tid="0" op="" dtype="" >5.2214e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000000E" call="MPI_Irecv" bytes="0" orank="14" region="0" commid="0" count="246" tid="0" op="" dtype="" >7.4625e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000000000000017" call="MPI_Irecv" bytes="0" orank="23" region="0" commid="0" count="252" tid="0" op="" dtype="" >6.1274e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000004F" call="MPI_Irecv" bytes="0" orank="79" region="0" commid="0" count="145" tid="0" op="" dtype="" >3.1471e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000040000000004F" call="MPI_Isend" bytes="16384" orank="79" region="0" commid="0" count="12402" tid="0" op="" dtype="" >1.0461e-01 3.8147e-06 3.4094e-05</hent>
<hent key="038001000000000000000000000002CF" call="MPI_Irecv" bytes="0" orank="719" region="0" commid="0" count="140" tid="0" op="" dtype="" >3.3140e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002CF" call="MPI_Isend" bytes="16384" orank="719" region="0" commid="0" count="12247" tid="0" op="" dtype="" >7.5905e-02 2.8610e-06 2.5034e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.2987e-04 0.0000e+00 2.7800e-04</hent>
<hent key="02400100000000000000200000000007" call="MPI_Isend" bytes="8192" orank="7" region="0" commid="0" count="12321" tid="0" op="" dtype="" >3.7675e-03 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000200000000008" call="MPI_Isend" bytes="8192" orank="8" region="0" commid="0" count="3126" tid="0" op="" dtype="" >4.7805e-03 0.0000e+00 1.4067e-05</hent>
<hent key="0240010000000000000020000000000E" call="MPI_Isend" bytes="8192" orank="14" region="0" commid="0" count="3557" tid="0" op="" dtype="" >3.9091e-03 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000200000000017" call="MPI_Isend" bytes="8192" orank="23" region="0" commid="0" count="12667" tid="0" op="" dtype="" >8.3852e-03 0.0000e+00 4.4823e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.9884e-05 8.9884e-05 8.9884e-05</hent>
<hent key="02400100000000000000000000000007" call="MPI_Isend" bytes="0" orank="7" region="0" commid="0" count="268" tid="0" op="" dtype="" >1.8382e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000008" call="MPI_Isend" bytes="0" orank="8" region="0" commid="0" count="253" tid="0" op="" dtype="" >5.3549e-04 0.0000e+00 8.7976e-05</hent>
<hent key="0240010000000000000000000000000E" call="MPI_Isend" bytes="0" orank="14" region="0" commid="0" count="228" tid="0" op="" dtype="" >1.4305e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000000000017" call="MPI_Isend" bytes="0" orank="23" region="0" commid="0" count="245" tid="0" op="" dtype="" >1.5879e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000000000000004F" call="MPI_Isend" bytes="0" orank="79" region="0" commid="0" count="150" tid="0" op="" dtype="" >5.7220e-04 9.5367e-07 8.8215e-06</hent>
<hent key="024001000000000000000000000002CF" call="MPI_Isend" bytes="0" orank="719" region="0" commid="0" count="149" tid="0" op="" dtype="" >5.3906e-04 1.9073e-06 5.0068e-06</hent>
<hent key="03800100000000000000060000000007" call="MPI_Irecv" bytes="1536" orank="7" region="0" commid="0" count="107" tid="0" op="" dtype="" >2.1696e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000008" call="MPI_Irecv" bytes="1536" orank="8" region="0" commid="0" count="94" tid="0" op="" dtype="" >3.1471e-05 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000006000000000E" call="MPI_Irecv" bytes="1536" orank="14" region="0" commid="0" count="105" tid="0" op="" dtype="" >4.0054e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000017" call="MPI_Irecv" bytes="1536" orank="23" region="0" commid="0" count="95" tid="0" op="" dtype="" >1.9789e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000004F" call="MPI_Irecv" bytes="1536" orank="79" region="0" commid="0" count="203" tid="0" op="" dtype="" >4.1723e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002CF" call="MPI_Irecv" bytes="1536" orank="719" region="0" commid="0" count="240" tid="0" op="" dtype="" >1.0180e-04 0.0000e+00 6.9141e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000007" call="MPI_Isend" bytes="1536" orank="7" region="0" commid="0" count="99" tid="0" op="" dtype="" >1.8311e-04 9.5367e-07 1.3113e-05</hent>
<hent key="02400100000000000000060000000008" call="MPI_Isend" bytes="1536" orank="8" region="0" commid="0" count="82" tid="0" op="" dtype="" >2.9922e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000006000000000E" call="MPI_Isend" bytes="1536" orank="14" region="0" commid="0" count="90" tid="0" op="" dtype="" >1.7500e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000017" call="MPI_Isend" bytes="1536" orank="23" region="0" commid="0" count="93" tid="0" op="" dtype="" >1.7095e-04 9.5367e-07 1.4067e-05</hent>
<hent key="0240010000000000000006000000004F" call="MPI_Isend" bytes="1536" orank="79" region="0" commid="0" count="229" tid="0" op="" dtype="" >1.1487e-03 3.8147e-06 6.1989e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002CF" call="MPI_Isend" bytes="1536" orank="719" region="0" commid="0" count="245" tid="0" op="" dtype="" >1.0920e-03 3.8147e-06 1.0014e-05</hent>
<hent key="038001000000000000000C000000000E" call="MPI_Irecv" bytes="3072" orank="14" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C0000000017" call="MPI_Irecv" bytes="3072" orank="23" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C000000004F" call="MPI_Irecv" bytes="3072" orank="79" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002CF" call="MPI_Irecv" bytes="3072" orank="719" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C000000000E" call="MPI_Isend" bytes="3072" orank="14" region="0" commid="0" count="2" tid="0" op="" dtype="" >6.1989e-06 3.0994e-06 3.0994e-06</hent>
<hent key="024001000000000000000C000000004F" call="MPI_Isend" bytes="3072" orank="79" region="0" commid="0" count="9" tid="0" op="" dtype="" >5.0545e-05 5.0068e-06 6.1989e-06</hent>
<hent key="024001000000000000000C00000002CF" call="MPI_Isend" bytes="3072" orank="719" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.4080e-05 4.0531e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.2282e-04 3.2282e-04 3.2282e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >7.4410e-04 2.4199e-04 2.5201e-04</hent>
<hent key="03800100000000000000038000000007" call="MPI_Irecv" bytes="896" orank="7" region="0" commid="0" count="334" tid="0" op="" dtype="" >7.5340e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000008" call="MPI_Irecv" bytes="896" orank="8" region="0" commid="0" count="2695" tid="0" op="" dtype="" >4.8423e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000000E" call="MPI_Irecv" bytes="896" orank="14" region="0" commid="0" count="2755" tid="0" op="" dtype="" >4.1437e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000017" call="MPI_Irecv" bytes="896" orank="23" region="0" commid="0" count="310" tid="0" op="" dtype="" >5.3167e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000003800000004F" call="MPI_Irecv" bytes="896" orank="79" region="0" commid="0" count="319" tid="0" op="" dtype="" >6.1512e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000380000002CF" call="MPI_Irecv" bytes="896" orank="719" region="0" commid="0" count="303" tid="0" op="" dtype="" >1.2589e-04 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >5.3883e-05 5.3883e-05 5.3883e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.1921e-06 1.1921e-06 1.1921e-06</hent>
<hent key="0380010000000000000038000000004F" call="MPI_Irecv" bytes="14336" orank="79" region="0" commid="0" count="29" tid="0" op="" dtype="" >2.0504e-05 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000003800000002CF" call="MPI_Irecv" bytes="14336" orank="719" region="0" commid="0" count="512" tid="0" op="" dtype="" >1.2064e-04 0.0000e+00 3.0994e-06</hent>
<hent key="02400100000000000000038000000007" call="MPI_Isend" bytes="896" orank="7" region="0" commid="0" count="314" tid="0" op="" dtype="" >4.3964e-04 9.5367e-07 7.1526e-06</hent>
<hent key="02400100000000000000038000000008" call="MPI_Isend" bytes="896" orank="8" region="0" commid="0" count="2896" tid="0" op="" dtype="" >2.8450e-03 0.0000e+00 1.7166e-05</hent>
<hent key="0240010000000000000003800000000E" call="MPI_Isend" bytes="896" orank="14" region="0" commid="0" count="2773" tid="0" op="" dtype="" >1.9770e-03 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000038000000017" call="MPI_Isend" bytes="896" orank="23" region="0" commid="0" count="360" tid="0" op="" dtype="" >4.6778e-04 9.5367e-07 6.1989e-06</hent>
<hent key="0240010000000000000003800000004F" call="MPI_Isend" bytes="896" orank="79" region="0" commid="0" count="334" tid="0" op="" dtype="" >1.5843e-03 3.8147e-06 1.1206e-05</hent>
<hent key="024001000000000000000380000002CF" call="MPI_Isend" bytes="896" orank="719" region="0" commid="0" count="311" tid="0" op="" dtype="" >1.3442e-03 2.8610e-06 5.6028e-05</hent>
<hent key="0240010000000000000038000000004F" call="MPI_Isend" bytes="14336" orank="79" region="0" commid="0" count="297" tid="0" op="" dtype="" >2.4869e-03 5.9605e-06 1.4067e-05</hent>
<hent key="024001000000000000003800000002CF" call="MPI_Isend" bytes="14336" orank="719" region="0" commid="0" count="452" tid="0" op="" dtype="" >2.6979e-03 3.8147e-06 1.1921e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.9760e+00 1.0014e-05 2.0320e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.2082e-03 3.2082e-03 3.2082e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0432e-02 1.0432e-02 1.0432e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.6537e-02 2.6537e-02 2.6537e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.9369e-01 1.1699e-03 1.7606e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.1114e-04 3.1114e-04 3.1114e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5871e+00 3.7313e-04 2.5074e-01</hent>
<hent key="03800100000000000000040000000007" call="MPI_Irecv" bytes="1024" orank="7" region="0" commid="0" count="3296" tid="0" op="" dtype="" >5.3144e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000008" call="MPI_Irecv" bytes="1024" orank="8" region="0" commid="0" count="1008" tid="0" op="" dtype="" >1.6737e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000000E" call="MPI_Irecv" bytes="1024" orank="14" region="0" commid="0" count="1014" tid="0" op="" dtype="" >1.0967e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000017" call="MPI_Irecv" bytes="1024" orank="23" region="0" commid="0" count="3392" tid="0" op="" dtype="" >5.7125e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.4782e-05 0.0000e+00 6.9141e-06</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.9101e-05 3.9101e-05 3.9101e-05</hent>
<hent key="03800100000000000000070000000007" call="MPI_Irecv" bytes="1792" orank="7" region="0" commid="0" count="32" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000008" call="MPI_Irecv" bytes="1792" orank="8" region="0" commid="0" count="42" tid="0" op="" dtype="" >1.4782e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000000E" call="MPI_Irecv" bytes="1792" orank="14" region="0" commid="0" count="45" tid="0" op="" dtype="" >1.8120e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000017" call="MPI_Irecv" bytes="1792" orank="23" region="0" commid="0" count="49" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000004F" call="MPI_Irecv" bytes="1792" orank="79" region="0" commid="0" count="157" tid="0" op="" dtype="" >2.9087e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002CF" call="MPI_Irecv" bytes="1792" orank="719" region="0" commid="0" count="290" tid="0" op="" dtype="" >1.0395e-04 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >4.3869e-05 0.0000e+00 2.5034e-05</hent>
<hent key="02400100000000000000040000000007" call="MPI_Isend" bytes="1024" orank="7" region="0" commid="0" count="3292" tid="0" op="" dtype="" >1.5678e-03 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000040000000008" call="MPI_Isend" bytes="1024" orank="8" region="0" commid="0" count="846" tid="0" op="" dtype="" >5.7983e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0240010000000000000004000000000E" call="MPI_Isend" bytes="1024" orank="14" region="0" commid="0" count="966" tid="0" op="" dtype="" >6.1321e-04 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000040000000017" call="MPI_Isend" bytes="1024" orank="23" region="0" commid="0" count="3390" tid="0" op="" dtype="" >1.8368e-03 0.0000e+00 1.2159e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >1.2875e-05 1.9073e-06 1.0967e-05</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5552e+00 0.0000e+00 2.2688e+00</hent>
<hent key="038001000000000000000A0000000007" call="MPI_Irecv" bytes="2560" orank="7" region="0" commid="0" count="4" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000008" call="MPI_Irecv" bytes="2560" orank="8" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000000E" call="MPI_Irecv" bytes="2560" orank="14" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000017" call="MPI_Irecv" bytes="2560" orank="23" region="0" commid="0" count="3" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000004F" call="MPI_Irecv" bytes="2560" orank="79" region="0" commid="0" count="50" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002CF" call="MPI_Irecv" bytes="2560" orank="719" region="0" commid="0" count="50" tid="0" op="" dtype="" >2.1219e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000007" call="MPI_Isend" bytes="1792" orank="7" region="0" commid="0" count="41" tid="0" op="" dtype="" >8.9407e-05 9.5367e-07 1.5020e-05</hent>
<hent key="02400100000000000000070000000008" call="MPI_Isend" bytes="1792" orank="8" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.5235e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000007000000000E" call="MPI_Isend" bytes="1792" orank="14" region="0" commid="0" count="36" tid="0" op="" dtype="" >7.5340e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000017" call="MPI_Isend" bytes="1792" orank="23" region="0" commid="0" count="39" tid="0" op="" dtype="" >9.3937e-05 9.5367e-07 1.5020e-05</hent>
<hent key="0240010000000000000007000000004F" call="MPI_Isend" bytes="1792" orank="79" region="0" commid="0" count="215" tid="0" op="" dtype="" >9.4128e-04 9.5367e-07 9.0599e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1355e-01 1.1355e-01 1.1355e-01</hent>
<hent key="024001000000000000000700000002CF" call="MPI_Isend" bytes="1792" orank="719" region="0" commid="0" count="248" tid="0" op="" dtype="" >9.2697e-04 9.5367e-07 5.0068e-06</hent>
<hent key="024001000000000000000A0000000007" call="MPI_Isend" bytes="2560" orank="7" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.4080e-05 1.9073e-06 1.5974e-05</hent>
<hent key="024001000000000000000A0000000008" call="MPI_Isend" bytes="2560" orank="8" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.0981e-05 3.0994e-06 4.0531e-06</hent>
<hent key="024001000000000000000A000000000E" call="MPI_Isend" bytes="2560" orank="14" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.4557e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000017" call="MPI_Isend" bytes="2560" orank="23" region="0" commid="0" count="4" tid="0" op="" dtype="" >6.9141e-06 9.5367e-07 2.1458e-06</hent>
<hent key="024001000000000000000A000000004F" call="MPI_Isend" bytes="2560" orank="79" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.2054e-04 5.0068e-06 6.1989e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.6597e-04 4.5061e-05 1.6189e-04</hent>
<hent key="024001000000000000000A00000002CF" call="MPI_Isend" bytes="2560" orank="719" region="0" commid="0" count="49" tid="0" op="" dtype="" >2.3556e-04 3.8147e-06 6.1989e-06</hent>
<hent key="0240010000000000000010000000004F" call="MPI_Isend" bytes="4096" orank="79" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.2591e-03 4.4918e-04 8.0991e-04</hent>
<hent key="024001000000000000001000000002CF" call="MPI_Isend" bytes="4096" orank="719" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.6001e-05 3.6001e-05 3.6001e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >6.6757e-06 1.9073e-06 4.7684e-06</hent>
<hent key="03800100000000000000000400000007" call="MPI_Irecv" bytes="4" orank="7" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.2963e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000008" call="MPI_Irecv" bytes="4" orank="8" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.5347e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000040000000E" call="MPI_Irecv" bytes="4" orank="14" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.0906e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000000400000017" call="MPI_Irecv" bytes="4" orank="23" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.5657e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000004F" call="MPI_Irecv" bytes="4" orank="79" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.2084e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.3909e-03 3.1614e-04 8.5282e-04</hent>
<hent key="038001000000000000001C0000000008" call="MPI_Irecv" bytes="7168" orank="8" region="0" commid="0" count="8985" tid="0" op="" dtype="" >1.3463e-03 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000001C000000000E" call="MPI_Irecv" bytes="7168" orank="14" region="0" commid="0" count="8940" tid="0" op="" dtype="" >3.3176e-03 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000004000002CF" call="MPI_Irecv" bytes="4" orank="719" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.6505e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000000400000007" call="MPI_Isend" bytes="4" orank="7" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2689e-03 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000400000008" call="MPI_Isend" bytes="4" orank="8" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.5210e-03 0.0000e+00 7.1049e-05</hent>
<hent key="0240010000000000000000040000000E" call="MPI_Isend" bytes="4" orank="14" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4601e-03 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000400000017" call="MPI_Isend" bytes="4" orank="23" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7927e-03 0.0000e+00 2.6941e-05</hent>
<hent key="0240010000000000000000040000004F" call="MPI_Isend" bytes="4" orank="79" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3764e-02 4.7684e-06 4.5061e-05</hent>
<hent key="024001000000000000001C0000000008" call="MPI_Isend" bytes="7168" orank="8" region="0" commid="0" count="9573" tid="0" op="" dtype="" >2.7148e-02 9.5367e-07 2.0981e-05</hent>
<hent key="024001000000000000001C000000000E" call="MPI_Isend" bytes="7168" orank="14" region="0" commid="0" count="9142" tid="0" op="" dtype="" >2.1653e-02 9.5367e-07 1.1921e-05</hent>
<hent key="024001000000000000000004000002CF" call="MPI_Isend" bytes="4" orank="719" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6937e-02 2.8610e-06 1.0419e-04</hent>
<hent key="03800100000000000000050000000007" call="MPI_Irecv" bytes="1280" orank="7" region="0" commid="0" count="291" tid="0" op="" dtype="" >5.6267e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000008" call="MPI_Irecv" bytes="1280" orank="8" region="0" commid="0" count="202" tid="0" op="" dtype="" >4.0293e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000000E" call="MPI_Irecv" bytes="1280" orank="14" region="0" commid="0" count="188" tid="0" op="" dtype="" >8.1778e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000017" call="MPI_Irecv" bytes="1280" orank="23" region="0" commid="0" count="224" tid="0" op="" dtype="" >3.6001e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000004F" call="MPI_Irecv" bytes="1280" orank="79" region="0" commid="0" count="295" tid="0" op="" dtype="" >5.1737e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0709e+01 6.9141e-06 1.5582e-01</hent>
<hent key="03800100000000000000280000000007" call="MPI_Irecv" bytes="10240" orank="7" region="0" commid="0" count="326" tid="0" op="" dtype="" >4.9829e-05 0.0000e+00 2.8610e-06</hent>
<hent key="03800100000000000000280000000017" call="MPI_Irecv" bytes="10240" orank="23" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000500000002CF" call="MPI_Irecv" bytes="1280" orank="719" region="0" commid="0" count="321" tid="0" op="" dtype="" >1.3304e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000007" call="MPI_Irecv" bytes="2048" orank="7" region="0" commid="0" count="15" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000008" call="MPI_Irecv" bytes="2048" orank="8" region="0" commid="0" count="13" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000E" call="MPI_Irecv" bytes="2048" orank="14" region="0" commid="0" count="13" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000017" call="MPI_Irecv" bytes="2048" orank="23" region="0" commid="0" count="12" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000004F" call="MPI_Irecv" bytes="2048" orank="79" region="0" commid="0" count="3454" tid="0" op="" dtype="" >4.9949e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002CF" call="MPI_Irecv" bytes="2048" orank="719" region="0" commid="0" count="3312" tid="0" op="" dtype="" >6.8903e-04 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000050000000007" call="MPI_Isend" bytes="1280" orank="7" region="0" commid="0" count="309" tid="0" op="" dtype="" >3.5715e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000050000000008" call="MPI_Isend" bytes="1280" orank="8" region="0" commid="0" count="171" tid="0" op="" dtype="" >6.0844e-04 2.8610e-06 7.8678e-06</hent>
<hent key="0240010000000000000005000000000E" call="MPI_Isend" bytes="1280" orank="14" region="0" commid="0" count="213" tid="0" op="" dtype="" >3.9768e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000050000000017" call="MPI_Isend" bytes="1280" orank="23" region="0" commid="0" count="219" tid="0" op="" dtype="" >3.2854e-04 0.0000e+00 1.2875e-05</hent>
<hent key="0240010000000000000005000000004F" call="MPI_Isend" bytes="1280" orank="79" region="0" commid="0" count="276" tid="0" op="" dtype="" >1.3678e-03 3.8147e-06 1.2159e-05</hent>
<hent key="02400100000000000000280000000007" call="MPI_Isend" bytes="10240" orank="7" region="0" commid="0" count="378" tid="0" op="" dtype="" >9.7990e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000280000000017" call="MPI_Isend" bytes="10240" orank="23" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.9073e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000500000002CF" call="MPI_Isend" bytes="1280" orank="719" region="0" commid="0" count="294" tid="0" op="" dtype="" >1.2829e-03 2.8610e-06 5.0068e-06</hent>
</hash>
<internal rank="15" log_i="1723712895.693953" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="16" mpi_size="768" stamp_init="1723712829.548127" stamp_final="1723712895.692261" username="apac4" allocationname="unknown" flags="0" pid="684284" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61441e+01" utime="4.60288e+01" stime="9.59963e+00" mtime="2.94383e+01" gflop="0.00000e+00" gbyte="3.77617e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.94383e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60099e+01" utime="4.59944e+01" stime="9.59164e+00" mtime="2.94383e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.94383e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4921e+08" > 8.2212e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5021e+08" > 6.3944e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9581e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5766e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0016e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.4223e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5863e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.4343e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0441e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1367e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7300e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="196" >
<hent key="02400100000000000000080000000008" call="MPI_Isend" bytes="2048" orank="8" region="0" commid="0" count="21" tid="0" op="" dtype="" >6.4850e-05 9.5367e-07 1.1921e-05</hent>
<hent key="02400100000000000000080000000011" call="MPI_Isend" bytes="2048" orank="17" region="0" commid="0" count="11" tid="0" op="" dtype="" >4.1723e-05 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000080000000017" call="MPI_Isend" bytes="2048" orank="23" region="0" commid="0" count="13" tid="0" op="" dtype="" >3.1948e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000080000000018" call="MPI_Isend" bytes="2048" orank="24" region="0" commid="0" count="13" tid="0" op="" dtype="" >8.4877e-05 5.0068e-06 1.0967e-05</hent>
<hent key="02400100000000000000080000000050" call="MPI_Isend" bytes="2048" orank="80" region="0" commid="0" count="3469" tid="0" op="" dtype="" >2.5602e-02 9.5367e-07 1.5187e-04</hent>
<hent key="024001000000000000000800000002D0" call="MPI_Isend" bytes="2048" orank="720" region="0" commid="0" count="3474" tid="0" op="" dtype="" >2.2453e-02 9.5367e-07 1.6499e-04</hent>
<hent key="038001000000000000000E0000000050" call="MPI_Irecv" bytes="3584" orank="80" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="03800100000000000000028000000008" call="MPI_Irecv" bytes="640" orank="8" region="0" commid="0" count="382" tid="0" op="" dtype="" >1.9169e-04 0.0000e+00 1.6212e-05</hent>
<hent key="03800100000000000000028000000011" call="MPI_Irecv" bytes="640" orank="17" region="0" commid="0" count="382" tid="0" op="" dtype="" >9.1553e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000017" call="MPI_Irecv" bytes="640" orank="23" region="0" commid="0" count="383" tid="0" op="" dtype="" >1.5712e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000028000000018" call="MPI_Irecv" bytes="640" orank="24" region="0" commid="0" count="419" tid="0" op="" dtype="" >1.4520e-04 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000028000000050" call="MPI_Irecv" bytes="640" orank="80" region="0" commid="0" count="272" tid="0" op="" dtype="" >1.0967e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.1062e-06 8.1062e-06 8.1062e-06</hent>
<hent key="038001000000000000000280000002D0" call="MPI_Irecv" bytes="640" orank="720" region="0" commid="0" count="267" tid="0" op="" dtype="" >1.8644e-04 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000028000000008" call="MPI_Isend" bytes="640" orank="8" region="0" commid="0" count="422" tid="0" op="" dtype="" >7.3910e-04 9.5367e-07 1.7166e-05</hent>
<hent key="02400100000000000000028000000011" call="MPI_Isend" bytes="640" orank="17" region="0" commid="0" count="405" tid="0" op="" dtype="" >1.2696e-03 1.9073e-06 1.6928e-05</hent>
<hent key="02400100000000000000028000000017" call="MPI_Isend" bytes="640" orank="23" region="0" commid="0" count="401" tid="0" op="" dtype="" >5.6577e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000028000000018" call="MPI_Isend" bytes="640" orank="24" region="0" commid="0" count="384" tid="0" op="" dtype="" >1.9901e-03 3.8147e-06 1.6928e-05</hent>
<hent key="02400100000000000000028000000050" call="MPI_Isend" bytes="640" orank="80" region="0" commid="0" count="268" tid="0" op="" dtype="" >1.5895e-03 2.8610e-06 2.1219e-05</hent>
<hent key="024001000000000000000280000002D0" call="MPI_Isend" bytes="640" orank="720" region="0" commid="0" count="258" tid="0" op="" dtype="" >1.4877e-03 3.8147e-06 2.5988e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8849e-05 2.8849e-05 2.8849e-05</hent>
<hent key="03800100000000000000014000000008" call="MPI_Irecv" bytes="320" orank="8" region="0" commid="0" count="339" tid="0" op="" dtype="" >1.6928e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000014000000011" call="MPI_Irecv" bytes="320" orank="17" region="0" commid="0" count="384" tid="0" op="" dtype="" >9.2268e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000017" call="MPI_Irecv" bytes="320" orank="23" region="0" commid="0" count="348" tid="0" op="" dtype="" >1.5759e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000018" call="MPI_Irecv" bytes="320" orank="24" region="0" commid="0" count="354" tid="0" op="" dtype="" >1.4806e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000014000000050" call="MPI_Irecv" bytes="320" orank="80" region="0" commid="0" count="172" tid="0" op="" dtype="" >5.9843e-05 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000140000002D0" call="MPI_Irecv" bytes="320" orank="720" region="0" commid="0" count="196" tid="0" op="" dtype="" >1.4710e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >6.4373e-06 9.5367e-07 2.1458e-06</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.9581e+00 0.0000e+00 1.8673e-01</hent>
<hent key="03800100000000000000400000000050" call="MPI_Irecv" bytes="16384" orank="80" region="0" commid="0" count="12671" tid="0" op="" dtype="" >4.7452e-03 0.0000e+00 6.1035e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6475e-02 0.0000e+00 1.6460e-02</hent>
<hent key="038001000000000000004000000002D0" call="MPI_Irecv" bytes="16384" orank="720" region="0" commid="0" count="12655" tid="0" op="" dtype="" >2.2288e-02 0.0000e+00 1.3804e-04</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.3127e-04 0.0000e+00 2.3007e-04</hent>
<hent key="02400100000000000000014000000008" call="MPI_Isend" bytes="320" orank="8" region="0" commid="0" count="352" tid="0" op="" dtype="" >5.0378e-04 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000014000000011" call="MPI_Isend" bytes="320" orank="17" region="0" commid="0" count="321" tid="0" op="" dtype="" >8.9455e-04 1.9073e-06 9.0599e-06</hent>
<hent key="02400100000000000000014000000017" call="MPI_Isend" bytes="320" orank="23" region="0" commid="0" count="368" tid="0" op="" dtype="" >4.1723e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000018" call="MPI_Isend" bytes="320" orank="24" region="0" commid="0" count="386" tid="0" op="" dtype="" >1.8594e-03 3.8147e-06 1.5020e-05</hent>
<hent key="02400100000000000000014000000050" call="MPI_Isend" bytes="320" orank="80" region="0" commid="0" count="176" tid="0" op="" dtype="" >1.0693e-03 2.8610e-06 2.9802e-05</hent>
<hent key="03800100000000000000200000000008" call="MPI_Irecv" bytes="8192" orank="8" region="0" commid="0" count="12624" tid="0" op="" dtype="" >7.0765e-03 0.0000e+00 6.8903e-05</hent>
<hent key="03800100000000000000200000000011" call="MPI_Irecv" bytes="8192" orank="17" region="0" commid="0" count="4074" tid="0" op="" dtype="" >8.1873e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000200000000017" call="MPI_Irecv" bytes="8192" orank="23" region="0" commid="0" count="4033" tid="0" op="" dtype="" >1.7231e-03 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000200000000018" call="MPI_Irecv" bytes="8192" orank="24" region="0" commid="0" count="12684" tid="0" op="" dtype="" >3.9420e-03 0.0000e+00 9.0837e-05</hent>
<hent key="024001000000000000000140000002D0" call="MPI_Isend" bytes="320" orank="720" region="0" commid="0" count="189" tid="0" op="" dtype="" >1.0736e-03 3.0994e-06 1.9073e-05</hent>
<hent key="03800100000000000000000000000008" call="MPI_Irecv" bytes="0" orank="8" region="0" commid="0" count="234" tid="0" op="" dtype="" >1.0800e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000011" call="MPI_Irecv" bytes="0" orank="17" region="0" commid="0" count="234" tid="0" op="" dtype="" >5.9128e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000017" call="MPI_Irecv" bytes="0" orank="23" region="0" commid="0" count="258" tid="0" op="" dtype="" >9.2506e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000000000000018" call="MPI_Irecv" bytes="0" orank="24" region="0" commid="0" count="240" tid="0" op="" dtype="" >8.0109e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000000000050" call="MPI_Irecv" bytes="0" orank="80" region="0" commid="0" count="159" tid="0" op="" dtype="" >7.1049e-05 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000400000000050" call="MPI_Isend" bytes="16384" orank="80" region="0" commid="0" count="12683" tid="0" op="" dtype="" >2.1055e-01 3.8147e-06 2.1601e-04</hent>
<hent key="038001000000000000000000000002D0" call="MPI_Irecv" bytes="0" orank="720" region="0" commid="0" count="154" tid="0" op="" dtype="" >5.3406e-05 0.0000e+00 8.8215e-06</hent>
<hent key="024001000000000000004000000002D0" call="MPI_Isend" bytes="16384" orank="720" region="0" commid="0" count="12682" tid="0" op="" dtype="" >1.4142e-01 3.0994e-06 1.2589e-04</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.3869e-04 0.0000e+00 2.8920e-04</hent>
<hent key="02400100000000000000200000000008" call="MPI_Isend" bytes="8192" orank="8" region="0" commid="0" count="12630" tid="0" op="" dtype="" >1.3638e-02 0.0000e+00 1.4997e-04</hent>
<hent key="02400100000000000000200000000011" call="MPI_Isend" bytes="8192" orank="17" region="0" commid="0" count="3654" tid="0" op="" dtype="" >7.9825e-03 0.0000e+00 7.9155e-05</hent>
<hent key="02400100000000000000200000000017" call="MPI_Isend" bytes="8192" orank="23" region="0" commid="0" count="3421" tid="0" op="" dtype="" >4.2009e-03 0.0000e+00 6.9141e-05</hent>
<hent key="02400100000000000000200000000018" call="MPI_Isend" bytes="8192" orank="24" region="0" commid="0" count="12671" tid="0" op="" dtype="" >1.8851e-01 4.0531e-06 2.3103e-04</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.9802e-05 2.9802e-05 2.9802e-05</hent>
<hent key="02400100000000000000000000000008" call="MPI_Isend" bytes="0" orank="8" region="0" commid="0" count="228" tid="0" op="" dtype="" >2.4557e-04 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000000000000011" call="MPI_Isend" bytes="0" orank="17" region="0" commid="0" count="272" tid="0" op="" dtype="" >5.9915e-04 0.0000e+00 7.2002e-05</hent>
<hent key="02400100000000000000000000000017" call="MPI_Isend" bytes="0" orank="23" region="0" commid="0" count="250" tid="0" op="" dtype="" >1.7285e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000018" call="MPI_Isend" bytes="0" orank="24" region="0" commid="0" count="255" tid="0" op="" dtype="" >1.1156e-03 9.5367e-07 1.5020e-05</hent>
<hent key="02400100000000000000000000000050" call="MPI_Isend" bytes="0" orank="80" region="0" commid="0" count="156" tid="0" op="" dtype="" >8.1038e-04 9.5367e-07 1.8120e-05</hent>
<hent key="024001000000000000000000000002D0" call="MPI_Isend" bytes="0" orank="720" region="0" commid="0" count="145" tid="0" op="" dtype="" >7.3266e-04 2.8610e-06 2.0027e-05</hent>
<hent key="03800100000000000000060000000008" call="MPI_Irecv" bytes="1536" orank="8" region="0" commid="0" count="110" tid="0" op="" dtype="" >5.7697e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000060000000011" call="MPI_Irecv" bytes="1536" orank="17" region="0" commid="0" count="105" tid="0" op="" dtype="" >2.8133e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000017" call="MPI_Irecv" bytes="1536" orank="23" region="0" commid="0" count="109" tid="0" op="" dtype="" >5.3406e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000018" call="MPI_Irecv" bytes="1536" orank="24" region="0" commid="0" count="80" tid="0" op="" dtype="" >3.2663e-05 0.0000e+00 3.8147e-06</hent>
<hent key="03800100000000000000060000000050" call="MPI_Irecv" bytes="1536" orank="80" region="0" commid="0" count="185" tid="0" op="" dtype="" >7.0572e-05 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000600000002D0" call="MPI_Irecv" bytes="1536" orank="720" region="0" commid="0" count="208" tid="0" op="" dtype="" >1.7667e-04 0.0000e+00 1.1206e-05</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 1.9073e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000008" call="MPI_Isend" bytes="1536" orank="8" region="0" commid="0" count="111" tid="0" op="" dtype="" >2.8920e-04 9.5367e-07 1.5974e-05</hent>
<hent key="02400100000000000000060000000011" call="MPI_Isend" bytes="1536" orank="17" region="0" commid="0" count="108" tid="0" op="" dtype="" >4.1580e-04 2.8610e-06 8.1062e-06</hent>
<hent key="02400100000000000000060000000017" call="MPI_Isend" bytes="1536" orank="23" region="0" commid="0" count="100" tid="0" op="" dtype="" >2.0742e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000018" call="MPI_Isend" bytes="1536" orank="24" region="0" commid="0" count="98" tid="0" op="" dtype="" >5.2810e-04 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000060000000050" call="MPI_Isend" bytes="1536" orank="80" region="0" commid="0" count="211" tid="0" op="" dtype="" >1.2619e-03 3.8147e-06 2.5034e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002D0" call="MPI_Isend" bytes="1536" orank="720" region="0" commid="0" count="197" tid="0" op="" dtype="" >1.2691e-03 3.8147e-06 2.3127e-05</hent>
<hent key="038001000000000000000C0000000050" call="MPI_Irecv" bytes="3072" orank="80" region="0" commid="0" count="3" tid="0" op="" dtype="" >2.3842e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002D0" call="MPI_Irecv" bytes="3072" orank="720" region="0" commid="0" count="9" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000018" call="MPI_Isend" bytes="3072" orank="24" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.1921e-05 5.9605e-06 5.9605e-06</hent>
<hent key="024001000000000000000C0000000050" call="MPI_Isend" bytes="3072" orank="80" region="0" commid="0" count="3" tid="0" op="" dtype="" >2.2888e-05 4.7684e-06 1.3113e-05</hent>
<hent key="024001000000000000000C00000002D0" call="MPI_Isend" bytes="3072" orank="720" region="0" commid="0" count="13" tid="0" op="" dtype="" >8.8930e-05 5.0068e-06 1.3113e-05</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.3307e-04 3.3307e-04 3.3307e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >7.8535e-04 2.5511e-04 2.6917e-04</hent>
<hent key="03800100000000000000038000000008" call="MPI_Irecv" bytes="896" orank="8" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.7810e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000038000000011" call="MPI_Irecv" bytes="896" orank="17" region="0" commid="0" count="2648" tid="0" op="" dtype="" >6.0725e-04 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000038000000017" call="MPI_Irecv" bytes="896" orank="23" region="0" commid="0" count="2667" tid="0" op="" dtype="" >6.4087e-04 0.0000e+00 3.0994e-05</hent>
<hent key="03800100000000000000038000000018" call="MPI_Irecv" bytes="896" orank="24" region="0" commid="0" count="352" tid="0" op="" dtype="" >1.0896e-04 0.0000e+00 3.8147e-06</hent>
<hent key="03800100000000000000038000000050" call="MPI_Irecv" bytes="896" orank="80" region="0" commid="0" count="355" tid="0" op="" dtype="" >1.7023e-04 0.0000e+00 1.0967e-05</hent>
<hent key="038001000000000000000380000002D0" call="MPI_Irecv" bytes="896" orank="720" region="0" commid="0" count="304" tid="0" op="" dtype="" >2.0456e-04 0.0000e+00 9.0599e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.4067e-05 1.4067e-05 1.4067e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="03800100000000000000380000000050" call="MPI_Irecv" bytes="14336" orank="80" region="0" commid="0" count="28" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000003800000002D0" call="MPI_Irecv" bytes="14336" orank="720" region="0" commid="0" count="44" tid="0" op="" dtype="" >4.6968e-05 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000038000000008" call="MPI_Isend" bytes="896" orank="8" region="0" commid="0" count="311" tid="0" op="" dtype="" >5.9652e-04 9.5367e-07 1.2159e-05</hent>
<hent key="02400100000000000000038000000011" call="MPI_Isend" bytes="896" orank="17" region="0" commid="0" count="2778" tid="0" op="" dtype="" >3.1765e-03 0.0000e+00 2.4080e-05</hent>
<hent key="02400100000000000000038000000017" call="MPI_Isend" bytes="896" orank="23" region="0" commid="0" count="2802" tid="0" op="" dtype="" >2.2109e-03 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000038000000018" call="MPI_Isend" bytes="896" orank="24" region="0" commid="0" count="304" tid="0" op="" dtype="" >1.6084e-03 3.8147e-06 1.3113e-05</hent>
<hent key="02400100000000000000038000000050" call="MPI_Isend" bytes="896" orank="80" region="0" commid="0" count="339" tid="0" op="" dtype="" >2.0690e-03 3.0994e-06 3.2902e-05</hent>
<hent key="024001000000000000000380000002D0" call="MPI_Isend" bytes="896" orank="720" region="0" commid="0" count="330" tid="0" op="" dtype="" >1.9152e-03 3.0994e-06 1.7881e-05</hent>
<hent key="02400100000000000000380000000050" call="MPI_Isend" bytes="14336" orank="80" region="0" commid="0" count="16" tid="0" op="" dtype="" >3.2067e-04 6.9141e-06 7.7963e-05</hent>
<hent key="024001000000000000003800000002D0" call="MPI_Isend" bytes="14336" orank="720" region="0" commid="0" count="17" tid="0" op="" dtype="" >1.8191e-04 5.0068e-06 2.2173e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >6.7139e+00 8.1062e-06 2.0308e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.2029e-03 3.2029e-03 3.2029e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0441e-02 1.0441e-02 1.0441e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.6228e-02 2.6228e-02 2.6228e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.9262e-01 2.4559e-03 1.7368e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >2.7418e-04 2.7418e-04 2.7418e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 9.5367e-07 1.1921e-06</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5863e+00 4.4990e-04 2.5029e-01</hent>
<hent key="03800100000000000000040000000008" call="MPI_Irecv" bytes="1024" orank="8" region="0" commid="0" count="3378" tid="0" op="" dtype="" >7.1383e-04 0.0000e+00 1.2875e-05</hent>
<hent key="03800100000000000000040000000011" call="MPI_Irecv" bytes="1024" orank="17" region="0" commid="0" count="1096" tid="0" op="" dtype="" >2.8753e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000040000000017" call="MPI_Irecv" bytes="1024" orank="23" region="0" commid="0" count="1074" tid="0" op="" dtype="" >2.0242e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000040000000018" call="MPI_Irecv" bytes="1024" orank="24" region="0" commid="0" count="3394" tid="0" op="" dtype="" >1.1735e-03 0.0000e+00 4.6015e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.0016e+00 9.5367e-07 2.0010e+00</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.0054e-05 4.0054e-05 4.0054e-05</hent>
<hent key="03800100000000000000070000000008" call="MPI_Irecv" bytes="1792" orank="8" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.3351e-05 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000011" call="MPI_Irecv" bytes="1792" orank="17" region="0" commid="0" count="37" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000017" call="MPI_Irecv" bytes="1792" orank="23" region="0" commid="0" count="44" tid="0" op="" dtype="" >1.9312e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000018" call="MPI_Irecv" bytes="1792" orank="24" region="0" commid="0" count="40" tid="0" op="" dtype="" >9.5367e-06 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000070000000050" call="MPI_Irecv" bytes="1792" orank="80" region="0" commid="0" count="133" tid="0" op="" dtype="" >4.6492e-05 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000700000002D0" call="MPI_Irecv" bytes="1792" orank="720" region="0" commid="0" count="159" tid="0" op="" dtype="" >1.2684e-04 0.0000e+00 1.0014e-05</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >5.1022e-05 0.0000e+00 2.5988e-05</hent>
<hent key="02400100000000000000040000000008" call="MPI_Isend" bytes="1024" orank="8" region="0" commid="0" count="3380" tid="0" op="" dtype="" >3.9372e-03 0.0000e+00 1.3280e-04</hent>
<hent key="02400100000000000000040000000011" call="MPI_Isend" bytes="1024" orank="17" region="0" commid="0" count="956" tid="0" op="" dtype="" >8.0681e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000040000000017" call="MPI_Isend" bytes="1024" orank="23" region="0" commid="0" count="932" tid="0" op="" dtype="" >6.6686e-04 0.0000e+00 1.8120e-05</hent>
<hent key="02400100000000000000040000000018" call="MPI_Isend" bytes="1024" orank="24" region="0" commid="0" count="3392" tid="0" op="" dtype="" >1.0571e-02 9.5367e-07 3.6955e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >3.8147e-06 1.9073e-06 1.9073e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >1.5595e+00 0.0000e+00 9.1488e-01</hent>
<hent key="038001000000000000000A0000000008" call="MPI_Irecv" bytes="2560" orank="8" region="0" commid="0" count="10" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000000A0000000011" call="MPI_Irecv" bytes="2560" orank="17" region="0" commid="0" count="7" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000017" call="MPI_Irecv" bytes="2560" orank="23" region="0" commid="0" count="4" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000018" call="MPI_Irecv" bytes="2560" orank="24" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000050" call="MPI_Irecv" bytes="2560" orank="80" region="0" commid="0" count="50" tid="0" op="" dtype="" >2.6703e-05 0.0000e+00 1.0014e-05</hent>
<hent key="038001000000000000000A00000002D0" call="MPI_Irecv" bytes="2560" orank="720" region="0" commid="0" count="58" tid="0" op="" dtype="" >5.4836e-05 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000070000000008" call="MPI_Isend" bytes="1792" orank="8" region="0" commid="0" count="27" tid="0" op="" dtype="" >8.2254e-05 1.9073e-06 1.5020e-05</hent>
<hent key="02400100000000000000070000000011" call="MPI_Isend" bytes="1792" orank="17" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.2279e-04 2.8610e-06 1.3113e-05</hent>
<hent key="02400100000000000000070000000017" call="MPI_Isend" bytes="1792" orank="23" region="0" commid="0" count="41" tid="0" op="" dtype="" >8.3923e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000018" call="MPI_Isend" bytes="1792" orank="24" region="0" commid="0" count="38" tid="0" op="" dtype="" >2.1958e-04 4.7684e-06 1.0014e-05</hent>
<hent key="02400100000000000000070000000050" call="MPI_Isend" bytes="1792" orank="80" region="0" commid="0" count="128" tid="0" op="" dtype="" >7.8773e-04 9.5367e-07 2.4080e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1367e-01 1.1367e-01 1.1367e-01</hent>
<hent key="024001000000000000000700000002D0" call="MPI_Isend" bytes="1792" orank="720" region="0" commid="0" count="137" tid="0" op="" dtype="" >9.4581e-04 9.5367e-07 8.9169e-05</hent>
<hent key="024001000000000000000A0000000008" call="MPI_Isend" bytes="2560" orank="8" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.4080e-05 1.9073e-06 1.1921e-05</hent>
<hent key="024001000000000000000A0000000011" call="MPI_Isend" bytes="2560" orank="17" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.5034e-05 3.8147e-06 5.0068e-06</hent>
<hent key="024001000000000000000A0000000017" call="MPI_Isend" bytes="2560" orank="23" region="0" commid="0" count="2" tid="0" op="" dtype="" >5.9605e-06 2.8610e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000018" call="MPI_Isend" bytes="2560" orank="24" region="0" commid="0" count="5" tid="0" op="" dtype="" >4.4107e-05 5.0068e-06 1.3113e-05</hent>
<hent key="024001000000000000000A0000000050" call="MPI_Isend" bytes="2560" orank="80" region="0" commid="0" count="51" tid="0" op="" dtype="" >3.2687e-04 4.0531e-06 1.3113e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >3.7813e-04 4.9114e-05 1.7095e-04</hent>
<hent key="024001000000000000000A00000002D0" call="MPI_Isend" bytes="2560" orank="720" region="0" commid="0" count="51" tid="0" op="" dtype="" >3.4666e-04 5.0068e-06 1.5974e-05</hent>
<hent key="02400100000000000000100000000050" call="MPI_Isend" bytes="4096" orank="80" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.3239e-03 4.7398e-04 8.4996e-04</hent>
<hent key="024001000000000000001000000002D0" call="MPI_Isend" bytes="4096" orank="720" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.2875e-05 1.2875e-05 1.2875e-05</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >6.1989e-06 2.1458e-06 4.0531e-06</hent>
<hent key="03800100000000000000000400000008" call="MPI_Irecv" bytes="4" orank="8" region="0" commid="0" count="3398" tid="0" op="" dtype="" >9.1076e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000000400000011" call="MPI_Irecv" bytes="4" orank="17" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.4373e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000400000017" call="MPI_Irecv" bytes="4" orank="23" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3292e-03 0.0000e+00 2.2173e-05</hent>
<hent key="03800100000000000000000400000018" call="MPI_Irecv" bytes="4" orank="24" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.5950e-04 0.0000e+00 9.7752e-06</hent>
<hent key="03800100000000000000000400000050" call="MPI_Irecv" bytes="4" orank="80" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3587e-03 0.0000e+00 3.1948e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.6052e-03 3.3188e-04 9.8109e-04</hent>
<hent key="038001000000000000001C0000000011" call="MPI_Irecv" bytes="7168" orank="17" region="0" commid="0" count="8625" tid="0" op="" dtype="" >1.6603e-03 0.0000e+00 7.8678e-06</hent>
<hent key="038001000000000000001C0000000017" call="MPI_Irecv" bytes="7168" orank="23" region="0" commid="0" count="8666" tid="0" op="" dtype="" >3.5923e-03 0.0000e+00 9.0599e-06</hent>
<hent key="038001000000000000000004000002D0" call="MPI_Irecv" bytes="4" orank="720" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.8919e-03 0.0000e+00 1.1015e-04</hent>
<hent key="02400100000000000000000400000008" call="MPI_Isend" bytes="4" orank="8" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.8179e-03 0.0000e+00 4.3154e-05</hent>
<hent key="02400100000000000000000400000011" call="MPI_Isend" bytes="4" orank="17" region="0" commid="0" count="3398" tid="0" op="" dtype="" >3.1118e-03 0.0000e+00 2.3842e-05</hent>
<hent key="02400100000000000000000400000017" call="MPI_Isend" bytes="4" orank="23" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.5390e-03 0.0000e+00 5.5075e-05</hent>
<hent key="02400100000000000000000400000018" call="MPI_Isend" bytes="4" orank="24" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.7178e-02 4.7684e-06 7.3910e-05</hent>
<hent key="02400100000000000000000400000050" call="MPI_Isend" bytes="4" orank="80" region="0" commid="0" count="3398" tid="0" op="" dtype="" >3.1724e-02 3.8147e-06 1.5497e-04</hent>
<hent key="024001000000000000001C0000000011" call="MPI_Isend" bytes="7168" orank="17" region="0" commid="0" count="9045" tid="0" op="" dtype="" >2.9935e-02 9.5367e-07 2.7895e-05</hent>
<hent key="024001000000000000001C0000000017" call="MPI_Isend" bytes="7168" orank="23" region="0" commid="0" count="9278" tid="0" op="" dtype="" >2.0621e-02 9.5367e-07 4.2915e-05</hent>
<hent key="024001000000000000000004000002D0" call="MPI_Isend" bytes="4" orank="720" region="0" commid="0" count="3398" tid="0" op="" dtype="" >3.2259e-02 3.8147e-06 1.3258e-03</hent>
<hent key="03800100000000000000050000000008" call="MPI_Irecv" bytes="1280" orank="8" region="0" commid="0" count="236" tid="0" op="" dtype="" >1.0824e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000050000000011" call="MPI_Irecv" bytes="1280" orank="17" region="0" commid="0" count="195" tid="0" op="" dtype="" >5.6744e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000017" call="MPI_Irecv" bytes="1280" orank="23" region="0" commid="0" count="197" tid="0" op="" dtype="" >6.4135e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000018" call="MPI_Irecv" bytes="1280" orank="24" region="0" commid="0" count="202" tid="0" op="" dtype="" >8.4400e-05 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000050000000050" call="MPI_Irecv" bytes="1280" orank="80" region="0" commid="0" count="310" tid="0" op="" dtype="" >1.1683e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0364e+01 7.8678e-06 1.5562e-01</hent>
<hent key="03800100000000000000280000000008" call="MPI_Irecv" bytes="10240" orank="8" region="0" commid="0" count="75" tid="0" op="" dtype="" >3.5763e-05 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000280000000018" call="MPI_Irecv" bytes="10240" orank="24" region="0" commid="0" count="15" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000500000002D0" call="MPI_Irecv" bytes="1280" orank="720" region="0" commid="0" count="278" tid="0" op="" dtype="" >2.5511e-04 0.0000e+00 1.5020e-05</hent>
<hent key="03800100000000000000080000000008" call="MPI_Irecv" bytes="2048" orank="8" region="0" commid="0" count="15" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000011" call="MPI_Irecv" bytes="2048" orank="17" region="0" commid="0" count="9" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000017" call="MPI_Irecv" bytes="2048" orank="23" region="0" commid="0" count="13" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000018" call="MPI_Irecv" bytes="2048" orank="24" region="0" commid="0" count="12" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000050" call="MPI_Irecv" bytes="2048" orank="80" region="0" commid="0" count="3456" tid="0" op="" dtype="" >1.1938e-03 0.0000e+00 1.0800e-04</hent>
<hent key="038001000000000000000800000002D0" call="MPI_Irecv" bytes="2048" orank="720" region="0" commid="0" count="3464" tid="0" op="" dtype="" >1.9746e-03 0.0000e+00 8.7023e-05</hent>
<hent key="02400100000000000000050000000008" call="MPI_Isend" bytes="1280" orank="8" region="0" commid="0" count="238" tid="0" op="" dtype="" >4.5133e-04 0.0000e+00 1.2159e-05</hent>
<hent key="02400100000000000000050000000011" call="MPI_Isend" bytes="1280" orank="17" region="0" commid="0" count="212" tid="0" op="" dtype="" >7.7009e-04 2.8610e-06 5.9605e-06</hent>
<hent key="02400100000000000000050000000017" call="MPI_Isend" bytes="1280" orank="23" region="0" commid="0" count="188" tid="0" op="" dtype="" >3.4571e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000018" call="MPI_Isend" bytes="1280" orank="24" region="0" commid="0" count="220" tid="0" op="" dtype="" >1.2331e-03 1.9073e-06 1.6928e-05</hent>
<hent key="02400100000000000000050000000050" call="MPI_Isend" bytes="1280" orank="80" region="0" commid="0" count="295" tid="0" op="" dtype="" >1.8559e-03 2.8610e-06 2.6941e-05</hent>
<hent key="02400100000000000000280000000008" call="MPI_Isend" bytes="10240" orank="8" region="0" commid="0" count="69" tid="0" op="" dtype="" >9.2506e-05 0.0000e+00 2.3842e-05</hent>
<hent key="02400100000000000000280000000018" call="MPI_Isend" bytes="10240" orank="24" region="0" commid="0" count="28" tid="0" op="" dtype="" >5.2547e-04 5.9605e-06 3.2902e-05</hent>
<hent key="024001000000000000000500000002D0" call="MPI_Isend" bytes="1280" orank="720" region="0" commid="0" count="302" tid="0" op="" dtype="" >1.7574e-03 2.8610e-06 1.7881e-05</hent>
</hash>
<internal rank="16" log_i="1723712895.692261" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="17" mpi_size="768" stamp_init="1723712829.548207" stamp_final="1723712895.687628" username="apac4" allocationname="unknown" flags="0" pid="684285" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61394e+01" utime="5.07241e+01" stime="7.78212e+00" mtime="3.06781e+01" gflop="0.00000e+00" gbyte="3.76827e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.06781e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60072e+01" utime="5.06948e+01" stime="7.76876e+00" mtime="3.06781e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.06781e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4957e+08" > 5.0669e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4970e+08" > 2.9220e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3607e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5821e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2650e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.4850e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5862e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.6965e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0411e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1355e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8483e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="195" >
<hent key="02400100000000000000080000000009" call="MPI_Isend" bytes="2048" orank="9" region="0" commid="0" count="15" tid="0" op="" dtype="" >6.7949e-05 9.5367e-07 1.5020e-05</hent>
<hent key="02400100000000000000080000000010" call="MPI_Isend" bytes="2048" orank="16" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.9789e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000012" call="MPI_Isend" bytes="2048" orank="18" region="0" commid="0" count="12" tid="0" op="" dtype="" >4.9353e-05 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000080000000019" call="MPI_Isend" bytes="2048" orank="25" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.8624e-05 4.7684e-06 5.9605e-06</hent>
<hent key="02400100000000000000080000000051" call="MPI_Isend" bytes="2048" orank="81" region="0" commid="0" count="3464" tid="0" op="" dtype="" >1.0405e-02 9.5367e-07 1.1921e-05</hent>
<hent key="024001000000000000000800000002D1" call="MPI_Isend" bytes="2048" orank="721" region="0" commid="0" count="3467" tid="0" op="" dtype="" >1.0887e-02 9.5367e-07 2.0027e-05</hent>
<hent key="038001000000000000000E0000000051" call="MPI_Irecv" bytes="3584" orank="81" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000E0000000051" call="MPI_Isend" bytes="3584" orank="81" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.6928e-05 5.0068e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000009" call="MPI_Irecv" bytes="640" orank="9" region="0" commid="0" count="417" tid="0" op="" dtype="" >1.1897e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000010" call="MPI_Irecv" bytes="640" orank="16" region="0" commid="0" count="405" tid="0" op="" dtype="" >1.5259e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000012" call="MPI_Irecv" bytes="640" orank="18" region="0" commid="0" count="393" tid="0" op="" dtype="" >1.2851e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000028000000019" call="MPI_Irecv" bytes="640" orank="25" region="0" commid="0" count="412" tid="0" op="" dtype="" >1.0085e-04 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000E00000002D1" call="MPI_Isend" bytes="3584" orank="721" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000051" call="MPI_Irecv" bytes="640" orank="81" region="0" commid="0" count="276" tid="0" op="" dtype="" >8.7500e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002D1" call="MPI_Irecv" bytes="640" orank="721" region="0" commid="0" count="290" tid="0" op="" dtype="" >9.7990e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000028000000009" call="MPI_Isend" bytes="640" orank="9" region="0" commid="0" count="432" tid="0" op="" dtype="" >4.9639e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000028000000010" call="MPI_Isend" bytes="640" orank="16" region="0" commid="0" count="382" tid="0" op="" dtype="" >5.3978e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000028000000012" call="MPI_Isend" bytes="640" orank="18" region="0" commid="0" count="438" tid="0" op="" dtype="" >1.3249e-03 1.9073e-06 1.6928e-05</hent>
<hent key="02400100000000000000028000000019" call="MPI_Isend" bytes="640" orank="25" region="0" commid="0" count="389" tid="0" op="" dtype="" >1.7979e-03 3.8147e-06 9.0599e-06</hent>
<hent key="02400100000000000000028000000051" call="MPI_Isend" bytes="640" orank="81" region="0" commid="0" count="254" tid="0" op="" dtype="" >1.0877e-03 2.8610e-06 7.8678e-06</hent>
<hent key="024001000000000000000280000002D1" call="MPI_Isend" bytes="640" orank="721" region="0" commid="0" count="292" tid="0" op="" dtype="" >1.2319e-03 2.8610e-06 9.0599e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.1948e-05 3.1948e-05 3.1948e-05</hent>
<hent key="03800100000000000000014000000009" call="MPI_Irecv" bytes="320" orank="9" region="0" commid="0" count="368" tid="0" op="" dtype="" >1.0324e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000010" call="MPI_Irecv" bytes="320" orank="16" region="0" commid="0" count="321" tid="0" op="" dtype="" >1.2231e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000012" call="MPI_Irecv" bytes="320" orank="18" region="0" commid="0" count="358" tid="0" op="" dtype="" >1.0252e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000019" call="MPI_Irecv" bytes="320" orank="25" region="0" commid="0" count="377" tid="0" op="" dtype="" >8.6784e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000051" call="MPI_Irecv" bytes="320" orank="81" region="0" commid="0" count="151" tid="0" op="" dtype="" >4.9353e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002D1" call="MPI_Irecv" bytes="320" orank="721" region="0" commid="0" count="197" tid="0" op="" dtype="" >6.3419e-05 0.0000e+00 9.0599e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.3607e+00 0.0000e+00 1.8521e-01</hent>
<hent key="03800100000000000000400000000051" call="MPI_Irecv" bytes="16384" orank="81" region="0" commid="0" count="12691" tid="0" op="" dtype="" >2.3570e-03 0.0000e+00 5.9605e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6443e-02 9.5367e-07 1.6425e-02</hent>
<hent key="038001000000000000004000000002D1" call="MPI_Irecv" bytes="16384" orank="721" region="0" commid="0" count="12690" tid="0" op="" dtype="" >5.7166e-03 0.0000e+00 7.8678e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000014000000009" call="MPI_Isend" bytes="320" orank="9" region="0" commid="0" count="359" tid="0" op="" dtype="" >3.5429e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000014000000010" call="MPI_Isend" bytes="320" orank="16" region="0" commid="0" count="384" tid="0" op="" dtype="" >4.1699e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000012" call="MPI_Isend" bytes="320" orank="18" region="0" commid="0" count="359" tid="0" op="" dtype="" >9.7251e-04 9.5367e-07 4.0531e-06</hent>
<hent key="02400100000000000000014000000019" call="MPI_Isend" bytes="320" orank="25" region="0" commid="0" count="349" tid="0" op="" dtype="" >1.5528e-03 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000014000000051" call="MPI_Isend" bytes="320" orank="81" region="0" commid="0" count="162" tid="0" op="" dtype="" >6.7115e-04 2.8610e-06 9.0599e-06</hent>
<hent key="03800100000000000000200000000009" call="MPI_Irecv" bytes="8192" orank="9" region="0" commid="0" count="12691" tid="0" op="" dtype="" >3.9918e-03 0.0000e+00 9.0599e-06</hent>
<hent key="03800100000000000000200000000010" call="MPI_Irecv" bytes="8192" orank="16" region="0" commid="0" count="3654" tid="0" op="" dtype="" >1.0774e-03 0.0000e+00 2.8610e-06</hent>
<hent key="03800100000000000000200000000012" call="MPI_Irecv" bytes="8192" orank="18" region="0" commid="0" count="4049" tid="0" op="" dtype="" >6.2561e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000200000000019" call="MPI_Irecv" bytes="8192" orank="25" region="0" commid="0" count="12683" tid="0" op="" dtype="" >1.4706e-03 0.0000e+00 4.0531e-06</hent>
<hent key="024001000000000000000140000002D1" call="MPI_Isend" bytes="320" orank="721" region="0" commid="0" count="174" tid="0" op="" dtype="" >7.2074e-04 2.8610e-06 1.0967e-05</hent>
<hent key="03800100000000000000000000000009" call="MPI_Irecv" bytes="0" orank="9" region="0" commid="0" count="242" tid="0" op="" dtype="" >8.1062e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000010" call="MPI_Irecv" bytes="0" orank="16" region="0" commid="0" count="272" tid="0" op="" dtype="" >8.1062e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000012" call="MPI_Irecv" bytes="0" orank="18" region="0" commid="0" count="262" tid="0" op="" dtype="" >8.4877e-05 0.0000e+00 1.3113e-05</hent>
<hent key="03800100000000000000000000000019" call="MPI_Irecv" bytes="0" orank="25" region="0" commid="0" count="240" tid="0" op="" dtype="" >5.5552e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000051" call="MPI_Irecv" bytes="0" orank="81" region="0" commid="0" count="168" tid="0" op="" dtype="" >2.9087e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000051" call="MPI_Isend" bytes="16384" orank="81" region="0" commid="0" count="12699" tid="0" op="" dtype="" >9.7423e-02 3.8147e-06 6.5088e-05</hent>
<hent key="038001000000000000000000000002D1" call="MPI_Irecv" bytes="0" orank="721" region="0" commid="0" count="138" tid="0" op="" dtype="" >3.7670e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002D1" call="MPI_Isend" bytes="16384" orank="721" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.0278e-01 2.8610e-06 1.0683e-02</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.4060e-04 0.0000e+00 2.7680e-04</hent>
<hent key="02400100000000000000200000000009" call="MPI_Isend" bytes="8192" orank="9" region="0" commid="0" count="12626" tid="0" op="" dtype="" >5.8346e-03 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000200000000010" call="MPI_Isend" bytes="8192" orank="16" region="0" commid="0" count="4074" tid="0" op="" dtype="" >4.9834e-03 0.0000e+00 1.2875e-05</hent>
<hent key="02400100000000000000200000000012" call="MPI_Isend" bytes="8192" orank="18" region="0" commid="0" count="3358" tid="0" op="" dtype="" >5.8899e-03 0.0000e+00 5.6982e-05</hent>
<hent key="02400100000000000000200000000019" call="MPI_Isend" bytes="8192" orank="25" region="0" commid="0" count="12691" tid="0" op="" dtype="" >1.0964e-01 3.8147e-06 4.6968e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.1948e-05 3.1948e-05 3.1948e-05</hent>
<hent key="02400100000000000000000000000009" call="MPI_Isend" bytes="0" orank="9" region="0" commid="0" count="254" tid="0" op="" dtype="" >1.6570e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000010" call="MPI_Isend" bytes="0" orank="16" region="0" commid="0" count="234" tid="0" op="" dtype="" >1.5974e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000012" call="MPI_Isend" bytes="0" orank="18" region="0" commid="0" count="251" tid="0" op="" dtype="" >5.3096e-04 9.5367e-07 9.4891e-05</hent>
<hent key="02400100000000000000000000000019" call="MPI_Isend" bytes="0" orank="25" region="0" commid="0" count="269" tid="0" op="" dtype="" >1.0197e-03 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000000000000051" call="MPI_Isend" bytes="0" orank="81" region="0" commid="0" count="153" tid="0" op="" dtype="" >5.6338e-04 9.5367e-07 5.9605e-06</hent>
<hent key="024001000000000000000000000002D1" call="MPI_Isend" bytes="0" orank="721" region="0" commid="0" count="145" tid="0" op="" dtype="" >5.4836e-04 9.5367e-07 5.0068e-06</hent>
<hent key="03800100000000000000060000000009" call="MPI_Irecv" bytes="1536" orank="9" region="0" commid="0" count="99" tid="0" op="" dtype="" >2.3603e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000010" call="MPI_Irecv" bytes="1536" orank="16" region="0" commid="0" count="108" tid="0" op="" dtype="" >3.4332e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000012" call="MPI_Irecv" bytes="1536" orank="18" region="0" commid="0" count="98" tid="0" op="" dtype="" >3.1233e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000060000000019" call="MPI_Irecv" bytes="1536" orank="25" region="0" commid="0" count="110" tid="0" op="" dtype="" >3.4809e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000051" call="MPI_Irecv" bytes="1536" orank="81" region="0" commid="0" count="224" tid="0" op="" dtype="" >8.5354e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002D1" call="MPI_Irecv" bytes="1536" orank="721" region="0" commid="0" count="227" tid="0" op="" dtype="" >8.8215e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 2.1458e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000060000000009" call="MPI_Isend" bytes="1536" orank="9" region="0" commid="0" count="94" tid="0" op="" dtype="" >1.8048e-04 9.5367e-07 1.4067e-05</hent>
<hent key="02400100000000000000060000000010" call="MPI_Isend" bytes="1536" orank="16" region="0" commid="0" count="105" tid="0" op="" dtype="" >2.1434e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000012" call="MPI_Isend" bytes="1536" orank="18" region="0" commid="0" count="91" tid="0" op="" dtype="" >3.3736e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000060000000019" call="MPI_Isend" bytes="1536" orank="25" region="0" commid="0" count="100" tid="0" op="" dtype="" >5.1022e-04 3.8147e-06 6.1989e-06</hent>
<hent key="02400100000000000000060000000051" call="MPI_Isend" bytes="1536" orank="81" region="0" commid="0" count="223" tid="0" op="" dtype="" >1.0259e-03 3.0994e-06 5.9605e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002D1" call="MPI_Isend" bytes="1536" orank="721" region="0" commid="0" count="205" tid="0" op="" dtype="" >9.3484e-04 3.8147e-06 1.0967e-05</hent>
<hent key="038001000000000000000C0000000051" call="MPI_Irecv" bytes="3072" orank="81" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002D1" call="MPI_Irecv" bytes="3072" orank="721" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000009" call="MPI_Isend" bytes="3072" orank="9" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="024001000000000000000C0000000019" call="MPI_Isend" bytes="3072" orank="25" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="024001000000000000000C0000000051" call="MPI_Isend" bytes="3072" orank="81" region="0" commid="0" count="10" tid="0" op="" dtype="" >6.0797e-05 4.7684e-06 1.3113e-05</hent>
<hent key="024001000000000000000C00000002D1" call="MPI_Isend" bytes="3072" orank="721" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.5034e-05 5.0068e-06 5.0068e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.5405e-04 3.5405e-04 3.5405e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >8.3113e-04 2.7204e-04 2.8110e-04</hent>
<hent key="03800100000000000000038000000009" call="MPI_Irecv" bytes="896" orank="9" region="0" commid="0" count="349" tid="0" op="" dtype="" >8.6546e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000010" call="MPI_Irecv" bytes="896" orank="16" region="0" commid="0" count="2778" tid="0" op="" dtype="" >6.3586e-04 0.0000e+00 1.5974e-05</hent>
<hent key="03800100000000000000038000000012" call="MPI_Irecv" bytes="896" orank="18" region="0" commid="0" count="2663" tid="0" op="" dtype="" >6.5947e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000038000000019" call="MPI_Irecv" bytes="896" orank="25" region="0" commid="0" count="316" tid="0" op="" dtype="" >7.8201e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000051" call="MPI_Irecv" bytes="896" orank="81" region="0" commid="0" count="291" tid="0" op="" dtype="" >7.5579e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0967e-05 1.0967e-05 1.0967e-05</hent>
<hent key="038001000000000000000380000002D1" call="MPI_Irecv" bytes="896" orank="721" region="0" commid="0" count="320" tid="0" op="" dtype="" >1.3161e-04 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="03800100000000000000380000000051" call="MPI_Irecv" bytes="14336" orank="81" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000003800000002D1" call="MPI_Irecv" bytes="14336" orank="721" region="0" commid="0" count="9" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000038000000009" call="MPI_Isend" bytes="896" orank="9" region="0" commid="0" count="315" tid="0" op="" dtype="" >4.5562e-04 9.5367e-07 7.1526e-06</hent>
<hent key="02400100000000000000038000000010" call="MPI_Isend" bytes="896" orank="16" region="0" commid="0" count="2648" tid="0" op="" dtype="" >1.9312e-03 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000038000000012" call="MPI_Isend" bytes="896" orank="18" region="0" commid="0" count="2801" tid="0" op="" dtype="" >2.7723e-03 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000038000000019" call="MPI_Isend" bytes="896" orank="25" region="0" commid="0" count="339" tid="0" op="" dtype="" >1.6263e-03 3.8147e-06 8.8215e-06</hent>
<hent key="02400100000000000000038000000051" call="MPI_Isend" bytes="896" orank="81" region="0" commid="0" count="345" tid="0" op="" dtype="" >1.4865e-03 2.8610e-06 1.0014e-05</hent>
<hent key="024001000000000000000380000002D1" call="MPI_Isend" bytes="896" orank="721" region="0" commid="0" count="333" tid="0" op="" dtype="" >1.4226e-03 2.8610e-06 5.9605e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.5697e+00 1.0967e-05 2.0316e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.2430e-03 3.2430e-03 3.2430e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0411e-02 1.0411e-02 1.0411e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.6100e-02 2.6100e-02 2.6100e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.9801e-01 3.1610e-03 1.7838e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.1710e-04 3.1710e-04 3.1710e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5862e+00 3.8004e-04 2.5066e-01</hent>
<hent key="03800100000000000000040000000009" call="MPI_Irecv" bytes="1024" orank="9" region="0" commid="0" count="3396" tid="0" op="" dtype="" >6.0892e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000010" call="MPI_Irecv" bytes="1024" orank="16" region="0" commid="0" count="956" tid="0" op="" dtype="" >1.8072e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000012" call="MPI_Irecv" bytes="1024" orank="18" region="0" commid="0" count="1068" tid="0" op="" dtype="" >2.2078e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000019" call="MPI_Irecv" bytes="1024" orank="25" region="0" commid="0" count="3394" tid="0" op="" dtype="" >5.6267e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.2650e-05 0.0000e+00 1.1921e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.8147e-05 3.8147e-05 3.8147e-05</hent>
<hent key="03800100000000000000070000000009" call="MPI_Irecv" bytes="1792" orank="9" region="0" commid="0" count="34" tid="0" op="" dtype="" >1.0014e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000010" call="MPI_Irecv" bytes="1792" orank="16" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.1683e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000012" call="MPI_Irecv" bytes="1792" orank="18" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000019" call="MPI_Irecv" bytes="1792" orank="25" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.0252e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000051" call="MPI_Irecv" bytes="1792" orank="81" region="0" commid="0" count="163" tid="0" op="" dtype="" >3.5763e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002D1" call="MPI_Irecv" bytes="1792" orank="721" region="0" commid="0" count="138" tid="0" op="" dtype="" >5.8413e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >7.0333e-05 0.0000e+00 2.9087e-05</hent>
<hent key="02400100000000000000040000000009" call="MPI_Isend" bytes="1024" orank="9" region="0" commid="0" count="3380" tid="0" op="" dtype="" >2.0168e-03 0.0000e+00 1.2875e-05</hent>
<hent key="02400100000000000000040000000010" call="MPI_Isend" bytes="1024" orank="16" region="0" commid="0" count="1096" tid="0" op="" dtype="" >6.7067e-04 0.0000e+00 1.9073e-06</hent>
<hent key="02400100000000000000040000000012" call="MPI_Isend" bytes="1024" orank="18" region="0" commid="0" count="920" tid="0" op="" dtype="" >6.6376e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000040000000019" call="MPI_Isend" bytes="1024" orank="25" region="0" commid="0" count="3396" tid="0" op="" dtype="" >9.2139e-03 9.5367e-07 1.6928e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5650e+00 0.0000e+00 2.2748e+00</hent>
<hent key="038001000000000000000A0000000009" call="MPI_Irecv" bytes="2560" orank="9" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000010" call="MPI_Irecv" bytes="2560" orank="16" region="0" commid="0" count="6" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000012" call="MPI_Irecv" bytes="2560" orank="18" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000019" call="MPI_Irecv" bytes="2560" orank="25" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000051" call="MPI_Irecv" bytes="2560" orank="81" region="0" commid="0" count="43" tid="0" op="" dtype="" >1.4067e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002D1" call="MPI_Irecv" bytes="2560" orank="721" region="0" commid="0" count="48" tid="0" op="" dtype="" >2.4319e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000070000000009" call="MPI_Isend" bytes="1792" orank="9" region="0" commid="0" count="44" tid="0" op="" dtype="" >1.2064e-04 9.5367e-07 1.5020e-05</hent>
<hent key="02400100000000000000070000000010" call="MPI_Isend" bytes="1792" orank="16" region="0" commid="0" count="37" tid="0" op="" dtype="" >7.8678e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000012" call="MPI_Isend" bytes="1792" orank="18" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.5879e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000070000000019" call="MPI_Isend" bytes="1792" orank="25" region="0" commid="0" count="36" tid="0" op="" dtype="" >1.8716e-04 4.0531e-06 6.1989e-06</hent>
<hent key="02400100000000000000070000000051" call="MPI_Isend" bytes="1792" orank="81" region="0" commid="0" count="139" tid="0" op="" dtype="" >6.6948e-04 3.8147e-06 1.1921e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1355e-01 1.1355e-01 1.1355e-01</hent>
<hent key="024001000000000000000700000002D1" call="MPI_Isend" bytes="1792" orank="721" region="0" commid="0" count="147" tid="0" op="" dtype="" >6.6662e-04 3.8147e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000009" call="MPI_Isend" bytes="2560" orank="9" region="0" commid="0" count="2" tid="0" op="" dtype="" >2.8610e-06 9.5367e-07 1.9073e-06</hent>
<hent key="024001000000000000000A0000000010" call="MPI_Isend" bytes="2560" orank="16" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.5736e-05 1.9073e-06 2.8610e-06</hent>
<hent key="024001000000000000000A0000000012" call="MPI_Isend" bytes="2560" orank="18" region="0" commid="0" count="1" tid="0" op="" dtype="" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="024001000000000000000A0000000019" call="MPI_Isend" bytes="2560" orank="25" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.2902e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000051" call="MPI_Isend" bytes="2560" orank="81" region="0" commid="0" count="56" tid="0" op="" dtype="" >2.8825e-04 3.8147e-06 6.1989e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.1938e-04 5.3167e-05 1.8406e-04</hent>
<hent key="024001000000000000000A00000002D1" call="MPI_Isend" bytes="2560" orank="721" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.2006e-04 3.8147e-06 1.0967e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.4119e-03 5.0497e-04 9.0694e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.1035e-05 6.1035e-05 6.1035e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 2.1458e-06 3.8147e-06</hent>
<hent key="03800100000000000000000400000009" call="MPI_Irecv" bytes="4" orank="9" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.7636e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000010" call="MPI_Irecv" bytes="4" orank="16" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.4111e-04 0.0000e+00 2.8610e-06</hent>
<hent key="03800100000000000000000400000012" call="MPI_Irecv" bytes="4" orank="18" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.0879e-04 0.0000e+00 2.8610e-06</hent>
<hent key="03800100000000000000000400000019" call="MPI_Irecv" bytes="4" orank="25" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.2486e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000051" call="MPI_Irecv" bytes="4" orank="81" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.7626e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.6801e-03 3.5191e-04 9.5105e-04</hent>
<hent key="038001000000000000001C0000000010" call="MPI_Irecv" bytes="7168" orank="16" region="0" commid="0" count="9045" tid="0" op="" dtype="" >2.4998e-03 0.0000e+00 6.1989e-06</hent>
<hent key="038001000000000000001C0000000012" call="MPI_Irecv" bytes="7168" orank="18" region="0" commid="0" count="8650" tid="0" op="" dtype="" >1.3063e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000004000002D1" call="MPI_Irecv" bytes="4" orank="721" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.3906e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000000400000009" call="MPI_Isend" bytes="4" orank="9" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1928e-03 0.0000e+00 3.0994e-05</hent>
<hent key="02400100000000000000000400000010" call="MPI_Isend" bytes="4" orank="16" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2486e-03 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000400000012" call="MPI_Isend" bytes="4" orank="18" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3651e-03 0.0000e+00 8.1062e-05</hent>
<hent key="02400100000000000000000400000019" call="MPI_Isend" bytes="4" orank="25" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3998e-02 3.8147e-06 3.6001e-05</hent>
<hent key="02400100000000000000000400000051" call="MPI_Isend" bytes="4" orank="81" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.8407e-02 3.8147e-06 1.1921e-05</hent>
<hent key="024001000000000000001C0000000010" call="MPI_Isend" bytes="7168" orank="16" region="0" commid="0" count="8625" tid="0" op="" dtype="" >1.8924e-02 9.5367e-07 1.8120e-05</hent>
<hent key="024001000000000000001C0000000012" call="MPI_Isend" bytes="7168" orank="18" region="0" commid="0" count="9341" tid="0" op="" dtype="" >2.7437e-02 9.5367e-07 1.5974e-05</hent>
<hent key="024001000000000000000004000002D1" call="MPI_Isend" bytes="4" orank="721" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7934e-02 3.8147e-06 1.3614e-04</hent>
<hent key="03800100000000000000050000000009" call="MPI_Irecv" bytes="1280" orank="9" region="0" commid="0" count="178" tid="0" op="" dtype="" >5.0068e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000010" call="MPI_Irecv" bytes="1280" orank="16" region="0" commid="0" count="212" tid="0" op="" dtype="" >7.3910e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000012" call="MPI_Irecv" bytes="1280" orank="18" region="0" commid="0" count="191" tid="0" op="" dtype="" >5.4121e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000050000000019" call="MPI_Irecv" bytes="1280" orank="25" region="0" commid="0" count="194" tid="0" op="" dtype="" >5.5075e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000051" call="MPI_Irecv" bytes="1280" orank="81" region="0" commid="0" count="314" tid="0" op="" dtype="" >9.4652e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0686e+01 7.8678e-06 1.5596e-01</hent>
<hent key="03800100000000000000280000000009" call="MPI_Irecv" bytes="10240" orank="9" region="0" commid="0" count="8" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000280000000019" call="MPI_Irecv" bytes="10240" orank="25" region="0" commid="0" count="16" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000500000002D1" call="MPI_Irecv" bytes="1280" orank="721" region="0" commid="0" count="268" tid="0" op="" dtype="" >1.0133e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000009" call="MPI_Irecv" bytes="2048" orank="9" region="0" commid="0" count="12" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000010" call="MPI_Irecv" bytes="2048" orank="16" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000012" call="MPI_Irecv" bytes="2048" orank="18" region="0" commid="0" count="15" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000019" call="MPI_Irecv" bytes="2048" orank="25" region="0" commid="0" count="19" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000051" call="MPI_Irecv" bytes="2048" orank="81" region="0" commid="0" count="3461" tid="0" op="" dtype="" >5.7244e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002D1" call="MPI_Irecv" bytes="2048" orank="721" region="0" commid="0" count="3464" tid="0" op="" dtype="" >6.7163e-04 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000050000000009" call="MPI_Isend" bytes="1280" orank="9" region="0" commid="0" count="201" tid="0" op="" dtype="" >3.1114e-04 0.0000e+00 1.2159e-05</hent>
<hent key="02400100000000000000050000000010" call="MPI_Isend" bytes="1280" orank="16" region="0" commid="0" count="195" tid="0" op="" dtype="" >3.7098e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000012" call="MPI_Isend" bytes="1280" orank="18" region="0" commid="0" count="183" tid="0" op="" dtype="" >6.3229e-04 1.9073e-06 5.0068e-06</hent>
<hent key="02400100000000000000050000000019" call="MPI_Isend" bytes="1280" orank="25" region="0" commid="0" count="205" tid="0" op="" dtype="" >1.0200e-03 1.9073e-06 1.0014e-05</hent>
<hent key="02400100000000000000050000000051" call="MPI_Isend" bytes="1280" orank="81" region="0" commid="0" count="288" tid="0" op="" dtype="" >1.2975e-03 2.8610e-06 6.1989e-06</hent>
<hent key="02400100000000000000280000000009" call="MPI_Isend" bytes="10240" orank="9" region="0" commid="0" count="73" tid="0" op="" dtype="" >2.2173e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000280000000019" call="MPI_Isend" bytes="10240" orank="25" region="0" commid="0" count="8" tid="0" op="" dtype="" >6.8188e-05 5.9605e-06 1.1921e-05</hent>
<hent key="024001000000000000000500000002D1" call="MPI_Isend" bytes="1280" orank="721" region="0" commid="0" count="285" tid="0" op="" dtype="" >1.2639e-03 3.0994e-06 9.0599e-06</hent>
</hash>
<internal rank="17" log_i="1723712895.687628" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="18" mpi_size="768" stamp_init="1723712829.548093" stamp_final="1723712895.682933" username="apac4" allocationname="unknown" flags="0" pid="684286" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61348e+01" utime="4.92725e+01" stime="8.25483e+00" mtime="3.02461e+01" gflop="0.00000e+00" gbyte="3.76972e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.02461e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60013e+01" utime="4.92424e+01" stime="8.24237e+00" mtime="3.02461e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.02461e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4894e+08" > 6.0802e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4987e+08" > 3.5689e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9751e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5653e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.5330e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.0572e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5862e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.0172e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0429e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1364e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8345e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="194" >
<hent key="0240010000000000000008000000000A" call="MPI_Isend" bytes="2048" orank="10" region="0" commid="0" count="13" tid="0" op="" dtype="" >4.1962e-05 9.5367e-07 1.1921e-05</hent>
<hent key="02400100000000000000080000000011" call="MPI_Isend" bytes="2048" orank="17" region="0" commid="0" count="15" tid="0" op="" dtype="" >3.5524e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000013" call="MPI_Isend" bytes="2048" orank="19" region="0" commid="0" count="22" tid="0" op="" dtype="" >1.1659e-04 3.8147e-06 2.2173e-05</hent>
<hent key="0240010000000000000008000000001A" call="MPI_Isend" bytes="2048" orank="26" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.2439e-05 5.0068e-06 7.1526e-06</hent>
<hent key="02400100000000000000080000000052" call="MPI_Isend" bytes="2048" orank="82" region="0" commid="0" count="3443" tid="0" op="" dtype="" >1.6007e-02 9.5367e-07 5.2214e-05</hent>
<hent key="024001000000000000000800000002D2" call="MPI_Isend" bytes="2048" orank="722" region="0" commid="0" count="3464" tid="0" op="" dtype="" >1.6703e-02 9.5367e-07 8.8930e-05</hent>
<hent key="038001000000000000000E0000000052" call="MPI_Irecv" bytes="3584" orank="82" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000E00000002D2" call="MPI_Irecv" bytes="3584" orank="722" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="024001000000000000000E0000000052" call="MPI_Isend" bytes="3584" orank="82" region="0" commid="0" count="1" tid="0" op="" dtype="" >6.9141e-06 6.9141e-06 6.9141e-06</hent>
<hent key="0380010000000000000002800000000A" call="MPI_Irecv" bytes="640" orank="10" region="0" commid="0" count="392" tid="0" op="" dtype="" >1.1539e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000028000000011" call="MPI_Irecv" bytes="640" orank="17" region="0" commid="0" count="438" tid="0" op="" dtype="" >1.9550e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000013" call="MPI_Irecv" bytes="640" orank="19" region="0" commid="0" count="407" tid="0" op="" dtype="" >9.8944e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000001A" call="MPI_Irecv" bytes="640" orank="26" region="0" commid="0" count="400" tid="0" op="" dtype="" >8.8930e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000052" call="MPI_Irecv" bytes="640" orank="82" region="0" commid="0" count="300" tid="0" op="" dtype="" >1.0443e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000000280000002D2" call="MPI_Irecv" bytes="640" orank="722" region="0" commid="0" count="296" tid="0" op="" dtype="" >1.5068e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000002800000000A" call="MPI_Isend" bytes="640" orank="10" region="0" commid="0" count="424" tid="0" op="" dtype="" >6.3252e-04 9.5367e-07 1.0014e-05</hent>
<hent key="02400100000000000000028000000011" call="MPI_Isend" bytes="640" orank="17" region="0" commid="0" count="393" tid="0" op="" dtype="" >5.4169e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000028000000013" call="MPI_Isend" bytes="640" orank="19" region="0" commid="0" count="411" tid="0" op="" dtype="" >1.4615e-03 1.9073e-06 1.4067e-05</hent>
<hent key="0240010000000000000002800000001A" call="MPI_Isend" bytes="640" orank="26" region="0" commid="0" count="415" tid="0" op="" dtype="" >2.1636e-03 3.8147e-06 1.4067e-05</hent>
<hent key="02400100000000000000028000000052" call="MPI_Isend" bytes="640" orank="82" region="0" commid="0" count="299" tid="0" op="" dtype="" >1.8044e-03 3.8147e-06 1.8835e-05</hent>
<hent key="024001000000000000000280000002D2" call="MPI_Isend" bytes="640" orank="722" region="0" commid="0" count="283" tid="0" op="" dtype="" >1.4501e-03 3.8147e-06 1.5020e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.3140e-05 3.3140e-05 3.3140e-05</hent>
<hent key="0380010000000000000001400000000A" call="MPI_Irecv" bytes="320" orank="10" region="0" commid="0" count="385" tid="0" op="" dtype="" >9.7275e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000011" call="MPI_Irecv" bytes="320" orank="17" region="0" commid="0" count="359" tid="0" op="" dtype="" >1.7166e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000014000000013" call="MPI_Irecv" bytes="320" orank="19" region="0" commid="0" count="383" tid="0" op="" dtype="" >9.4891e-05 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000001400000001A" call="MPI_Irecv" bytes="320" orank="26" region="0" commid="0" count="366" tid="0" op="" dtype="" >9.2506e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000014000000052" call="MPI_Irecv" bytes="320" orank="82" region="0" commid="0" count="175" tid="0" op="" dtype="" >6.3896e-05 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000140000002D2" call="MPI_Irecv" bytes="320" orank="722" region="0" commid="0" count="175" tid="0" op="" dtype="" >6.3181e-05 0.0000e+00 3.8147e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.9751e+00 0.0000e+00 1.5379e-01</hent>
<hent key="03800100000000000000400000000052" call="MPI_Irecv" bytes="16384" orank="82" region="0" commid="0" count="12651" tid="0" op="" dtype="" >5.0635e-03 0.0000e+00 3.0041e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6466e-02 0.0000e+00 1.6452e-02</hent>
<hent key="038001000000000000004000000002D2" call="MPI_Irecv" bytes="16384" orank="722" region="0" commid="0" count="12699" tid="0" op="" dtype="" >4.7157e-03 0.0000e+00 3.6955e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.0742e-05 0.0000e+00 1.9789e-05</hent>
<hent key="0240010000000000000001400000000A" call="MPI_Isend" bytes="320" orank="10" region="0" commid="0" count="345" tid="0" op="" dtype="" >4.3058e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000014000000011" call="MPI_Isend" bytes="320" orank="17" region="0" commid="0" count="358" tid="0" op="" dtype="" >4.0364e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000014000000013" call="MPI_Isend" bytes="320" orank="19" region="0" commid="0" count="353" tid="0" op="" dtype="" >1.1194e-03 1.9073e-06 5.0068e-06</hent>
<hent key="0240010000000000000001400000001A" call="MPI_Isend" bytes="320" orank="26" region="0" commid="0" count="363" tid="0" op="" dtype="" >1.8177e-03 3.8147e-06 1.1206e-05</hent>
<hent key="02400100000000000000014000000052" call="MPI_Isend" bytes="320" orank="82" region="0" commid="0" count="188" tid="0" op="" dtype="" >1.1010e-03 3.8147e-06 2.1219e-05</hent>
<hent key="0380010000000000000020000000000A" call="MPI_Irecv" bytes="8192" orank="10" region="0" commid="0" count="12647" tid="0" op="" dtype="" >4.8337e-03 0.0000e+00 3.1948e-05</hent>
<hent key="03800100000000000000200000000011" call="MPI_Irecv" bytes="8192" orank="17" region="0" commid="0" count="3358" tid="0" op="" dtype="" >1.1322e-03 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000200000000013" call="MPI_Irecv" bytes="8192" orank="19" region="0" commid="0" count="4625" tid="0" op="" dtype="" >8.6546e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000020000000001A" call="MPI_Irecv" bytes="8192" orank="26" region="0" commid="0" count="12667" tid="0" op="" dtype="" >2.2545e-03 0.0000e+00 2.7895e-05</hent>
<hent key="024001000000000000000140000002D2" call="MPI_Isend" bytes="320" orank="722" region="0" commid="0" count="202" tid="0" op="" dtype="" >1.0202e-03 3.0994e-06 2.2173e-05</hent>
<hent key="0380010000000000000000000000000A" call="MPI_Irecv" bytes="0" orank="10" region="0" commid="0" count="242" tid="0" op="" dtype="" >6.2466e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000011" call="MPI_Irecv" bytes="0" orank="17" region="0" commid="0" count="251" tid="0" op="" dtype="" >8.9407e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000013" call="MPI_Irecv" bytes="0" orank="19" region="0" commid="0" count="282" tid="0" op="" dtype="" >5.2214e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000001A" call="MPI_Irecv" bytes="0" orank="26" region="0" commid="0" count="267" tid="0" op="" dtype="" >5.3167e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000052" call="MPI_Irecv" bytes="0" orank="82" region="0" commid="0" count="162" tid="0" op="" dtype="" >4.1962e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000052" call="MPI_Isend" bytes="16384" orank="82" region="0" commid="0" count="12683" tid="0" op="" dtype="" >1.1131e-01 3.8147e-06 1.1897e-04</hent>
<hent key="038001000000000000000000000002D2" call="MPI_Irecv" bytes="0" orank="722" region="0" commid="0" count="157" tid="0" op="" dtype="" >7.0333e-05 0.0000e+00 6.9141e-06</hent>
<hent key="024001000000000000004000000002D2" call="MPI_Isend" bytes="16384" orank="722" region="0" commid="0" count="12692" tid="0" op="" dtype="" >1.3129e-01 3.8147e-06 7.2956e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >3.9577e-04 0.0000e+00 2.6894e-04</hent>
<hent key="0240010000000000000020000000000A" call="MPI_Isend" bytes="8192" orank="10" region="0" commid="0" count="12699" tid="0" op="" dtype="" >9.1853e-03 0.0000e+00 2.6941e-05</hent>
<hent key="02400100000000000000200000000011" call="MPI_Isend" bytes="8192" orank="17" region="0" commid="0" count="4049" tid="0" op="" dtype="" >4.5528e-03 0.0000e+00 1.4067e-05</hent>
<hent key="02400100000000000000200000000013" call="MPI_Isend" bytes="8192" orank="19" region="0" commid="0" count="3047" tid="0" op="" dtype="" >5.9719e-03 0.0000e+00 7.4148e-05</hent>
<hent key="0240010000000000000020000000001A" call="MPI_Isend" bytes="8192" orank="26" region="0" commid="0" count="12631" tid="0" op="" dtype="" >1.2754e-01 4.7684e-06 1.6308e-04</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.6042e-05 6.6042e-05 6.6042e-05</hent>
<hent key="0240010000000000000000000000000A" call="MPI_Isend" bytes="0" orank="10" region="0" commid="0" count="264" tid="0" op="" dtype="" >2.5272e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000011" call="MPI_Isend" bytes="0" orank="17" region="0" commid="0" count="262" tid="0" op="" dtype="" >1.7810e-04 0.0000e+00 3.8147e-06</hent>
<hent key="02400100000000000000000000000013" call="MPI_Isend" bytes="0" orank="19" region="0" commid="0" count="260" tid="0" op="" dtype="" >6.1107e-04 9.5367e-07 7.2956e-05</hent>
<hent key="0240010000000000000000000000001A" call="MPI_Isend" bytes="0" orank="26" region="0" commid="0" count="281" tid="0" op="" dtype="" >1.2350e-03 1.9073e-06 1.1921e-05</hent>
<hent key="02400100000000000000000000000052" call="MPI_Isend" bytes="0" orank="82" region="0" commid="0" count="150" tid="0" op="" dtype="" >7.4959e-04 9.5367e-07 1.2159e-05</hent>
<hent key="024001000000000000000000000002D2" call="MPI_Isend" bytes="0" orank="722" region="0" commid="0" count="148" tid="0" op="" dtype="" >6.7258e-04 2.8610e-06 1.3828e-05</hent>
<hent key="0380010000000000000006000000000A" call="MPI_Irecv" bytes="1536" orank="10" region="0" commid="0" count="100" tid="0" op="" dtype="" >2.1219e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000011" call="MPI_Irecv" bytes="1536" orank="17" region="0" commid="0" count="91" tid="0" op="" dtype="" >3.0041e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000013" call="MPI_Irecv" bytes="1536" orank="19" region="0" commid="0" count="65" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000001A" call="MPI_Irecv" bytes="1536" orank="26" region="0" commid="0" count="98" tid="0" op="" dtype="" >2.3127e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000052" call="MPI_Irecv" bytes="1536" orank="82" region="0" commid="0" count="220" tid="0" op="" dtype="" >8.2731e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002D2" call="MPI_Irecv" bytes="1536" orank="722" region="0" commid="0" count="209" tid="0" op="" dtype="" >1.1635e-04 0.0000e+00 8.8215e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0240010000000000000006000000000A" call="MPI_Isend" bytes="1536" orank="10" region="0" commid="0" count="115" tid="0" op="" dtype="" >2.5177e-04 9.5367e-07 1.5974e-05</hent>
<hent key="02400100000000000000060000000011" call="MPI_Isend" bytes="1536" orank="17" region="0" commid="0" count="98" tid="0" op="" dtype="" >1.9169e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000013" call="MPI_Isend" bytes="1536" orank="19" region="0" commid="0" count="94" tid="0" op="" dtype="" >3.9887e-04 2.8610e-06 5.9605e-06</hent>
<hent key="0240010000000000000006000000001A" call="MPI_Isend" bytes="1536" orank="26" region="0" commid="0" count="90" tid="0" op="" dtype="" >5.3787e-04 4.7684e-06 1.0967e-05</hent>
<hent key="02400100000000000000060000000052" call="MPI_Isend" bytes="1536" orank="82" region="0" commid="0" count="236" tid="0" op="" dtype="" >1.5736e-03 4.7684e-06 2.5988e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002D2" call="MPI_Isend" bytes="1536" orank="722" region="0" commid="0" count="204" tid="0" op="" dtype="" >1.2174e-03 3.8147e-06 1.5974e-05</hent>
<hent key="038001000000000000000C0000000052" call="MPI_Irecv" bytes="3072" orank="82" region="0" commid="0" count="5" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002D2" call="MPI_Irecv" bytes="3072" orank="722" region="0" commid="0" count="6" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000C0000000052" call="MPI_Isend" bytes="3072" orank="82" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.0967e-05 1.0967e-05 1.0967e-05</hent>
<hent key="024001000000000000000C00000002D2" call="MPI_Isend" bytes="3072" orank="722" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.1962e-05 4.7684e-06 7.1526e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.7813e-04 3.7813e-04 3.7813e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >8.7118e-04 2.8300e-04 2.9421e-04</hent>
<hent key="0380010000000000000003800000000A" call="MPI_Irecv" bytes="896" orank="10" region="0" commid="0" count="324" tid="0" op="" dtype="" >7.8201e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000011" call="MPI_Irecv" bytes="896" orank="17" region="0" commid="0" count="2801" tid="0" op="" dtype="" >5.1713e-04 0.0000e+00 1.0967e-05</hent>
<hent key="03800100000000000000038000000013" call="MPI_Irecv" bytes="896" orank="19" region="0" commid="0" count="2469" tid="0" op="" dtype="" >4.7779e-04 0.0000e+00 7.8678e-06</hent>
<hent key="0380010000000000000003800000001A" call="MPI_Irecv" bytes="896" orank="26" region="0" commid="0" count="301" tid="0" op="" dtype="" >7.0333e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000038000000052" call="MPI_Irecv" bytes="896" orank="82" region="0" commid="0" count="323" tid="0" op="" dtype="" >1.3185e-04 0.0000e+00 3.0994e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.7976e-05 8.7976e-05 8.7976e-05</hent>
<hent key="038001000000000000000380000002D2" call="MPI_Irecv" bytes="896" orank="722" region="0" commid="0" count="318" tid="0" op="" dtype="" >1.3781e-04 0.0000e+00 4.0531e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000380000000052" call="MPI_Irecv" bytes="14336" orank="82" region="0" commid="0" count="48" tid="0" op="" dtype="" >3.3140e-05 0.0000e+00 1.2875e-05</hent>
<hent key="0240010000000000000003800000000A" call="MPI_Isend" bytes="896" orank="10" region="0" commid="0" count="294" tid="0" op="" dtype="" >5.2428e-04 9.5367e-07 3.0041e-05</hent>
<hent key="02400100000000000000038000000011" call="MPI_Isend" bytes="896" orank="17" region="0" commid="0" count="2663" tid="0" op="" dtype="" >1.9529e-03 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000038000000013" call="MPI_Isend" bytes="896" orank="19" region="0" commid="0" count="2894" tid="0" op="" dtype="" >3.3655e-03 0.0000e+00 1.1921e-05</hent>
<hent key="0240010000000000000003800000001A" call="MPI_Isend" bytes="896" orank="26" region="0" commid="0" count="336" tid="0" op="" dtype="" >1.8668e-03 4.0531e-06 1.2159e-05</hent>
<hent key="02400100000000000000038000000052" call="MPI_Isend" bytes="896" orank="82" region="0" commid="0" count="335" tid="0" op="" dtype="" >2.1515e-03 3.8147e-06 2.0981e-05</hent>
<hent key="024001000000000000000380000002D2" call="MPI_Isend" bytes="896" orank="722" region="0" commid="0" count="331" tid="0" op="" dtype="" >1.6983e-03 3.8147e-06 1.2159e-05</hent>
<hent key="02400100000000000000380000000052" call="MPI_Isend" bytes="14336" orank="82" region="0" commid="0" count="16" tid="0" op="" dtype="" >1.4615e-04 5.9605e-06 2.2173e-05</hent>
<hent key="024001000000000000003800000002D2" call="MPI_Isend" bytes="14336" orank="722" region="0" commid="0" count="7" tid="0" op="" dtype="" >7.4387e-05 5.9605e-06 2.5034e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.5783e+00 1.6928e-05 2.0317e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.2129e-03 3.2129e-03 3.2129e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0429e-02 1.0429e-02 1.0429e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.5976e-02 2.5976e-02 2.5976e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.8978e-01 3.2799e-03 1.7002e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >2.7204e-04 2.7204e-04 2.7204e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5862e+00 4.5490e-04 2.5059e-01</hent>
<hent key="0380010000000000000004000000000A" call="MPI_Irecv" bytes="1024" orank="10" region="0" commid="0" count="3380" tid="0" op="" dtype="" >6.9547e-04 0.0000e+00 1.2875e-05</hent>
<hent key="03800100000000000000040000000011" call="MPI_Irecv" bytes="1024" orank="17" region="0" commid="0" count="920" tid="0" op="" dtype="" >1.1492e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000013" call="MPI_Irecv" bytes="1024" orank="19" region="0" commid="0" count="1240" tid="0" op="" dtype="" >2.3580e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000004000000001A" call="MPI_Irecv" bytes="1024" orank="26" region="0" commid="0" count="3390" tid="0" op="" dtype="" >6.3515e-04 0.0000e+00 1.5020e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >8.5330e-04 1.1921e-06 8.1992e-04</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.8147e-05 3.8147e-05 3.8147e-05</hent>
<hent key="0380010000000000000007000000000A" call="MPI_Irecv" bytes="1792" orank="10" region="0" commid="0" count="44" tid="0" op="" dtype="" >1.1683e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000011" call="MPI_Irecv" bytes="1792" orank="17" region="0" commid="0" count="41" tid="0" op="" dtype="" >2.0504e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000013" call="MPI_Irecv" bytes="1792" orank="19" region="0" commid="0" count="38" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000007000000001A" call="MPI_Irecv" bytes="1792" orank="26" region="0" commid="0" count="37" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000052" call="MPI_Irecv" bytes="1792" orank="82" region="0" commid="0" count="140" tid="0" op="" dtype="" >6.8903e-05 0.0000e+00 9.0599e-06</hent>
<hent key="038001000000000000000700000002D2" call="MPI_Irecv" bytes="1792" orank="722" region="0" commid="0" count="116" tid="0" op="" dtype="" >6.2227e-05 0.0000e+00 5.0068e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >7.4625e-05 0.0000e+00 3.2902e-05</hent>
<hent key="0240010000000000000004000000000A" call="MPI_Isend" bytes="1024" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.8071e-03 0.0000e+00 2.5034e-05</hent>
<hent key="02400100000000000000040000000011" call="MPI_Isend" bytes="1024" orank="17" region="0" commid="0" count="1068" tid="0" op="" dtype="" >6.3777e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000040000000013" call="MPI_Isend" bytes="1024" orank="19" region="0" commid="0" count="834" tid="0" op="" dtype="" >6.8879e-04 0.0000e+00 7.1526e-06</hent>
<hent key="0240010000000000000004000000001A" call="MPI_Isend" bytes="1024" orank="26" region="0" commid="0" count="3380" tid="0" op="" dtype="" >1.1111e-02 9.5367e-07 1.9073e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >3.8147e-06 1.9073e-06 1.9073e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5481e+00 0.0000e+00 2.2689e+00</hent>
<hent key="038001000000000000000A000000000A" call="MPI_Irecv" bytes="2560" orank="10" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000011" call="MPI_Irecv" bytes="2560" orank="17" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000013" call="MPI_Irecv" bytes="2560" orank="19" region="0" commid="0" count="7" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000001A" call="MPI_Irecv" bytes="2560" orank="26" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000052" call="MPI_Irecv" bytes="2560" orank="82" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.1683e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002D2" call="MPI_Irecv" bytes="2560" orank="722" region="0" commid="0" count="43" tid="0" op="" dtype="" >3.3855e-05 0.0000e+00 1.5020e-05</hent>
<hent key="0240010000000000000007000000000A" call="MPI_Isend" bytes="1792" orank="10" region="0" commid="0" count="28" tid="0" op="" dtype="" >8.3447e-05 9.5367e-07 1.2875e-05</hent>
<hent key="02400100000000000000070000000011" call="MPI_Isend" bytes="1792" orank="17" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.0037e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000013" call="MPI_Isend" bytes="1792" orank="19" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.1611e-04 3.8147e-06 5.9605e-06</hent>
<hent key="0240010000000000000007000000001A" call="MPI_Isend" bytes="1792" orank="26" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.5845e-04 4.7684e-06 1.1921e-05</hent>
<hent key="02400100000000000000070000000052" call="MPI_Isend" bytes="1792" orank="82" region="0" commid="0" count="108" tid="0" op="" dtype="" >8.0729e-04 1.9073e-06 2.0981e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1364e-01 1.1364e-01 1.1364e-01</hent>
<hent key="024001000000000000000700000002D2" call="MPI_Isend" bytes="1792" orank="722" region="0" commid="0" count="127" tid="0" op="" dtype="" >7.3242e-04 2.1458e-06 1.5974e-05</hent>
<hent key="024001000000000000000A000000000A" call="MPI_Isend" bytes="2560" orank="10" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.7193e-05 1.9073e-06 1.2875e-05</hent>
<hent key="024001000000000000000A0000000011" call="MPI_Isend" bytes="2560" orank="17" region="0" commid="0" count="2" tid="0" op="" dtype="" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="024001000000000000000A0000000013" call="MPI_Isend" bytes="2560" orank="19" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.0981e-05 3.8147e-06 5.0068e-06</hent>
<hent key="024001000000000000000A000000001A" call="MPI_Isend" bytes="2560" orank="26" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.5034e-05 5.9605e-06 6.9141e-06</hent>
<hent key="024001000000000000000A0000000052" call="MPI_Isend" bytes="2560" orank="82" region="0" commid="0" count="33" tid="0" op="" dtype="" >2.3270e-04 5.0068e-06 1.2159e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.2224e-04 4.8161e-05 1.9193e-04</hent>
<hent key="024001000000000000000A00000002D2" call="MPI_Isend" bytes="2560" orank="722" region="0" commid="0" count="31" tid="0" op="" dtype="" >1.7786e-04 5.0068e-06 6.9141e-06</hent>
<hent key="03800100000000000000100000000052" call="MPI_Irecv" bytes="4096" orank="82" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.4808e-03 5.2595e-04 9.5487e-04</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.7922e-05 4.7922e-05 4.7922e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 2.1458e-06 3.8147e-06</hent>
<hent key="0380010000000000000000040000000A" call="MPI_Irecv" bytes="4" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.0630e-04 0.0000e+00 1.0967e-05</hent>
<hent key="03800100000000000000000400000011" call="MPI_Irecv" bytes="4" orank="17" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0695e-03 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000000400000013" call="MPI_Irecv" bytes="4" orank="19" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.1154e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000040000001A" call="MPI_Irecv" bytes="4" orank="26" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.6233e-04 0.0000e+00 1.5020e-05</hent>
<hent key="03800100000000000000000400000052" call="MPI_Irecv" bytes="4" orank="82" region="0" commid="0" count="3398" tid="0" op="" dtype="" >9.0742e-04 0.0000e+00 1.7881e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.8620e-03 3.6502e-04 1.0450e-03</hent>
<hent key="038001000000000000001C0000000011" call="MPI_Irecv" bytes="7168" orank="17" region="0" commid="0" count="9341" tid="0" op="" dtype="" >3.2670e-03 0.0000e+00 1.2875e-05</hent>
<hent key="038001000000000000001C0000000013" call="MPI_Irecv" bytes="7168" orank="19" region="0" commid="0" count="8074" tid="0" op="" dtype="" >1.4660e-03 0.0000e+00 8.1062e-06</hent>
<hent key="038001000000000000000004000002D2" call="MPI_Irecv" bytes="4" orank="722" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.0310e-04 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000000040000000A" call="MPI_Isend" bytes="4" orank="10" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3371e-03 0.0000e+00 3.2902e-05</hent>
<hent key="02400100000000000000000400000011" call="MPI_Isend" bytes="4" orank="17" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3554e-03 0.0000e+00 4.6968e-05</hent>
<hent key="02400100000000000000000400000013" call="MPI_Isend" bytes="4" orank="19" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.6417e-03 0.0000e+00 2.3127e-05</hent>
<hent key="0240010000000000000000040000001A" call="MPI_Isend" bytes="4" orank="26" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.7321e-02 4.7684e-06 3.0041e-05</hent>
<hent key="02400100000000000000000400000052" call="MPI_Isend" bytes="4" orank="82" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.2958e-02 3.8147e-06 4.6968e-05</hent>
<hent key="024001000000000000001C0000000011" call="MPI_Isend" bytes="7168" orank="17" region="0" commid="0" count="8650" tid="0" op="" dtype="" >1.8394e-02 9.5367e-07 1.0967e-05</hent>
<hent key="024001000000000000001C0000000013" call="MPI_Isend" bytes="7168" orank="19" region="0" commid="0" count="9652" tid="0" op="" dtype="" >3.0126e-02 9.5367e-07 2.5034e-05</hent>
<hent key="024001000000000000000004000002D2" call="MPI_Isend" bytes="4" orank="722" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.2850e-02 3.8147e-06 1.0705e-04</hent>
<hent key="0380010000000000000005000000000A" call="MPI_Irecv" bytes="1280" orank="10" region="0" commid="0" count="209" tid="0" op="" dtype="" >6.1512e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000011" call="MPI_Irecv" bytes="1280" orank="17" region="0" commid="0" count="183" tid="0" op="" dtype="" >7.4863e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000013" call="MPI_Irecv" bytes="1280" orank="19" region="0" commid="0" count="197" tid="0" op="" dtype="" >3.8147e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000001A" call="MPI_Irecv" bytes="1280" orank="26" region="0" commid="0" count="220" tid="0" op="" dtype="" >4.2439e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000052" call="MPI_Irecv" bytes="1280" orank="82" region="0" commid="0" count="303" tid="0" op="" dtype="" >1.1730e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0547e+01 1.0014e-05 1.5582e-01</hent>
<hent key="0380010000000000000028000000000A" call="MPI_Irecv" bytes="10240" orank="10" region="0" commid="0" count="52" tid="0" op="" dtype="" >2.1696e-05 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000028000000001A" call="MPI_Irecv" bytes="10240" orank="26" region="0" commid="0" count="32" tid="0" op="" dtype="" >5.4836e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002D2" call="MPI_Irecv" bytes="1280" orank="722" region="0" commid="0" count="311" tid="0" op="" dtype="" >1.4806e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0380010000000000000008000000000A" call="MPI_Irecv" bytes="2048" orank="10" region="0" commid="0" count="17" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000011" call="MPI_Irecv" bytes="2048" orank="17" region="0" commid="0" count="12" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000013" call="MPI_Irecv" bytes="2048" orank="19" region="0" commid="0" count="9" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000001A" call="MPI_Irecv" bytes="2048" orank="26" region="0" commid="0" count="15" tid="0" op="" dtype="" >4.5300e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000052" call="MPI_Irecv" bytes="2048" orank="82" region="0" commid="0" count="3437" tid="0" op="" dtype="" >6.9785e-04 0.0000e+00 1.0967e-05</hent>
<hent key="038001000000000000000800000002D2" call="MPI_Irecv" bytes="2048" orank="722" region="0" commid="0" count="3465" tid="0" op="" dtype="" >9.5391e-04 0.0000e+00 2.4080e-05</hent>
<hent key="0240010000000000000005000000000A" call="MPI_Isend" bytes="1280" orank="10" region="0" commid="0" count="209" tid="0" op="" dtype="" >3.8338e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000050000000011" call="MPI_Isend" bytes="1280" orank="17" region="0" commid="0" count="191" tid="0" op="" dtype="" >3.6120e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000013" call="MPI_Isend" bytes="1280" orank="19" region="0" commid="0" count="197" tid="0" op="" dtype="" >7.9870e-04 2.8610e-06 1.0014e-05</hent>
<hent key="0240010000000000000005000000001A" call="MPI_Isend" bytes="1280" orank="26" region="0" commid="0" count="178" tid="0" op="" dtype="" >9.7346e-04 9.5367e-07 1.1921e-05</hent>
<hent key="02400100000000000000050000000052" call="MPI_Isend" bytes="1280" orank="82" region="0" commid="0" count="303" tid="0" op="" dtype="" >1.9908e-03 4.0531e-06 1.7881e-05</hent>
<hent key="0240010000000000000028000000001A" call="MPI_Isend" bytes="10240" orank="26" region="0" commid="0" count="68" tid="0" op="" dtype="" >6.2871e-04 5.9605e-06 1.5020e-05</hent>
<hent key="024001000000000000000500000002D2" call="MPI_Isend" bytes="1280" orank="722" region="0" commid="0" count="300" tid="0" op="" dtype="" >1.6146e-03 3.8147e-06 1.4782e-05</hent>
</hash>
<internal rank="18" log_i="1723712895.682933" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="19" mpi_size="768" stamp_init="1723712829.548143" stamp_final="1723712895.685731" username="apac4" allocationname="unknown" flags="0" pid="684287" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61376e+01" utime="5.06550e+01" stime="7.53515e+00" mtime="3.01594e+01" gflop="0.00000e+00" gbyte="3.77964e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.01594e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a615a61551" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60066e+01" utime="5.06237e+01" stime="7.52360e+00" mtime="3.01594e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.01594e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5043e+08" > 4.8321e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4880e+08" > 3.1623e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.5532e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5838e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4080e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.8903e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5703e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.2848e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0420e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1348e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8808e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="191" >
<hent key="0240010000000000000008000000000B" call="MPI_Isend" bytes="2048" orank="11" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.9789e-05 9.5367e-07 1.0014e-05</hent>
<hent key="02400100000000000000080000000012" call="MPI_Isend" bytes="2048" orank="18" region="0" commid="0" count="9" tid="0" op="" dtype="" >2.4796e-05 2.1458e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000014" call="MPI_Isend" bytes="2048" orank="20" region="0" commid="0" count="10" tid="0" op="" dtype="" >3.8862e-05 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000008000000001B" call="MPI_Isend" bytes="2048" orank="27" region="0" commid="0" count="13" tid="0" op="" dtype="" >7.4863e-05 5.0068e-06 6.1989e-06</hent>
<hent key="02400100000000000000080000000053" call="MPI_Isend" bytes="2048" orank="83" region="0" commid="0" count="3481" tid="0" op="" dtype="" >1.0744e-02 9.5367e-07 1.3113e-05</hent>
<hent key="024001000000000000000800000002D3" call="MPI_Isend" bytes="2048" orank="723" region="0" commid="0" count="3463" tid="0" op="" dtype="" >1.1281e-02 9.5367e-07 1.6928e-05</hent>
<hent key="0380010000000000000002800000000B" call="MPI_Irecv" bytes="640" orank="11" region="0" commid="0" count="410" tid="0" op="" dtype="" >1.0872e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000012" call="MPI_Irecv" bytes="640" orank="18" region="0" commid="0" count="411" tid="0" op="" dtype="" >1.6212e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000014" call="MPI_Irecv" bytes="640" orank="20" region="0" commid="0" count="417" tid="0" op="" dtype="" >2.8181e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000002800000001B" call="MPI_Irecv" bytes="640" orank="27" region="0" commid="0" count="416" tid="0" op="" dtype="" >1.0753e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000053" call="MPI_Irecv" bytes="640" orank="83" region="0" commid="0" count="263" tid="0" op="" dtype="" >6.1989e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002D3" call="MPI_Irecv" bytes="640" orank="723" region="0" commid="0" count="260" tid="0" op="" dtype="" >7.5817e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000002800000000B" call="MPI_Isend" bytes="640" orank="11" region="0" commid="0" count="394" tid="0" op="" dtype="" >4.7040e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000028000000012" call="MPI_Isend" bytes="640" orank="18" region="0" commid="0" count="407" tid="0" op="" dtype="" >5.5742e-04 9.5367e-07 2.1458e-06</hent>
<hent key="02400100000000000000028000000014" call="MPI_Isend" bytes="640" orank="20" region="0" commid="0" count="419" tid="0" op="" dtype="" >1.3497e-03 1.9073e-06 7.1526e-06</hent>
<hent key="0240010000000000000002800000001B" call="MPI_Isend" bytes="640" orank="27" region="0" commid="0" count="410" tid="0" op="" dtype="" >1.9195e-03 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000028000000053" call="MPI_Isend" bytes="640" orank="83" region="0" commid="0" count="247" tid="0" op="" dtype="" >1.0610e-03 2.8610e-06 1.0967e-05</hent>
<hent key="024001000000000000000280000002D3" call="MPI_Isend" bytes="640" orank="723" region="0" commid="0" count="273" tid="0" op="" dtype="" >1.1380e-03 2.8610e-06 1.0014e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.1948e-05 3.1948e-05 3.1948e-05</hent>
<hent key="0380010000000000000001400000000B" call="MPI_Irecv" bytes="320" orank="11" region="0" commid="0" count="341" tid="0" op="" dtype="" >9.7752e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000012" call="MPI_Irecv" bytes="320" orank="18" region="0" commid="0" count="353" tid="0" op="" dtype="" >1.3733e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000014" call="MPI_Irecv" bytes="320" orank="20" region="0" commid="0" count="378" tid="0" op="" dtype="" >2.2817e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000001400000001B" call="MPI_Irecv" bytes="320" orank="27" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.0276e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000014000000053" call="MPI_Irecv" bytes="320" orank="83" region="0" commid="0" count="172" tid="0" op="" dtype="" >4.2200e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002D3" call="MPI_Irecv" bytes="320" orank="723" region="0" commid="0" count="197" tid="0" op="" dtype="" >6.0797e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.5532e+00 0.0000e+00 1.5381e-01</hent>
<hent key="03800100000000000000400000000053" call="MPI_Irecv" bytes="16384" orank="83" region="0" commid="0" count="12691" tid="0" op="" dtype="" >2.6574e-03 0.0000e+00 5.9605e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6443e-02 9.5367e-07 1.6426e-02</hent>
<hent key="038001000000000000004000000002D3" call="MPI_Irecv" bytes="16384" orank="723" region="0" commid="0" count="12699" tid="0" op="" dtype="" >6.2499e-03 0.0000e+00 2.1935e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.0981e-05 0.0000e+00 2.0981e-05</hent>
<hent key="0240010000000000000001400000000B" call="MPI_Isend" bytes="320" orank="11" region="0" commid="0" count="352" tid="0" op="" dtype="" >3.5882e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000014000000012" call="MPI_Isend" bytes="320" orank="18" region="0" commid="0" count="383" tid="0" op="" dtype="" >4.3225e-04 0.0000e+00 3.9101e-05</hent>
<hent key="02400100000000000000014000000014" call="MPI_Isend" bytes="320" orank="20" region="0" commid="0" count="383" tid="0" op="" dtype="" >1.1289e-03 1.9073e-06 8.1062e-06</hent>
<hent key="0240010000000000000001400000001B" call="MPI_Isend" bytes="320" orank="27" region="0" commid="0" count="352" tid="0" op="" dtype="" >1.5686e-03 3.8147e-06 5.9605e-06</hent>
<hent key="02400100000000000000014000000053" call="MPI_Isend" bytes="320" orank="83" region="0" commid="0" count="165" tid="0" op="" dtype="" >6.9594e-04 2.8610e-06 5.9605e-06</hent>
<hent key="0380010000000000000020000000000B" call="MPI_Irecv" bytes="8192" orank="11" region="0" commid="0" count="12674" tid="0" op="" dtype="" >4.3349e-03 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000200000000012" call="MPI_Irecv" bytes="8192" orank="18" region="0" commid="0" count="3047" tid="0" op="" dtype="" >1.0619e-03 0.0000e+00 1.0014e-05</hent>
<hent key="03800100000000000000200000000014" call="MPI_Irecv" bytes="8192" orank="20" region="0" commid="0" count="3669" tid="0" op="" dtype="" >4.9567e-04 0.0000e+00 3.0994e-06</hent>
<hent key="0380010000000000000020000000001B" call="MPI_Irecv" bytes="8192" orank="27" region="0" commid="0" count="12637" tid="0" op="" dtype="" >1.5032e-03 0.0000e+00 2.1458e-06</hent>
<hent key="024001000000000000000140000002D3" call="MPI_Isend" bytes="320" orank="723" region="0" commid="0" count="191" tid="0" op="" dtype="" >7.8607e-04 2.8610e-06 8.8215e-06</hent>
<hent key="0380010000000000000000000000000B" call="MPI_Irecv" bytes="0" orank="11" region="0" commid="0" count="237" tid="0" op="" dtype="" >5.9843e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000012" call="MPI_Irecv" bytes="0" orank="18" region="0" commid="0" count="260" tid="0" op="" dtype="" >8.7261e-05 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000000000000014" call="MPI_Irecv" bytes="0" orank="20" region="0" commid="0" count="247" tid="0" op="" dtype="" >1.0037e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000001B" call="MPI_Irecv" bytes="0" orank="27" region="0" commid="0" count="267" tid="0" op="" dtype="" >6.5088e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000000000053" call="MPI_Irecv" bytes="0" orank="83" region="0" commid="0" count="131" tid="0" op="" dtype="" >2.6703e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000053" call="MPI_Isend" bytes="16384" orank="83" region="0" commid="0" count="12683" tid="0" op="" dtype="" >8.3739e-02 3.8147e-06 6.8903e-05</hent>
<hent key="038001000000000000000000000002D3" call="MPI_Irecv" bytes="0" orank="723" region="0" commid="0" count="145" tid="0" op="" dtype="" >3.3617e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002D3" call="MPI_Isend" bytes="16384" orank="723" region="0" commid="0" count="12699" tid="0" op="" dtype="" >9.0961e-02 3.0994e-06 6.1989e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.2820e-04 0.0000e+00 2.7299e-04</hent>
<hent key="0240010000000000000020000000000B" call="MPI_Isend" bytes="8192" orank="11" region="0" commid="0" count="12699" tid="0" op="" dtype="" >5.8615e-03 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000200000000012" call="MPI_Isend" bytes="8192" orank="18" region="0" commid="0" count="4625" tid="0" op="" dtype="" >5.2929e-03 0.0000e+00 1.0014e-05</hent>
<hent key="02400100000000000000200000000014" call="MPI_Isend" bytes="8192" orank="20" region="0" commid="0" count="3819" tid="0" op="" dtype="" >7.0500e-03 0.0000e+00 2.2888e-05</hent>
<hent key="0240010000000000000020000000001B" call="MPI_Isend" bytes="8192" orank="27" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.1035e-01 3.8147e-06 5.9843e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >7.5102e-05 7.5102e-05 7.5102e-05</hent>
<hent key="0240010000000000000000000000000B" call="MPI_Isend" bytes="0" orank="11" region="0" commid="0" count="224" tid="0" op="" dtype="" >1.6046e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000000000000012" call="MPI_Isend" bytes="0" orank="18" region="0" commid="0" count="282" tid="0" op="" dtype="" >1.8167e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000014" call="MPI_Isend" bytes="0" orank="20" region="0" commid="0" count="259" tid="0" op="" dtype="" >5.8222e-04 9.5367e-07 6.1989e-05</hent>
<hent key="0240010000000000000000000000001B" call="MPI_Isend" bytes="0" orank="27" region="0" commid="0" count="250" tid="0" op="" dtype="" >9.2793e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000000000000053" call="MPI_Isend" bytes="0" orank="83" region="0" commid="0" count="149" tid="0" op="" dtype="" >5.5170e-04 9.5367e-07 7.1526e-06</hent>
<hent key="024001000000000000000000000002D3" call="MPI_Isend" bytes="0" orank="723" region="0" commid="0" count="149" tid="0" op="" dtype="" >5.7244e-04 1.1921e-06 5.0068e-06</hent>
<hent key="0380010000000000000006000000000B" call="MPI_Irecv" bytes="1536" orank="11" region="0" commid="0" count="110" tid="0" op="" dtype="" >3.0518e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000012" call="MPI_Irecv" bytes="1536" orank="18" region="0" commid="0" count="94" tid="0" op="" dtype="" >4.0293e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000014" call="MPI_Irecv" bytes="1536" orank="20" region="0" commid="0" count="96" tid="0" op="" dtype="" >6.9380e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000006000000001B" call="MPI_Irecv" bytes="1536" orank="27" region="0" commid="0" count="80" tid="0" op="" dtype="" >3.3617e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000053" call="MPI_Irecv" bytes="1536" orank="83" region="0" commid="0" count="205" tid="0" op="" dtype="" >5.4836e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002D3" call="MPI_Irecv" bytes="1536" orank="723" region="0" commid="0" count="235" tid="0" op="" dtype="" >8.7738e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0240010000000000000006000000000B" call="MPI_Isend" bytes="1536" orank="11" region="0" commid="0" count="110" tid="0" op="" dtype="" >2.4128e-04 9.5367e-07 1.5974e-05</hent>
<hent key="02400100000000000000060000000012" call="MPI_Isend" bytes="1536" orank="18" region="0" commid="0" count="65" tid="0" op="" dtype="" >1.3709e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000060000000014" call="MPI_Isend" bytes="1536" orank="20" region="0" commid="0" count="86" tid="0" op="" dtype="" >3.3426e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000006000000001B" call="MPI_Isend" bytes="1536" orank="27" region="0" commid="0" count="103" tid="0" op="" dtype="" >5.5313e-04 4.7684e-06 1.0967e-05</hent>
<hent key="02400100000000000000060000000053" call="MPI_Isend" bytes="1536" orank="83" region="0" commid="0" count="188" tid="0" op="" dtype="" >9.0384e-04 3.8147e-06 6.1989e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002D3" call="MPI_Isend" bytes="1536" orank="723" region="0" commid="0" count="222" tid="0" op="" dtype="" >1.0314e-03 3.0994e-06 1.0014e-05</hent>
<hent key="038001000000000000000C000000000B" call="MPI_Irecv" bytes="3072" orank="11" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000C0000000014" call="MPI_Irecv" bytes="3072" orank="20" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="038001000000000000000C0000000053" call="MPI_Irecv" bytes="3072" orank="83" region="0" commid="0" count="9" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002D3" call="MPI_Irecv" bytes="3072" orank="723" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C0000000053" call="MPI_Isend" bytes="3072" orank="83" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.7166e-05 5.0068e-06 6.1989e-06</hent>
<hent key="024001000000000000000C00000002D3" call="MPI_Isend" bytes="3072" orank="723" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.3855e-05 5.0068e-06 5.9605e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.9887e-04 3.9887e-04 3.9887e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.1696e-04 2.9802e-04 3.0994e-04</hent>
<hent key="0380010000000000000003800000000B" call="MPI_Irecv" bytes="896" orank="11" region="0" commid="0" count="325" tid="0" op="" dtype="" >8.9407e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000012" call="MPI_Irecv" bytes="896" orank="18" region="0" commid="0" count="2894" tid="0" op="" dtype="" >5.3835e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000014" call="MPI_Irecv" bytes="896" orank="20" region="0" commid="0" count="2727" tid="0" op="" dtype="" >5.8651e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000003800000001B" call="MPI_Irecv" bytes="896" orank="27" region="0" commid="0" count="339" tid="0" op="" dtype="" >1.0157e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000053" call="MPI_Irecv" bytes="896" orank="83" region="0" commid="0" count="340" tid="0" op="" dtype="" >9.1553e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0014e-05 1.0014e-05 1.0014e-05</hent>
<hent key="038001000000000000000380000002D3" call="MPI_Irecv" bytes="896" orank="723" region="0" commid="0" count="336" tid="0" op="" dtype="" >9.9659e-05 0.0000e+00 5.0068e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="03800100000000000000380000000053" call="MPI_Irecv" bytes="14336" orank="83" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.1921e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000003800000000B" call="MPI_Isend" bytes="896" orank="11" region="0" commid="0" count="319" tid="0" op="" dtype="" >4.7755e-04 9.5367e-07 1.1921e-05</hent>
<hent key="02400100000000000000038000000012" call="MPI_Isend" bytes="896" orank="18" region="0" commid="0" count="2469" tid="0" op="" dtype="" >1.8749e-03 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000038000000014" call="MPI_Isend" bytes="896" orank="20" region="0" commid="0" count="2680" tid="0" op="" dtype="" >2.8338e-03 0.0000e+00 2.0981e-05</hent>
<hent key="0240010000000000000003800000001B" call="MPI_Isend" bytes="896" orank="27" region="0" commid="0" count="339" tid="0" op="" dtype="" >1.6637e-03 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000038000000053" call="MPI_Isend" bytes="896" orank="83" region="0" commid="0" count="345" tid="0" op="" dtype="" >1.5266e-03 2.8610e-06 1.9073e-05</hent>
<hent key="024001000000000000000380000002D3" call="MPI_Isend" bytes="896" orank="723" region="0" commid="0" count="321" tid="0" op="" dtype="" >1.3525e-03 2.8610e-06 1.0967e-05</hent>
<hent key="02400100000000000000380000000053" call="MPI_Isend" bytes="14336" orank="83" region="0" commid="0" count="16" tid="0" op="" dtype="" >1.0180e-04 5.0068e-06 7.8678e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.8529e+00 1.5974e-05 2.0318e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.2208e-03 3.2208e-03 3.2208e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0420e-02 1.0420e-02 1.0420e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.5939e-02 2.5939e-02 2.5939e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.1083e-01 3.3920e-03 1.9096e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >2.6917e-04 2.6917e-04 2.6917e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5703e+00 3.8290e-04 2.5888e-01</hent>
<hent key="0380010000000000000004000000000B" call="MPI_Irecv" bytes="1024" orank="11" region="0" commid="0" count="3392" tid="0" op="" dtype="" >6.1131e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000012" call="MPI_Irecv" bytes="1024" orank="18" region="0" commid="0" count="834" tid="0" op="" dtype="" >1.2636e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000014" call="MPI_Irecv" bytes="1024" orank="20" region="0" commid="0" count="988" tid="0" op="" dtype="" >1.5092e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000004000000001B" call="MPI_Irecv" bytes="1024" orank="27" region="0" commid="0" count="3382" tid="0" op="" dtype="" >5.2452e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.4080e-05 0.0000e+00 1.1921e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.8147e-05 3.8147e-05 3.8147e-05</hent>
<hent key="0380010000000000000007000000000B" call="MPI_Irecv" bytes="1792" orank="11" region="0" commid="0" count="48" tid="0" op="" dtype="" >1.4782e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000012" call="MPI_Irecv" bytes="1792" orank="18" region="0" commid="0" count="27" tid="0" op="" dtype="" >1.1444e-05 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000070000000014" call="MPI_Irecv" bytes="1792" orank="20" region="0" commid="0" count="44" tid="0" op="" dtype="" >3.4094e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000007000000001B" call="MPI_Irecv" bytes="1792" orank="27" region="0" commid="0" count="29" tid="0" op="" dtype="" >1.0014e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000053" call="MPI_Irecv" bytes="1792" orank="83" region="0" commid="0" count="138" tid="0" op="" dtype="" >3.5763e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002D3" call="MPI_Irecv" bytes="1792" orank="723" region="0" commid="0" count="123" tid="0" op="" dtype="" >4.7922e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >4.3154e-05 0.0000e+00 2.7180e-05</hent>
<hent key="0240010000000000000004000000000B" call="MPI_Isend" bytes="1024" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.0826e-03 0.0000e+00 1.1921e-05</hent>
<hent key="02400100000000000000040000000012" call="MPI_Isend" bytes="1024" orank="18" region="0" commid="0" count="1240" tid="0" op="" dtype="" >8.2088e-04 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000040000000014" call="MPI_Isend" bytes="1024" orank="20" region="0" commid="0" count="1034" tid="0" op="" dtype="" >8.0681e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000004000000001B" call="MPI_Isend" bytes="1024" orank="27" region="0" commid="0" count="3398" tid="0" op="" dtype="" >9.2216e-03 9.5367e-07 1.0967e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >5.2452e-06 2.1458e-06 3.0994e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5667e+00 0.0000e+00 2.2689e+00</hent>
<hent key="038001000000000000000A000000000B" call="MPI_Irecv" bytes="2560" orank="11" region="0" commid="0" count="6" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000012" call="MPI_Irecv" bytes="2560" orank="18" region="0" commid="0" count="5" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000014" call="MPI_Irecv" bytes="2560" orank="20" region="0" commid="0" count="8" tid="0" op="" dtype="" >9.5367e-06 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000A000000001B" call="MPI_Irecv" bytes="2560" orank="27" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000053" call="MPI_Irecv" bytes="2560" orank="83" region="0" commid="0" count="43" tid="0" op="" dtype="" >1.3113e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002D3" call="MPI_Irecv" bytes="2560" orank="723" region="0" commid="0" count="36" tid="0" op="" dtype="" >1.7881e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000007000000000B" call="MPI_Isend" bytes="1792" orank="11" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.0657e-04 9.5367e-07 1.3113e-05</hent>
<hent key="02400100000000000000070000000012" call="MPI_Isend" bytes="1792" orank="18" region="0" commid="0" count="38" tid="0" op="" dtype="" >7.9393e-05 1.1921e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000014" call="MPI_Isend" bytes="1792" orank="20" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.5903e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000007000000001B" call="MPI_Isend" bytes="1792" orank="27" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.7953e-04 4.7684e-06 6.1989e-06</hent>
<hent key="02400100000000000000070000000053" call="MPI_Isend" bytes="1792" orank="83" region="0" commid="0" count="141" tid="0" op="" dtype="" >6.9714e-04 9.5367e-07 6.1989e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1348e-01 1.1348e-01 1.1348e-01</hent>
<hent key="024001000000000000000700000002D3" call="MPI_Isend" bytes="1792" orank="723" region="0" commid="0" count="127" tid="0" op="" dtype="" >6.0368e-04 3.8147e-06 6.1989e-06</hent>
<hent key="024001000000000000000A000000000B" call="MPI_Isend" bytes="2560" orank="11" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.5497e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000012" call="MPI_Isend" bytes="2560" orank="18" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.5020e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000014" call="MPI_Isend" bytes="2560" orank="20" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.6941e-05 3.8147e-06 5.0068e-06</hent>
<hent key="024001000000000000000A000000001B" call="MPI_Isend" bytes="2560" orank="27" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.6928e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000053" call="MPI_Isend" bytes="2560" orank="83" region="0" commid="0" count="59" tid="0" op="" dtype="" >3.2187e-04 4.0531e-06 6.9141e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.4608e-04 5.1022e-05 2.0099e-04</hent>
<hent key="024001000000000000000A00000002D3" call="MPI_Isend" bytes="2560" orank="723" region="0" commid="0" count="56" tid="0" op="" dtype="" >3.0231e-04 4.0531e-06 6.1989e-06</hent>
<hent key="038001000000000000001000000002D3" call="MPI_Irecv" bytes="4096" orank="723" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.5640e-03 5.5504e-04 1.0090e-03</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.6968e-05 4.6968e-05 4.6968e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.9605e-06 2.1458e-06 3.8147e-06</hent>
<hent key="0380010000000000000000040000000B" call="MPI_Irecv" bytes="4" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.8270e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000012" call="MPI_Irecv" bytes="4" orank="18" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.5827e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000000400000014" call="MPI_Irecv" bytes="4" orank="20" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.7183e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000040000001B" call="MPI_Irecv" bytes="4" orank="27" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.7040e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000053" call="MPI_Irecv" bytes="4" orank="83" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.6924e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.9579e-03 4.7207e-05 7.7486e-04</hent>
<hent key="038001000000000000001C0000000012" call="MPI_Irecv" bytes="7168" orank="18" region="0" commid="0" count="9652" tid="0" op="" dtype="" >3.4258e-03 0.0000e+00 6.1989e-06</hent>
<hent key="038001000000000000001C0000000014" call="MPI_Irecv" bytes="7168" orank="20" region="0" commid="0" count="9030" tid="0" op="" dtype="" >1.3180e-03 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000004000002D3" call="MPI_Irecv" bytes="4" orank="723" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.8031e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000000040000000B" call="MPI_Isend" bytes="4" orank="11" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2453e-03 0.0000e+00 2.8133e-05</hent>
<hent key="02400100000000000000000400000012" call="MPI_Isend" bytes="4" orank="18" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.1184e-03 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000000400000014" call="MPI_Isend" bytes="4" orank="20" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.2349e-03 0.0000e+00 5.8889e-05</hent>
<hent key="0240010000000000000000040000001B" call="MPI_Isend" bytes="4" orank="27" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3206e-02 3.8147e-06 2.8133e-05</hent>
<hent key="02400100000000000000000400000053" call="MPI_Isend" bytes="4" orank="83" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.9048e-02 3.8147e-06 4.6015e-05</hent>
<hent key="024001000000000000001C0000000012" call="MPI_Isend" bytes="7168" orank="18" region="0" commid="0" count="8074" tid="0" op="" dtype="" >1.8470e-02 9.5367e-07 2.7895e-05</hent>
<hent key="024001000000000000001C0000000014" call="MPI_Isend" bytes="7168" orank="20" region="0" commid="0" count="8880" tid="0" op="" dtype="" >2.6855e-02 9.5367e-07 2.1935e-05</hent>
<hent key="024001000000000000000004000002D3" call="MPI_Isend" bytes="4" orank="723" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7431e-02 3.8147e-06 1.0395e-04</hent>
<hent key="0380010000000000000005000000000B" call="MPI_Irecv" bytes="1280" orank="11" region="0" commid="0" count="216" tid="0" op="" dtype="" >7.3433e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000012" call="MPI_Irecv" bytes="1280" orank="18" region="0" commid="0" count="197" tid="0" op="" dtype="" >7.8440e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000014" call="MPI_Irecv" bytes="1280" orank="20" region="0" commid="0" count="171" tid="0" op="" dtype="" >1.2660e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000005000000001B" call="MPI_Irecv" bytes="1280" orank="27" region="0" commid="0" count="203" tid="0" op="" dtype="" >6.6519e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000053" call="MPI_Irecv" bytes="1280" orank="83" region="0" commid="0" count="329" tid="0" op="" dtype="" >8.0347e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0715e+01 9.7752e-06 1.5601e-01</hent>
<hent key="0380010000000000000028000000000B" call="MPI_Irecv" bytes="10240" orank="11" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.0014e-05 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000028000000001B" call="MPI_Irecv" bytes="10240" orank="27" region="0" commid="0" count="62" tid="0" op="" dtype="" >1.2159e-05 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000500000002D3" call="MPI_Irecv" bytes="1280" orank="723" region="0" commid="0" count="299" tid="0" op="" dtype="" >1.2612e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000B" call="MPI_Irecv" bytes="2048" orank="11" region="0" commid="0" count="11" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000012" call="MPI_Irecv" bytes="2048" orank="18" region="0" commid="0" count="22" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000014" call="MPI_Irecv" bytes="2048" orank="20" region="0" commid="0" count="20" tid="0" op="" dtype="" >1.3590e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000008000000001B" call="MPI_Irecv" bytes="2048" orank="27" region="0" commid="0" count="13" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000053" call="MPI_Irecv" bytes="2048" orank="83" region="0" commid="0" count="3467" tid="0" op="" dtype="" >5.2142e-04 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000800000002D3" call="MPI_Irecv" bytes="2048" orank="723" region="0" commid="0" count="3458" tid="0" op="" dtype="" >7.3481e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0240010000000000000005000000000B" call="MPI_Isend" bytes="1280" orank="11" region="0" commid="0" count="239" tid="0" op="" dtype="" >3.9625e-04 9.5367e-07 1.3828e-05</hent>
<hent key="02400100000000000000050000000012" call="MPI_Isend" bytes="1280" orank="18" region="0" commid="0" count="197" tid="0" op="" dtype="" >3.8266e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000050000000014" call="MPI_Isend" bytes="1280" orank="20" region="0" commid="0" count="181" tid="0" op="" dtype="" >6.7592e-04 1.9073e-06 5.0068e-06</hent>
<hent key="0240010000000000000005000000001B" call="MPI_Isend" bytes="1280" orank="27" region="0" commid="0" count="197" tid="0" op="" dtype="" >1.0211e-03 3.8147e-06 1.0967e-05</hent>
<hent key="02400100000000000000050000000053" call="MPI_Isend" bytes="1280" orank="83" region="0" commid="0" count="319" tid="0" op="" dtype="" >1.4365e-03 3.0994e-06 6.1989e-06</hent>
<hent key="024001000000000000000500000002D3" call="MPI_Isend" bytes="1280" orank="723" region="0" commid="0" count="289" tid="0" op="" dtype="" >1.2710e-03 3.8147e-06 8.8215e-06</hent>
</hash>
<internal rank="19" log_i="1723712895.685731" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="20" mpi_size="768" stamp_init="1723712829.548152" stamp_final="1723712895.689937" username="apac4" allocationname="unknown" flags="0" pid="684288" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61418e+01" utime="4.71116e+01" stime="9.22206e+00" mtime="2.95129e+01" gflop="0.00000e+00" gbyte="3.77499e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.95129e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60088e+01" utime="4.70752e+01" stime="9.21612e+00" mtime="2.95129e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.95129e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4964e+08" > 8.6064e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4957e+08" > 6.1751e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9323e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5733e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.6999e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1206e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5855e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.6087e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0361e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1363e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7364e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="198" >
<hent key="0240010000000000000008000000000C" call="MPI_Isend" bytes="2048" orank="12" region="0" commid="0" count="14" tid="0" op="" dtype="" >4.9114e-05 9.5367e-07 1.3113e-05</hent>
<hent key="02400100000000000000080000000013" call="MPI_Isend" bytes="2048" orank="19" region="0" commid="0" count="20" tid="0" op="" dtype="" >4.7684e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000015" call="MPI_Isend" bytes="2048" orank="21" region="0" commid="0" count="14" tid="0" op="" dtype="" >5.3883e-05 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000008000000001C" call="MPI_Isend" bytes="2048" orank="28" region="0" commid="0" count="11" tid="0" op="" dtype="" >7.5579e-05 5.9605e-06 1.2159e-05</hent>
<hent key="02400100000000000000080000000054" call="MPI_Isend" bytes="2048" orank="84" region="0" commid="0" count="3460" tid="0" op="" dtype="" >1.8683e-02 9.5367e-07 7.0095e-05</hent>
<hent key="024001000000000000000800000002D4" call="MPI_Isend" bytes="2048" orank="724" region="0" commid="0" count="3466" tid="0" op="" dtype="" >1.7312e-02 9.5367e-07 5.4121e-05</hent>
<hent key="024001000000000000000E0000000054" call="MPI_Isend" bytes="3584" orank="84" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.9605e-06 5.9605e-06 5.9605e-06</hent>
<hent key="0380010000000000000002800000000C" call="MPI_Irecv" bytes="640" orank="12" region="0" commid="0" count="399" tid="0" op="" dtype="" >1.1659e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000028000000013" call="MPI_Irecv" bytes="640" orank="19" region="0" commid="0" count="419" tid="0" op="" dtype="" >1.3304e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000015" call="MPI_Irecv" bytes="640" orank="21" region="0" commid="0" count="421" tid="0" op="" dtype="" >8.7261e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000001C" call="MPI_Irecv" bytes="640" orank="28" region="0" commid="0" count="420" tid="0" op="" dtype="" >1.7476e-04 0.0000e+00 2.0027e-05</hent>
<hent key="024001000000000000000E00000002D4" call="MPI_Isend" bytes="3584" orank="724" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.1921e-05 5.9605e-06 5.9605e-06</hent>
<hent key="03800100000000000000028000000054" call="MPI_Irecv" bytes="640" orank="84" region="0" commid="0" count="280" tid="0" op="" dtype="" >1.1110e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000280000002D4" call="MPI_Irecv" bytes="640" orank="724" region="0" commid="0" count="286" tid="0" op="" dtype="" >2.0337e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000002800000000C" call="MPI_Isend" bytes="640" orank="12" region="0" commid="0" count="389" tid="0" op="" dtype="" >5.3525e-04 9.5367e-07 7.8678e-06</hent>
<hent key="02400100000000000000028000000013" call="MPI_Isend" bytes="640" orank="19" region="0" commid="0" count="417" tid="0" op="" dtype="" >5.9938e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000028000000015" call="MPI_Isend" bytes="640" orank="21" region="0" commid="0" count="427" tid="0" op="" dtype="" >1.2841e-03 1.9073e-06 7.8678e-06</hent>
<hent key="0240010000000000000002800000001C" call="MPI_Isend" bytes="640" orank="28" region="0" commid="0" count="395" tid="0" op="" dtype="" >2.3801e-03 4.0531e-06 1.5974e-05</hent>
<hent key="02400100000000000000028000000054" call="MPI_Isend" bytes="640" orank="84" region="0" commid="0" count="268" tid="0" op="" dtype="" >1.3194e-03 3.8147e-06 2.0981e-05</hent>
<hent key="024001000000000000000280000002D4" call="MPI_Isend" bytes="640" orank="724" region="0" commid="0" count="279" tid="0" op="" dtype="" >1.4975e-03 3.8147e-06 2.1935e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-05 3.0994e-05 3.0994e-05</hent>
<hent key="0380010000000000000001400000000C" call="MPI_Irecv" bytes="320" orank="12" region="0" commid="0" count="390" tid="0" op="" dtype="" >1.2422e-04 0.0000e+00 7.1526e-06</hent>
<hent key="03800100000000000000014000000013" call="MPI_Irecv" bytes="320" orank="19" region="0" commid="0" count="383" tid="0" op="" dtype="" >1.4234e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000015" call="MPI_Irecv" bytes="320" orank="21" region="0" commid="0" count="357" tid="0" op="" dtype="" >8.0347e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000001C" call="MPI_Irecv" bytes="320" orank="28" region="0" commid="0" count="340" tid="0" op="" dtype="" >1.1039e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000014000000054" call="MPI_Irecv" bytes="320" orank="84" region="0" commid="0" count="178" tid="0" op="" dtype="" >5.7459e-05 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000140000002D4" call="MPI_Irecv" bytes="320" orank="724" region="0" commid="0" count="170" tid="0" op="" dtype="" >1.0800e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.9323e+00 0.0000e+00 1.5398e-01</hent>
<hent key="03800100000000000000400000000054" call="MPI_Irecv" bytes="16384" orank="84" region="0" commid="0" count="12624" tid="0" op="" dtype="" >5.1129e-03 0.0000e+00 4.6015e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6475e-02 0.0000e+00 1.6459e-02</hent>
<hent key="038001000000000000004000000002D4" call="MPI_Irecv" bytes="16384" orank="724" region="0" commid="0" count="12646" tid="0" op="" dtype="" >1.6822e-02 0.0000e+00 7.2002e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >6.6996e-05 0.0000e+00 6.6996e-05</hent>
<hent key="0240010000000000000001400000000C" call="MPI_Isend" bytes="320" orank="12" region="0" commid="0" count="364" tid="0" op="" dtype="" >4.0674e-04 0.0000e+00 7.8678e-06</hent>
<hent key="02400100000000000000014000000013" call="MPI_Isend" bytes="320" orank="19" region="0" commid="0" count="378" tid="0" op="" dtype="" >4.3583e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000014000000015" call="MPI_Isend" bytes="320" orank="21" region="0" commid="0" count="361" tid="0" op="" dtype="" >9.6416e-04 1.9073e-06 6.9141e-06</hent>
<hent key="0240010000000000000001400000001C" call="MPI_Isend" bytes="320" orank="28" region="0" commid="0" count="354" tid="0" op="" dtype="" >2.0394e-03 3.8147e-06 1.4067e-05</hent>
<hent key="02400100000000000000014000000054" call="MPI_Isend" bytes="320" orank="84" region="0" commid="0" count="191" tid="0" op="" dtype="" >1.0033e-03 3.0994e-06 1.7166e-05</hent>
<hent key="0380010000000000000020000000000C" call="MPI_Irecv" bytes="8192" orank="12" region="0" commid="0" count="12682" tid="0" op="" dtype="" >1.1306e-02 0.0000e+00 5.5075e-05</hent>
<hent key="03800100000000000000200000000013" call="MPI_Irecv" bytes="8192" orank="19" region="0" commid="0" count="3819" tid="0" op="" dtype="" >2.0061e-03 0.0000e+00 2.3842e-05</hent>
<hent key="03800100000000000000200000000015" call="MPI_Irecv" bytes="8192" orank="21" region="0" commid="0" count="3778" tid="0" op="" dtype="" >6.7210e-04 0.0000e+00 3.0041e-05</hent>
<hent key="0380010000000000000020000000001C" call="MPI_Irecv" bytes="8192" orank="28" region="0" commid="0" count="12578" tid="0" op="" dtype="" >4.1256e-03 0.0000e+00 4.3869e-05</hent>
<hent key="024001000000000000000140000002D4" call="MPI_Isend" bytes="320" orank="724" region="0" commid="0" count="178" tid="0" op="" dtype="" >9.1910e-04 3.8147e-06 1.8835e-05</hent>
<hent key="0380010000000000000000000000000C" call="MPI_Irecv" bytes="0" orank="12" region="0" commid="0" count="252" tid="0" op="" dtype="" >8.5115e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000000000013" call="MPI_Irecv" bytes="0" orank="19" region="0" commid="0" count="259" tid="0" op="" dtype="" >7.0095e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000015" call="MPI_Irecv" bytes="0" orank="21" region="0" commid="0" count="247" tid="0" op="" dtype="" >5.8174e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000001C" call="MPI_Irecv" bytes="0" orank="28" region="0" commid="0" count="269" tid="0" op="" dtype="" >8.3685e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000000000054" call="MPI_Irecv" bytes="0" orank="84" region="0" commid="0" count="148" tid="0" op="" dtype="" >4.8876e-05 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000400000000054" call="MPI_Isend" bytes="16384" orank="84" region="0" commid="0" count="12667" tid="0" op="" dtype="" >1.6397e-01 3.8147e-06 9.2030e-05</hent>
<hent key="038001000000000000000000000002D4" call="MPI_Irecv" bytes="0" orank="724" region="0" commid="0" count="137" tid="0" op="" dtype="" >3.7909e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002D4" call="MPI_Isend" bytes="16384" orank="724" region="0" commid="0" count="12691" tid="0" op="" dtype="" >1.5888e-01 3.8147e-06 8.3208e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.0889e-04 0.0000e+00 2.7514e-04</hent>
<hent key="0240010000000000000020000000000C" call="MPI_Isend" bytes="8192" orank="12" region="0" commid="0" count="12676" tid="0" op="" dtype="" >1.4605e-02 0.0000e+00 7.0095e-05</hent>
<hent key="02400100000000000000200000000013" call="MPI_Isend" bytes="8192" orank="19" region="0" commid="0" count="3669" tid="0" op="" dtype="" >4.6818e-03 0.0000e+00 5.9128e-05</hent>
<hent key="02400100000000000000200000000015" call="MPI_Isend" bytes="8192" orank="21" region="0" commid="0" count="3531" tid="0" op="" dtype="" >7.2734e-03 0.0000e+00 6.6996e-05</hent>
<hent key="0240010000000000000020000000001C" call="MPI_Isend" bytes="8192" orank="28" region="0" commid="0" count="12439" tid="0" op="" dtype="" >2.7451e-01 4.0531e-06 1.3399e-04</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >7.3910e-05 7.3910e-05 7.3910e-05</hent>
<hent key="0240010000000000000000000000000C" call="MPI_Isend" bytes="0" orank="12" region="0" commid="0" count="264" tid="0" op="" dtype="" >2.5773e-04 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000000000000013" call="MPI_Isend" bytes="0" orank="19" region="0" commid="0" count="247" tid="0" op="" dtype="" >1.9670e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000015" call="MPI_Isend" bytes="0" orank="21" region="0" commid="0" count="259" tid="0" op="" dtype="" >4.6921e-04 9.5367e-07 3.0994e-05</hent>
<hent key="0240010000000000000000000000001C" call="MPI_Isend" bytes="0" orank="28" region="0" commid="0" count="270" tid="0" op="" dtype="" >1.2424e-03 1.9073e-06 1.0967e-05</hent>
<hent key="02400100000000000000000000000054" call="MPI_Isend" bytes="0" orank="84" region="0" commid="0" count="145" tid="0" op="" dtype="" >6.6710e-04 9.5367e-07 1.6928e-05</hent>
<hent key="024001000000000000000000000002D4" call="MPI_Isend" bytes="0" orank="724" region="0" commid="0" count="150" tid="0" op="" dtype="" >7.2479e-04 2.8610e-06 1.3113e-05</hent>
<hent key="0380010000000000000006000000000C" call="MPI_Irecv" bytes="1536" orank="12" region="0" commid="0" count="89" tid="0" op="" dtype="" >2.3365e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000013" call="MPI_Irecv" bytes="1536" orank="19" region="0" commid="0" count="86" tid="0" op="" dtype="" >2.4796e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000015" call="MPI_Irecv" bytes="1536" orank="21" region="0" commid="0" count="72" tid="0" op="" dtype="" >9.2983e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000001C" call="MPI_Irecv" bytes="1536" orank="28" region="0" commid="0" count="89" tid="0" op="" dtype="" >3.2663e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000054" call="MPI_Irecv" bytes="1536" orank="84" region="0" commid="0" count="234" tid="0" op="" dtype="" >7.2241e-05 0.0000e+00 7.8678e-06</hent>
<hent key="038001000000000000000600000002D4" call="MPI_Irecv" bytes="1536" orank="724" region="0" commid="0" count="238" tid="0" op="" dtype="" >1.8740e-04 0.0000e+00 7.8678e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0240010000000000000006000000000C" call="MPI_Isend" bytes="1536" orank="12" region="0" commid="0" count="89" tid="0" op="" dtype="" >1.7691e-04 9.5367e-07 1.5020e-05</hent>
<hent key="02400100000000000000060000000013" call="MPI_Isend" bytes="1536" orank="19" region="0" commid="0" count="96" tid="0" op="" dtype="" >2.0647e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000015" call="MPI_Isend" bytes="1536" orank="21" region="0" commid="0" count="80" tid="0" op="" dtype="" >2.9731e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0240010000000000000006000000001C" call="MPI_Isend" bytes="1536" orank="28" region="0" commid="0" count="103" tid="0" op="" dtype="" >6.5875e-04 4.7684e-06 1.5020e-05</hent>
<hent key="02400100000000000000060000000054" call="MPI_Isend" bytes="1536" orank="84" region="0" commid="0" count="212" tid="0" op="" dtype="" >1.1699e-03 3.8147e-06 1.9073e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002D4" call="MPI_Isend" bytes="1536" orank="724" region="0" commid="0" count="227" tid="0" op="" dtype="" >1.2310e-03 3.8147e-06 1.5020e-05</hent>
<hent key="038001000000000000000C0000000054" call="MPI_Irecv" bytes="3072" orank="84" region="0" commid="0" count="8" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002D4" call="MPI_Irecv" bytes="3072" orank="724" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.9073e-06 9.5367e-07 9.5367e-07</hent>
<hent key="024001000000000000000C000000000C" call="MPI_Isend" bytes="3072" orank="12" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.0967e-05 1.0967e-05 1.0967e-05</hent>
<hent key="024001000000000000000C0000000013" call="MPI_Isend" bytes="3072" orank="19" region="0" commid="0" count="1" tid="0" op="" dtype="" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="024001000000000000000C0000000015" call="MPI_Isend" bytes="3072" orank="21" region="0" commid="0" count="1" tid="0" op="" dtype="" >4.0531e-06 4.0531e-06 4.0531e-06</hent>
<hent key="024001000000000000000C000000001C" call="MPI_Isend" bytes="3072" orank="28" region="0" commid="0" count="2" tid="0" op="" dtype="" >1.4067e-05 6.9141e-06 7.1526e-06</hent>
<hent key="024001000000000000000C0000000054" call="MPI_Isend" bytes="3072" orank="84" region="0" commid="0" count="10" tid="0" op="" dtype="" >6.0081e-05 5.0068e-06 1.0014e-05</hent>
<hent key="024001000000000000000C00000002D4" call="MPI_Isend" bytes="3072" orank="724" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.5048e-05 5.0068e-06 6.9141e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.1890e-04 4.1890e-04 4.1890e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >9.6989e-04 3.1400e-04 3.3092e-04</hent>
<hent key="0380010000000000000003800000000C" call="MPI_Irecv" bytes="896" orank="12" region="0" commid="0" count="308" tid="0" op="" dtype="" >6.8903e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000013" call="MPI_Irecv" bytes="896" orank="19" region="0" commid="0" count="2680" tid="0" op="" dtype="" >1.0185e-03 0.0000e+00 1.5974e-05</hent>
<hent key="03800100000000000000038000000015" call="MPI_Irecv" bytes="896" orank="21" region="0" commid="0" count="2714" tid="0" op="" dtype="" >6.4969e-04 0.0000e+00 3.1948e-05</hent>
<hent key="0380010000000000000003800000001C" call="MPI_Irecv" bytes="896" orank="28" region="0" commid="0" count="324" tid="0" op="" dtype="" >1.2112e-04 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000038000000054" call="MPI_Irecv" bytes="896" orank="84" region="0" commid="0" count="310" tid="0" op="" dtype="" >1.0538e-04 0.0000e+00 7.1526e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.8930e-05 8.8930e-05 8.8930e-05</hent>
<hent key="038001000000000000000380000002D4" call="MPI_Irecv" bytes="896" orank="724" region="0" commid="0" count="311" tid="0" op="" dtype="" >2.0480e-04 0.0000e+00 7.8678e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >2.1458e-06 2.1458e-06 2.1458e-06</hent>
<hent key="03800100000000000000380000000054" call="MPI_Irecv" bytes="14336" orank="84" region="0" commid="0" count="75" tid="0" op="" dtype="" >1.9550e-05 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000003800000002D4" call="MPI_Irecv" bytes="14336" orank="724" region="0" commid="0" count="53" tid="0" op="" dtype="" >7.7963e-05 0.0000e+00 1.8120e-05</hent>
<hent key="0240010000000000000003800000000C" call="MPI_Isend" bytes="896" orank="12" region="0" commid="0" count="338" tid="0" op="" dtype="" >5.6148e-04 9.5367e-07 8.1062e-06</hent>
<hent key="02400100000000000000038000000013" call="MPI_Isend" bytes="896" orank="19" region="0" commid="0" count="2727" tid="0" op="" dtype="" >2.2101e-03 0.0000e+00 2.5988e-05</hent>
<hent key="02400100000000000000038000000015" call="MPI_Isend" bytes="896" orank="21" region="0" commid="0" count="2770" tid="0" op="" dtype="" >3.0794e-03 0.0000e+00 2.5034e-05</hent>
<hent key="0240010000000000000003800000001C" call="MPI_Isend" bytes="896" orank="28" region="0" commid="0" count="312" tid="0" op="" dtype="" >1.9159e-03 4.7684e-06 1.3828e-05</hent>
<hent key="02400100000000000000038000000054" call="MPI_Isend" bytes="896" orank="84" region="0" commid="0" count="300" tid="0" op="" dtype="" >1.5376e-03 3.0994e-06 1.9073e-05</hent>
<hent key="024001000000000000000380000002D4" call="MPI_Isend" bytes="896" orank="724" region="0" commid="0" count="326" tid="0" op="" dtype="" >1.7045e-03 3.8147e-06 1.8120e-05</hent>
<hent key="02400100000000000000380000000054" call="MPI_Isend" bytes="14336" orank="84" region="0" commid="0" count="32" tid="0" op="" dtype="" >3.5191e-04 5.0068e-06 3.1948e-05</hent>
<hent key="024001000000000000003800000002D4" call="MPI_Isend" bytes="14336" orank="724" region="0" commid="0" count="8" tid="0" op="" dtype="" >8.9645e-05 5.0068e-06 2.5034e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >6.5657e+00 7.8678e-06 2.0307e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.2020e-03 3.2020e-03 3.2020e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0361e-02 1.0361e-02 1.0361e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.5989e-02 2.5989e-02 2.5989e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.0183e-01 2.0640e-03 1.8376e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.6407e-04 3.6407e-04 3.6407e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5855e+00 4.5204e-04 2.5048e-01</hent>
<hent key="0380010000000000000004000000000C" call="MPI_Irecv" bytes="1024" orank="12" region="0" commid="0" count="3394" tid="0" op="" dtype="" >9.7346e-04 0.0000e+00 2.0981e-05</hent>
<hent key="03800100000000000000040000000013" call="MPI_Irecv" bytes="1024" orank="19" region="0" commid="0" count="1034" tid="0" op="" dtype="" >3.3498e-04 0.0000e+00 1.1921e-05</hent>
<hent key="03800100000000000000040000000015" call="MPI_Irecv" bytes="1024" orank="21" region="0" commid="0" count="1014" tid="0" op="" dtype="" >1.7834e-04 0.0000e+00 1.6212e-05</hent>
<hent key="0380010000000000000004000000001C" call="MPI_Irecv" bytes="1024" orank="28" region="0" commid="0" count="3360" tid="0" op="" dtype="" >9.1434e-04 0.0000e+00 1.8120e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >4.6999e-03 1.1921e-06 3.1519e-03</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.9141e-06 6.9141e-06 6.9141e-06</hent>
<hent key="0380010000000000000007000000000C" call="MPI_Irecv" bytes="1792" orank="12" region="0" commid="0" count="42" tid="0" op="" dtype="" >1.9312e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000013" call="MPI_Irecv" bytes="1792" orank="19" region="0" commid="0" count="39" tid="0" op="" dtype="" >1.5020e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000015" call="MPI_Irecv" bytes="1792" orank="21" region="0" commid="0" count="35" tid="0" op="" dtype="" >4.5300e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000001C" call="MPI_Irecv" bytes="1792" orank="28" region="0" commid="0" count="49" tid="0" op="" dtype="" >2.0742e-05 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000070000000054" call="MPI_Irecv" bytes="1792" orank="84" region="0" commid="0" count="139" tid="0" op="" dtype="" >5.0306e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002D4" call="MPI_Irecv" bytes="1792" orank="724" region="0" commid="0" count="179" tid="0" op="" dtype="" >1.3947e-04 0.0000e+00 1.0014e-05</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >5.0783e-05 0.0000e+00 2.6226e-05</hent>
<hent key="0240010000000000000004000000000C" call="MPI_Isend" bytes="1024" orank="12" region="0" commid="0" count="3392" tid="0" op="" dtype="" >4.0071e-03 0.0000e+00 5.6028e-05</hent>
<hent key="02400100000000000000040000000013" call="MPI_Isend" bytes="1024" orank="19" region="0" commid="0" count="988" tid="0" op="" dtype="" >7.8988e-04 0.0000e+00 2.2888e-05</hent>
<hent key="02400100000000000000040000000015" call="MPI_Isend" bytes="1024" orank="21" region="0" commid="0" count="956" tid="0" op="" dtype="" >8.5974e-04 0.0000e+00 1.8120e-05</hent>
<hent key="0240010000000000000004000000001C" call="MPI_Isend" bytes="1024" orank="28" region="0" commid="0" count="3324" tid="0" op="" dtype="" >1.3399e-02 9.5367e-07 5.5075e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >4.0531e-06 1.9073e-06 2.1458e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5561e+00 0.0000e+00 2.2673e+00</hent>
<hent key="038001000000000000000A000000000C" call="MPI_Irecv" bytes="2560" orank="12" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000013" call="MPI_Irecv" bytes="2560" orank="19" region="0" commid="0" count="6" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000015" call="MPI_Irecv" bytes="2560" orank="21" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000001C" call="MPI_Irecv" bytes="2560" orank="28" region="0" commid="0" count="2" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000054" call="MPI_Irecv" bytes="2560" orank="84" region="0" commid="0" count="45" tid="0" op="" dtype="" >1.4305e-05 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A00000002D4" call="MPI_Irecv" bytes="2560" orank="724" region="0" commid="0" count="48" tid="0" op="" dtype="" >4.4346e-05 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000007000000000C" call="MPI_Isend" bytes="1792" orank="12" region="0" commid="0" count="47" tid="0" op="" dtype="" >1.5712e-04 9.5367e-07 1.8120e-05</hent>
<hent key="02400100000000000000070000000013" call="MPI_Isend" bytes="1792" orank="19" region="0" commid="0" count="44" tid="0" op="" dtype="" >9.5844e-05 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000070000000015" call="MPI_Isend" bytes="1792" orank="21" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.1301e-04 2.8610e-06 9.0599e-06</hent>
<hent key="0240010000000000000007000000001C" call="MPI_Isend" bytes="1792" orank="28" region="0" commid="0" count="33" tid="0" op="" dtype="" >2.1744e-04 4.7684e-06 1.2875e-05</hent>
<hent key="02400100000000000000070000000054" call="MPI_Isend" bytes="1792" orank="84" region="0" commid="0" count="137" tid="0" op="" dtype="" >7.4506e-04 9.5367e-07 1.8120e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1363e-01 1.1363e-01 1.1363e-01</hent>
<hent key="024001000000000000000700000002D4" call="MPI_Isend" bytes="1792" orank="724" region="0" commid="0" count="133" tid="0" op="" dtype="" >7.6580e-04 3.0994e-06 1.4067e-05</hent>
<hent key="024001000000000000000A000000000C" call="MPI_Isend" bytes="2560" orank="12" region="0" commid="0" count="4" tid="0" op="" dtype="" >2.6703e-05 9.5367e-07 1.7881e-05</hent>
<hent key="024001000000000000000A0000000013" call="MPI_Isend" bytes="2560" orank="19" region="0" commid="0" count="8" tid="0" op="" dtype="" >1.9550e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000015" call="MPI_Isend" bytes="2560" orank="21" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.8835e-05 2.8610e-06 5.0068e-06</hent>
<hent key="024001000000000000000A000000001C" call="MPI_Isend" bytes="2560" orank="28" region="0" commid="0" count="6" tid="0" op="" dtype="" >4.7684e-05 5.9605e-06 1.2875e-05</hent>
<hent key="024001000000000000000A0000000054" call="MPI_Isend" bytes="2560" orank="84" region="0" commid="0" count="51" tid="0" op="" dtype="" >3.1018e-04 4.0531e-06 1.2875e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.6897e-04 5.3883e-05 2.1219e-04</hent>
<hent key="024001000000000000000A00000002D4" call="MPI_Isend" bytes="2560" orank="724" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.8038e-04 4.0531e-06 1.4067e-05</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.6420e-03 5.8293e-04 1.0591e-03</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.4107e-05 4.4107e-05 4.4107e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >6.9141e-06 1.9073e-06 5.0068e-06</hent>
<hent key="0380010000000000000000040000000C" call="MPI_Irecv" bytes="4" orank="12" region="0" commid="0" count="3398" tid="0" op="" dtype="" >9.8491e-04 0.0000e+00 1.2875e-05</hent>
<hent key="03800100000000000000000400000013" call="MPI_Irecv" bytes="4" orank="19" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2393e-03 0.0000e+00 4.4823e-05</hent>
<hent key="03800100000000000000000400000015" call="MPI_Irecv" bytes="4" orank="21" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.0545e-04 0.0000e+00 1.4067e-05</hent>
<hent key="0380010000000000000000040000001C" call="MPI_Irecv" bytes="4" orank="28" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.2517e-04 0.0000e+00 1.0967e-05</hent>
<hent key="03800100000000000000000400000054" call="MPI_Irecv" bytes="4" orank="84" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0138e-03 0.0000e+00 1.7166e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.1071e-03 4.0197e-04 1.1051e-03</hent>
<hent key="038001000000000000001C0000000013" call="MPI_Irecv" bytes="7168" orank="19" region="0" commid="0" count="8880" tid="0" op="" dtype="" >4.5238e-03 0.0000e+00 2.5034e-05</hent>
<hent key="038001000000000000001C0000000015" call="MPI_Irecv" bytes="7168" orank="21" region="0" commid="0" count="8921" tid="0" op="" dtype="" >1.6110e-03 0.0000e+00 2.9087e-05</hent>
<hent key="038001000000000000000004000002D4" call="MPI_Irecv" bytes="4" orank="724" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0493e-03 0.0000e+00 1.0967e-05</hent>
<hent key="0240010000000000000000040000000C" call="MPI_Isend" bytes="4" orank="12" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.7078e-03 0.0000e+00 3.6955e-05</hent>
<hent key="02400100000000000000000400000013" call="MPI_Isend" bytes="4" orank="19" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4083e-03 0.0000e+00 4.4107e-05</hent>
<hent key="02400100000000000000000400000015" call="MPI_Isend" bytes="4" orank="21" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.7261e-03 0.0000e+00 1.2159e-05</hent>
<hent key="0240010000000000000000040000001C" call="MPI_Isend" bytes="4" orank="28" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.9768e-02 3.8147e-06 6.1989e-05</hent>
<hent key="02400100000000000000000400000054" call="MPI_Isend" bytes="4" orank="84" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.5789e-02 3.8147e-06 6.1989e-05</hent>
<hent key="024001000000000000001C0000000013" call="MPI_Isend" bytes="7168" orank="19" region="0" commid="0" count="9030" tid="0" op="" dtype="" >2.1234e-02 9.5367e-07 4.3869e-05</hent>
<hent key="024001000000000000001C0000000015" call="MPI_Isend" bytes="7168" orank="21" region="0" commid="0" count="9168" tid="0" op="" dtype="" >2.8236e-02 9.5367e-07 2.6941e-05</hent>
<hent key="024001000000000000000004000002D4" call="MPI_Isend" bytes="4" orank="724" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.4055e-02 3.8147e-06 1.3113e-04</hent>
<hent key="0380010000000000000005000000000C" call="MPI_Irecv" bytes="1280" orank="12" region="0" commid="0" count="194" tid="0" op="" dtype="" >6.3419e-05 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000050000000013" call="MPI_Irecv" bytes="1280" orank="19" region="0" commid="0" count="181" tid="0" op="" dtype="" >7.0572e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000015" call="MPI_Irecv" bytes="1280" orank="21" region="0" commid="0" count="216" tid="0" op="" dtype="" >4.3154e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000001C" call="MPI_Irecv" bytes="1280" orank="28" region="0" commid="0" count="233" tid="0" op="" dtype="" >8.9645e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000050000000054" call="MPI_Irecv" bytes="1280" orank="84" region="0" commid="0" count="291" tid="0" op="" dtype="" >1.0633e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0567e+01 1.0967e-05 1.5547e-01</hent>
<hent key="0380010000000000000028000000000C" call="MPI_Irecv" bytes="10240" orank="12" region="0" commid="0" count="17" tid="0" op="" dtype="" >6.9141e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000028000000001C" call="MPI_Irecv" bytes="10240" orank="28" region="0" commid="0" count="121" tid="0" op="" dtype="" >2.5272e-05 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000500000002D4" call="MPI_Irecv" bytes="1280" orank="724" region="0" commid="0" count="286" tid="0" op="" dtype="" >2.2054e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0380010000000000000008000000000C" call="MPI_Irecv" bytes="2048" orank="12" region="0" commid="0" count="25" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000013" call="MPI_Irecv" bytes="2048" orank="19" region="0" commid="0" count="10" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000015" call="MPI_Irecv" bytes="2048" orank="21" region="0" commid="0" count="12" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000001C" call="MPI_Irecv" bytes="2048" orank="28" region="0" commid="0" count="11" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000054" call="MPI_Irecv" bytes="2048" orank="84" region="0" commid="0" count="3464" tid="0" op="" dtype="" >9.5177e-04 0.0000e+00 3.1948e-05</hent>
<hent key="038001000000000000000800000002D4" call="MPI_Irecv" bytes="2048" orank="724" region="0" commid="0" count="3440" tid="0" op="" dtype="" >1.1954e-03 0.0000e+00 3.9101e-05</hent>
<hent key="0240010000000000000005000000000C" call="MPI_Isend" bytes="1280" orank="12" region="0" commid="0" count="195" tid="0" op="" dtype="" >3.6192e-04 0.0000e+00 2.0027e-05</hent>
<hent key="02400100000000000000050000000013" call="MPI_Isend" bytes="1280" orank="19" region="0" commid="0" count="171" tid="0" op="" dtype="" >3.2711e-04 9.5367e-07 5.0068e-06</hent>
<hent key="02400100000000000000050000000015" call="MPI_Isend" bytes="1280" orank="21" region="0" commid="0" count="196" tid="0" op="" dtype="" >7.1216e-04 2.8610e-06 1.7881e-05</hent>
<hent key="0240010000000000000005000000001C" call="MPI_Isend" bytes="1280" orank="28" region="0" commid="0" count="287" tid="0" op="" dtype="" >1.6191e-03 9.5367e-07 2.5034e-05</hent>
<hent key="02400100000000000000050000000054" call="MPI_Isend" bytes="1280" orank="84" region="0" commid="0" count="322" tid="0" op="" dtype="" >1.6870e-03 3.8147e-06 2.1219e-05</hent>
<hent key="0240010000000000000028000000000C" call="MPI_Isend" bytes="10240" orank="12" region="0" commid="0" count="23" tid="0" op="" dtype="" >4.6968e-05 0.0000e+00 2.1935e-05</hent>
<hent key="0240010000000000000028000000001C" call="MPI_Isend" bytes="10240" orank="28" region="0" commid="0" count="260" tid="0" op="" dtype="" >5.0800e-03 5.0068e-06 4.6015e-05</hent>
<hent key="024001000000000000000500000002D4" call="MPI_Isend" bytes="1280" orank="724" region="0" commid="0" count="287" tid="0" op="" dtype="" >1.6122e-03 3.8147e-06 2.6941e-05</hent>
</hash>
<internal rank="20" log_i="1723712895.689937" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="21" mpi_size="768" stamp_init="1723712829.548188" stamp_final="1723712895.681780" username="apac4" allocationname="unknown" flags="0" pid="684289" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61336e+01" utime="5.08294e+01" stime="7.68043e+00" mtime="3.08484e+01" gflop="0.00000e+00" gbyte="3.77968e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.08484e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004614785546144614ac" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60004e+01" utime="5.07944e+01" stime="7.67168e+00" mtime="3.08484e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.08484e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4989e+08" > 4.9329e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4889e+08" > 3.2511e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2752e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5755e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7180e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.9114e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5853e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.5987e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0432e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1348e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8756e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="194" >
<hent key="0240010000000000000008000000000D" call="MPI_Isend" bytes="2048" orank="13" region="0" commid="0" count="19" tid="0" op="" dtype="" >6.8426e-05 9.5367e-07 1.5020e-05</hent>
<hent key="02400100000000000000080000000014" call="MPI_Isend" bytes="2048" orank="20" region="0" commid="0" count="12" tid="0" op="" dtype="" >3.2663e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000016" call="MPI_Isend" bytes="2048" orank="22" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.1008e-05 3.8147e-06 9.0599e-06</hent>
<hent key="0240010000000000000008000000001D" call="MPI_Isend" bytes="2048" orank="29" region="0" commid="0" count="9" tid="0" op="" dtype="" >4.9829e-05 5.0068e-06 5.9605e-06</hent>
<hent key="02400100000000000000080000000055" call="MPI_Isend" bytes="2048" orank="85" region="0" commid="0" count="3477" tid="0" op="" dtype="" >1.0605e-02 9.5367e-07 1.6928e-05</hent>
<hent key="024001000000000000000800000002D5" call="MPI_Isend" bytes="2048" orank="725" region="0" commid="0" count="3455" tid="0" op="" dtype="" >1.0772e-02 9.5367e-07 2.4080e-05</hent>
<hent key="038001000000000000000E00000002D5" call="MPI_Irecv" bytes="3584" orank="725" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0380010000000000000002800000000D" call="MPI_Irecv" bytes="640" orank="13" region="0" commid="0" count="374" tid="0" op="" dtype="" >1.1516e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000014" call="MPI_Irecv" bytes="640" orank="20" region="0" commid="0" count="427" tid="0" op="" dtype="" >2.2817e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000028000000016" call="MPI_Irecv" bytes="640" orank="22" region="0" commid="0" count="424" tid="0" op="" dtype="" >1.4043e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000001D" call="MPI_Irecv" bytes="640" orank="29" region="0" commid="0" count="411" tid="0" op="" dtype="" >8.2731e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000055" call="MPI_Irecv" bytes="640" orank="85" region="0" commid="0" count="272" tid="0" op="" dtype="" >6.4373e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="038001000000000000000280000002D5" call="MPI_Irecv" bytes="640" orank="725" region="0" commid="0" count="296" tid="0" op="" dtype="" >1.0252e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000002800000000D" call="MPI_Isend" bytes="640" orank="13" region="0" commid="0" count="406" tid="0" op="" dtype="" >5.2404e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000028000000014" call="MPI_Isend" bytes="640" orank="20" region="0" commid="0" count="421" tid="0" op="" dtype="" >6.1679e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000028000000016" call="MPI_Isend" bytes="640" orank="22" region="0" commid="0" count="435" tid="0" op="" dtype="" >1.5116e-03 1.9073e-06 2.6941e-05</hent>
<hent key="0240010000000000000002800000001D" call="MPI_Isend" bytes="640" orank="29" region="0" commid="0" count="456" tid="0" op="" dtype="" >2.1775e-03 3.8147e-06 7.8678e-06</hent>
<hent key="02400100000000000000028000000055" call="MPI_Isend" bytes="640" orank="85" region="0" commid="0" count="251" tid="0" op="" dtype="" >1.0669e-03 2.8610e-06 5.9605e-06</hent>
<hent key="024001000000000000000280000002D5" call="MPI_Isend" bytes="640" orank="725" region="0" commid="0" count="281" tid="0" op="" dtype="" >1.1857e-03 2.8610e-06 9.0599e-06</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.2187e-05 3.2187e-05 3.2187e-05</hent>
<hent key="0380010000000000000001400000000D" call="MPI_Irecv" bytes="320" orank="13" region="0" commid="0" count="361" tid="0" op="" dtype="" >8.7261e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000014" call="MPI_Irecv" bytes="320" orank="20" region="0" commid="0" count="361" tid="0" op="" dtype="" >1.8072e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000016" call="MPI_Irecv" bytes="320" orank="22" region="0" commid="0" count="339" tid="0" op="" dtype="" >9.3937e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000001D" call="MPI_Irecv" bytes="320" orank="29" region="0" commid="0" count="353" tid="0" op="" dtype="" >8.0347e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000055" call="MPI_Irecv" bytes="320" orank="85" region="0" commid="0" count="169" tid="0" op="" dtype="" >3.6240e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002D5" call="MPI_Irecv" bytes="320" orank="725" region="0" commid="0" count="168" tid="0" op="" dtype="" >5.0068e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.2752e+00 0.0000e+00 1.8246e-01</hent>
<hent key="03800100000000000000400000000055" call="MPI_Irecv" bytes="16384" orank="85" region="0" commid="0" count="12517" tid="0" op="" dtype="" >2.2590e-03 0.0000e+00 4.0531e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6441e-02 9.5367e-07 1.6424e-02</hent>
<hent key="038001000000000000004000000002D5" call="MPI_Irecv" bytes="16384" orank="725" region="0" commid="0" count="12618" tid="0" op="" dtype="" >6.2118e-03 0.0000e+00 8.1062e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 1.9073e-06</hent>
<hent key="0240010000000000000001400000000D" call="MPI_Isend" bytes="320" orank="13" region="0" commid="0" count="346" tid="0" op="" dtype="" >3.4785e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000014000000014" call="MPI_Isend" bytes="320" orank="20" region="0" commid="0" count="357" tid="0" op="" dtype="" >4.1485e-04 0.0000e+00 4.0531e-06</hent>
<hent key="02400100000000000000014000000016" call="MPI_Isend" bytes="320" orank="22" region="0" commid="0" count="353" tid="0" op="" dtype="" >1.0715e-03 9.5367e-07 5.0068e-06</hent>
<hent key="0240010000000000000001400000001D" call="MPI_Isend" bytes="320" orank="29" region="0" commid="0" count="387" tid="0" op="" dtype="" >1.7679e-03 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000014000000055" call="MPI_Isend" bytes="320" orank="85" region="0" commid="0" count="175" tid="0" op="" dtype="" >7.1549e-04 2.8610e-06 5.9605e-06</hent>
<hent key="0380010000000000000020000000000D" call="MPI_Irecv" bytes="8192" orank="13" region="0" commid="0" count="12679" tid="0" op="" dtype="" >4.2226e-03 0.0000e+00 6.9141e-06</hent>
<hent key="03800100000000000000200000000014" call="MPI_Irecv" bytes="8192" orank="20" region="0" commid="0" count="3531" tid="0" op="" dtype="" >1.4002e-03 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000200000000016" call="MPI_Irecv" bytes="8192" orank="22" region="0" commid="0" count="3440" tid="0" op="" dtype="" >5.7077e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000020000000001D" call="MPI_Irecv" bytes="8192" orank="29" region="0" commid="0" count="12448" tid="0" op="" dtype="" >1.3316e-03 0.0000e+00 5.9605e-06</hent>
<hent key="024001000000000000000140000002D5" call="MPI_Isend" bytes="320" orank="725" region="0" commid="0" count="171" tid="0" op="" dtype="" >6.9618e-04 2.8610e-06 5.0068e-06</hent>
<hent key="0380010000000000000000000000000D" call="MPI_Irecv" bytes="0" orank="13" region="0" commid="0" count="278" tid="0" op="" dtype="" >6.7234e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000014" call="MPI_Irecv" bytes="0" orank="20" region="0" commid="0" count="259" tid="0" op="" dtype="" >1.1468e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000000000000016" call="MPI_Irecv" bytes="0" orank="22" region="0" commid="0" count="270" tid="0" op="" dtype="" >8.2731e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000001D" call="MPI_Irecv" bytes="0" orank="29" region="0" commid="0" count="262" tid="0" op="" dtype="" >6.3419e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000055" call="MPI_Irecv" bytes="0" orank="85" region="0" commid="0" count="133" tid="0" op="" dtype="" >3.7909e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000055" call="MPI_Isend" bytes="16384" orank="85" region="0" commid="0" count="12699" tid="0" op="" dtype="" >8.3057e-02 3.8147e-06 1.8880e-03</hent>
<hent key="038001000000000000000000000002D5" call="MPI_Irecv" bytes="0" orank="725" region="0" commid="0" count="147" tid="0" op="" dtype="" >4.1962e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002D5" call="MPI_Isend" bytes="16384" orank="725" region="0" commid="0" count="12685" tid="0" op="" dtype="" >9.3141e-02 2.8610e-06 4.6015e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.2462e-04 0.0000e+00 2.7895e-04</hent>
<hent key="0240010000000000000020000000000D" call="MPI_Isend" bytes="8192" orank="13" region="0" commid="0" count="12699" tid="0" op="" dtype="" >5.5783e-03 0.0000e+00 6.1989e-06</hent>
<hent key="02400100000000000000200000000014" call="MPI_Isend" bytes="8192" orank="20" region="0" commid="0" count="3778" tid="0" op="" dtype="" >4.1027e-03 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000200000000016" call="MPI_Isend" bytes="8192" orank="22" region="0" commid="0" count="3840" tid="0" op="" dtype="" >6.6423e-03 0.0000e+00 2.2888e-05</hent>
<hent key="0240010000000000000020000000001D" call="MPI_Isend" bytes="8192" orank="29" region="0" commid="0" count="12527" tid="0" op="" dtype="" >1.0961e-01 3.8147e-06 2.5988e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.9857e-05 6.9857e-05 6.9857e-05</hent>
<hent key="0240010000000000000000000000000D" call="MPI_Isend" bytes="0" orank="13" region="0" commid="0" count="253" tid="0" op="" dtype="" >1.8978e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000000000000014" call="MPI_Isend" bytes="0" orank="20" region="0" commid="0" count="247" tid="0" op="" dtype="" >1.7929e-04 0.0000e+00 5.0068e-06</hent>
<hent key="02400100000000000000000000000016" call="MPI_Isend" bytes="0" orank="22" region="0" commid="0" count="246" tid="0" op="" dtype="" >5.2500e-04 9.5367e-07 5.3883e-05</hent>
<hent key="0240010000000000000000000000001D" call="MPI_Isend" bytes="0" orank="29" region="0" commid="0" count="257" tid="0" op="" dtype="" >9.8681e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000000000000055" call="MPI_Isend" bytes="0" orank="85" region="0" commid="0" count="149" tid="0" op="" dtype="" >5.3573e-04 9.5367e-07 6.9141e-06</hent>
<hent key="024001000000000000000000000002D5" call="MPI_Isend" bytes="0" orank="725" region="0" commid="0" count="149" tid="0" op="" dtype="" >5.6624e-04 9.5367e-07 5.0068e-06</hent>
<hent key="0380010000000000000006000000000D" call="MPI_Irecv" bytes="1536" orank="13" region="0" commid="0" count="111" tid="0" op="" dtype="" >3.2663e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000014" call="MPI_Irecv" bytes="1536" orank="20" region="0" commid="0" count="80" tid="0" op="" dtype="" >4.4823e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000060000000016" call="MPI_Irecv" bytes="1536" orank="22" region="0" commid="0" count="100" tid="0" op="" dtype="" >2.3842e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000001D" call="MPI_Irecv" bytes="1536" orank="29" region="0" commid="0" count="103" tid="0" op="" dtype="" >2.6941e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000055" call="MPI_Irecv" bytes="1536" orank="85" region="0" commid="0" count="232" tid="0" op="" dtype="" >6.4373e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002D5" call="MPI_Irecv" bytes="1536" orank="725" region="0" commid="0" count="206" tid="0" op="" dtype="" >8.4400e-05 0.0000e+00 1.1921e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000006000000000D" call="MPI_Isend" bytes="1536" orank="13" region="0" commid="0" count="107" tid="0" op="" dtype="" >2.1863e-04 9.5367e-07 1.4067e-05</hent>
<hent key="02400100000000000000060000000014" call="MPI_Isend" bytes="1536" orank="20" region="0" commid="0" count="72" tid="0" op="" dtype="" >1.4973e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000060000000016" call="MPI_Isend" bytes="1536" orank="22" region="0" commid="0" count="110" tid="0" op="" dtype="" >4.5371e-04 2.8610e-06 7.1526e-06</hent>
<hent key="0240010000000000000006000000001D" call="MPI_Isend" bytes="1536" orank="29" region="0" commid="0" count="83" tid="0" op="" dtype="" >4.2868e-04 4.0531e-06 6.1989e-06</hent>
<hent key="02400100000000000000060000000055" call="MPI_Isend" bytes="1536" orank="85" region="0" commid="0" count="221" tid="0" op="" dtype="" >1.0054e-03 3.8147e-06 5.9605e-06</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002D5" call="MPI_Isend" bytes="1536" orank="725" region="0" commid="0" count="218" tid="0" op="" dtype="" >9.7251e-04 3.0994e-06 9.7752e-06</hent>
<hent key="038001000000000000000C0000000014" call="MPI_Irecv" bytes="3072" orank="20" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000C0000000055" call="MPI_Irecv" bytes="3072" orank="85" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002D5" call="MPI_Irecv" bytes="3072" orank="725" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C000000000D" call="MPI_Isend" bytes="3072" orank="13" region="0" commid="0" count="1" tid="0" op="" dtype="" >1.0967e-05 1.0967e-05 1.0967e-05</hent>
<hent key="024001000000000000000C0000000016" call="MPI_Isend" bytes="3072" orank="22" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="024001000000000000000C0000000055" call="MPI_Isend" bytes="3072" orank="85" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.6001e-05 4.0531e-06 5.9605e-06</hent>
<hent key="024001000000000000000C00000002D5" call="MPI_Isend" bytes="3072" orank="725" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.5020e-05 5.0068e-06 5.0068e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.3797e-04 4.3797e-04 4.3797e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.0118e-03 3.3092e-04 3.4094e-04</hent>
<hent key="0380010000000000000003800000000D" call="MPI_Irecv" bytes="896" orank="13" region="0" commid="0" count="313" tid="0" op="" dtype="" >9.3222e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000014" call="MPI_Irecv" bytes="896" orank="20" region="0" commid="0" count="2770" tid="0" op="" dtype="" >6.6566e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000038000000016" call="MPI_Irecv" bytes="896" orank="22" region="0" commid="0" count="2793" tid="0" op="" dtype="" >6.9141e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0380010000000000000003800000001D" call="MPI_Irecv" bytes="896" orank="29" region="0" commid="0" count="314" tid="0" op="" dtype="" >8.2731e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000055" call="MPI_Irecv" bytes="896" orank="85" region="0" commid="0" count="320" tid="0" op="" dtype="" >7.8440e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >7.1049e-05 7.1049e-05 7.1049e-05</hent>
<hent key="038001000000000000000380000002D5" call="MPI_Irecv" bytes="896" orank="725" region="0" commid="0" count="365" tid="0" op="" dtype="" >1.3280e-04 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000380000000055" call="MPI_Irecv" bytes="14336" orank="85" region="0" commid="0" count="182" tid="0" op="" dtype="" >2.4319e-05 0.0000e+00 1.9073e-06</hent>
<hent key="038001000000000000003800000002D5" call="MPI_Irecv" bytes="14336" orank="725" region="0" commid="0" count="81" tid="0" op="" dtype="" >3.5524e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000003800000000D" call="MPI_Isend" bytes="896" orank="13" region="0" commid="0" count="314" tid="0" op="" dtype="" >4.5943e-04 9.5367e-07 7.8678e-06</hent>
<hent key="02400100000000000000038000000014" call="MPI_Isend" bytes="896" orank="20" region="0" commid="0" count="2714" tid="0" op="" dtype="" >2.0359e-03 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000038000000016" call="MPI_Isend" bytes="896" orank="22" region="0" commid="0" count="2659" tid="0" op="" dtype="" >2.8203e-03 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000003800000001D" call="MPI_Isend" bytes="896" orank="29" region="0" commid="0" count="300" tid="0" op="" dtype="" >1.4789e-03 3.8147e-06 9.0599e-06</hent>
<hent key="02400100000000000000038000000055" call="MPI_Isend" bytes="896" orank="85" region="0" commid="0" count="320" tid="0" op="" dtype="" >1.3885e-03 2.8610e-06 1.0014e-05</hent>
<hent key="024001000000000000000380000002D5" call="MPI_Isend" bytes="896" orank="725" region="0" commid="0" count="347" tid="0" op="" dtype="" >1.4956e-03 2.8610e-06 4.6968e-05</hent>
<hent key="024001000000000000003800000002D5" call="MPI_Isend" bytes="14336" orank="725" region="0" commid="0" count="14" tid="0" op="" dtype="" >1.0371e-04 4.0531e-06 1.0014e-05</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.6121e+00 7.8678e-06 2.0313e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.1991e-03 3.1991e-03 3.1991e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0432e-02 1.0432e-02 1.0432e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.6204e-02 2.6204e-02 2.6204e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.0529e-01 3.6790e-03 1.8513e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.6907e-04 3.6907e-04 3.6907e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5853e+00 3.8290e-04 2.5044e-01</hent>
<hent key="0380010000000000000004000000000D" call="MPI_Irecv" bytes="1024" orank="13" region="0" commid="0" count="3392" tid="0" op="" dtype="" >6.7425e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000040000000014" call="MPI_Irecv" bytes="1024" orank="20" region="0" commid="0" count="956" tid="0" op="" dtype="" >1.9169e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000016" call="MPI_Irecv" bytes="1024" orank="22" region="0" commid="0" count="930" tid="0" op="" dtype="" >2.4557e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000001D" call="MPI_Irecv" bytes="1024" orank="29" region="0" commid="0" count="3324" tid="0" op="" dtype="" >6.0773e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.7180e-05 0.0000e+00 1.1206e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="0380010000000000000007000000000D" call="MPI_Irecv" bytes="1792" orank="13" region="0" commid="0" count="37" tid="0" op="" dtype="" >8.8215e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000014" call="MPI_Irecv" bytes="1792" orank="20" region="0" commid="0" count="28" tid="0" op="" dtype="" >1.7643e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000016" call="MPI_Irecv" bytes="1792" orank="22" region="0" commid="0" count="35" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000001D" call="MPI_Irecv" bytes="1792" orank="29" region="0" commid="0" count="38" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000055" call="MPI_Irecv" bytes="1792" orank="85" region="0" commid="0" count="171" tid="0" op="" dtype="" >4.1485e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002D5" call="MPI_Irecv" bytes="1792" orank="725" region="0" commid="0" count="160" tid="0" op="" dtype="" >5.1737e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >4.3869e-05 0.0000e+00 2.3127e-05</hent>
<hent key="0240010000000000000004000000000D" call="MPI_Isend" bytes="1024" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.1658e-03 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000040000000014" call="MPI_Isend" bytes="1024" orank="20" region="0" commid="0" count="1014" tid="0" op="" dtype="" >6.6209e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000040000000016" call="MPI_Isend" bytes="1024" orank="22" region="0" commid="0" count="1036" tid="0" op="" dtype="" >7.8750e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000004000000001D" call="MPI_Isend" bytes="1024" orank="29" region="0" commid="0" count="3348" tid="0" op="" dtype="" >9.0654e-03 9.5367e-07 1.0014e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >3.8147e-06 1.9073e-06 1.9073e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5584e+00 0.0000e+00 2.2703e+00</hent>
<hent key="038001000000000000000A000000000D" call="MPI_Irecv" bytes="2560" orank="13" region="0" commid="0" count="4" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000014" call="MPI_Irecv" bytes="2560" orank="20" region="0" commid="0" count="5" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A0000000016" call="MPI_Irecv" bytes="2560" orank="22" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000001D" call="MPI_Irecv" bytes="2560" orank="29" region="0" commid="0" count="8" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000055" call="MPI_Irecv" bytes="2560" orank="85" region="0" commid="0" count="52" tid="0" op="" dtype="" >1.3590e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002D5" call="MPI_Irecv" bytes="2560" orank="725" region="0" commid="0" count="40" tid="0" op="" dtype="" >1.4305e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000007000000000D" call="MPI_Isend" bytes="1792" orank="13" region="0" commid="0" count="36" tid="0" op="" dtype="" >9.7275e-05 9.5367e-07 1.4067e-05</hent>
<hent key="02400100000000000000070000000014" call="MPI_Isend" bytes="1792" orank="20" region="0" commid="0" count="35" tid="0" op="" dtype="" >8.5831e-05 1.9073e-06 7.1526e-06</hent>
<hent key="02400100000000000000070000000016" call="MPI_Isend" bytes="1792" orank="22" region="0" commid="0" count="38" tid="0" op="" dtype="" >1.6260e-04 2.8610e-06 8.1062e-06</hent>
<hent key="0240010000000000000007000000001D" call="MPI_Isend" bytes="1792" orank="29" region="0" commid="0" count="20" tid="0" op="" dtype="" >1.0395e-04 5.0068e-06 5.9605e-06</hent>
<hent key="02400100000000000000070000000055" call="MPI_Isend" bytes="1792" orank="85" region="0" commid="0" count="130" tid="0" op="" dtype="" >6.0558e-04 3.8147e-06 5.9605e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1348e-01 1.1348e-01 1.1348e-01</hent>
<hent key="024001000000000000000700000002D5" call="MPI_Isend" bytes="1792" orank="725" region="0" commid="0" count="132" tid="0" op="" dtype="" >5.9271e-04 1.9073e-06 5.0068e-06</hent>
<hent key="024001000000000000000A000000000D" call="MPI_Isend" bytes="2560" orank="13" region="0" commid="0" count="10" tid="0" op="" dtype="" >6.8903e-05 9.5367e-07 1.4067e-05</hent>
<hent key="024001000000000000000A0000000014" call="MPI_Isend" bytes="2560" orank="20" region="0" commid="0" count="9" tid="0" op="" dtype="" >2.2888e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000016" call="MPI_Isend" bytes="2560" orank="22" region="0" commid="0" count="6" tid="0" op="" dtype="" >2.7895e-05 3.8147e-06 5.9605e-06</hent>
<hent key="024001000000000000000A000000001D" call="MPI_Isend" bytes="2560" orank="29" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.5988e-05 5.0068e-06 5.9605e-06</hent>
<hent key="024001000000000000000A0000000055" call="MPI_Isend" bytes="2560" orank="85" region="0" commid="0" count="42" tid="0" op="" dtype="" >2.0814e-04 3.8147e-06 6.1989e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >4.9591e-04 5.6982e-05 2.2197e-04</hent>
<hent key="024001000000000000000A00000002D5" call="MPI_Isend" bytes="2560" orank="725" region="0" commid="0" count="38" tid="0" op="" dtype="" >1.8668e-04 3.8147e-06 5.9605e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.7180e-03 6.0892e-04 1.1091e-03</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.4107e-05 4.4107e-05 4.4107e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >4.7684e-06 1.9073e-06 2.8610e-06</hent>
<hent key="0380010000000000000000040000000D" call="MPI_Irecv" bytes="4" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.1512e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000014" call="MPI_Irecv" bytes="4" orank="20" region="0" commid="0" count="3398" tid="0" op="" dtype="" >7.5316e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000400000016" call="MPI_Irecv" bytes="4" orank="22" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.9128e-04 0.0000e+00 1.9073e-06</hent>
<hent key="0380010000000000000000040000001D" call="MPI_Irecv" bytes="4" orank="29" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.9615e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000055" call="MPI_Irecv" bytes="4" orank="85" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.7711e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >2.9340e-03 4.2510e-04 8.5092e-04</hent>
<hent key="038001000000000000001C0000000014" call="MPI_Irecv" bytes="7168" orank="20" region="0" commid="0" count="9168" tid="0" op="" dtype="" >3.6864e-03 0.0000e+00 6.1989e-06</hent>
<hent key="038001000000000000001C0000000016" call="MPI_Irecv" bytes="7168" orank="22" region="0" commid="0" count="9259" tid="0" op="" dtype="" >1.6110e-03 0.0000e+00 2.1458e-06</hent>
<hent key="038001000000000000000004000002D5" call="MPI_Irecv" bytes="4" orank="725" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.5494e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000000040000000D" call="MPI_Isend" bytes="4" orank="13" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.3330e-03 0.0000e+00 3.0041e-05</hent>
<hent key="02400100000000000000000400000014" call="MPI_Isend" bytes="4" orank="20" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.4956e-03 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000400000016" call="MPI_Isend" bytes="4" orank="22" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.5115e-03 0.0000e+00 6.2227e-05</hent>
<hent key="0240010000000000000000040000001D" call="MPI_Isend" bytes="4" orank="29" region="0" commid="0" count="3398" tid="0" op="" dtype="" >3.1124e-02 3.8147e-06 3.1948e-05</hent>
<hent key="02400100000000000000000400000055" call="MPI_Isend" bytes="4" orank="85" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.9383e-02 3.8147e-06 2.9087e-05</hent>
<hent key="024001000000000000001C0000000014" call="MPI_Isend" bytes="7168" orank="20" region="0" commid="0" count="8921" tid="0" op="" dtype="" >1.9414e-02 9.5367e-07 1.1921e-05</hent>
<hent key="024001000000000000001C0000000016" call="MPI_Isend" bytes="7168" orank="22" region="0" commid="0" count="8859" tid="0" op="" dtype="" >2.6346e-02 9.5367e-07 1.5020e-05</hent>
<hent key="024001000000000000000004000002D5" call="MPI_Isend" bytes="4" orank="725" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.8227e-02 3.8147e-06 1.1086e-04</hent>
<hent key="0380010000000000000005000000000D" call="MPI_Irecv" bytes="1280" orank="13" region="0" commid="0" count="209" tid="0" op="" dtype="" >5.6028e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000014" call="MPI_Irecv" bytes="1280" orank="20" region="0" commid="0" count="196" tid="0" op="" dtype="" >1.1230e-04 0.0000e+00 1.5020e-05</hent>
<hent key="03800100000000000000050000000016" call="MPI_Irecv" bytes="1280" orank="22" region="0" commid="0" count="192" tid="0" op="" dtype="" >5.5790e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000001D" call="MPI_Irecv" bytes="1280" orank="29" region="0" commid="0" count="273" tid="0" op="" dtype="" >6.2704e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000055" call="MPI_Irecv" bytes="1280" orank="85" region="0" commid="0" count="313" tid="0" op="" dtype="" >9.8705e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0909e+01 9.0599e-06 1.5573e-01</hent>
<hent key="0380010000000000000028000000000D" call="MPI_Irecv" bytes="10240" orank="13" region="0" commid="0" count="20" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 3.0994e-06</hent>
<hent key="0380010000000000000028000000001D" call="MPI_Irecv" bytes="10240" orank="29" region="0" commid="0" count="251" tid="0" op="" dtype="" >2.1458e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002D5" call="MPI_Irecv" bytes="1280" orank="725" region="0" commid="0" count="274" tid="0" op="" dtype="" >1.1134e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000D" call="MPI_Irecv" bytes="2048" orank="13" region="0" commid="0" count="18" tid="0" op="" dtype="" >5.2452e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000014" call="MPI_Irecv" bytes="2048" orank="20" region="0" commid="0" count="14" tid="0" op="" dtype="" >1.0014e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000016" call="MPI_Irecv" bytes="2048" orank="22" region="0" commid="0" count="10" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000001D" call="MPI_Irecv" bytes="2048" orank="29" region="0" commid="0" count="11" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000055" call="MPI_Irecv" bytes="2048" orank="85" region="0" commid="0" count="3430" tid="0" op="" dtype="" >5.9009e-04 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000800000002D5" call="MPI_Irecv" bytes="2048" orank="725" region="0" commid="0" count="3434" tid="0" op="" dtype="" >6.4802e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000005000000000D" call="MPI_Isend" bytes="1280" orank="13" region="0" commid="0" count="207" tid="0" op="" dtype="" >3.3212e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000050000000014" call="MPI_Isend" bytes="1280" orank="20" region="0" commid="0" count="216" tid="0" op="" dtype="" >4.0960e-04 9.5367e-07 3.0994e-06</hent>
<hent key="02400100000000000000050000000016" call="MPI_Isend" bytes="1280" orank="22" region="0" commid="0" count="205" tid="0" op="" dtype="" >7.9608e-04 1.9073e-06 8.1062e-06</hent>
<hent key="0240010000000000000005000000001D" call="MPI_Isend" bytes="1280" orank="29" region="0" commid="0" count="232" tid="0" op="" dtype="" >1.0655e-03 9.5367e-07 1.0014e-05</hent>
<hent key="02400100000000000000050000000055" call="MPI_Isend" bytes="1280" orank="85" region="0" commid="0" count="325" tid="0" op="" dtype="" >1.4532e-03 2.8610e-06 1.1206e-05</hent>
<hent key="0240010000000000000028000000001D" call="MPI_Isend" bytes="10240" orank="29" region="0" commid="0" count="172" tid="0" op="" dtype="" >1.3666e-03 5.0068e-06 1.3113e-05</hent>
<hent key="024001000000000000000500000002D5" call="MPI_Isend" bytes="1280" orank="725" region="0" commid="0" count="303" tid="0" op="" dtype="" >1.3163e-03 2.8610e-06 1.0014e-05</hent>
</hash>
<internal rank="21" log_i="1723712895.681780" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="22" mpi_size="768" stamp_init="1723712829.548092" stamp_final="1723712895.683646" username="apac4" allocationname="unknown" flags="0" pid="684290" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61356e+01" utime="4.88241e+01" stime="8.73193e+00" mtime="3.04364e+01" gflop="0.00000e+00" gbyte="3.76671e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.04364e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ad15af15b0153b56b015b0150b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60032e+01" utime="4.87919e+01" stime="8.72164e+00" mtime="3.04364e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.04364e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4849e+08" > 6.7361e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4934e+08" > 4.7242e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6059e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5721e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9200e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1468e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5848e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.2615e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0414e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1363e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7819e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="197" >
<hent key="0240010000000000000008000000000E" call="MPI_Isend" bytes="2048" orank="14" region="0" commid="0" count="12" tid="0" op="" dtype="" >3.4332e-05 1.1921e-06 1.1921e-05</hent>
<hent key="02400100000000000000080000000015" call="MPI_Isend" bytes="2048" orank="21" region="0" commid="0" count="10" tid="0" op="" dtype="" >2.2411e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000080000000017" call="MPI_Isend" bytes="2048" orank="23" region="0" commid="0" count="18" tid="0" op="" dtype="" >9.9897e-05 4.0531e-06 6.9141e-06</hent>
<hent key="0240010000000000000008000000001E" call="MPI_Isend" bytes="2048" orank="30" region="0" commid="0" count="12" tid="0" op="" dtype="" >7.4148e-05 5.0068e-06 7.1526e-06</hent>
<hent key="02400100000000000000080000000056" call="MPI_Isend" bytes="2048" orank="86" region="0" commid="0" count="3472" tid="0" op="" dtype="" >1.6454e-02 9.5367e-07 4.7922e-05</hent>
<hent key="024001000000000000000800000002D6" call="MPI_Isend" bytes="2048" orank="726" region="0" commid="0" count="3466" tid="0" op="" dtype="" >1.5434e-02 9.5367e-07 5.1975e-05</hent>
<hent key="038001000000000000000E00000002D6" call="MPI_Irecv" bytes="3584" orank="726" region="0" commid="0" count="2" tid="0" op="" dtype="" >2.1458e-06 9.5367e-07 1.1921e-06</hent>
<hent key="0380010000000000000002800000000E" call="MPI_Irecv" bytes="640" orank="14" region="0" commid="0" count="386" tid="0" op="" dtype="" >1.0872e-04 0.0000e+00 3.0994e-06</hent>
<hent key="03800100000000000000028000000015" call="MPI_Irecv" bytes="640" orank="21" region="0" commid="0" count="435" tid="0" op="" dtype="" >3.6955e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000028000000017" call="MPI_Irecv" bytes="640" orank="23" region="0" commid="0" count="399" tid="0" op="" dtype="" >6.3419e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000001E" call="MPI_Irecv" bytes="640" orank="30" region="0" commid="0" count="398" tid="0" op="" dtype="" >1.1063e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000028000000056" call="MPI_Irecv" bytes="640" orank="86" region="0" commid="0" count="268" tid="0" op="" dtype="" >9.0122e-05 0.0000e+00 3.0994e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >2.8610e-06 2.8610e-06 2.8610e-06</hent>
<hent key="038001000000000000000280000002D6" call="MPI_Irecv" bytes="640" orank="726" region="0" commid="0" count="283" tid="0" op="" dtype="" >1.3661e-04 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000002800000000E" call="MPI_Isend" bytes="640" orank="14" region="0" commid="0" count="390" tid="0" op="" dtype="" >5.5265e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000028000000015" call="MPI_Isend" bytes="640" orank="21" region="0" commid="0" count="424" tid="0" op="" dtype="" >6.4540e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000028000000017" call="MPI_Isend" bytes="640" orank="23" region="0" commid="0" count="394" tid="0" op="" dtype="" >1.8113e-03 1.9073e-06 5.5075e-05</hent>
<hent key="0240010000000000000002800000001E" call="MPI_Isend" bytes="640" orank="30" region="0" commid="0" count="405" tid="0" op="" dtype="" >2.3258e-03 3.8147e-06 2.3842e-05</hent>
<hent key="02400100000000000000028000000056" call="MPI_Isend" bytes="640" orank="86" region="0" commid="0" count="307" tid="0" op="" dtype="" >1.7037e-03 3.8147e-06 2.8133e-05</hent>
<hent key="024001000000000000000280000002D6" call="MPI_Isend" bytes="640" orank="726" region="0" commid="0" count="286" tid="0" op="" dtype="" >1.4896e-03 3.8147e-06 1.8835e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.2187e-05 3.2187e-05 3.2187e-05</hent>
<hent key="0380010000000000000001400000000E" call="MPI_Irecv" bytes="320" orank="14" region="0" commid="0" count="396" tid="0" op="" dtype="" >9.2268e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000015" call="MPI_Irecv" bytes="320" orank="21" region="0" commid="0" count="353" tid="0" op="" dtype="" >2.9039e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000014000000017" call="MPI_Irecv" bytes="320" orank="23" region="0" commid="0" count="358" tid="0" op="" dtype="" >7.0095e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000001E" call="MPI_Irecv" bytes="320" orank="30" region="0" commid="0" count="358" tid="0" op="" dtype="" >8.5354e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000014000000056" call="MPI_Irecv" bytes="320" orank="86" region="0" commid="0" count="185" tid="0" op="" dtype="" >6.0797e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002D6" call="MPI_Irecv" bytes="320" orank="726" region="0" commid="0" count="172" tid="0" op="" dtype="" >6.5088e-05 0.0000e+00 7.8678e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >6.6059e+00 0.0000e+00 1.8162e-01</hent>
<hent key="03800100000000000000400000000056" call="MPI_Irecv" bytes="16384" orank="86" region="0" commid="0" count="12534" tid="0" op="" dtype="" >4.5621e-03 0.0000e+00 3.9101e-05</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6460e-02 1.1921e-06 1.6444e-02</hent>
<hent key="038001000000000000004000000002D6" call="MPI_Irecv" bytes="16384" orank="726" region="0" commid="0" count="12691" tid="0" op="" dtype="" >1.4034e-02 0.0000e+00 4.1008e-05</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >6.6757e-05 0.0000e+00 6.3896e-05</hent>
<hent key="0240010000000000000001400000000E" call="MPI_Isend" bytes="320" orank="14" region="0" commid="0" count="370" tid="0" op="" dtype="" >4.3631e-04 0.0000e+00 1.0967e-05</hent>
<hent key="02400100000000000000014000000015" call="MPI_Isend" bytes="320" orank="21" region="0" commid="0" count="339" tid="0" op="" dtype="" >4.0293e-04 0.0000e+00 2.1458e-06</hent>
<hent key="02400100000000000000014000000017" call="MPI_Isend" bytes="320" orank="23" region="0" commid="0" count="394" tid="0" op="" dtype="" >1.6134e-03 1.9073e-06 1.0014e-05</hent>
<hent key="0240010000000000000001400000001E" call="MPI_Isend" bytes="320" orank="30" region="0" commid="0" count="346" tid="0" op="" dtype="" >1.8203e-03 3.8147e-06 1.5020e-05</hent>
<hent key="02400100000000000000014000000056" call="MPI_Isend" bytes="320" orank="86" region="0" commid="0" count="174" tid="0" op="" dtype="" >9.5415e-04 2.8610e-06 2.0981e-05</hent>
<hent key="0380010000000000000020000000000E" call="MPI_Irecv" bytes="8192" orank="14" region="0" commid="0" count="12668" tid="0" op="" dtype="" >5.6620e-03 0.0000e+00 1.5020e-05</hent>
<hent key="03800100000000000000200000000015" call="MPI_Irecv" bytes="8192" orank="21" region="0" commid="0" count="3840" tid="0" op="" dtype="" >1.3556e-03 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000200000000017" call="MPI_Irecv" bytes="8192" orank="23" region="0" commid="0" count="3750" tid="0" op="" dtype="" >1.1349e-03 0.0000e+00 9.0599e-06</hent>
<hent key="0380010000000000000020000000001E" call="MPI_Irecv" bytes="8192" orank="30" region="0" commid="0" count="12639" tid="0" op="" dtype="" >1.9622e-03 0.0000e+00 1.5974e-05</hent>
<hent key="024001000000000000000140000002D6" call="MPI_Isend" bytes="320" orank="726" region="0" commid="0" count="152" tid="0" op="" dtype="" >7.8964e-04 2.8610e-06 1.9073e-05</hent>
<hent key="0380010000000000000000000000000E" call="MPI_Irecv" bytes="0" orank="14" region="0" commid="0" count="274" tid="0" op="" dtype="" >8.0824e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000015" call="MPI_Irecv" bytes="0" orank="21" region="0" commid="0" count="246" tid="0" op="" dtype="" >1.1706e-04 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000000000000017" call="MPI_Irecv" bytes="0" orank="23" region="0" commid="0" count="263" tid="0" op="" dtype="" >5.9366e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000000000000001E" call="MPI_Irecv" bytes="0" orank="30" region="0" commid="0" count="261" tid="0" op="" dtype="" >5.6744e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000056" call="MPI_Irecv" bytes="0" orank="86" region="0" commid="0" count="146" tid="0" op="" dtype="" >3.4809e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000056" call="MPI_Isend" bytes="16384" orank="86" region="0" commid="0" count="12676" tid="0" op="" dtype="" >1.4951e-01 3.8147e-06 8.2016e-05</hent>
<hent key="038001000000000000000000000002D6" call="MPI_Irecv" bytes="0" orank="726" region="0" commid="0" count="147" tid="0" op="" dtype="" >4.5776e-05 0.0000e+00 6.9141e-06</hent>
<hent key="024001000000000000004000000002D6" call="MPI_Isend" bytes="16384" orank="726" region="0" commid="0" count="12691" tid="0" op="" dtype="" >1.4544e-01 2.8610e-06 6.1989e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.1199e-04 0.0000e+00 2.7299e-04</hent>
<hent key="0240010000000000000020000000000E" call="MPI_Isend" bytes="8192" orank="14" region="0" commid="0" count="12611" tid="0" op="" dtype="" >7.6075e-03 0.0000e+00 2.0027e-05</hent>
<hent key="02400100000000000000200000000015" call="MPI_Isend" bytes="8192" orank="21" region="0" commid="0" count="3440" tid="0" op="" dtype="" >4.6299e-03 0.0000e+00 5.2929e-05</hent>
<hent key="02400100000000000000200000000017" call="MPI_Isend" bytes="8192" orank="23" region="0" commid="0" count="3009" tid="0" op="" dtype="" >6.6414e-03 0.0000e+00 1.5974e-05</hent>
<hent key="0240010000000000000020000000001E" call="MPI_Isend" bytes="8192" orank="30" region="0" commid="0" count="12667" tid="0" op="" dtype="" >1.2998e-01 3.8147e-06 3.7909e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >7.4863e-05 7.4863e-05 7.4863e-05</hent>
<hent key="0240010000000000000000000000000E" call="MPI_Isend" bytes="0" orank="14" region="0" commid="0" count="280" tid="0" op="" dtype="" >2.6464e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000000000000015" call="MPI_Isend" bytes="0" orank="21" region="0" commid="0" count="270" tid="0" op="" dtype="" >1.9932e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000000000000017" call="MPI_Isend" bytes="0" orank="23" region="0" commid="0" count="235" tid="0" op="" dtype="" >5.6243e-04 9.5367e-07 3.1948e-05</hent>
<hent key="0240010000000000000000000000001E" call="MPI_Isend" bytes="0" orank="30" region="0" commid="0" count="274" tid="0" op="" dtype="" >1.2202e-03 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000000000000056" call="MPI_Isend" bytes="0" orank="86" region="0" commid="0" count="148" tid="0" op="" dtype="" >6.3539e-04 9.5367e-07 1.0014e-05</hent>
<hent key="024001000000000000000000000002D6" call="MPI_Isend" bytes="0" orank="726" region="0" commid="0" count="152" tid="0" op="" dtype="" >6.4063e-04 1.9073e-06 1.1206e-05</hent>
<hent key="0380010000000000000006000000000E" call="MPI_Irecv" bytes="1536" orank="14" region="0" commid="0" count="93" tid="0" op="" dtype="" >2.9087e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000015" call="MPI_Irecv" bytes="1536" orank="21" region="0" commid="0" count="110" tid="0" op="" dtype="" >9.5129e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000060000000017" call="MPI_Irecv" bytes="1536" orank="23" region="0" commid="0" count="100" tid="0" op="" dtype="" >1.5020e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000001E" call="MPI_Irecv" bytes="1536" orank="30" region="0" commid="0" count="80" tid="0" op="" dtype="" >2.1696e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000056" call="MPI_Irecv" bytes="1536" orank="86" region="0" commid="0" count="243" tid="0" op="" dtype="" >1.0371e-04 0.0000e+00 7.1526e-06</hent>
<hent key="038001000000000000000600000002D6" call="MPI_Irecv" bytes="1536" orank="726" region="0" commid="0" count="224" tid="0" op="" dtype="" >1.1587e-04 0.0000e+00 4.0531e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >5.0068e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000006000000000E" call="MPI_Isend" bytes="1536" orank="14" region="0" commid="0" count="98" tid="0" op="" dtype="" >2.2244e-04 9.5367e-07 1.0967e-05</hent>
<hent key="02400100000000000000060000000015" call="MPI_Isend" bytes="1536" orank="21" region="0" commid="0" count="100" tid="0" op="" dtype="" >2.1195e-04 9.5367e-07 7.1526e-06</hent>
<hent key="02400100000000000000060000000017" call="MPI_Isend" bytes="1536" orank="23" region="0" commid="0" count="97" tid="0" op="" dtype="" >5.1832e-04 2.8610e-06 7.1526e-06</hent>
<hent key="0240010000000000000006000000001E" call="MPI_Isend" bytes="1536" orank="30" region="0" commid="0" count="84" tid="0" op="" dtype="" >5.0879e-04 5.0068e-06 1.0967e-05</hent>
<hent key="02400100000000000000060000000056" call="MPI_Isend" bytes="1536" orank="86" region="0" commid="0" count="189" tid="0" op="" dtype="" >1.0781e-03 3.8147e-06 2.5034e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002D6" call="MPI_Isend" bytes="1536" orank="726" region="0" commid="0" count="222" tid="0" op="" dtype="" >1.2107e-03 3.8147e-06 1.9073e-05</hent>
<hent key="038001000000000000000C0000000015" call="MPI_Irecv" bytes="3072" orank="21" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000000C0000000056" call="MPI_Irecv" bytes="3072" orank="86" region="0" commid="0" count="9" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 2.8610e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002D6" call="MPI_Irecv" bytes="3072" orank="726" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.8147e-06 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C0000000056" call="MPI_Isend" bytes="3072" orank="86" region="0" commid="0" count="9" tid="0" op="" dtype="" >5.3644e-05 5.0068e-06 6.9141e-06</hent>
<hent key="024001000000000000000C00000002D6" call="MPI_Isend" bytes="3072" orank="726" region="0" commid="0" count="3" tid="0" op="" dtype="" >1.9312e-05 5.9605e-06 7.1526e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.5919e-04 4.5919e-04 4.5919e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.0552e-03 3.4308e-04 3.5810e-04</hent>
<hent key="0380010000000000000003800000000E" call="MPI_Irecv" bytes="896" orank="14" region="0" commid="0" count="308" tid="0" op="" dtype="" >7.7009e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000015" call="MPI_Irecv" bytes="896" orank="21" region="0" commid="0" count="2659" tid="0" op="" dtype="" >5.4860e-04 0.0000e+00 1.0967e-05</hent>
<hent key="03800100000000000000038000000017" call="MPI_Irecv" bytes="896" orank="23" region="0" commid="0" count="2751" tid="0" op="" dtype="" >6.5708e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0380010000000000000003800000001E" call="MPI_Irecv" bytes="896" orank="30" region="0" commid="0" count="356" tid="0" op="" dtype="" >9.9421e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000056" call="MPI_Irecv" bytes="896" orank="86" region="0" commid="0" count="331" tid="0" op="" dtype="" >1.3280e-04 0.0000e+00 5.0068e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.4162e-05 8.4162e-05 8.4162e-05</hent>
<hent key="038001000000000000000380000002D6" call="MPI_Irecv" bytes="896" orank="726" region="0" commid="0" count="310" tid="0" op="" dtype="" >1.3733e-04 0.0000e+00 1.0967e-05</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000380000000056" call="MPI_Irecv" bytes="14336" orank="86" region="0" commid="0" count="165" tid="0" op="" dtype="" >3.8385e-05 0.0000e+00 2.8610e-06</hent>
<hent key="038001000000000000003800000002D6" call="MPI_Irecv" bytes="14336" orank="726" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.2915e-06 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000003800000000E" call="MPI_Isend" bytes="896" orank="14" region="0" commid="0" count="319" tid="0" op="" dtype="" >4.8018e-04 9.5367e-07 7.8678e-06</hent>
<hent key="02400100000000000000038000000015" call="MPI_Isend" bytes="896" orank="21" region="0" commid="0" count="2793" tid="0" op="" dtype="" >1.9655e-03 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000038000000017" call="MPI_Isend" bytes="896" orank="23" region="0" commid="0" count="2928" tid="0" op="" dtype="" >3.7069e-03 0.0000e+00 2.0981e-05</hent>
<hent key="0240010000000000000003800000001E" call="MPI_Isend" bytes="896" orank="30" region="0" commid="0" count="347" tid="0" op="" dtype="" >2.0037e-03 4.7684e-06 1.1206e-05</hent>
<hent key="02400100000000000000038000000056" call="MPI_Isend" bytes="896" orank="86" region="0" commid="0" count="323" tid="0" op="" dtype="" >1.7197e-03 3.8147e-06 1.9073e-05</hent>
<hent key="024001000000000000000380000002D6" call="MPI_Isend" bytes="896" orank="726" region="0" commid="0" count="315" tid="0" op="" dtype="" >1.6639e-03 3.0994e-06 1.8835e-05</hent>
<hent key="02400100000000000000380000000056" call="MPI_Isend" bytes="14336" orank="86" region="0" commid="0" count="23" tid="0" op="" dtype="" >3.2520e-04 5.0068e-06 4.9829e-05</hent>
<hent key="024001000000000000003800000002D6" call="MPI_Isend" bytes="14336" orank="726" region="0" commid="0" count="8" tid="0" op="" dtype="" >5.6982e-05 5.0068e-06 9.0599e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.2371e+00 7.8678e-06 2.0310e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.1638e-03 3.1638e-03 3.1638e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0414e-02 1.0414e-02 1.0414e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.6121e-02 2.6121e-02 2.6121e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >1.9727e-01 2.4979e-03 1.7834e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.7813e-04 3.7813e-04 3.7813e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5848e+00 4.5419e-04 2.5034e-01</hent>
<hent key="0380010000000000000004000000000E" call="MPI_Irecv" bytes="1024" orank="14" region="0" commid="0" count="3390" tid="0" op="" dtype="" >6.1226e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000040000000015" call="MPI_Irecv" bytes="1024" orank="21" region="0" commid="0" count="1036" tid="0" op="" dtype="" >1.5569e-04 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000040000000017" call="MPI_Irecv" bytes="1024" orank="23" region="0" commid="0" count="990" tid="0" op="" dtype="" >2.5582e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000001E" call="MPI_Irecv" bytes="1024" orank="30" region="0" commid="0" count="3378" tid="0" op="" dtype="" >7.1692e-04 0.0000e+00 1.5020e-05</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >1.9200e-03 9.5367e-07 1.8878e-03</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.0014e-05 1.0014e-05 1.0014e-05</hent>
<hent key="0380010000000000000007000000000E" call="MPI_Irecv" bytes="1792" orank="14" region="0" commid="0" count="28" tid="0" op="" dtype="" >5.9605e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000015" call="MPI_Irecv" bytes="1792" orank="21" region="0" commid="0" count="38" tid="0" op="" dtype="" >3.3617e-05 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000070000000017" call="MPI_Irecv" bytes="1792" orank="23" region="0" commid="0" count="46" tid="0" op="" dtype="" >1.0014e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000001E" call="MPI_Irecv" bytes="1792" orank="30" region="0" commid="0" count="37" tid="0" op="" dtype="" >1.0014e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000056" call="MPI_Irecv" bytes="1792" orank="86" region="0" commid="0" count="149" tid="0" op="" dtype="" >5.6505e-05 0.0000e+00 7.1526e-06</hent>
<hent key="038001000000000000000700000002D6" call="MPI_Irecv" bytes="1792" orank="726" region="0" commid="0" count="147" tid="0" op="" dtype="" >7.2718e-05 0.0000e+00 7.1526e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >4.2915e-05 0.0000e+00 1.5974e-05</hent>
<hent key="0240010000000000000004000000000E" call="MPI_Isend" bytes="1024" orank="14" region="0" commid="0" count="3376" tid="0" op="" dtype="" >2.7778e-03 0.0000e+00 3.0041e-05</hent>
<hent key="02400100000000000000040000000015" call="MPI_Isend" bytes="1024" orank="21" region="0" commid="0" count="930" tid="0" op="" dtype="" >5.7578e-04 0.0000e+00 7.1526e-06</hent>
<hent key="02400100000000000000040000000017" call="MPI_Isend" bytes="1024" orank="23" region="0" commid="0" count="788" tid="0" op="" dtype="" >6.6018e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0240010000000000000004000000001E" call="MPI_Isend" bytes="1024" orank="30" region="0" commid="0" count="3390" tid="0" op="" dtype="" >1.0140e-02 9.5367e-07 2.3842e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >5.0068e-06 2.1458e-06 2.8610e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5550e+00 0.0000e+00 2.2673e+00</hent>
<hent key="038001000000000000000A000000000E" call="MPI_Irecv" bytes="2560" orank="14" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000015" call="MPI_Irecv" bytes="2560" orank="21" region="0" commid="0" count="6" tid="0" op="" dtype="" >6.1989e-06 9.5367e-07 1.1921e-06</hent>
<hent key="038001000000000000000A0000000017" call="MPI_Irecv" bytes="2560" orank="23" region="0" commid="0" count="8" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A000000001E" call="MPI_Irecv" bytes="2560" orank="30" region="0" commid="0" count="5" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000056" call="MPI_Irecv" bytes="2560" orank="86" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.3842e-05 0.0000e+00 5.9605e-06</hent>
<hent key="038001000000000000000A00000002D6" call="MPI_Irecv" bytes="2560" orank="726" region="0" commid="0" count="48" tid="0" op="" dtype="" >2.4080e-05 0.0000e+00 3.0994e-06</hent>
<hent key="0240010000000000000007000000000E" call="MPI_Isend" bytes="1792" orank="14" region="0" commid="0" count="29" tid="0" op="" dtype="" >6.5088e-05 9.5367e-07 1.4067e-05</hent>
<hent key="02400100000000000000070000000015" call="MPI_Isend" bytes="1792" orank="21" region="0" commid="0" count="35" tid="0" op="" dtype="" >7.9155e-05 1.9073e-06 3.0994e-06</hent>
<hent key="02400100000000000000070000000017" call="MPI_Isend" bytes="1792" orank="23" region="0" commid="0" count="33" tid="0" op="" dtype="" >1.8287e-04 4.0531e-06 7.1526e-06</hent>
<hent key="0240010000000000000007000000001E" call="MPI_Isend" bytes="1792" orank="30" region="0" commid="0" count="47" tid="0" op="" dtype="" >2.8777e-04 5.0068e-06 1.0967e-05</hent>
<hent key="02400100000000000000070000000056" call="MPI_Isend" bytes="1792" orank="86" region="0" commid="0" count="133" tid="0" op="" dtype="" >7.5507e-04 9.5367e-07 1.7881e-05</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1363e-01 1.1363e-01 1.1363e-01</hent>
<hent key="024001000000000000000700000002D6" call="MPI_Isend" bytes="1792" orank="726" region="0" commid="0" count="130" tid="0" op="" dtype="" >7.9536e-04 1.9073e-06 1.5974e-05</hent>
<hent key="024001000000000000000A000000000E" call="MPI_Isend" bytes="2560" orank="14" region="0" commid="0" count="14" tid="0" op="" dtype="" >4.5776e-05 9.5367e-07 1.2875e-05</hent>
<hent key="024001000000000000000A0000000015" call="MPI_Isend" bytes="2560" orank="21" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.0490e-05 2.1458e-06 3.0994e-06</hent>
<hent key="024001000000000000000A0000000017" call="MPI_Isend" bytes="2560" orank="23" region="0" commid="0" count="7" tid="0" op="" dtype="" >4.0770e-05 5.0068e-06 7.8678e-06</hent>
<hent key="024001000000000000000A000000001E" call="MPI_Isend" bytes="2560" orank="30" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.6478e-05 5.9605e-06 6.1989e-06</hent>
<hent key="024001000000000000000A0000000056" call="MPI_Isend" bytes="2560" orank="86" region="0" commid="0" count="53" tid="0" op="" dtype="" >3.1900e-04 4.0531e-06 1.5020e-05</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >5.0998e-04 5.6982e-05 2.3293e-04</hent>
<hent key="024001000000000000000A00000002D6" call="MPI_Isend" bytes="2560" orank="726" region="0" commid="0" count="43" tid="0" op="" dtype="" >2.6822e-04 4.0531e-06 1.0967e-05</hent>
<hent key="03800100000000000000100000000056" call="MPI_Irecv" bytes="4096" orank="86" region="0" commid="0" count="1" tid="0" op="" dtype="" >9.5367e-07 9.5367e-07 9.5367e-07</hent>
<hent key="038001000000000000001000000002D6" call="MPI_Irecv" bytes="4096" orank="726" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.7829e-03 6.2680e-04 1.1561e-03</hent>
<hent key="024001000000000000001000000002D6" call="MPI_Isend" bytes="4096" orank="726" region="0" commid="0" count="1" tid="0" op="" dtype="" >5.0068e-06 5.0068e-06 5.0068e-06</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.5061e-05 4.5061e-05 4.5061e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >5.7220e-06 1.9073e-06 3.8147e-06</hent>
<hent key="0380010000000000000000040000000E" call="MPI_Irecv" bytes="4" orank="14" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.8889e-04 0.0000e+00 8.1062e-06</hent>
<hent key="03800100000000000000000400000015" call="MPI_Irecv" bytes="4" orank="21" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0636e-03 0.0000e+00 6.1989e-06</hent>
<hent key="03800100000000000000000400000017" call="MPI_Irecv" bytes="4" orank="23" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.8069e-04 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000000040000001E" call="MPI_Irecv" bytes="4" orank="30" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.9557e-04 0.0000e+00 1.0967e-05</hent>
<hent key="03800100000000000000000400000056" call="MPI_Irecv" bytes="4" orank="86" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.0612e-03 0.0000e+00 1.8835e-05</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.4523e-03 4.4012e-04 1.2610e-03</hent>
<hent key="038001000000000000001C0000000015" call="MPI_Irecv" bytes="7168" orank="21" region="0" commid="0" count="8859" tid="0" op="" dtype="" >3.0336e-03 0.0000e+00 6.9141e-06</hent>
<hent key="038001000000000000001C0000000017" call="MPI_Irecv" bytes="7168" orank="23" region="0" commid="0" count="8949" tid="0" op="" dtype="" >2.1377e-03 0.0000e+00 1.9073e-05</hent>
<hent key="038001000000000000000004000002D6" call="MPI_Irecv" bytes="4" orank="726" region="0" commid="0" count="3398" tid="0" op="" dtype="" >9.6798e-04 0.0000e+00 1.8835e-05</hent>
<hent key="0240010000000000000000040000000E" call="MPI_Isend" bytes="4" orank="14" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.6472e-03 0.0000e+00 3.0041e-05</hent>
<hent key="02400100000000000000000400000015" call="MPI_Isend" bytes="4" orank="21" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.5891e-03 0.0000e+00 4.6015e-05</hent>
<hent key="02400100000000000000000400000017" call="MPI_Isend" bytes="4" orank="23" region="0" commid="0" count="3398" tid="0" op="" dtype="" >3.4902e-03 0.0000e+00 1.0014e-05</hent>
<hent key="0240010000000000000000040000001E" call="MPI_Isend" bytes="4" orank="30" region="0" commid="0" count="3398" tid="0" op="" dtype="" >3.2428e-02 3.8147e-06 3.0994e-05</hent>
<hent key="02400100000000000000000400000056" call="MPI_Isend" bytes="4" orank="86" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3779e-02 3.8147e-06 5.1975e-05</hent>
<hent key="024001000000000000001C0000000015" call="MPI_Isend" bytes="7168" orank="21" region="0" commid="0" count="9259" tid="0" op="" dtype="" >2.1549e-02 9.5367e-07 1.4067e-05</hent>
<hent key="024001000000000000001C0000000017" call="MPI_Isend" bytes="7168" orank="23" region="0" commid="0" count="9690" tid="0" op="" dtype="" >3.1791e-02 9.5367e-07 2.2888e-05</hent>
<hent key="024001000000000000000004000002D6" call="MPI_Isend" bytes="4" orank="726" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.3886e-02 3.8147e-06 1.1396e-04</hent>
<hent key="0380010000000000000005000000000E" call="MPI_Irecv" bytes="1280" orank="14" region="0" commid="0" count="207" tid="0" op="" dtype="" >4.8399e-05 0.0000e+00 2.8610e-06</hent>
<hent key="03800100000000000000050000000015" call="MPI_Irecv" bytes="1280" orank="21" region="0" commid="0" count="205" tid="0" op="" dtype="" >1.5974e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000050000000017" call="MPI_Irecv" bytes="1280" orank="23" region="0" commid="0" count="164" tid="0" op="" dtype="" >2.9087e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000001E" call="MPI_Irecv" bytes="1280" orank="30" region="0" commid="0" count="209" tid="0" op="" dtype="" >5.0068e-05 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000050000000056" call="MPI_Irecv" bytes="1280" orank="86" region="0" commid="0" count="308" tid="0" op="" dtype="" >1.1015e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0355e+01 1.3113e-05 1.5565e-01</hent>
<hent key="0380010000000000000028000000000E" call="MPI_Irecv" bytes="10240" orank="14" region="0" commid="0" count="31" tid="0" op="" dtype="" >5.7220e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000028000000001E" call="MPI_Irecv" bytes="10240" orank="30" region="0" commid="0" count="60" tid="0" op="" dtype="" >4.7684e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000500000002D6" call="MPI_Irecv" bytes="1280" orank="726" region="0" commid="0" count="294" tid="0" op="" dtype="" >1.5116e-04 0.0000e+00 6.9141e-06</hent>
<hent key="0380010000000000000008000000000E" call="MPI_Irecv" bytes="2048" orank="14" region="0" commid="0" count="14" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000015" call="MPI_Irecv" bytes="2048" orank="21" region="0" commid="0" count="8" tid="0" op="" dtype="" >6.1989e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000017" call="MPI_Irecv" bytes="2048" orank="23" region="0" commid="0" count="18" tid="0" op="" dtype="" >3.0994e-06 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000001E" call="MPI_Irecv" bytes="2048" orank="30" region="0" commid="0" count="15" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000056" call="MPI_Irecv" bytes="2048" orank="86" region="0" commid="0" count="3414" tid="0" op="" dtype="" >7.3433e-04 0.0000e+00 2.8849e-05</hent>
<hent key="038001000000000000000800000002D6" call="MPI_Irecv" bytes="2048" orank="726" region="0" commid="0" count="3462" tid="0" op="" dtype="" >1.0817e-03 0.0000e+00 3.6955e-05</hent>
<hent key="0240010000000000000005000000000E" call="MPI_Isend" bytes="1280" orank="14" region="0" commid="0" count="209" tid="0" op="" dtype="" >3.4571e-04 0.0000e+00 1.3113e-05</hent>
<hent key="02400100000000000000050000000015" call="MPI_Isend" bytes="1280" orank="21" region="0" commid="0" count="192" tid="0" op="" dtype="" >3.7932e-04 9.5367e-07 5.9605e-06</hent>
<hent key="02400100000000000000050000000017" call="MPI_Isend" bytes="1280" orank="23" region="0" commid="0" count="203" tid="0" op="" dtype="" >1.0154e-03 3.8147e-06 1.0967e-05</hent>
<hent key="0240010000000000000005000000001E" call="MPI_Isend" bytes="1280" orank="30" region="0" commid="0" count="186" tid="0" op="" dtype="" >1.1263e-03 9.5367e-07 2.1935e-05</hent>
<hent key="02400100000000000000050000000056" call="MPI_Isend" bytes="1280" orank="86" region="0" commid="0" count="289" tid="0" op="" dtype="" >1.5931e-03 3.8147e-06 1.8120e-05</hent>
<hent key="0240010000000000000028000000000E" call="MPI_Isend" bytes="10240" orank="14" region="0" commid="0" count="88" tid="0" op="" dtype="" >5.5552e-05 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000028000000001E" call="MPI_Isend" bytes="10240" orank="30" region="0" commid="0" count="32" tid="0" op="" dtype="" >3.3975e-04 5.0068e-06 1.9073e-05</hent>
<hent key="024001000000000000000500000002D6" call="MPI_Isend" bytes="1280" orank="726" region="0" commid="0" count="327" tid="0" op="" dtype="" >1.8094e-03 2.8610e-06 1.5974e-05</hent>
</hash>
<internal rank="22" log_i="1723712895.683646" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="23" mpi_size="768" stamp_init="1723712829.548138" stamp_final="1723712895.693006" username="apac4" allocationname="unknown" flags="0" pid="684291" >
<job nhosts="32" ntasks="768" start="1723712829" final="1723712895" cookie="nocookie" code="unknown" >187551</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09b</host>
<perf wtime="6.61449e+01" utime="5.06248e+01" stime="7.84030e+00" mtime="3.02127e+01" gflop="0.00000e+00" gbyte="3.76591e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.02127e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.60136e+01" utime="5.05935e+01" stime="7.82892e+00" mtime="3.02127e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.02127e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4972e+08" > 5.1852e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4832e+08" > 3.1828e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8411e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5738e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2227e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5830e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.5812e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0442e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1346e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8533e+01 </func>
</region>
</regions>
<comms size="2" >
<comm  id="0" size="0" > </comm>
<comm  id="1" size="768" > </comm>
</comms>
<hash nlog="383312" nkey="193" >
<hent key="0240010000000000000008000000000F" call="MPI_Isend" bytes="2048" orank="15" region="0" commid="0" count="12" tid="0" op="" dtype="" >5.4836e-05 1.1921e-06 1.3113e-05</hent>
<hent key="02400100000000000000080000000010" call="MPI_Isend" bytes="2048" orank="16" region="0" commid="0" count="13" tid="0" op="" dtype="" >5.3167e-05 3.8147e-06 5.0068e-06</hent>
<hent key="02400100000000000000080000000016" call="MPI_Isend" bytes="2048" orank="22" region="0" commid="0" count="18" tid="0" op="" dtype="" >3.9816e-05 1.9073e-06 3.0994e-06</hent>
<hent key="0240010000000000000008000000001F" call="MPI_Isend" bytes="2048" orank="31" region="0" commid="0" count="12" tid="0" op="" dtype="" >6.6996e-05 5.0068e-06 6.1989e-06</hent>
<hent key="02400100000000000000080000000057" call="MPI_Isend" bytes="2048" orank="87" region="0" commid="0" count="3472" tid="0" op="" dtype="" >1.0841e-02 9.5367e-07 2.1935e-05</hent>
<hent key="024001000000000000000800000002D7" call="MPI_Isend" bytes="2048" orank="727" region="0" commid="0" count="3463" tid="0" op="" dtype="" >1.0993e-02 9.5367e-07 1.0967e-05</hent>
<hent key="024001000000000000000E0000000057" call="MPI_Isend" bytes="3584" orank="87" region="0" commid="0" count="1" tid="0" op="" dtype="" >6.9141e-06 6.9141e-06 6.9141e-06</hent>
<hent key="0380010000000000000002800000000F" call="MPI_Irecv" bytes="640" orank="15" region="0" commid="0" count="386" tid="0" op="" dtype="" >8.9169e-05 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000028000000010" call="MPI_Irecv" bytes="640" orank="16" region="0" commid="0" count="401" tid="0" op="" dtype="" >1.0991e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000016" call="MPI_Irecv" bytes="640" orank="22" region="0" commid="0" count="394" tid="0" op="" dtype="" >1.2684e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000002800000001F" call="MPI_Irecv" bytes="640" orank="31" region="0" commid="0" count="429" tid="0" op="" dtype="" >1.2827e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000028000000057" call="MPI_Irecv" bytes="640" orank="87" region="0" commid="0" count="304" tid="0" op="" dtype="" >7.0572e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0A400100000102000000000E00000000" call="MPI_Gatherv" bytes="14" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000280000002D7" call="MPI_Irecv" bytes="640" orank="727" region="0" commid="0" count="296" tid="0" op="" dtype="" >1.3828e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000002800000000F" call="MPI_Isend" bytes="640" orank="15" region="0" commid="0" count="410" tid="0" op="" dtype="" >5.4121e-04 0.0000e+00 9.0599e-06</hent>
<hent key="02400100000000000000028000000010" call="MPI_Isend" bytes="640" orank="16" region="0" commid="0" count="383" tid="0" op="" dtype="" >1.1356e-03 1.9073e-06 4.0531e-06</hent>
<hent key="02400100000000000000028000000016" call="MPI_Isend" bytes="640" orank="22" region="0" commid="0" count="399" tid="0" op="" dtype="" >5.2261e-04 9.5367e-07 2.1458e-06</hent>
<hent key="0240010000000000000002800000001F" call="MPI_Isend" bytes="640" orank="31" region="0" commid="0" count="379" tid="0" op="" dtype="" >1.8501e-03 3.8147e-06 6.1989e-06</hent>
<hent key="02400100000000000000028000000057" call="MPI_Isend" bytes="640" orank="87" region="0" commid="0" count="261" tid="0" op="" dtype="" >1.1907e-03 3.8147e-06 1.5020e-05</hent>
<hent key="024001000000000000000280000002D7" call="MPI_Isend" bytes="640" orank="727" region="0" commid="0" count="273" tid="0" op="" dtype="" >1.2033e-03 3.0994e-06 1.0014e-05</hent>
<hent key="0900010000010200000001C000000000" call="MPI_Bcast" bytes="448" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0041e-05 3.0041e-05 3.0041e-05</hent>
<hent key="0380010000000000000001400000000F" call="MPI_Irecv" bytes="320" orank="15" region="0" commid="0" count="346" tid="0" op="" dtype="" >6.7949e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000010" call="MPI_Irecv" bytes="320" orank="16" region="0" commid="0" count="368" tid="0" op="" dtype="" >1.1134e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000016" call="MPI_Irecv" bytes="320" orank="22" region="0" commid="0" count="394" tid="0" op="" dtype="" >1.4472e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000001400000001F" call="MPI_Irecv" bytes="320" orank="31" region="0" commid="0" count="331" tid="0" op="" dtype="" >9.6560e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000014000000057" call="MPI_Irecv" bytes="320" orank="87" region="0" commid="0" count="165" tid="0" op="" dtype="" >3.7432e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000140000002D7" call="MPI_Irecv" bytes="320" orank="727" region="0" commid="0" count="174" tid="0" op="" dtype="" >7.4387e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0AC00100000102000000000800000000" call="MPI_Scatterv" bytes="8" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="04C00100000000000000000000000000" call="MPI_Waitall" bytes="0" orank="0" region="0" commid="0" count="106776" tid="0" op="" dtype="" >5.8411e+00 0.0000e+00 1.8643e-01</hent>
<hent key="03800100000000000000400000000057" call="MPI_Irecv" bytes="16384" orank="87" region="0" commid="0" count="12659" tid="0" op="" dtype="" >2.3937e-03 0.0000e+00 3.0994e-06</hent>
<hent key="09000100000102000000000100000000" call="MPI_Bcast" bytes="1" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.6442e-02 9.5367e-07 1.6425e-02</hent>
<hent key="038001000000000000004000000002D7" call="MPI_Irecv" bytes="16384" orank="727" region="0" commid="0" count="12690" tid="0" op="" dtype="" >6.5682e-03 0.0000e+00 6.9141e-06</hent>
<hent key="0A400100000102000000000800000000" call="MPI_Gatherv" bytes="8" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >2.1458e-06 0.0000e+00 2.1458e-06</hent>
<hent key="0240010000000000000001400000000F" call="MPI_Isend" bytes="320" orank="15" region="0" commid="0" count="349" tid="0" op="" dtype="" >3.8791e-04 0.0000e+00 8.1062e-06</hent>
<hent key="02400100000000000000014000000010" call="MPI_Isend" bytes="320" orank="16" region="0" commid="0" count="348" tid="0" op="" dtype="" >9.4271e-04 1.9073e-06 8.1062e-06</hent>
<hent key="02400100000000000000014000000016" call="MPI_Isend" bytes="320" orank="22" region="0" commid="0" count="358" tid="0" op="" dtype="" >3.5763e-04 0.0000e+00 4.0531e-06</hent>
<hent key="0240010000000000000001400000001F" call="MPI_Isend" bytes="320" orank="31" region="0" commid="0" count="389" tid="0" op="" dtype="" >1.8084e-03 3.8147e-06 1.0014e-05</hent>
<hent key="02400100000000000000014000000057" call="MPI_Isend" bytes="320" orank="87" region="0" commid="0" count="174" tid="0" op="" dtype="" >7.5102e-04 2.8610e-06 5.9605e-06</hent>
<hent key="0380010000000000000020000000000F" call="MPI_Irecv" bytes="8192" orank="15" region="0" commid="0" count="12667" tid="0" op="" dtype="" >4.4053e-03 0.0000e+00 7.8678e-06</hent>
<hent key="03800100000000000000200000000010" call="MPI_Irecv" bytes="8192" orank="16" region="0" commid="0" count="3421" tid="0" op="" dtype="" >5.1904e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000200000000016" call="MPI_Irecv" bytes="8192" orank="22" region="0" commid="0" count="3009" tid="0" op="" dtype="" >1.0850e-03 0.0000e+00 3.0994e-06</hent>
<hent key="0380010000000000000020000000001F" call="MPI_Irecv" bytes="8192" orank="31" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.5552e-03 0.0000e+00 2.1458e-06</hent>
<hent key="024001000000000000000140000002D7" call="MPI_Isend" bytes="320" orank="727" region="0" commid="0" count="203" tid="0" op="" dtype="" >8.7881e-04 3.0994e-06 9.0599e-06</hent>
<hent key="0380010000000000000000000000000F" call="MPI_Irecv" bytes="0" orank="15" region="0" commid="0" count="245" tid="0" op="" dtype="" >6.0320e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000010" call="MPI_Irecv" bytes="0" orank="16" region="0" commid="0" count="250" tid="0" op="" dtype="" >6.5804e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000016" call="MPI_Irecv" bytes="0" orank="22" region="0" commid="0" count="235" tid="0" op="" dtype="" >7.3195e-05 0.0000e+00 2.1458e-06</hent>
<hent key="0380010000000000000000000000001F" call="MPI_Irecv" bytes="0" orank="31" region="0" commid="0" count="252" tid="0" op="" dtype="" >5.3883e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000000000057" call="MPI_Irecv" bytes="0" orank="87" region="0" commid="0" count="145" tid="0" op="" dtype="" >3.2187e-05 0.0000e+00 1.1921e-06</hent>
<hent key="02400100000000000000400000000057" call="MPI_Isend" bytes="16384" orank="87" region="0" commid="0" count="12665" tid="0" op="" dtype="" >1.0150e-01 3.8147e-06 2.8133e-05</hent>
<hent key="038001000000000000000000000002D7" call="MPI_Irecv" bytes="0" orank="727" region="0" commid="0" count="140" tid="0" op="" dtype="" >3.3855e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000004000000002D7" call="MPI_Isend" bytes="16384" orank="727" region="0" commid="0" count="12683" tid="0" op="" dtype="" >1.0200e-01 4.0531e-06 2.7895e-05</hent>
<hent key="09000100000102000000000800000000" call="MPI_Bcast" bytes="8" orank="0" region="0" commid="1" count="25" tid="0" op="" dtype="MPI_BYTE" >4.4036e-04 0.0000e+00 2.7895e-04</hent>
<hent key="0240010000000000000020000000000F" call="MPI_Isend" bytes="8192" orank="15" region="0" commid="0" count="12674" tid="0" op="" dtype="" >6.3636e-03 0.0000e+00 1.5974e-05</hent>
<hent key="02400100000000000000200000000010" call="MPI_Isend" bytes="8192" orank="16" region="0" commid="0" count="4033" tid="0" op="" dtype="" >7.2503e-03 0.0000e+00 2.1935e-05</hent>
<hent key="02400100000000000000200000000016" call="MPI_Isend" bytes="8192" orank="22" region="0" commid="0" count="3750" tid="0" op="" dtype="" >4.6582e-03 0.0000e+00 9.7990e-05</hent>
<hent key="0240010000000000000020000000001F" call="MPI_Isend" bytes="8192" orank="31" region="0" commid="0" count="12699" tid="0" op="" dtype="" >1.1233e-01 3.8147e-06 6.9857e-05</hent>
<hent key="09000100000102000000380000000000" call="MPI_Bcast" bytes="14336" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >8.3923e-05 8.3923e-05 8.3923e-05</hent>
<hent key="0240010000000000000000000000000F" call="MPI_Isend" bytes="0" orank="15" region="0" commid="0" count="252" tid="0" op="" dtype="" >1.9813e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000000000000010" call="MPI_Isend" bytes="0" orank="16" region="0" commid="0" count="258" tid="0" op="" dtype="" >5.4359e-04 9.5367e-07 5.1975e-05</hent>
<hent key="02400100000000000000000000000016" call="MPI_Isend" bytes="0" orank="22" region="0" commid="0" count="263" tid="0" op="" dtype="" >1.7262e-04 0.0000e+00 6.1989e-06</hent>
<hent key="0240010000000000000000000000001F" call="MPI_Isend" bytes="0" orank="31" region="0" commid="0" count="259" tid="0" op="" dtype="" >9.4914e-04 9.5367e-07 6.1989e-06</hent>
<hent key="02400100000000000000000000000057" call="MPI_Isend" bytes="0" orank="87" region="0" commid="0" count="154" tid="0" op="" dtype="" >5.5289e-04 9.5367e-07 8.8215e-06</hent>
<hent key="024001000000000000000000000002D7" call="MPI_Isend" bytes="0" orank="727" region="0" commid="0" count="137" tid="0" op="" dtype="" >5.3263e-04 9.5367e-07 8.8215e-06</hent>
<hent key="0380010000000000000006000000000F" call="MPI_Irecv" bytes="1536" orank="15" region="0" commid="0" count="93" tid="0" op="" dtype="" >2.2411e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000010" call="MPI_Irecv" bytes="1536" orank="16" region="0" commid="0" count="100" tid="0" op="" dtype="" >2.9802e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000016" call="MPI_Irecv" bytes="1536" orank="22" region="0" commid="0" count="97" tid="0" op="" dtype="" >3.8624e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000006000000001F" call="MPI_Irecv" bytes="1536" orank="31" region="0" commid="0" count="95" tid="0" op="" dtype="" >2.7180e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000060000000057" call="MPI_Irecv" bytes="1536" orank="87" region="0" commid="0" count="213" tid="0" op="" dtype="" >4.0054e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000600000002D7" call="MPI_Irecv" bytes="1536" orank="727" region="0" commid="0" count="194" tid="0" op="" dtype="" >1.2469e-04 0.0000e+00 5.9605e-06</hent>
<hent key="01000100000000000000000000000000" call="MPI_Comm_size" bytes="0" orank="0" region="0" commid="0" count="40" tid="0" op="" dtype="" >7.3910e-06 0.0000e+00 2.1458e-06</hent>
<hent key="00C00100000000000000000000000000" call="MPI_Comm_rank" bytes="0" orank="0" region="0" commid="0" count="74" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0240010000000000000006000000000F" call="MPI_Isend" bytes="1536" orank="15" region="0" commid="0" count="95" tid="0" op="" dtype="" >1.7858e-04 9.5367e-07 9.0599e-06</hent>
<hent key="02400100000000000000060000000010" call="MPI_Isend" bytes="1536" orank="16" region="0" commid="0" count="109" tid="0" op="" dtype="" >4.0007e-04 2.8610e-06 9.0599e-06</hent>
<hent key="02400100000000000000060000000016" call="MPI_Isend" bytes="1536" orank="22" region="0" commid="0" count="100" tid="0" op="" dtype="" >1.9312e-04 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000006000000001F" call="MPI_Isend" bytes="1536" orank="31" region="0" commid="0" count="93" tid="0" op="" dtype="" >4.9329e-04 4.0531e-06 1.0967e-05</hent>
<hent key="02400100000000000000060000000057" call="MPI_Isend" bytes="1536" orank="87" region="0" commid="0" count="213" tid="0" op="" dtype="" >1.0440e-03 3.8147e-06 1.0967e-05</hent>
<hent key="00800100000000000000000000000000" call="MPI_Finalize" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="024001000000000000000600000002D7" call="MPI_Isend" bytes="1536" orank="727" region="0" commid="0" count="201" tid="0" op="" dtype="" >9.7871e-04 3.8147e-06 1.0014e-05</hent>
<hent key="038001000000000000000C0000000057" call="MPI_Irecv" bytes="3072" orank="87" region="0" commid="0" count="10" tid="0" op="" dtype="" >2.1458e-06 0.0000e+00 1.1921e-06</hent>
<hent key="00000100000000000000000000000000" call="MPI_Init" bytes="0" orank="0" region="0" commid="0" count="1" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000C00000002D7" call="MPI_Irecv" bytes="3072" orank="727" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="024001000000000000000C000000000F" call="MPI_Isend" bytes="3072" orank="15" region="0" commid="0" count="1" tid="0" op="" dtype="" >7.1526e-06 7.1526e-06 7.1526e-06</hent>
<hent key="024001000000000000000C000000001F" call="MPI_Isend" bytes="3072" orank="31" region="0" commid="0" count="1" tid="0" op="" dtype="" >6.9141e-06 6.9141e-06 6.9141e-06</hent>
<hent key="024001000000000000000C0000000057" call="MPI_Isend" bytes="3072" orank="87" region="0" commid="0" count="7" tid="0" op="" dtype="" >3.9339e-05 5.0068e-06 6.1989e-06</hent>
<hent key="024001000000000000000C00000002D7" call="MPI_Isend" bytes="3072" orank="727" region="0" commid="0" count="6" tid="0" op="" dtype="" >3.4094e-05 5.0068e-06 6.1989e-06</hent>
<hent key="0AC00100000102000000700000000000" call="MPI_Scatterv" bytes="28672" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >4.8399e-04 4.8399e-04 4.8399e-04</hent>
<hent key="0AC00100000102000000500000000000" call="MPI_Scatterv" bytes="20480" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >1.1060e-03 3.5810e-04 3.7694e-04</hent>
<hent key="0380010000000000000003800000000F" call="MPI_Irecv" bytes="896" orank="15" region="0" commid="0" count="360" tid="0" op="" dtype="" >7.7724e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000038000000010" call="MPI_Irecv" bytes="896" orank="16" region="0" commid="0" count="2802" tid="0" op="" dtype="" >5.3740e-04 0.0000e+00 4.0531e-06</hent>
<hent key="03800100000000000000038000000016" call="MPI_Irecv" bytes="896" orank="22" region="0" commid="0" count="2928" tid="0" op="" dtype="" >5.5981e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000003800000001F" call="MPI_Irecv" bytes="896" orank="31" region="0" commid="0" count="328" tid="0" op="" dtype="" >1.0657e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000038000000057" call="MPI_Irecv" bytes="896" orank="87" region="0" commid="0" count="332" tid="0" op="" dtype="" >7.4863e-05 0.0000e+00 1.9073e-06</hent>
<hent key="09000100000102000000001000000000" call="MPI_Bcast" bytes="16" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >9.0599e-06 9.0599e-06 9.0599e-06</hent>
<hent key="038001000000000000000380000002D7" call="MPI_Irecv" bytes="896" orank="727" region="0" commid="0" count="324" tid="0" op="" dtype="" >1.5831e-04 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000107000000006000000000" call="MPI_Bcast" bytes="96" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_DOUBLE" >1.9073e-06 1.9073e-06 1.9073e-06</hent>
<hent key="03800100000000000000380000000057" call="MPI_Irecv" bytes="14336" orank="87" region="0" commid="0" count="40" tid="0" op="" dtype="" >4.0531e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000003800000002D7" call="MPI_Irecv" bytes="14336" orank="727" region="0" commid="0" count="9" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0240010000000000000003800000000F" call="MPI_Isend" bytes="896" orank="15" region="0" commid="0" count="310" tid="0" op="" dtype="" >4.8757e-04 9.5367e-07 6.9141e-06</hent>
<hent key="02400100000000000000038000000010" call="MPI_Isend" bytes="896" orank="16" region="0" commid="0" count="2667" tid="0" op="" dtype="" >2.6538e-03 0.0000e+00 3.0994e-05</hent>
<hent key="02400100000000000000038000000016" call="MPI_Isend" bytes="896" orank="22" region="0" commid="0" count="2751" tid="0" op="" dtype="" >1.9486e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000003800000001F" call="MPI_Isend" bytes="896" orank="31" region="0" commid="0" count="340" tid="0" op="" dtype="" >1.7078e-03 3.8147e-06 1.0967e-05</hent>
<hent key="02400100000000000000038000000057" call="MPI_Isend" bytes="896" orank="87" region="0" commid="0" count="321" tid="0" op="" dtype="" >1.4844e-03 3.8147e-06 5.9605e-06</hent>
<hent key="024001000000000000000380000002D7" call="MPI_Isend" bytes="896" orank="727" region="0" commid="0" count="342" tid="0" op="" dtype="" >1.5690e-03 2.8610e-06 1.0014e-05</hent>
<hent key="02400100000000000000380000000057" call="MPI_Isend" bytes="14336" orank="87" region="0" commid="0" count="34" tid="0" op="" dtype="" >2.7609e-04 5.9605e-06 1.0014e-05</hent>
<hent key="024001000000000000003800000002D7" call="MPI_Isend" bytes="14336" orank="727" region="0" commid="0" count="16" tid="0" op="" dtype="" >1.2398e-04 6.9141e-06 9.0599e-06</hent>
<hent key="0BC00100000104100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="10945" tid="0" op="MPI_MAX" dtype="MPI_INT" >7.7395e+00 9.0599e-06 2.0316e-01</hent>
<hent key="0BC00100000104300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_SUM" dtype="MPI_INT" >3.2020e-03 3.2020e-03 3.2020e-03</hent>
<hent key="0B400100000104000000000400000000" call="MPI_Allgather" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_INT" >1.0442e-02 1.0442e-02 1.0442e-02</hent>
<hent key="0BC0010000010A100000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_MAX" dtype="MPI_UNSIGNED" >2.5501e-02 2.5501e-02 2.5501e-02</hent>
<hent key="0BC0010000010A300000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="3" tid="0" op="MPI_SUM" dtype="MPI_UNSIGNED" >2.0197e-01 1.2670e-03 1.8421e-01</hent>
<hent key="0BC0010000010A700000000400000000" call="MPI_Allreduce" bytes="4" orank="0" region="0" commid="1" count="1" tid="0" op="MPI_LOR" dtype="MPI_UNSIGNED" >3.6407e-04 3.6407e-04 3.6407e-04</hent>
<hent key="0AC00100000102000000000400000000" call="MPI_Scatterv" bytes="4" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="0A800100000104000000000400000000" call="MPI_Scatter" bytes="4" orank="0" region="0" commid="1" count="19" tid="0" op="" dtype="MPI_INT" >1.5830e+00 3.9196e-04 2.5034e-01</hent>
<hent key="0380010000000000000004000000000F" call="MPI_Irecv" bytes="1024" orank="15" region="0" commid="0" count="3390" tid="0" op="" dtype="" >5.8079e-04 0.0000e+00 2.1458e-06</hent>
<hent key="03800100000000000000040000000010" call="MPI_Irecv" bytes="1024" orank="16" region="0" commid="0" count="932" tid="0" op="" dtype="" >1.7929e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000040000000016" call="MPI_Irecv" bytes="1024" orank="22" region="0" commid="0" count="788" tid="0" op="" dtype="" >1.3256e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000004000000001F" call="MPI_Irecv" bytes="1024" orank="31" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.6887e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0A000100000104000000000400000000" call="MPI_Gather" bytes="4" orank="0" region="0" commid="1" count="5" tid="0" op="" dtype="MPI_INT" >2.2888e-05 0.0000e+00 1.1206e-05</hent>
<hent key="09000100000102000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >3.0994e-06 3.0994e-06 3.0994e-06</hent>
<hent key="0380010000000000000007000000000F" call="MPI_Irecv" bytes="1792" orank="15" region="0" commid="0" count="39" tid="0" op="" dtype="" >7.1526e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000010" call="MPI_Irecv" bytes="1792" orank="16" region="0" commid="0" count="41" tid="0" op="" dtype="" >1.1921e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000016" call="MPI_Irecv" bytes="1792" orank="22" region="0" commid="0" count="33" tid="0" op="" dtype="" >1.3113e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000007000000001F" call="MPI_Irecv" bytes="1792" orank="31" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.3590e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000070000000057" call="MPI_Irecv" bytes="1792" orank="87" region="0" commid="0" count="162" tid="0" op="" dtype="" >4.0293e-05 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000700000002D7" call="MPI_Irecv" bytes="1792" orank="727" region="0" commid="0" count="136" tid="0" op="" dtype="" >8.1539e-05 0.0000e+00 1.1921e-06</hent>
<hent key="09000100000102000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="10" tid="0" op="" dtype="MPI_BYTE" >4.2915e-05 0.0000e+00 2.7180e-05</hent>
<hent key="0240010000000000000004000000000F" call="MPI_Isend" bytes="1024" orank="15" region="0" commid="0" count="3392" tid="0" op="" dtype="" >2.2037e-03 0.0000e+00 1.5020e-05</hent>
<hent key="02400100000000000000040000000010" call="MPI_Isend" bytes="1024" orank="16" region="0" commid="0" count="1074" tid="0" op="" dtype="" >7.3338e-04 0.0000e+00 6.9141e-06</hent>
<hent key="02400100000000000000040000000016" call="MPI_Isend" bytes="1024" orank="22" region="0" commid="0" count="990" tid="0" op="" dtype="" >6.2418e-04 0.0000e+00 8.1062e-06</hent>
<hent key="0240010000000000000004000000001F" call="MPI_Isend" bytes="1024" orank="31" region="0" commid="0" count="3398" tid="0" op="" dtype="" >9.5100e-03 9.5367e-07 9.5844e-05</hent>
<hent key="09000100000107000000004000000000" call="MPI_Bcast" bytes="64" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_DOUBLE" >5.0068e-06 1.9073e-06 3.0994e-06</hent>
<hent key="09000100000104000000000400000000" call="MPI_Bcast" bytes="4" orank="0" region="0" commid="1" count="44" tid="0" op="" dtype="MPI_INT" >3.5567e+00 0.0000e+00 2.2673e+00</hent>
<hent key="038001000000000000000A000000000F" call="MPI_Irecv" bytes="2560" orank="15" region="0" commid="0" count="4" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000010" call="MPI_Irecv" bytes="2560" orank="16" region="0" commid="0" count="2" tid="0" op="" dtype="" >9.5367e-07 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A0000000016" call="MPI_Irecv" bytes="2560" orank="22" region="0" commid="0" count="7" tid="0" op="" dtype="" >1.9073e-06 0.0000e+00 9.5367e-07</hent>
<hent key="038001000000000000000A000000001F" call="MPI_Irecv" bytes="2560" orank="31" region="0" commid="0" count="3" tid="0" op="" dtype="" >0.0000e+00 0.0000e+00 0.0000e+00</hent>
<hent key="038001000000000000000A0000000057" call="MPI_Irecv" bytes="2560" orank="87" region="0" commid="0" count="40" tid="0" op="" dtype="" >7.8678e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000A00000002D7" call="MPI_Irecv" bytes="2560" orank="727" region="0" commid="0" count="40" tid="0" op="" dtype="" >2.6226e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000007000000000F" call="MPI_Isend" bytes="1792" orank="15" region="0" commid="0" count="49" tid="0" op="" dtype="" >1.1182e-04 9.5367e-07 1.5974e-05</hent>
<hent key="02400100000000000000070000000010" call="MPI_Isend" bytes="1792" orank="16" region="0" commid="0" count="44" tid="0" op="" dtype="" >1.6785e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000070000000016" call="MPI_Isend" bytes="1792" orank="22" region="0" commid="0" count="46" tid="0" op="" dtype="" >9.6321e-05 9.5367e-07 3.0994e-06</hent>
<hent key="0240010000000000000007000000001F" call="MPI_Isend" bytes="1792" orank="31" region="0" commid="0" count="32" tid="0" op="" dtype="" >1.7214e-04 4.7684e-06 6.1989e-06</hent>
<hent key="02400100000000000000070000000057" call="MPI_Isend" bytes="1792" orank="87" region="0" commid="0" count="145" tid="0" op="" dtype="" >7.0858e-04 9.5367e-07 6.1989e-06</hent>
<hent key="0B800100000102000000280000000000" call="MPI_Allgatherv" bytes="10240" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >1.1346e-01 1.1346e-01 1.1346e-01</hent>
<hent key="024001000000000000000700000002D7" call="MPI_Isend" bytes="1792" orank="727" region="0" commid="0" count="119" tid="0" op="" dtype="" >5.7721e-04 9.5367e-07 6.1989e-06</hent>
<hent key="024001000000000000000A000000000F" call="MPI_Isend" bytes="2560" orank="15" region="0" commid="0" count="3" tid="0" op="" dtype="" >2.0981e-05 1.9073e-06 1.6928e-05</hent>
<hent key="024001000000000000000A0000000010" call="MPI_Isend" bytes="2560" orank="16" region="0" commid="0" count="4" tid="0" op="" dtype="" >1.6928e-05 3.8147e-06 5.0068e-06</hent>
<hent key="024001000000000000000A0000000016" call="MPI_Isend" bytes="2560" orank="22" region="0" commid="0" count="8" tid="0" op="" dtype="" >2.1696e-05 1.9073e-06 3.0994e-06</hent>
<hent key="024001000000000000000A000000001F" call="MPI_Isend" bytes="2560" orank="31" region="0" commid="0" count="5" tid="0" op="" dtype="" >2.9087e-05 5.0068e-06 6.1989e-06</hent>
<hent key="024001000000000000000A0000000057" call="MPI_Isend" bytes="2560" orank="87" region="0" commid="0" count="45" tid="0" op="" dtype="" >2.4056e-04 4.7684e-06 6.1989e-06</hent>
<hent key="0AC00100000102000000280000000000" call="MPI_Scatterv" bytes="10240" orank="0" region="0" commid="1" count="3" tid="0" op="" dtype="MPI_BYTE" >5.2786e-04 5.7936e-05 2.3794e-04</hent>
<hent key="024001000000000000000A00000002D7" call="MPI_Isend" bytes="2560" orank="727" region="0" commid="0" count="54" tid="0" op="" dtype="" >2.8777e-04 4.7684e-06 6.1989e-06</hent>
<hent key="0AC00100000102000001400000000000" call="MPI_Scatterv" bytes="81920" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >1.8673e-03 6.5613e-04 1.2112e-03</hent>
<hent key="0A400100000102000000002000000000" call="MPI_Gatherv" bytes="32" orank="0" region="0" commid="1" count="1" tid="0" op="" dtype="MPI_BYTE" >6.0081e-05 6.0081e-05 6.0081e-05</hent>
<hent key="09000100000102000000007000000000" call="MPI_Bcast" bytes="112" orank="0" region="0" commid="1" count="2" tid="0" op="" dtype="MPI_BYTE" >3.8147e-06 9.5367e-07 2.8610e-06</hent>
<hent key="0380010000000000000000040000000F" call="MPI_Irecv" bytes="4" orank="15" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.0987e-04 0.0000e+00 5.9605e-06</hent>
<hent key="03800100000000000000000400000010" call="MPI_Irecv" bytes="4" orank="16" region="0" commid="0" count="3398" tid="0" op="" dtype="" >4.5943e-04 0.0000e+00 5.0068e-06</hent>
<hent key="03800100000000000000000400000016" call="MPI_Irecv" bytes="4" orank="22" region="0" commid="0" count="3398" tid="0" op="" dtype="" >8.6164e-04 0.0000e+00 5.0068e-06</hent>
<hent key="0380010000000000000000040000001F" call="MPI_Irecv" bytes="4" orank="31" region="0" commid="0" count="3398" tid="0" op="" dtype="" >5.1117e-04 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000000400000057" call="MPI_Irecv" bytes="4" orank="87" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.1607e-04 0.0000e+00 5.9605e-06</hent>
<hent key="0AC00100000102000000E00000000000" call="MPI_Scatterv" bytes="57344" orank="0" region="0" commid="1" count="4" tid="0" op="" dtype="MPI_BYTE" >3.5942e-03 4.5800e-04 1.3080e-03</hent>
<hent key="038001000000000000001C0000000010" call="MPI_Irecv" bytes="7168" orank="16" region="0" commid="0" count="9278" tid="0" op="" dtype="" >1.4508e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000001C0000000016" call="MPI_Irecv" bytes="7168" orank="22" region="0" commid="0" count="9690" tid="0" op="" dtype="" >3.4037e-03 0.0000e+00 5.0068e-06</hent>
<hent key="038001000000000000000004000002D7" call="MPI_Irecv" bytes="4" orank="727" region="0" commid="0" count="3398" tid="0" op="" dtype="" >6.0511e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000000040000000F" call="MPI_Isend" bytes="4" orank="15" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2527e-03 0.0000e+00 3.0041e-05</hent>
<hent key="02400100000000000000000400000010" call="MPI_Isend" bytes="4" orank="16" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.1892e-03 0.0000e+00 5.9128e-05</hent>
<hent key="02400100000000000000000400000016" call="MPI_Isend" bytes="4" orank="22" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.2867e-03 0.0000e+00 5.9605e-06</hent>
<hent key="0240010000000000000000040000001F" call="MPI_Isend" bytes="4" orank="31" region="0" commid="0" count="3398" tid="0" op="" dtype="" >2.4922e-02 4.7684e-06 2.4080e-05</hent>
<hent key="02400100000000000000000400000057" call="MPI_Isend" bytes="4" orank="87" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.9787e-02 3.8147e-06 1.4067e-05</hent>
<hent key="024001000000000000001C0000000010" call="MPI_Isend" bytes="7168" orank="16" region="0" commid="0" count="8666" tid="0" op="" dtype="" >2.5520e-02 9.5367e-07 1.6928e-05</hent>
<hent key="024001000000000000001C0000000016" call="MPI_Isend" bytes="7168" orank="22" region="0" commid="0" count="8949" tid="0" op="" dtype="" >2.0504e-02 9.5367e-07 1.1921e-05</hent>
<hent key="024001000000000000000004000002D7" call="MPI_Isend" bytes="4" orank="727" region="0" commid="0" count="3398" tid="0" op="" dtype="" >1.8007e-02 3.8147e-06 1.0896e-04</hent>
<hent key="0380010000000000000005000000000F" call="MPI_Irecv" bytes="1280" orank="15" region="0" commid="0" count="219" tid="0" op="" dtype="" >5.4836e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000010" call="MPI_Irecv" bytes="1280" orank="16" region="0" commid="0" count="188" tid="0" op="" dtype="" >5.9366e-05 0.0000e+00 1.9073e-06</hent>
<hent key="03800100000000000000050000000016" call="MPI_Irecv" bytes="1280" orank="22" region="0" commid="0" count="203" tid="0" op="" dtype="" >7.6056e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000005000000001F" call="MPI_Irecv" bytes="1280" orank="31" region="0" commid="0" count="211" tid="0" op="" dtype="" >6.9857e-05 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000050000000057" call="MPI_Irecv" bytes="1280" orank="87" region="0" commid="0" count="276" tid="0" op="" dtype="" >7.3671e-05 0.0000e+00 1.1921e-06</hent>
<hent key="0BC00100000107300000005000000000" call="MPI_Allreduce" bytes="80" orank="0" region="0" commid="1" count="11000" tid="0" op="MPI_SUM" dtype="MPI_DOUBLE" >1.0562e+01 7.8678e-06 1.5572e-01</hent>
<hent key="0380010000000000000028000000000F" call="MPI_Irecv" bytes="10240" orank="15" region="0" commid="0" count="32" tid="0" op="" dtype="" >8.8215e-06 0.0000e+00 1.1921e-06</hent>
<hent key="038001000000000000000500000002D7" call="MPI_Irecv" bytes="1280" orank="727" region="0" commid="0" count="313" tid="0" op="" dtype="" >1.9002e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0380010000000000000008000000000F" call="MPI_Irecv" bytes="2048" orank="15" region="0" commid="0" count="15" tid="0" op="" dtype="" >3.3379e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000010" call="MPI_Irecv" bytes="2048" orank="16" region="0" commid="0" count="13" tid="0" op="" dtype="" >2.8610e-06 0.0000e+00 9.5367e-07</hent>
<hent key="03800100000000000000080000000016" call="MPI_Irecv" bytes="2048" orank="22" region="0" commid="0" count="18" tid="0" op="" dtype="" >8.5831e-06 0.0000e+00 9.5367e-07</hent>
<hent key="0380010000000000000008000000001F" call="MPI_Irecv" bytes="2048" orank="31" region="0" commid="0" count="18" tid="0" op="" dtype="" >8.8215e-06 0.0000e+00 1.1921e-06</hent>
<hent key="03800100000000000000080000000057" call="MPI_Irecv" bytes="2048" orank="87" region="0" commid="0" count="3450" tid="0" op="" dtype="" >5.7364e-04 0.0000e+00 4.0531e-06</hent>
<hent key="038001000000000000000800000002D7" call="MPI_Irecv" bytes="2048" orank="727" region="0" commid="0" count="3478" tid="0" op="" dtype="" >6.6543e-04 0.0000e+00 1.1921e-06</hent>
<hent key="0240010000000000000005000000000F" call="MPI_Isend" bytes="1280" orank="15" region="0" commid="0" count="224" tid="0" op="" dtype="" >3.6573e-04 0.0000e+00 5.9605e-06</hent>
<hent key="02400100000000000000050000000010" call="MPI_Isend" bytes="1280" orank="16" region="0" commid="0" count="197" tid="0" op="" dtype="" >6.8402e-04 2.8610e-06 5.0068e-06</hent>
<hent key="02400100000000000000050000000016" call="MPI_Isend" bytes="1280" orank="22" region="0" commid="0" count="164" tid="0" op="" dtype="" >3.1042e-04 9.5367e-07 8.1062e-06</hent>
<hent key="0240010000000000000005000000001F" call="MPI_Isend" bytes="1280" orank="31" region="0" commid="0" count="189" tid="0" op="" dtype="" >9.8848e-04 4.0531e-06 6.1989e-06</hent>
<hent key="02400100000000000000050000000057" call="MPI_Isend" bytes="1280" orank="87" region="0" commid="0" count="304" tid="0" op="" dtype="" >1.4796e-03 3.8147e-06 1.3113e-05</hent>
<hent key="0240010000000000000028000000000F" call="MPI_Isend" bytes="10240" orank="15" region="0" commid="0" count="25" tid="0" op="" dtype="" >1.5259e-05 0.0000e+00 1.1921e-06</hent>
<hent key="024001000000000000000500000002D7" call="MPI_Isend" bytes="1280" orank="727" region="0" commid="0" count="299" tid="0" op="" dtype="" >1.3840e-03 3.8147e-06 6.1989e-06</hent>
</hash>
<internal rank="23" log_i="1723712895.693006" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="24" mpi_size="768" stamp_init="1723712830.151391" stamp_final="1723712895.688261" username="apac4" allocationname="unknown" flags="0" pid="3301240" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55369e+01" utime="4.29664e+01" stime="1.22549e+01" mtime="2.83487e+01" gflop="0.00000e+00" gbyte="3.85517e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83487e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54115e+01" utime="4.29383e+01" stime="1.22407e+01" mtime="2.83487e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83487e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5041e+08" > 5.5159e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4763e+08" > 5.0898e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3396e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6189e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.0415e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5834e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.9536e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0440e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1346e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8071e+01 </func>
</region>
</regions>
<internal rank="24" log_i="1723712895.688261" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="25" mpi_size="768" stamp_init="1723712830.151359" stamp_final="1723712895.689515" username="apac4" allocationname="unknown" flags="0" pid="3301241" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55382e+01" utime="4.92334e+01" stime="7.14291e+00" mtime="2.83728e+01" gflop="0.00000e+00" gbyte="3.79848e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83728e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000045144514db" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54030e+01" utime="4.92030e+01" stime="7.12970e+00" mtime="2.83728e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83728e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4729e+08" > 4.6938e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4984e+08" > 3.1193e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.4364e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6109e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7628e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5837e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.6975e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0453e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1320e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9099e+01 </func>
</region>
</regions>
<internal rank="25" log_i="1723712895.689515" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="26" mpi_size="768" stamp_init="1723712830.151771" stamp_final="1723712895.677570" username="apac4" allocationname="unknown" flags="0" pid="3301242" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55258e+01" utime="4.72571e+01" stime="7.92045e+00" mtime="2.81414e+01" gflop="0.00000e+00" gbyte="3.77911e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81414e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b914b814ce" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53986e+01" utime="4.72261e+01" stime="7.90909e+00" mtime="2.81414e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81414e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.3447e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4816e+08" > 5.8376e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4835e+08" > 3.7050e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0087e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5997e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.1506e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8798e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5840e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.2822e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0446e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1346e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8178e+01 </func>
</region>
</regions>
<internal rank="26" log_i="1723712895.677570" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="27" mpi_size="768" stamp_init="1723712830.151364" stamp_final="1723712895.690107" username="apac4" allocationname="unknown" flags="0" pid="3301243" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55387e+01" utime="4.88923e+01" stime="7.48471e+00" mtime="2.84186e+01" gflop="0.00000e+00" gbyte="3.77789e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84186e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54112e+01" utime="4.88654e+01" stime="7.46871e+00" mtime="2.84186e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84186e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4904e+08" > 4.5391e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4872e+08" > 3.1801e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0179e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6164e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9149e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5831e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.2120e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0430e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1319e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8573e+01 </func>
</region>
</regions>
<internal rank="27" log_i="1723712895.690107" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="28" mpi_size="768" stamp_init="1723712830.151397" stamp_final="1723712895.683668" username="apac4" allocationname="unknown" flags="0" pid="3301244" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55323e+01" utime="4.56325e+01" stime="8.57782e+00" mtime="2.79556e+01" gflop="0.00000e+00" gbyte="3.76835e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79556e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ad14ad14e4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54066e+01" utime="4.56060e+01" stime="8.56206e+00" mtime="2.79556e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79556e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4861e+08" > 7.3564e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4800e+08" > 6.3264e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1080e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5939e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.3941e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7790e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5826e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.2456e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0453e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1343e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7729e+01 </func>
</region>
</regions>
<internal rank="28" log_i="1723712895.683668" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="29" mpi_size="768" stamp_init="1723712830.151335" stamp_final="1723712895.681141" username="apac4" allocationname="unknown" flags="0" pid="3301245" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55298e+01" utime="4.90090e+01" stime="7.32673e+00" mtime="2.88927e+01" gflop="0.00000e+00" gbyte="3.77773e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88927e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000081142e14ba" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54015e+01" utime="4.89772e+01" stime="7.31595e+00" mtime="2.88927e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88927e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4698e+08" > 4.5069e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4722e+08" > 2.6853e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8251e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6170e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1921e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.6407e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5827e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.6571e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0427e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1320e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9257e+01 </func>
</region>
</regions>
<internal rank="29" log_i="1723712895.681141" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="30" mpi_size="768" stamp_init="1723712830.151467" stamp_final="1723712895.695089" username="apac4" allocationname="unknown" flags="0" pid="3301246" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55436e+01" utime="4.66421e+01" stime="8.23130e+00" mtime="2.82584e+01" gflop="0.00000e+00" gbyte="3.76617e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82584e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000fe15fe1542" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54162e+01" utime="4.66107e+01" stime="8.22053e+00" mtime="2.82584e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82584e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4795e+08" > 6.2152e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4837e+08" > 4.1039e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6521e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6035e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0729e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8009e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5823e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.9547e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0486e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1342e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7614e+01 </func>
</region>
</regions>
<internal rank="30" log_i="1723712895.695089" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="31" mpi_size="768" stamp_init="1723712830.151323" stamp_final="1723712895.689522" username="apac4" allocationname="unknown" flags="0" pid="3301247" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55382e+01" utime="4.89886e+01" stime="7.40219e+00" mtime="2.82795e+01" gflop="0.00000e+00" gbyte="3.76343e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82795e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf41315151516154d551615161503" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54099e+01" utime="4.89576e+01" stime="7.39170e+00" mtime="2.82795e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82795e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4841e+08" > 4.5338e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4896e+08" > 2.6417e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.6780e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5943e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1921e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0061e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5825e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.9120e-03 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0481e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1319e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8801e+01 </func>
</region>
</regions>
<internal rank="31" log_i="1723712895.689522" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="32" mpi_size="768" stamp_init="1723712830.151391" stamp_final="1723712895.695202" username="apac4" allocationname="unknown" flags="0" pid="3301248" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55438e+01" utime="4.74381e+01" stime="7.98609e+00" mtime="2.89699e+01" gflop="0.00000e+00" gbyte="3.77163e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89699e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49c149e149f147c559f149f14f0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54159e+01" utime="4.74104e+01" stime="7.97188e+00" mtime="2.89699e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89699e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4838e+08" > 4.4558e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4887e+08" > 4.0671e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1116e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6149e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.7130e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5800e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1688e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0488e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1339e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8033e+01 </func>
</region>
</regions>
<internal rank="32" log_i="1723712895.695202" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="33" mpi_size="768" stamp_init="1723712830.151350" stamp_final="1723712895.684033" username="apac4" allocationname="unknown" flags="0" pid="3301249" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55327e+01" utime="4.90990e+01" stime="7.29217e+00" mtime="2.90897e+01" gflop="0.00000e+00" gbyte="3.76724e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90897e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d2147255d214d214b4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54040e+01" utime="4.90647e+01" stime="7.28500e+00" mtime="2.90897e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90897e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4850e+08" > 3.5280e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4943e+08" > 2.6328e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2798e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5961e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8670e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5801e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2182e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0453e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1315e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9109e+01 </func>
</region>
</regions>
<internal rank="33" log_i="1723712895.684033" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="34" mpi_size="768" stamp_init="1723712830.151413" stamp_final="1723712895.682341" username="apac4" allocationname="unknown" flags="0" pid="3301250" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55309e+01" utime="4.76912e+01" stime="7.72715e+00" mtime="2.85064e+01" gflop="0.00000e+00" gbyte="3.74329e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85064e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000621462149d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54048e+01" utime="4.76568e+01" stime="7.71981e+00" mtime="2.85064e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85064e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4913e+08" > 4.3480e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4842e+08" > 3.5146e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2902e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5957e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.6731e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1432e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5799e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2291e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0436e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1338e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8416e+01 </func>
</region>
</regions>
<internal rank="34" log_i="1723712895.682341" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="35" mpi_size="768" stamp_init="1723712830.151345" stamp_final="1723712895.675147" username="apac4" allocationname="unknown" flags="0" pid="3301251" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55238e+01" utime="4.90455e+01" stime="7.31668e+00" mtime="2.87625e+01" gflop="0.00000e+00" gbyte="3.76629e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87625e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4801482148314b9558314831486" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53940e+01" utime="4.90118e+01" stime="7.30866e+00" mtime="2.87625e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87625e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4980e+08" > 3.5201e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4906e+08" > 2.8947e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1471e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6059e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2636e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1448e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5796e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2585e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0454e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1306e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8901e+01 </func>
</region>
</regions>
<internal rank="35" log_i="1723712895.675147" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="36" mpi_size="768" stamp_init="1723712830.151440" stamp_final="1723712895.687945" username="apac4" allocationname="unknown" flags="0" pid="3301252" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55365e+01" utime="4.62412e+01" stime="8.27307e+00" mtime="2.82196e+01" gflop="0.00000e+00" gbyte="3.77651e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82196e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003f153c553f153f1510" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54088e+01" utime="4.62087e+01" stime="8.26324e+00" mtime="2.82196e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82196e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4869e+08" > 5.6590e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4785e+08" > 4.1073e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4474e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6148e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7688e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8901e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5789e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2722e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0441e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1336e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7822e+01 </func>
</region>
</regions>
<internal rank="36" log_i="1723712895.687945" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="37" mpi_size="768" stamp_init="1723712830.151980" stamp_final="1723712895.675192" username="apac4" allocationname="unknown" flags="0" pid="3301253" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55232e+01" utime="4.90617e+01" stime="7.29235e+00" mtime="2.88861e+01" gflop="0.00000e+00" gbyte="3.75076e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88861e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000070147014dd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53941e+01" utime="4.90288e+01" stime="7.28358e+00" mtime="2.88861e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88861e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4829e+08" > 3.6445e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4784e+08" > 2.7985e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0389e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6131e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8358e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8898e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5792e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2659e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0477e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1305e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9116e+01 </func>
</region>
</regions>
<internal rank="37" log_i="1723712895.675192" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="38" mpi_size="768" stamp_init="1723712830.151430" stamp_final="1723712895.687485" username="apac4" allocationname="unknown" flags="0" pid="3301254" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55361e+01" utime="4.71910e+01" stime="8.16808e+00" mtime="2.89573e+01" gflop="0.00000e+00" gbyte="3.77747e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89573e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54075e+01" utime="4.71592e+01" stime="8.15763e+00" mtime="2.89573e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89573e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4773e+08" > 4.9432e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4644e+08" > 3.7000e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0380e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6300e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6093e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.9539e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5780e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3795e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0473e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1336e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8037e+01 </func>
</region>
</regions>
<internal rank="38" log_i="1723712895.687485" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="39" mpi_size="768" stamp_init="1723712830.151336" stamp_final="1723712895.694869" username="apac4" allocationname="unknown" flags="0" pid="3301255" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55435e+01" utime="4.90879e+01" stime="7.28308e+00" mtime="2.91368e+01" gflop="0.00000e+00" gbyte="3.77949e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91368e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001214a9551214d01498" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54149e+01" utime="4.90603e+01" stime="7.26825e+00" mtime="2.91368e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91368e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4683e+08" > 3.7403e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4726e+08" > 2.4281e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5226e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6265e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.1119e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5785e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3750e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0447e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1305e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8868e+01 </func>
</region>
</regions>
<internal rank="39" log_i="1723712895.694869" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="40" mpi_size="768" stamp_init="1723712830.151468" stamp_final="1723712895.684781" username="apac4" allocationname="unknown" flags="0" pid="3301256" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55333e+01" utime="4.72068e+01" stime="8.16681e+00" mtime="2.86817e+01" gflop="0.00000e+00" gbyte="3.77853e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86817e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ce15c91503" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54067e+01" utime="4.71785e+01" stime="8.15248e+00" mtime="2.86817e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86817e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4905e+08" > 5.9627e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4922e+08" > 6.0446e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8892e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6110e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.3287e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7430e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5782e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3934e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0429e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1334e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7799e+01 </func>
</region>
</regions>
<internal rank="40" log_i="1723712895.684781" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="41" mpi_size="768" stamp_init="1723712830.151323" stamp_final="1723712895.694762" username="apac4" allocationname="unknown" flags="0" pid="3301257" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55434e+01" utime="4.88943e+01" stime="7.49419e+00" mtime="2.85826e+01" gflop="0.00000e+00" gbyte="3.77151e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85826e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005414de5554145314cc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54153e+01" utime="4.88655e+01" stime="7.48068e+00" mtime="2.85826e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85826e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4879e+08" > 4.8692e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4969e+08" > 3.3457e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3054e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6057e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0441e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5775e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4401e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0477e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1305e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8425e+01 </func>
</region>
</regions>
<internal rank="41" log_i="1723712895.694762" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="42" mpi_size="768" stamp_init="1723712830.151413" stamp_final="1723712895.676814" username="apac4" allocationname="unknown" flags="0" pid="3301258" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55254e+01" utime="4.73009e+01" stime="8.12482e+00" mtime="2.85939e+01" gflop="0.00000e+00" gbyte="3.76545e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85939e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf45714581459144d5559145914e3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53992e+01" utime="4.72721e+01" stime="8.11156e+00" mtime="2.85939e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85939e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4936e+08" > 5.8540e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4802e+08" > 4.9630e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8627e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6128e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.4260e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7649e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5776e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4596e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0461e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1333e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7753e+01 </func>
</region>
</regions>
<internal rank="42" log_i="1723712895.676814" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="43" mpi_size="768" stamp_init="1723712830.151328" stamp_final="1723712895.694643" username="apac4" allocationname="unknown" flags="0" pid="3301259" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55433e+01" utime="4.87357e+01" stime="7.61174e+00" mtime="2.88473e+01" gflop="0.00000e+00" gbyte="3.77079e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88473e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4231424142514a75525142514f1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54144e+01" utime="4.87056e+01" stime="7.59945e+00" mtime="2.88473e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88473e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4849e+08" > 5.0839e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4930e+08" > 2.9859e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2108e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5976e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6451e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7899e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5773e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4966e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0433e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1300e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8775e+01 </func>
</region>
</regions>
<internal rank="43" log_i="1723712895.694643" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="44" mpi_size="768" stamp_init="1723712830.151453" stamp_final="1723712895.677200" username="apac4" allocationname="unknown" flags="0" pid="3301260" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55257e+01" utime="4.45738e+01" stime="8.88009e+00" mtime="2.81300e+01" gflop="0.00000e+00" gbyte="3.74302e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81300e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49c149d149e141a559e149e1467" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53987e+01" utime="4.45453e+01" stime="8.86606e+00" mtime="2.81300e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81300e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4965e+08" > 7.6706e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4917e+08" > 7.5010e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7372e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6180e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8731e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0322e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5771e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4901e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0461e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1332e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7203e+01 </func>
</region>
</regions>
<internal rank="44" log_i="1723712895.677200" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="45" mpi_size="768" stamp_init="1723712830.151329" stamp_final="1723712895.688008" username="apac4" allocationname="unknown" flags="0" pid="3301261" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55367e+01" utime="4.89166e+01" stime="7.47903e+00" mtime="2.84707e+01" gflop="0.00000e+00" gbyte="3.76419e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84707e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ca15ca1517" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54080e+01" utime="4.88875e+01" stime="7.46653e+00" mtime="2.84707e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84707e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4839e+08" > 4.7241e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4926e+08" > 3.0324e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.6083e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6089e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0214e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5768e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5105e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0472e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1300e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9025e+01 </func>
</region>
</regions>
<internal rank="45" log_i="1723712895.688008" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="46" mpi_size="768" stamp_init="1723712830.151458" stamp_final="1723712895.678590" username="apac4" allocationname="unknown" flags="0" pid="3301262" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55271e+01" utime="4.65836e+01" stime="8.26325e+00" mtime="2.81909e+01" gflop="0.00000e+00" gbyte="3.75454e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81909e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004314245543144214d1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53995e+01" utime="4.65509e+01" stime="8.25355e+00" mtime="2.81909e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81909e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4762e+08" > 6.8315e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4892e+08" > 7.2406e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3034e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6092e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.1989e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1084e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5765e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5732e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0448e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1332e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7796e+01 </func>
</region>
</regions>
<internal rank="46" log_i="1723712895.678590" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="47" mpi_size="768" stamp_init="1723712830.151344" stamp_final="1723712895.697100" username="apac4" allocationname="unknown" flags="0" pid="3301263" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10a</host>
<perf wtime="6.55458e+01" utime="4.87101e+01" stime="7.64549e+00" mtime="2.89028e+01" gflop="0.00000e+00" gbyte="3.77262e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89028e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005e145e145f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54150e+01" utime="4.86811e+01" stime="7.63237e+00" mtime="2.89028e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89028e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.5736e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4967e+08" > 5.0988e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4845e+08" > 3.4752e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4036e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6210e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9795e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5760e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5952e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0440e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1298e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8618e+01 </func>
</region>
</regions>
<internal rank="47" log_i="1723712895.697100" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="48" mpi_size="768" stamp_init="1723712830.470954" stamp_final="1723712895.680764" username="apac4" allocationname="unknown" flags="0" pid="615940" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52098e+01" utime="4.25903e+01" stime="1.25553e+01" mtime="2.83260e+01" gflop="0.00000e+00" gbyte="3.86562e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83260e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49e159f15a0153656a015a01511" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50747e+01" utime="4.25564e+01" stime="1.25466e+01" mtime="2.83260e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83260e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4838e+08" > 5.7775e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4885e+08" > 4.3430e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5672e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7085e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1757e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5759e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5755e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0464e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1295e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7711e+01 </func>
</region>
</regions>
<internal rank="48" log_i="1723712895.680764" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="49" mpi_size="768" stamp_init="1723712830.471013" stamp_final="1723712895.680952" username="apac4" allocationname="unknown" flags="0" pid="615941" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52099e+01" utime="4.89891e+01" stime="7.43334e+00" mtime="2.84589e+01" gflop="0.00000e+00" gbyte="3.77872e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84589e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50734e+01" utime="4.89567e+01" stime="7.42218e+00" mtime="2.84589e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84589e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4962e+08" > 4.6640e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4861e+08" > 3.3438e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7353e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7153e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8571e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5766e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5408e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0453e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1157e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8787e+01 </func>
</region>
</regions>
<internal rank="49" log_i="1723712895.680952" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="50" mpi_size="768" stamp_init="1723712830.470926" stamp_final="1723712895.684025" username="apac4" allocationname="unknown" flags="0" pid="615942" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52131e+01" utime="4.64563e+01" stime="8.33389e+00" mtime="2.80660e+01" gflop="0.00000e+00" gbyte="3.77338e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80660e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4331534153515c855351535150d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50793e+01" utime="4.64207e+01" stime="8.32684e+00" mtime="2.80660e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80660e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4893e+08" > 6.6533e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4921e+08" > 5.1404e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2475e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7180e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.4250e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3024e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5763e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5628e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0489e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1294e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7667e+01 </func>
</region>
</regions>
<internal rank="50" log_i="1723712895.684025" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="51" mpi_size="768" stamp_init="1723712830.471002" stamp_final="1723712895.679953" username="apac4" allocationname="unknown" flags="0" pid="615943" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52090e+01" utime="4.89234e+01" stime="7.49917e+00" mtime="2.88268e+01" gflop="0.00000e+00" gbyte="3.77476e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88268e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d1142655d114d11475" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50763e+01" utime="4.88941e+01" stime="7.48559e+00" mtime="2.88268e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88268e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5003e+08" > 4.5357e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4976e+08" > 3.4826e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8943e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7254e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4049e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5760e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5687e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0448e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1105e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9005e+01 </func>
</region>
</regions>
<internal rank="51" log_i="1723712895.679953" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="52" mpi_size="768" stamp_init="1723712830.471891" stamp_final="1723712895.689608" username="apac4" allocationname="unknown" flags="0" pid="615944" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52177e+01" utime="4.58914e+01" stime="8.69702e+00" mtime="2.82743e+01" gflop="0.00000e+00" gbyte="3.74035e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82743e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50857e+01" utime="4.58640e+01" stime="8.68220e+00" mtime="2.82743e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82743e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4954e+08" > 7.2036e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4980e+08" > 5.8858e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2326e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7119e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.8563e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2119e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5746e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7066e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0470e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1292e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7823e+01 </func>
</region>
</regions>
<internal rank="52" log_i="1723712895.689608" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="53" mpi_size="768" stamp_init="1723712830.471025" stamp_final="1723712895.679585" username="apac4" allocationname="unknown" flags="0" pid="615945" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52086e+01" utime="4.91931e+01" stime="7.22612e+00" mtime="2.84719e+01" gflop="0.00000e+00" gbyte="3.75996e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84719e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006f1598556f156e1518" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50762e+01" utime="4.91573e+01" stime="7.21987e+00" mtime="2.84719e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84719e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4785e+08" > 4.4250e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5043e+08" > 3.2019e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.5943e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7140e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.1900e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5755e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6541e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0495e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1093e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8968e+01 </func>
</region>
</regions>
<internal rank="53" log_i="1723712895.679585" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="54" mpi_size="768" stamp_init="1723712830.470948" stamp_final="1723712895.689151" username="apac4" allocationname="unknown" flags="0" pid="615946" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52182e+01" utime="4.59083e+01" stime="8.61327e+00" mtime="2.80657e+01" gflop="0.00000e+00" gbyte="3.76667e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80657e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e214e314e4143256e414e41483" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50834e+01" utime="4.58715e+01" stime="8.60756e+00" mtime="2.80657e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80657e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4967e+08" > 7.2279e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4812e+08" > 5.7695e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8169e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7172e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5511e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.1950e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5751e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6715e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0441e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1290e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8028e+01 </func>
</region>
</regions>
<internal rank="54" log_i="1723712895.689151" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="55" mpi_size="768" stamp_init="1723712830.471038" stamp_final="1723712895.689814" username="apac4" allocationname="unknown" flags="0" pid="615947" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52188e+01" utime="4.90562e+01" stime="7.38290e+00" mtime="2.85619e+01" gflop="0.00000e+00" gbyte="3.76488e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85619e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002914291494" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50850e+01" utime="4.90242e+01" stime="7.37258e+00" mtime="2.85619e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85619e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4894e+08" > 4.5292e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4848e+08" > 3.1544e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9965e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7115e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.1759e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5750e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6682e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0477e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1090e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8649e+01 </func>
</region>
</regions>
<internal rank="55" log_i="1723712895.689814" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="56" mpi_size="768" stamp_init="1723712830.470937" stamp_final="1723712895.683655" username="apac4" allocationname="unknown" flags="0" pid="615948" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52127e+01" utime="4.67684e+01" stime="8.37679e+00" mtime="2.84664e+01" gflop="0.00000e+00" gbyte="3.76915e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84664e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4bf15c015c115f056c115c11541" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50788e+01" utime="4.67344e+01" stime="8.36869e+00" mtime="2.84664e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84664e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4805e+08" > 5.7895e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4913e+08" > 6.6075e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1698e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7188e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8600e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.0810e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5750e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7057e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0491e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1284e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7210e+01 </func>
</region>
</regions>
<internal rank="56" log_i="1723712895.683655" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="57" mpi_size="768" stamp_init="1723712830.471534" stamp_final="1723712895.690026" username="apac4" allocationname="unknown" flags="0" pid="615949" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52185e+01" utime="4.86764e+01" stime="7.75291e+00" mtime="2.90564e+01" gflop="0.00000e+00" gbyte="3.77239e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90564e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008c148c1476" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50857e+01" utime="4.86390e+01" stime="7.74810e+00" mtime="2.90564e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90564e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4856e+08" > 4.7376e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4832e+08" > 2.7255e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6510e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7256e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3842e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.3171e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5733e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8438e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0496e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1049e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8465e+01 </func>
</region>
</regions>
<internal rank="57" log_i="1723712895.690026" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="58" mpi_size="768" stamp_init="1723712830.471436" stamp_final="1723712895.689353" username="apac4" allocationname="unknown" flags="0" pid="615950" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52179e+01" utime="4.71962e+01" stime="7.90712e+00" mtime="2.82478e+01" gflop="0.00000e+00" gbyte="3.74832e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82478e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004a157c554a154a1538" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50851e+01" utime="4.71636e+01" stime="7.89762e+00" mtime="2.82478e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82478e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4951e+08" > 6.1569e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4915e+08" > 4.2367e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4098e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7112e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1948e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0500e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5737e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8299e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0468e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1278e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7743e+01 </func>
</region>
</regions>
<internal rank="58" log_i="1723712895.689353" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="59" mpi_size="768" stamp_init="1723712830.470990" stamp_final="1723712895.685173" username="apac4" allocationname="unknown" flags="0" pid="615951" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52142e+01" utime="4.90283e+01" stime="7.34622e+00" mtime="2.88333e+01" gflop="0.00000e+00" gbyte="3.77083e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88333e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf480159815aa15aa56aa15a51543" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50797e+01" utime="4.89938e+01" stime="7.33839e+00" mtime="2.88333e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88333e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4902e+08" > 4.5012e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4837e+08" > 2.5912e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2464e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7197e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8293e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5740e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8126e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0458e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1085e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8677e+01 </func>
</region>
</regions>
<internal rank="59" log_i="1723712895.685173" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="60" mpi_size="768" stamp_init="1723712830.470952" stamp_final="1723712895.689387" username="apac4" allocationname="unknown" flags="0" pid="615952" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52184e+01" utime="4.44745e+01" stime="9.08792e+00" mtime="2.80273e+01" gflop="0.00000e+00" gbyte="3.77495e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80273e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005b14e0555b145b14b9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50849e+01" utime="4.44424e+01" stime="9.07746e+00" mtime="2.80273e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80273e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4785e+08" > 8.5554e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4934e+08" > 6.1680e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3157e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7082e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0994e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.0767e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5734e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8509e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0481e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1245e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7360e+01 </func>
</region>
</regions>
<internal rank="60" log_i="1723712895.689387" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="61" mpi_size="768" stamp_init="1723712830.471014" stamp_final="1723712895.690868" username="apac4" allocationname="unknown" flags="0" pid="615953" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52199e+01" utime="4.89121e+01" stime="7.48693e+00" mtime="2.87514e+01" gflop="0.00000e+00" gbyte="3.77430e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87514e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000191419148b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50907e+01" utime="4.88849e+01" stime="7.47190e+00" mtime="2.87514e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87514e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4827e+08" > 4.6131e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4859e+08" > 3.0979e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1273e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6948e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7415e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5731e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9020e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0497e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1051e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8723e+01 </func>
</region>
</regions>
<internal rank="61" log_i="1723712895.690868" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="62" mpi_size="768" stamp_init="1723712830.471757" stamp_final="1723712895.683842" username="apac4" allocationname="unknown" flags="0" pid="615954" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52121e+01" utime="4.67618e+01" stime="8.27395e+00" mtime="2.84133e+01" gflop="0.00000e+00" gbyte="3.76549e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84133e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009c149c14f5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50801e+01" utime="4.67329e+01" stime="8.26033e+00" mtime="2.84133e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84133e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4941e+08" > 6.3186e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4909e+08" > 3.8851e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7480e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6935e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1805e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.0970e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5724e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9846e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0463e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1244e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7579e+01 </func>
</region>
</regions>
<internal rank="62" log_i="1723712895.683842" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="63" mpi_size="768" stamp_init="1723712830.471020" stamp_final="1723712895.685239" username="apac4" allocationname="unknown" flags="0" pid="615955" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52142e+01" utime="4.88813e+01" stime="7.54541e+00" mtime="2.92046e+01" gflop="0.00000e+00" gbyte="3.77621e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92046e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4611563156415fa556415641519" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50841e+01" utime="4.88493e+01" stime="7.53490e+00" mtime="2.92046e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92046e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4977e+08" > 4.5136e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4951e+08" > 2.8893e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6550e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7158e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4305e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.1161e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5722e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0018e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0449e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1051e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8633e+01 </func>
</region>
</regions>
<internal rank="63" log_i="1723712895.685239" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="64" mpi_size="768" stamp_init="1723712830.470935" stamp_final="1723712895.684020" username="apac4" allocationname="unknown" flags="0" pid="615956" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52131e+01" utime="4.63167e+01" stime="8.50048e+00" mtime="2.79450e+01" gflop="0.00000e+00" gbyte="3.77449e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79450e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b6159055b615b51532" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50796e+01" utime="4.62859e+01" stime="8.48810e+00" mtime="2.79450e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79450e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4796e+08" > 8.3269e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4848e+08" > 4.6841e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9431e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7010e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.1529e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5712e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0423e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0446e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1212e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7702e+01 </func>
</region>
</regions>
<internal rank="64" log_i="1723712895.684020" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="65" mpi_size="768" stamp_init="1723712830.471035" stamp_final="1723712895.687467" username="apac4" allocationname="unknown" flags="0" pid="615957" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52164e+01" utime="4.87749e+01" stime="7.42092e+00" mtime="2.87150e+01" gflop="0.00000e+00" gbyte="3.76137e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87150e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50855e+01" utime="4.87439e+01" stime="7.40987e+00" mtime="2.87150e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87150e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5043e+08" > 5.5437e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4915e+08" > 2.5624e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0698e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6700e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7405e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2458e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5698e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2033e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0477e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1007e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8676e+01 </func>
</region>
</regions>
<internal rank="65" log_i="1723712895.687467" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="66" mpi_size="768" stamp_init="1723712830.470938" stamp_final="1723712895.678895" username="apac4" allocationname="unknown" flags="0" pid="615958" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52080e+01" utime="4.67294e+01" stime="8.35381e+00" mtime="2.84371e+01" gflop="0.00000e+00" gbyte="3.75809e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84371e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007b1485557b147b147e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50748e+01" utime="4.67017e+01" stime="8.33954e+00" mtime="2.84371e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84371e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4944e+08" > 8.2349e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4957e+08" > 4.0489e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9210e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6980e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.4135e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7059e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5698e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2065e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0495e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1208e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8232e+01 </func>
</region>
</regions>
<internal rank="66" log_i="1723712895.678895" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="67" mpi_size="768" stamp_init="1723712830.471013" stamp_final="1723712895.679704" username="apac4" allocationname="unknown" flags="0" pid="615959" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52087e+01" utime="4.87245e+01" stime="7.43639e+00" mtime="2.86351e+01" gflop="0.00000e+00" gbyte="3.76690e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86351e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44f15511552155f55521552154d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50772e+01" utime="4.86955e+01" stime="7.42219e+00" mtime="2.86351e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86351e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4841e+08" > 5.9015e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4850e+08" > 2.9767e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9262e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7020e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.2152e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5704e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1780e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0464e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1045e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8674e+01 </func>
</region>
</regions>
<internal rank="67" log_i="1723712895.679704" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="68" mpi_size="768" stamp_init="1723712830.470941" stamp_final="1723712895.689451" username="apac4" allocationname="unknown" flags="0" pid="615960" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52185e+01" utime="4.48293e+01" stime="8.93274e+00" mtime="2.75811e+01" gflop="0.00000e+00" gbyte="3.77117e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.75811e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000024142414e7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50842e+01" utime="4.47935e+01" stime="8.92585e+00" mtime="2.75811e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.75811e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4854e+08" > 1.2821e+00 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4912e+08" > 7.1196e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.2706e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7060e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.5817e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.0789e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5694e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2457e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0464e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1177e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7529e+01 </func>
</region>
</regions>
<internal rank="68" log_i="1723712895.689451" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="69" mpi_size="768" stamp_init="1723712830.471547" stamp_final="1723712895.684477" username="apac4" allocationname="unknown" flags="0" pid="615961" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52129e+01" utime="4.86154e+01" stime="7.73625e+00" mtime="2.86743e+01" gflop="0.00000e+00" gbyte="3.76446e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86743e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48f159015911538569115911529" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50838e+01" utime="4.85842e+01" stime="7.72501e+00" mtime="2.86743e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86743e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4895e+08" > 5.5111e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4991e+08" > 2.5253e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8068e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7167e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.4911e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5696e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2570e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0450e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1039e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8861e+01 </func>
</region>
</regions>
<internal rank="69" log_i="1723712895.684477" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="70" mpi_size="768" stamp_init="1723712830.470927" stamp_final="1723712895.686975" username="apac4" allocationname="unknown" flags="0" pid="615962" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52160e+01" utime="4.66033e+01" stime="8.35629e+00" mtime="2.79174e+01" gflop="0.00000e+00" gbyte="3.74550e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79174e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50821e+01" utime="4.65721e+01" stime="8.34404e+00" mtime="2.79174e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79174e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4978e+08" > 7.8204e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4886e+08" > 5.2043e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9186e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7062e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.7976e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.0519e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5685e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.3387e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0445e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1175e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7737e+01 </func>
</region>
</regions>
<internal rank="70" log_i="1723712895.686975" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="71" mpi_size="768" stamp_init="1723712830.471094" stamp_final="1723712895.685416" username="apac4" allocationname="unknown" flags="0" pid="615963" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u19a</host>
<perf wtime="6.52143e+01" utime="4.89150e+01" stime="7.52453e+00" mtime="2.87349e+01" gflop="0.00000e+00" gbyte="3.76598e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87349e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50803e+01" utime="4.88877e+01" stime="7.50934e+00" mtime="2.87349e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87349e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4987e+08" > 5.8269e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4890e+08" > 2.8027e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1219e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7225e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2609e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5687e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.3491e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0448e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1040e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8566e+01 </func>
</region>
</regions>
<internal rank="71" log_i="1723712895.685416" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="72" mpi_size="768" stamp_init="1723712830.181143" stamp_final="1723712895.692584" username="apac4" allocationname="unknown" flags="0" pid="296243" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55114e+01" utime="4.32457e+01" stime="1.30757e+01" mtime="2.96166e+01" gflop="0.00000e+00" gbyte="3.85807e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.96166e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006214cb56621461147a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53900e+01" utime="4.32186e+01" stime="1.30609e+01" mtime="2.96166e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.96166e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4953e+08" > 6.4095e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5068e+08" > 4.2469e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6724e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1568e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5432e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2141e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5683e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.3650e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0399e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1013e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7382e+01 </func>
</region>
</regions>
<internal rank="72" log_i="1723712895.692584" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="73" mpi_size="768" stamp_init="1723712830.179757" stamp_final="1723712895.689225" username="apac4" allocationname="unknown" flags="0" pid="296250" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55095e+01" utime="5.01371e+01" stime="7.67510e+00" mtime="3.03237e+01" gflop="0.00000e+00" gbyte="3.78563e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.03237e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44f1550155215a0555215511530" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53821e+01" utime="5.01056e+01" stime="7.66346e+00" mtime="3.03237e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.03237e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5066e+08" > 4.5100e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4925e+08" > 3.4104e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1040e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1680e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0041e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0552e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5695e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2595e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0435e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0971e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8844e+01 </func>
</region>
</regions>
<internal rank="73" log_i="1723712895.689225" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="74" mpi_size="768" stamp_init="1723712830.179832" stamp_final="1723712895.681766" username="apac4" allocationname="unknown" flags="0" pid="296251" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55019e+01" utime="4.75036e+01" stime="8.56610e+00" mtime="2.96164e+01" gflop="0.00000e+00" gbyte="3.75591e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.96164e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53786e+01" utime="4.74715e+01" stime="8.55634e+00" mtime="2.96164e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.96164e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.4067e-05 </func>
<func name="MPI_Isend" count="127164" bytes="8.4955e+08" > 6.8058e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4951e+08" > 4.5525e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0688e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1816e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.6280e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.4680e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5695e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2947e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0471e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1013e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7919e+01 </func>
</region>
</regions>
<internal rank="74" log_i="1723712895.681766" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="75" mpi_size="768" stamp_init="1723712830.180031" stamp_final="1723712895.679534" username="apac4" allocationname="unknown" flags="0" pid="296252" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.54995e+01" utime="5.03312e+01" stime="7.51841e+00" mtime="3.04414e+01" gflop="0.00000e+00" gbyte="3.75729e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.04414e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b014b114b314ef56b314b21456" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53755e+01" utime="5.03018e+01" stime="7.50571e+00" mtime="3.04414e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.04414e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4909e+08" > 4.4524e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4910e+08" > 2.7436e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0235e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1836e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2529e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5691e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.3378e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0431e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0971e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9048e+01 </func>
</region>
</regions>
<internal rank="75" log_i="1723712895.679534" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="76" mpi_size="768" stamp_init="1723712830.180033" stamp_final="1723712895.688613" username="apac4" allocationname="unknown" flags="0" pid="296253" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55086e+01" utime="4.78825e+01" stime="8.42325e+00" mtime="2.96000e+01" gflop="0.00000e+00" gbyte="3.75999e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.96000e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d7144d55d714d61469" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53854e+01" utime="4.78485e+01" stime="8.41500e+00" mtime="2.96000e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.96000e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4765e+08" > 6.4712e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4997e+08" > 5.1920e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4906e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1769e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.5831e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2517e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5679e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.4596e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0419e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1012e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7512e+01 </func>
</region>
</regions>
<internal rank="76" log_i="1723712895.688613" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="77" mpi_size="768" stamp_init="1723712830.181156" stamp_final="1723712895.688907" username="apac4" allocationname="unknown" flags="0" pid="296254" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55078e+01" utime="4.99781e+01" stime="7.73417e+00" mtime="3.04890e+01" gflop="0.00000e+00" gbyte="3.76480e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.04890e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53877e+01" utime="4.99460e+01" stime="7.72472e+00" mtime="3.04890e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.04890e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4835e+08" > 4.5073e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4803e+08" > 2.9773e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3376e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1578e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9087e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2842e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5679e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.4581e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0453e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0971e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8793e+01 </func>
</region>
</regions>
<internal rank="77" log_i="1723712895.688907" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="78" mpi_size="768" stamp_init="1723712830.179904" stamp_final="1723712895.680415" username="apac4" allocationname="unknown" flags="0" pid="296255" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55005e+01" utime="4.82222e+01" stime="8.16420e+00" mtime="2.94920e+01" gflop="0.00000e+00" gbyte="3.77857e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.94920e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4231425142614955526142614cd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53798e+01" utime="4.81879e+01" stime="8.15652e+00" mtime="2.94920e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.94920e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4789e+08" > 6.1245e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4758e+08" > 5.5716e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0175e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1806e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9913e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.5011e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5671e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.5210e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0415e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1005e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7903e+01 </func>
</region>
</regions>
<internal rank="78" log_i="1723712895.680415" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="79" mpi_size="768" stamp_init="1723712830.179593" stamp_final="1723712895.682207" username="apac4" allocationname="unknown" flags="0" pid="296256" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55026e+01" utime="5.01993e+01" stime="7.47159e+00" mtime="3.02133e+01" gflop="0.00000e+00" gbyte="3.77941e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.02133e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4af14b114b2144e55b214b21493" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53801e+01" utime="5.01684e+01" stime="7.46062e+00" mtime="3.02133e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.02133e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4882e+08" > 4.4457e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5003e+08" > 2.9792e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9040e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1759e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9802e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.5068e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5676e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.4528e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0437e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0968e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8939e+01 </func>
</region>
</regions>
<internal rank="79" log_i="1723712895.682207" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="80" mpi_size="768" stamp_init="1723712830.179640" stamp_final="1723712895.685601" username="apac4" allocationname="unknown" flags="0" pid="296257" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55060e+01" utime="4.80925e+01" stime="8.40561e+00" mtime="2.95304e+01" gflop="0.00000e+00" gbyte="3.75938e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.95304e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000072147114ab" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53837e+01" utime="4.80632e+01" stime="8.39294e+00" mtime="2.95304e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.95304e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5001e+08" > 5.2934e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4727e+08" > 3.9667e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3339e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1776e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.8991e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2448e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5664e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.5616e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0400e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1005e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7729e+01 </func>
</region>
</regions>
<internal rank="80" log_i="1723712895.685601" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="81" mpi_size="768" stamp_init="1723712830.179599" stamp_final="1723712895.678211" username="apac4" allocationname="unknown" flags="0" pid="296258" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.54986e+01" utime="4.95979e+01" stime="7.30076e+00" mtime="3.05595e+01" gflop="0.00000e+00" gbyte="3.77831e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.05595e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f215f21528" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53731e+01" utime="4.95629e+01" stime="7.29397e+00" mtime="3.05595e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.05595e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4847e+08" > 3.7723e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5103e+08" > 2.8026e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2132e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1746e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3842e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.9310e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5658e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.6250e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0472e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0965e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9046e+01 </func>
</region>
</regions>
<internal rank="81" log_i="1723712895.678211" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="82" mpi_size="768" stamp_init="1723712830.180126" stamp_final="1723712895.687274" username="apac4" allocationname="unknown" flags="0" pid="296259" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55071e+01" utime="4.90445e+01" stime="7.72887e+00" mtime="2.95070e+01" gflop="0.00000e+00" gbyte="3.77163e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.95070e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002f142f14a3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53849e+01" utime="4.90175e+01" stime="7.71401e+00" mtime="2.95070e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.95070e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5105e+08" > 4.8337e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4889e+08" > 4.0664e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.5916e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1801e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8248e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.4531e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5658e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.6743e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0439e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0998e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8487e+01 </func>
</region>
</regions>
<internal rank="82" log_i="1723712895.687274" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="83" mpi_size="768" stamp_init="1723712830.181380" stamp_final="1723712895.692607" username="apac4" allocationname="unknown" flags="0" pid="296260" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55112e+01" utime="5.02160e+01" stime="6.78171e+00" mtime="3.01194e+01" gflop="0.00000e+00" gbyte="3.77083e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.01194e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000036158756361536150c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53896e+01" utime="5.01812e+01" stime="6.77486e+00" mtime="3.01194e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.01194e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4880e+08" > 3.5533e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5031e+08" > 2.7048e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.6209e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1770e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6226e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.4651e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5655e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.7008e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0426e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0964e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9218e+01 </func>
</region>
</regions>
<internal rank="83" log_i="1723712895.692607" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="84" mpi_size="768" stamp_init="1723712830.179592" stamp_final="1723712895.683274" username="apac4" allocationname="unknown" flags="0" pid="296261" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55037e+01" utime="4.61062e+01" stime="8.54131e+00" mtime="2.98022e+01" gflop="0.00000e+00" gbyte="3.74989e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.98022e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c615c715c8156356c815c81510" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53822e+01" utime="4.60778e+01" stime="8.52723e+00" mtime="2.98022e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.98022e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4904e+08" > 6.1294e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4915e+08" > 5.3001e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6173e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1572e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0499e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.4151e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5649e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.7021e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0402e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0997e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7640e+01 </func>
</region>
</regions>
<internal rank="84" log_i="1723712895.683274" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="85" mpi_size="768" stamp_init="1723712830.179604" stamp_final="1723712895.685425" username="apac4" allocationname="unknown" flags="0" pid="296262" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55058e+01" utime="4.97122e+01" stime="7.25152e+00" mtime="3.04401e+01" gflop="0.00000e+00" gbyte="3.77186e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.04401e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000cb15cb1507" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53827e+01" utime="4.96803e+01" stime="7.24120e+00" mtime="3.04401e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.04401e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4905e+08" > 3.5987e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4946e+08" > 3.0732e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1174e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1756e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7418e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.3807e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5649e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.7292e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0411e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0961e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9037e+01 </func>
</region>
</regions>
<internal rank="85" log_i="1723712895.685425" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="86" mpi_size="768" stamp_init="1723712830.179768" stamp_final="1723712895.678678" username="apac4" allocationname="unknown" flags="0" pid="296263" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.54989e+01" utime="4.76919e+01" stime="7.98797e+00" mtime="2.97925e+01" gflop="0.00000e+00" gbyte="3.76625e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.97925e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53750e+01" utime="4.76595e+01" stime="7.97874e+00" mtime="2.97925e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.97925e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4988e+08" > 4.9647e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4909e+08" > 3.8370e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3868e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1724e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.9618e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.1081e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5650e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.7462e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0442e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0989e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7978e+01 </func>
</region>
</regions>
<internal rank="86" log_i="1723712895.678678" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="87" mpi_size="768" stamp_init="1723712830.181408" stamp_final="1723712895.682206" username="apac4" allocationname="unknown" flags="0" pid="296264" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55008e+01" utime="5.05591e+01" stime="7.32061e+00" mtime="3.06224e+01" gflop="0.00000e+00" gbyte="3.77182e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.06224e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000076147614bb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53787e+01" utime="5.05265e+01" stime="7.31197e+00" mtime="3.06224e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.06224e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4848e+08" > 3.6476e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4985e+08" > 2.7563e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2765e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1576e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1127e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5646e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.7601e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0420e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0961e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9073e+01 </func>
</region>
</regions>
<internal rank="87" log_i="1723712895.682206" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="88" mpi_size="768" stamp_init="1723712830.180452" stamp_final="1723712895.690907" username="apac4" allocationname="unknown" flags="0" pid="296265" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55105e+01" utime="4.74919e+01" stime="8.49131e+00" mtime="2.90033e+01" gflop="0.00000e+00" gbyte="3.77113e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90033e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002a14ff552a142914c2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53914e+01" utime="4.74615e+01" stime="8.48009e+00" mtime="2.90033e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90033e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.0252e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4848e+08" > 7.9525e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4961e+08" > 6.2069e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.4807e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1653e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.9592e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2589e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5629e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.9488e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0399e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0986e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7776e+01 </func>
</region>
</regions>
<internal rank="88" log_i="1723712895.690907" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="89" mpi_size="768" stamp_init="1723712830.180005" stamp_final="1723712895.683252" username="apac4" allocationname="unknown" flags="0" pid="296266" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55032e+01" utime="5.01419e+01" stime="7.78594e+00" mtime="3.06205e+01" gflop="0.00000e+00" gbyte="3.77331e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.06205e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003a153a1533" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53800e+01" utime="5.01162e+01" stime="7.76972e+00" mtime="3.06205e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.06205e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5019e+08" > 4.9078e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4849e+08" > 3.2829e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4680e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1393e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0967e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.4911e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5637e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.8753e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0425e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0958e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8769e+01 </func>
</region>
</regions>
<internal rank="89" log_i="1723712895.683252" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="90" mpi_size="768" stamp_init="1723712830.180736" stamp_final="1723712895.687652" username="apac4" allocationname="unknown" flags="0" pid="296267" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55069e+01" utime="4.86743e+01" stime="8.30708e+00" mtime="3.01664e+01" gflop="0.00000e+00" gbyte="3.75629e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.01664e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005f155e1546" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53844e+01" utime="4.86498e+01" stime="8.28964e+00" mtime="3.01664e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.01664e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4854e+08" > 5.8893e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5020e+08" > 3.8953e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5316e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1727e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.8630e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2949e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5634e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.9153e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0464e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0986e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8113e+01 </func>
</region>
</regions>
<internal rank="90" log_i="1723712895.687652" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="91" mpi_size="768" stamp_init="1723712830.179607" stamp_final="1723712895.689807" username="apac4" allocationname="unknown" flags="0" pid="296268" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55102e+01" utime="5.01297e+01" stime="7.55807e+00" mtime="3.02491e+01" gflop="0.00000e+00" gbyte="3.77201e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.02491e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53861e+01" utime="5.00988e+01" stime="7.54704e+00" mtime="3.02491e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.02491e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4904e+08" > 4.7858e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4938e+08" > 2.6494e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2973e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1661e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2159e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2789e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5636e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.8927e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0440e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0958e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8561e+01 </func>
</region>
</regions>
<internal rank="91" log_i="1723712895.689807" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="92" mpi_size="768" stamp_init="1723712830.179815" stamp_final="1723712895.688311" username="apac4" allocationname="unknown" flags="0" pid="296269" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55085e+01" utime="4.84582e+01" stime="8.28580e+00" mtime="2.97816e+01" gflop="0.00000e+00" gbyte="3.77300e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.97816e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53872e+01" utime="4.84254e+01" stime="8.27647e+00" mtime="2.97816e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.97816e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4966e+08" > 6.3235e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4835e+08" > 3.8848e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3151e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1773e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.5342e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.3490e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5627e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.9689e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0447e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0979e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7892e+01 </func>
</region>
</regions>
<internal rank="92" log_i="1723712895.688311" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="93" mpi_size="768" stamp_init="1723712830.180012" stamp_final="1723712895.682796" username="apac4" allocationname="unknown" flags="0" pid="296270" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55028e+01" utime="5.04164e+01" stime="7.48588e+00" mtime="3.01613e+01" gflop="0.00000e+00" gbyte="3.76045e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.01613e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000331433146c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53779e+01" utime="5.03831e+01" stime="7.47804e+00" mtime="3.01613e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.01613e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4710e+08" > 4.7701e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4796e+08" > 3.1898e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1129e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1707e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0561e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5624e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.0127e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0408e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0954e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8646e+01 </func>
</region>
</regions>
<internal rank="93" log_i="1723712895.682796" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="94" mpi_size="768" stamp_init="1723712830.180799" stamp_final="1723712895.681246" username="apac4" allocationname="unknown" flags="0" pid="296271" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55004e+01" utime="4.78496e+01" stime="8.50260e+00" mtime="2.92816e+01" gflop="0.00000e+00" gbyte="3.76240e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92816e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44f1450145114995651145114d0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53821e+01" utime="4.78251e+01" stime="8.48526e+00" mtime="2.92816e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92816e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4958e+08" > 6.3828e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4798e+08" > 3.9622e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1629e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1666e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.5885e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.3612e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5624e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.0126e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0441e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0972e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7554e+01 </func>
</region>
</regions>
<internal rank="94" log_i="1723712895.681246" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="95" mpi_size="768" stamp_init="1723712830.181427" stamp_final="1723712895.688939" username="apac4" allocationname="unknown" flags="0" pid="296272" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u08b</host>
<perf wtime="6.55075e+01" utime="5.03784e+01" stime="7.58056e+00" mtime="2.99556e+01" gflop="0.00000e+00" gbyte="3.77197e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.99556e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009e149e14d1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53848e+01" utime="5.03458e+01" stime="7.57172e+00" mtime="2.99556e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.99556e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4861e+08" > 4.8377e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4886e+08" > 3.1073e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1532e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.1730e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.3478e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5624e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.9869e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0454e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0945e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8395e+01 </func>
</region>
</regions>
<internal rank="95" log_i="1723712895.688939" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="96" mpi_size="768" stamp_init="1723712830.511068" stamp_final="1723712895.696759" username="apac4" allocationname="unknown" flags="0" pid="1710777" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51857e+01" utime="4.37481e+01" stime="1.22215e+01" mtime="2.80884e+01" gflop="0.00000e+00" gbyte="3.85853e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80884e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f514f41470" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50606e+01" utime="4.37222e+01" stime="1.22059e+01" mtime="2.80884e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80884e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4921e+08" > 5.8322e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5003e+08" > 4.2321e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9847e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8163e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6676e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5615e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.0583e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0421e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0948e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7683e+01 </func>
</region>
</regions>
<internal rank="96" log_i="1723712895.696759" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="97" mpi_size="768" stamp_init="1723712830.511768" stamp_final="1723712895.683263" username="apac4" allocationname="unknown" flags="0" pid="1710778" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51715e+01" utime="4.96053e+01" stime="7.54362e+00" mtime="2.90232e+01" gflop="0.00000e+00" gbyte="3.74874e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90232e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008614861472" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50429e+01" utime="4.95730e+01" stime="7.53320e+00" mtime="2.90232e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90232e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4981e+08" > 4.5871e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4979e+08" > 2.8631e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9014e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0484e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6453e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5613e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.0939e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0429e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0800e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8870e+01 </func>
</region>
</regions>
<internal rank="97" log_i="1723712895.683263" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="98" mpi_size="768" stamp_init="1723712830.510254" stamp_final="1723712895.696425" username="apac4" allocationname="unknown" flags="0" pid="1710779" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51862e+01" utime="4.74571e+01" stime="8.29229e+00" mtime="2.85846e+01" gflop="0.00000e+00" gbyte="3.76057e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85846e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000042144114c3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50614e+01" utime="4.74226e+01" stime="8.28516e+00" mtime="2.85846e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85846e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4914e+08" > 6.1523e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4981e+08" > 3.3937e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7523e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0524e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.2268e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2940e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5618e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.0035e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0421e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0947e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8413e+01 </func>
</region>
</regions>
<internal rank="98" log_i="1723712895.696425" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="99" mpi_size="768" stamp_init="1723712830.511757" stamp_final="1723712895.684942" username="apac4" allocationname="unknown" flags="0" pid="1710780" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51732e+01" utime="4.96955e+01" stime="7.46664e+00" mtime="2.93446e+01" gflop="0.00000e+00" gbyte="3.77342e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.93446e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007414145674147314a7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50497e+01" utime="4.96627e+01" stime="7.45751e+00" mtime="2.93446e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.93446e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5030e+08" > 4.3194e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4991e+08" > 2.7336e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1871e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0682e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.9548e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5622e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.0327e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0437e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0800e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8911e+01 </func>
</region>
</regions>
<internal rank="99" log_i="1723712895.684942" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="100" mpi_size="768" stamp_init="1723712830.509453" stamp_final="1723712895.689174" username="apac4" allocationname="unknown" flags="0" pid="1710781" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51797e+01" utime="4.65743e+01" stime="8.70879e+00" mtime="2.82261e+01" gflop="0.00000e+00" gbyte="3.76328e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82261e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007c15ab567c157c1540" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50533e+01" utime="4.65414e+01" stime="8.70027e+00" mtime="2.82261e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82261e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5024e+08" > 6.9741e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4945e+08" > 5.0361e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8212e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0698e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.9060e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7448e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5618e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.0722e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0415e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0947e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7863e+01 </func>
</region>
</regions>
<internal rank="100" log_i="1723712895.689174" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="101" mpi_size="768" stamp_init="1723712830.509452" stamp_final="1723712895.693073" username="apac4" allocationname="unknown" flags="0" pid="1710782" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51836e+01" utime="4.98246e+01" stime="7.30155e+00" mtime="2.90596e+01" gflop="0.00000e+00" gbyte="3.77579e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90596e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000010143a55101410145e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50575e+01" utime="4.97927e+01" stime="7.29131e+00" mtime="2.90596e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90596e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4748e+08" > 4.3517e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5086e+08" > 2.7052e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9002e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0646e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1921e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7565e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5617e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.0946e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0423e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0782e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8914e+01 </func>
</region>
</regions>
<internal rank="101" log_i="1723712895.693073" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="102" mpi_size="768" stamp_init="1723712830.509824" stamp_final="1723712895.682934" username="apac4" allocationname="unknown" flags="0" pid="1710783" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51731e+01" utime="4.64456e+01" stime="8.65319e+00" mtime="2.84049e+01" gflop="0.00000e+00" gbyte="3.77361e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84049e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d3152555d315d31504" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50463e+01" utime="4.64170e+01" stime="8.63968e+00" mtime="2.84049e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84049e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4872e+08" > 7.4763e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4702e+08" > 4.3684e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3030e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0314e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.4805e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0618e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5610e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.1229e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0430e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0929e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7556e+01 </func>
</region>
</regions>
<internal rank="102" log_i="1723712895.682934" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="103" mpi_size="768" stamp_init="1723712830.509456" stamp_final="1723712895.689613" username="apac4" allocationname="unknown" flags="0" pid="1710784" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51802e+01" utime="5.01028e+01" stime="7.04084e+00" mtime="2.89811e+01" gflop="0.00000e+00" gbyte="3.76049e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89811e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003a1444553a143a14b2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50562e+01" utime="5.00700e+01" stime="7.03197e+00" mtime="2.89811e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89811e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4874e+08" > 4.4016e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4791e+08" > 2.9750e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0114e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0677e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0620e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5608e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.1480e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0428e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0781e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8711e+01 </func>
</region>
</regions>
<internal rank="103" log_i="1723712895.689613" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="104" mpi_size="768" stamp_init="1723712830.509557" stamp_final="1723712895.688890" username="apac4" allocationname="unknown" flags="0" pid="1710785" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51793e+01" utime="4.76151e+01" stime="8.25678e+00" mtime="2.83257e+01" gflop="0.00000e+00" gbyte="3.76682e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83257e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50544e+01" utime="4.75808e+01" stime="8.24929e+00" mtime="2.83257e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83257e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5046e+08" > 5.1836e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4876e+08" > 4.0308e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0949e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0711e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.5398e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6889e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5605e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.1805e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0446e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0923e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7883e+01 </func>
</region>
</regions>
<internal rank="104" log_i="1723712895.688890" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="105" mpi_size="768" stamp_init="1723712830.509436" stamp_final="1723712895.689894" username="apac4" allocationname="unknown" flags="0" pid="1710786" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51805e+01" utime="4.95386e+01" stime="7.50224e+00" mtime="2.93560e+01" gflop="0.00000e+00" gbyte="3.77029e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.93560e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003514a25535143414a8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50542e+01" utime="4.95160e+01" stime="7.48287e+00" mtime="2.93560e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.93560e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4964e+08" > 3.4717e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4957e+08" > 2.8680e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5428e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0649e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.1500e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5605e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.2158e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0457e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0780e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8655e+01 </func>
</region>
</regions>
<internal rank="105" log_i="1723712895.689894" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="106" mpi_size="768" stamp_init="1723712830.510241" stamp_final="1723712895.678091" username="apac4" allocationname="unknown" flags="0" pid="1710787" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51679e+01" utime="4.68340e+01" stime="8.27560e+00" mtime="2.81642e+01" gflop="0.00000e+00" gbyte="3.77163e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81642e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b314b3147e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50408e+01" utime="4.68051e+01" stime="8.26273e+00" mtime="2.81642e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81642e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4824e+08" > 6.1201e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5037e+08" > 4.8681e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.5258e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0620e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.5844e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.0658e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5598e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.2321e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0470e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0922e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8198e+01 </func>
</region>
</regions>
<internal rank="106" log_i="1723712895.678091" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="107" mpi_size="768" stamp_init="1723712830.511803" stamp_final="1723712895.689929" username="apac4" allocationname="unknown" flags="0" pid="1710788" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51781e+01" utime="4.98028e+01" stime="7.33552e+00" mtime="2.95862e+01" gflop="0.00000e+00" gbyte="3.76976e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.95862e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50541e+01" utime="4.97738e+01" stime="7.32301e+00" mtime="2.95862e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.95862e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4880e+08" > 3.4041e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4907e+08" > 3.3784e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1930e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0767e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2299e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5596e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.3069e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0433e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0778e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9225e+01 </func>
</region>
</regions>
<internal rank="107" log_i="1723712895.689929" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="108" mpi_size="768" stamp_init="1723712830.509515" stamp_final="1723712895.683408" username="apac4" allocationname="unknown" flags="0" pid="1710789" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51739e+01" utime="4.78063e+01" stime="8.16418e+00" mtime="2.90134e+01" gflop="0.00000e+00" gbyte="3.77796e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90134e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000054145314cf" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50470e+01" utime="4.77807e+01" stime="8.14764e+00" mtime="2.90134e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90134e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4929e+08" > 4.4468e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4762e+08" > 4.2570e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8958e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0494e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.4904e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6827e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5594e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.2921e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0456e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0913e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7860e+01 </func>
</region>
</regions>
<internal rank="108" log_i="1723712895.683408" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="109" mpi_size="768" stamp_init="1723712830.510302" stamp_final="1723712895.690183" username="apac4" allocationname="unknown" flags="0" pid="1710790" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51799e+01" utime="4.98463e+01" stime="7.20047e+00" mtime="2.87188e+01" gflop="0.00000e+00" gbyte="3.76957e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87188e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50563e+01" utime="4.98138e+01" stime="7.19098e+00" mtime="2.87188e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87188e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4794e+08" > 3.6287e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4875e+08" > 2.7165e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.5865e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0654e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6729e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5585e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.3828e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0478e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0772e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8961e+01 </func>
</region>
</regions>
<internal rank="109" log_i="1723712895.690183" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="110" mpi_size="768" stamp_init="1723712830.509438" stamp_final="1723712895.678079" username="apac4" allocationname="unknown" flags="0" pid="1710791" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51686e+01" utime="4.75358e+01" stime="8.27129e+00" mtime="2.86791e+01" gflop="0.00000e+00" gbyte="3.76019e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86791e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000fc14fc14bb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50435e+01" utime="4.75100e+01" stime="8.25500e+00" mtime="2.86791e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86791e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4780e+08" > 4.6501e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4856e+08" > 4.9897e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5596e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0654e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5927e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.4513e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5588e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.3356e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0424e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0909e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7817e+01 </func>
</region>
</regions>
<internal rank="110" log_i="1723712895.678079" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="111" mpi_size="768" stamp_init="1723712830.509442" stamp_final="1723712895.694867" username="apac4" allocationname="unknown" flags="0" pid="1710792" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51854e+01" utime="4.95789e+01" stime="7.57503e+00" mtime="2.93238e+01" gflop="0.00000e+00" gbyte="3.76831e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.93238e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4fd15fe15ff159b56ff15ff153e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50615e+01" utime="4.95456e+01" stime="7.56637e+00" mtime="2.93238e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.93238e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4909e+08" > 3.5769e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4912e+08" > 2.8157e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6007e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0664e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.4511e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5589e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.3824e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0439e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0746e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8553e+01 </func>
</region>
</regions>
<internal rank="111" log_i="1723712895.694867" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="112" mpi_size="768" stamp_init="1723712830.511474" stamp_final="1723712895.688653" username="apac4" allocationname="unknown" flags="0" pid="1710793" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51772e+01" utime="4.75384e+01" stime="8.23210e+00" mtime="2.79361e+01" gflop="0.00000e+00" gbyte="3.76575e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79361e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50510e+01" utime="4.75037e+01" stime="8.22539e+00" mtime="2.79361e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79361e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4884e+08" > 6.2523e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4984e+08" > 5.8373e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1713e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7598e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6624e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3628e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5573e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.4876e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0466e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0865e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7300e+01 </func>
</region>
</regions>
<internal rank="112" log_i="1723712895.688653" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="113" mpi_size="768" stamp_init="1723712830.511641" stamp_final="1723712895.678681" username="apac4" allocationname="unknown" flags="0" pid="1710794" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51670e+01" utime="4.94391e+01" stime="7.62985e+00" mtime="2.87509e+01" gflop="0.00000e+00" gbyte="3.75401e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87509e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44c144d144f146f554f144e1470" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50436e+01" utime="4.94037e+01" stime="7.62292e+00" mtime="2.87509e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87509e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5021e+08" > 4.7056e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4933e+08" > 3.2855e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8231e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0600e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2902e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.7840e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5577e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.4953e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0463e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0746e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8647e+01 </func>
</region>
</regions>
<internal rank="113" log_i="1723712895.678681" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="114" mpi_size="768" stamp_init="1723712830.511870" stamp_final="1723712895.688668" username="apac4" allocationname="unknown" flags="0" pid="1710795" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51768e+01" utime="4.77350e+01" stime="8.22632e+00" mtime="2.85034e+01" gflop="0.00000e+00" gbyte="3.77670e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85034e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d314d514d6144d55d614d614a4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50516e+01" utime="4.76998e+01" stime="8.21969e+00" mtime="2.85034e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85034e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5031e+08" > 6.2318e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4955e+08" > 5.9272e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0101e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0572e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.9897e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.3613e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5575e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.5099e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0421e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0873e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8033e+01 </func>
</region>
</regions>
<internal rank="114" log_i="1723712895.688668" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="115" mpi_size="768" stamp_init="1723712830.509466" stamp_final="1723712895.683710" username="apac4" allocationname="unknown" flags="0" pid="1710796" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51742e+01" utime="4.93909e+01" stime="7.64692e+00" mtime="2.91898e+01" gflop="0.00000e+00" gbyte="3.75965e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91898e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4cc14cd14ce14fc56ce14ce14ef" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50498e+01" utime="4.93677e+01" stime="7.62702e+00" mtime="2.91898e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91898e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4985e+08" > 4.7649e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4852e+08" > 3.0164e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8573e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0636e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5048e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.2285e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5569e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.5395e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0460e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0707e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9052e+01 </func>
</region>
</regions>
<internal rank="115" log_i="1723712895.683710" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="116" mpi_size="768" stamp_init="1723712830.509435" stamp_final="1723712895.681690" username="apac4" allocationname="unknown" flags="0" pid="1710797" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51723e+01" utime="4.71157e+01" stime="8.35296e+00" mtime="2.82104e+01" gflop="0.00000e+00" gbyte="3.75004e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82104e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000022143d5522142214da" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50467e+01" utime="4.70851e+01" stime="8.34153e+00" mtime="2.82104e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82104e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4714e+08" > 6.2266e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4920e+08" > 4.4202e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8703e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0641e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9145e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6310e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5573e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.5428e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0419e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0867e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7891e+01 </func>
</region>
</regions>
<internal rank="116" log_i="1723712895.681690" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="117" mpi_size="768" stamp_init="1723712830.510129" stamp_final="1723712895.689882" username="apac4" allocationname="unknown" flags="0" pid="1710798" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51798e+01" utime="4.91133e+01" stime="7.90860e+00" mtime="2.91609e+01" gflop="0.00000e+00" gbyte="3.76438e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91609e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50569e+01" utime="4.90754e+01" stime="7.90447e+00" mtime="2.91609e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91609e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4918e+08" > 4.7494e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4855e+08" > 3.6412e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2862e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0588e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4796e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6541e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5571e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.5269e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0473e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0739e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8589e+01 </func>
</region>
</regions>
<internal rank="117" log_i="1723712895.689882" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="118" mpi_size="768" stamp_init="1723712830.511542" stamp_final="1723712895.695760" username="apac4" allocationname="unknown" flags="0" pid="1710799" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51842e+01" utime="4.71488e+01" stime="8.37334e+00" mtime="2.85686e+01" gflop="0.00000e+00" gbyte="3.77525e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85686e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50593e+01" utime="4.71185e+01" stime="8.36186e+00" mtime="2.85686e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85686e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4987e+08" > 5.8681e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4926e+08" > 4.4572e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5677e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0554e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.9262e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7979e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5568e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.5674e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0436e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0867e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7592e+01 </func>
</region>
</regions>
<internal rank="118" log_i="1723712895.695760" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="119" mpi_size="768" stamp_init="1723712830.509443" stamp_final="1723712895.689249" username="apac4" allocationname="unknown" flags="0" pid="1710800" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u13b</host>
<perf wtime="6.51798e+01" utime="4.93592e+01" stime="7.79791e+00" mtime="2.90097e+01" gflop="0.00000e+00" gbyte="3.77525e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90097e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000038143714bf" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50538e+01" utime="4.93315e+01" stime="7.78282e+00" mtime="2.90097e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90097e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4967e+08" > 4.9710e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4922e+08" > 3.7063e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5464e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0326e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9087e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7781e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5566e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.6109e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0459e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0738e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8180e+01 </func>
</region>
</regions>
<internal rank="119" log_i="1723712895.689249" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="120" mpi_size="768" stamp_init="1723712830.533346" stamp_final="1723712895.693738" username="apac4" allocationname="unknown" flags="0" pid="781187" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51604e+01" utime="4.24724e+01" stime="1.27929e+01" mtime="2.84565e+01" gflop="0.00000e+00" gbyte="3.85197e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84565e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001814a7551814171496" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50257e+01" utime="4.24419e+01" stime="1.27811e+01" mtime="2.84565e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84565e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.5763e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4772e+08" > 7.6839e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4802e+08" > 4.3995e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8896e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7459e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4909e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3639e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5562e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.6805e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0237e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0740e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7239e+01 </func>
</region>
</regions>
<internal rank="120" log_i="1723712895.693738" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="121" mpi_size="768" stamp_init="1723712830.533427" stamp_final="1723712895.690909" username="apac4" allocationname="unknown" flags="0" pid="781188" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51575e+01" utime="4.90678e+01" stime="7.68708e+00" mtime="2.85686e+01" gflop="0.00000e+00" gbyte="3.78082e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85686e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006114405661146114ab" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50184e+01" utime="4.90369e+01" stime="7.67415e+00" mtime="2.85686e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85686e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4928e+08" > 5.5723e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4670e+08" > 2.7116e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8865e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7995e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4404e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5569e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.5908e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0244e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0633e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8588e+01 </func>
</region>
</regions>
<internal rank="121" log_i="1723712895.690909" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="122" mpi_size="768" stamp_init="1723712830.533327" stamp_final="1723712895.680860" username="apac4" allocationname="unknown" flags="0" pid="781189" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51475e+01" utime="4.70302e+01" stime="8.38623e+00" mtime="2.84918e+01" gflop="0.00000e+00" gbyte="3.76881e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84918e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50128e+01" utime="4.70003e+01" stime="8.37358e+00" mtime="2.84918e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84918e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4793e+08" > 7.2927e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4903e+08" > 3.8100e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1109e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7297e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.5790e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3691e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5558e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.6679e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0247e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0726e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8130e+01 </func>
</region>
</regions>
<internal rank="122" log_i="1723712895.680860" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="123" mpi_size="768" stamp_init="1723712830.533411" stamp_final="1723712895.691066" username="apac4" allocationname="unknown" flags="0" pid="781190" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51577e+01" utime="4.91332e+01" stime="7.63969e+00" mtime="2.87394e+01" gflop="0.00000e+00" gbyte="3.77575e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87394e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009215921513" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50235e+01" utime="4.90989e+01" stime="7.63128e+00" mtime="2.87394e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87394e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4843e+08" > 5.4181e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4790e+08" > 2.7534e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9078e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7319e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3980e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5560e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.6485e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0239e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0579e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8778e+01 </func>
</region>
</regions>
<internal rank="123" log_i="1723712895.691066" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="124" mpi_size="768" stamp_init="1723712830.533363" stamp_final="1723712895.683652" username="apac4" allocationname="unknown" flags="0" pid="781191" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51503e+01" utime="4.69191e+01" stime="8.40395e+00" mtime="2.82132e+01" gflop="0.00000e+00" gbyte="3.76320e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82132e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006f146f14b2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50187e+01" utime="4.68876e+01" stime="8.39191e+00" mtime="2.82132e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82132e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4816e+08" > 7.3000e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4784e+08" > 4.4712e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5479e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7403e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3890e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3717e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5554e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.7071e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0257e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0727e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7395e+01 </func>
</region>
</regions>
<internal rank="124" log_i="1723712895.683652" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="125" mpi_size="768" stamp_init="1723712830.533391" stamp_final="1723712895.691280" username="apac4" allocationname="unknown" flags="0" pid="781192" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51579e+01" utime="4.86925e+01" stime="8.01507e+00" mtime="2.92699e+01" gflop="0.00000e+00" gbyte="3.77014e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92699e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50248e+01" utime="4.86602e+01" stime="8.00452e+00" mtime="2.92699e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92699e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 5.4949e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4810e+08" > 2.7133e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7874e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7408e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3780e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5554e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.7121e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0248e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0623e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8412e+01 </func>
</region>
</regions>
<internal rank="125" log_i="1723712895.691280" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="126" mpi_size="768" stamp_init="1723712830.533355" stamp_final="1723712895.694848" username="apac4" allocationname="unknown" flags="0" pid="781193" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51615e+01" utime="4.60111e+01" stime="8.82269e+00" mtime="2.82867e+01" gflop="0.00000e+00" gbyte="3.75191e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82867e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50274e+01" utime="4.59818e+01" stime="8.80951e+00" mtime="2.82867e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82867e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.0027e-05 </func>
<func name="MPI_Isend" count="127164" bytes="8.4989e+08" > 8.3007e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5019e+08" > 5.0570e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5399e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7086e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4605e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3774e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5549e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.7301e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0271e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0717e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7390e+01 </func>
</region>
</regions>
<internal rank="126" log_i="1723712895.694848" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="127" mpi_size="768" stamp_init="1723712830.533359" stamp_final="1723712895.691413" username="apac4" allocationname="unknown" flags="0" pid="781194" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51581e+01" utime="4.90724e+01" stime="7.78326e+00" mtime="2.89155e+01" gflop="0.00000e+00" gbyte="3.76434e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89155e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50241e+01" utime="4.90394e+01" stime="7.77391e+00" mtime="2.89155e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89155e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4901e+08" > 5.4446e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5061e+08" > 2.7580e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7590e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7258e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3727e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5554e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.7454e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0279e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0617e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8106e+01 </func>
</region>
</regions>
<internal rank="127" log_i="1723712895.691413" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="128" mpi_size="768" stamp_init="1723712830.533360" stamp_final="1723712895.678817" username="apac4" allocationname="unknown" flags="0" pid="781195" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51455e+01" utime="4.60124e+01" stime="8.92468e+00" mtime="2.85027e+01" gflop="0.00000e+00" gbyte="3.76175e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85027e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44d144e1450143a5550144f1492" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50128e+01" utime="4.59856e+01" stime="8.90967e+00" mtime="2.85027e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85027e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4992e+08" > 6.3218e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4887e+08" > 4.7332e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1867e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3095e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.7695e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5525e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.0273e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0260e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0715e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7140e+01 </func>
</region>
</regions>
<internal rank="128" log_i="1723712895.678817" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="129" mpi_size="768" stamp_init="1723712830.533361" stamp_final="1723712895.679834" username="apac4" allocationname="unknown" flags="0" pid="781196" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51465e+01" utime="4.90636e+01" stime="7.69159e+00" mtime="2.88733e+01" gflop="0.00000e+00" gbyte="3.76968e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88733e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000098148c5698149814ff" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50131e+01" utime="4.90336e+01" stime="7.67914e+00" mtime="2.88733e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88733e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4892e+08" > 4.3761e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5018e+08" > 4.0016e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5523e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7588e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4080e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9587e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5522e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.0217e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0236e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0617e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8372e+01 </func>
</region>
</regions>
<internal rank="129" log_i="1723712895.679834" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="130" mpi_size="768" stamp_init="1723712830.533297" stamp_final="1723712895.679558" username="apac4" allocationname="unknown" flags="0" pid="781197" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51463e+01" utime="4.68132e+01" stime="8.45334e+00" mtime="2.85245e+01" gflop="0.00000e+00" gbyte="3.76728e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85245e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50107e+01" utime="4.67881e+01" stime="8.43625e+00" mtime="2.85245e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85245e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4896e+08" > 6.0653e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4874e+08" > 3.7457e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5917e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7371e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8862e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4178e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5522e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.0556e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0246e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0715e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7797e+01 </func>
</region>
</regions>
<internal rank="130" log_i="1723712895.679558" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="131" mpi_size="768" stamp_init="1723712830.533423" stamp_final="1723712895.678492" username="apac4" allocationname="unknown" flags="0" pid="781198" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51451e+01" utime="4.93739e+01" stime="7.38526e+00" mtime="2.92008e+01" gflop="0.00000e+00" gbyte="3.77068e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92008e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50111e+01" utime="4.93395e+01" stime="7.37709e+00" mtime="2.92008e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92008e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4838e+08" > 4.3345e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4904e+08" > 3.2940e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3577e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7347e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.5276e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5508e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.1713e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0245e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0602e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8888e+01 </func>
</region>
</regions>
<internal rank="131" log_i="1723712895.678492" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="132" mpi_size="768" stamp_init="1723712830.533330" stamp_final="1723712895.682457" username="apac4" allocationname="unknown" flags="0" pid="781199" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51491e+01" utime="4.49288e+01" stime="9.06557e+00" mtime="2.84205e+01" gflop="0.00000e+00" gbyte="3.77644e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84205e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50146e+01" utime="4.48946e+01" stime="9.05702e+00" mtime="2.84205e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84205e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4854e+08" > 7.8522e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4791e+08" > 5.6771e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8952e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7416e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.0868e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3641e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5510e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.1455e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0254e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0710e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7188e+01 </func>
</region>
</regions>
<internal rank="132" log_i="1723712895.682457" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="133" mpi_size="768" stamp_init="1723712830.533334" stamp_final="1723712895.684302" username="apac4" allocationname="unknown" flags="0" pid="781200" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51510e+01" utime="4.89030e+01" stime="7.84400e+00" mtime="2.93956e+01" gflop="0.00000e+00" gbyte="3.76400e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.93956e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48f159015911594559115911514" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50176e+01" utime="4.88746e+01" stime="7.83025e+00" mtime="2.93956e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.93956e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4860e+08" > 4.2488e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4817e+08" > 2.8247e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9787e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7398e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4522e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5509e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.1957e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0245e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0588e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8471e+01 </func>
</region>
</regions>
<internal rank="133" log_i="1723712895.684302" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="134" mpi_size="768" stamp_init="1723712830.533313" stamp_final="1723712895.689844" username="apac4" allocationname="unknown" flags="0" pid="781201" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51565e+01" utime="4.72883e+01" stime="8.27944e+00" mtime="2.82017e+01" gflop="0.00000e+00" gbyte="3.76587e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82017e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf448144a144b1428554b144b149d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50221e+01" utime="4.72583e+01" stime="8.26746e+00" mtime="2.82017e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82017e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4816e+08" > 5.6145e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5076e+08" > 3.9987e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1470e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7296e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1815e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3627e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5508e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.2141e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0271e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0701e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7970e+01 </func>
</region>
</regions>
<internal rank="134" log_i="1723712895.689844" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="135" mpi_size="768" stamp_init="1723712830.533337" stamp_final="1723712895.683437" username="apac4" allocationname="unknown" flags="0" pid="781202" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51501e+01" utime="4.86843e+01" stime="7.76175e+00" mtime="2.88472e+01" gflop="0.00000e+00" gbyte="3.76446e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88472e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf47f1481148214b65582148214d9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50175e+01" utime="4.86511e+01" stime="7.75262e+00" mtime="2.88472e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88472e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5081e+08" > 4.3092e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4891e+08" > 3.1843e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2405e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7468e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5272e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3650e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5510e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.1782e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0266e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0585e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8645e+01 </func>
</region>
</regions>
<internal rank="135" log_i="1723712895.683437" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="136" mpi_size="768" stamp_init="1723712830.533332" stamp_final="1723712895.685757" username="apac4" allocationname="unknown" flags="0" pid="781203" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51524e+01" utime="4.47299e+01" stime="9.13383e+00" mtime="2.77873e+01" gflop="0.00000e+00" gbyte="3.76385e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77873e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e914e814ef" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50183e+01" utime="4.46960e+01" stime="9.12558e+00" mtime="2.77873e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77873e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4782e+08" > 1.0204e+00 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5054e+08" > 7.5518e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3629e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7395e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6584e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3650e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5504e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.1807e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0247e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0690e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6833e+01 </func>
</region>
</regions>
<internal rank="136" log_i="1723712895.685757" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="137" mpi_size="768" stamp_init="1723712830.533416" stamp_final="1723712895.690399" username="apac4" allocationname="unknown" flags="0" pid="781204" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51570e+01" utime="4.91698e+01" stime="7.60803e+00" mtime="2.86573e+01" gflop="0.00000e+00" gbyte="3.77522e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86573e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50241e+01" utime="4.91304e+01" stime="7.60467e+00" mtime="2.86573e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86573e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4997e+08" > 4.7240e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4881e+08" > 2.9336e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2721e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7201e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4044e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5499e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.2578e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0252e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0578e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8411e+01 </func>
</region>
</regions>
<internal rank="137" log_i="1723712895.690399" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="138" mpi_size="768" stamp_init="1723712830.533351" stamp_final="1723712895.679893" username="apac4" allocationname="unknown" flags="0" pid="781205" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51465e+01" utime="4.68237e+01" stime="8.37400e+00" mtime="2.83775e+01" gflop="0.00000e+00" gbyte="3.74668e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83775e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50143e+01" utime="4.67961e+01" stime="8.35909e+00" mtime="2.83775e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83775e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4889e+08" > 7.2116e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4894e+08" > 4.5992e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5442e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7457e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.9966e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4629e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5500e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.2805e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0247e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0656e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7566e+01 </func>
</region>
</regions>
<internal rank="138" log_i="1723712895.679893" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="139" mpi_size="768" stamp_init="1723712830.533398" stamp_final="1723712895.679469" username="apac4" allocationname="unknown" flags="0" pid="781206" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51461e+01" utime="4.92589e+01" stime="7.45304e+00" mtime="2.85461e+01" gflop="0.00000e+00" gbyte="3.77319e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85461e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000059145d565914591459" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50149e+01" utime="4.92243e+01" stime="7.44466e+00" mtime="2.85461e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85461e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4876e+08" > 4.7155e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4944e+08" > 2.7273e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7936e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7468e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4631e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5499e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.2663e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0279e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0572e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8754e+01 </func>
</region>
</regions>
<internal rank="139" log_i="1723712895.679469" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="140" mpi_size="768" stamp_init="1723712830.533340" stamp_final="1723712895.687333" username="apac4" allocationname="unknown" flags="0" pid="781207" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51540e+01" utime="4.71799e+01" stime="8.29354e+00" mtime="2.79107e+01" gflop="0.00000e+00" gbyte="3.77354e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79107e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ee14f014f114dc55f114f014de" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50183e+01" utime="4.71450e+01" stime="8.28584e+00" mtime="2.79107e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79107e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4981e+08" > 6.3794e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4738e+08" > 5.4051e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4316e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7236e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.7258e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3638e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5498e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.3029e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0258e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0645e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7310e+01 </func>
</region>
</regions>
<internal rank="140" log_i="1723712895.687333" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="141" mpi_size="768" stamp_init="1723712830.533399" stamp_final="1723712895.686087" username="apac4" allocationname="unknown" flags="0" pid="781208" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51527e+01" utime="4.92653e+01" stime="7.53075e+00" mtime="2.85849e+01" gflop="0.00000e+00" gbyte="3.77018e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85849e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c115c01536" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50184e+01" utime="4.92341e+01" stime="7.51939e+00" mtime="2.85849e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85849e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 4.6844e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4993e+08" > 3.3681e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3219e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7335e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4544e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4141e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5494e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.3470e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0262e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0568e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8274e+01 </func>
</region>
</regions>
<internal rank="141" log_i="1723712895.686087" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="142" mpi_size="768" stamp_init="1723712830.533409" stamp_final="1723712895.683591" username="apac4" allocationname="unknown" flags="0" pid="781209" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51502e+01" utime="4.69961e+01" stime="8.55793e+00" mtime="2.84640e+01" gflop="0.00000e+00" gbyte="3.77346e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84640e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007a147a14aa" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50160e+01" utime="4.69574e+01" stime="8.55401e+00" mtime="2.84640e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84640e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5016e+08" > 6.5845e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4780e+08" > 4.9135e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7476e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7449e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3699e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3644e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5486e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.3882e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0238e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0644e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7509e+01 </func>
</region>
</regions>
<internal rank="142" log_i="1723712895.683591" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="143" mpi_size="768" stamp_init="1723712830.533411" stamp_final="1723712895.689461" username="apac4" allocationname="unknown" flags="0" pid="781210" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u14a</host>
<perf wtime="6.51560e+01" utime="4.86767e+01" stime="7.85098e+00" mtime="2.89308e+01" gflop="0.00000e+00" gbyte="3.77842e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89308e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50232e+01" utime="4.86427e+01" stime="7.84238e+00" mtime="2.89308e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89308e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4884e+08" > 4.8298e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4873e+08" > 3.0705e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5667e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7330e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3654e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5485e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.4406e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0264e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0548e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8365e+01 </func>
</region>
</regions>
<internal rank="143" log_i="1723712895.689461" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="144" mpi_size="768" stamp_init="1723712830.340322" stamp_final="1723712895.676992" username="apac4" allocationname="unknown" flags="0" pid="3087460" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53367e+01" utime="4.39873e+01" stime="1.31168e+01" mtime="3.04750e+01" gflop="0.00000e+00" gbyte="3.86642e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.04750e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52013e+01" utime="4.39556e+01" stime="1.31059e+01" mtime="3.04750e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.04750e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5056e+08" > 5.8769e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4907e+08" > 4.4397e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6655e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.6004e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0101e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1003e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5483e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.4289e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0368e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0545e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7757e+01 </func>
</region>
</regions>
<internal rank="144" log_i="1723712895.676992" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="145" mpi_size="768" stamp_init="1723712830.340058" stamp_final="1723712895.678823" username="apac4" allocationname="unknown" flags="0" pid="3087461" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53388e+01" utime="5.03308e+01" stime="7.70913e+00" mtime="3.06361e+01" gflop="0.00000e+00" gbyte="3.78231e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.06361e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf41014111412145755121412147a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51965e+01" utime="5.02987e+01" stime="7.69726e+00" mtime="3.06361e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.06361e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4928e+08" > 4.4918e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4953e+08" > 3.1186e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1905e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5983e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1300e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5486e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.3856e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0442e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5440e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8596e+01 </func>
</region>
</regions>
<internal rank="145" log_i="1723712895.678823" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="146" mpi_size="768" stamp_init="1723712830.340049" stamp_final="1723712895.678155" username="apac4" allocationname="unknown" flags="0" pid="3087462" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53381e+01" utime="4.86599e+01" stime="8.49972e+00" mtime="3.04960e+01" gflop="0.00000e+00" gbyte="3.76881e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.04960e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001314e35513141314c0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52059e+01" utime="4.86306e+01" stime="8.48541e+00" mtime="3.04960e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.04960e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4895e+08" > 6.2759e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4927e+08" > 4.9438e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0684e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.6020e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.9141e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1287e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5481e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.4138e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0470e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0458e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8328e+01 </func>
</region>
</regions>
<internal rank="146" log_i="1723712895.678155" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="147" mpi_size="768" stamp_init="1723712830.341470" stamp_final="1723712895.687382" username="apac4" allocationname="unknown" flags="0" pid="3087463" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53459e+01" utime="5.06229e+01" stime="7.79542e+00" mtime="3.06845e+01" gflop="0.00000e+00" gbyte="3.76930e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.06845e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000080147f1491" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52107e+01" utime="5.05956e+01" stime="7.78007e+00" mtime="3.06845e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.06845e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4833e+08" > 4.3414e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4974e+08" > 2.9193e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8299e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5961e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1308e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5473e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.5244e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0462e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5450e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9024e+01 </func>
</region>
</regions>
<internal rank="147" log_i="1723712895.687382" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="148" mpi_size="768" stamp_init="1723712830.340036" stamp_final="1723712895.687408" username="apac4" allocationname="unknown" flags="0" pid="3087464" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53474e+01" utime="4.78685e+01" stime="8.74180e+00" mtime="2.99147e+01" gflop="0.00000e+00" gbyte="3.74249e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.99147e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003014301484" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52117e+01" utime="4.78391e+01" stime="8.72886e+00" mtime="2.99147e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.99147e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5051e+08" > 7.1377e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4906e+08" > 5.0940e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1243e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5960e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5429e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1035e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5480e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.4594e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0441e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.0458e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7611e+01 </func>
</region>
</regions>
<internal rank="148" log_i="1723712895.687408" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="149" mpi_size="768" stamp_init="1723712830.340071" stamp_final="1723712895.682378" username="apac4" allocationname="unknown" flags="0" pid="3087465" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53423e+01" utime="5.01530e+01" stime="8.21569e+00" mtime="3.11682e+01" gflop="0.00000e+00" gbyte="3.76369e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.11682e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44d154f155015a4555015501532" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52088e+01" utime="5.01192e+01" stime="8.20726e+00" mtime="3.11682e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.11682e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4969e+08" > 4.4481e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4983e+08" > 3.1362e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4237e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5973e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1035e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5474e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.5268e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0455e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5451e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8902e+01 </func>
</region>
</regions>
<internal rank="149" log_i="1723712895.682378" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="150" mpi_size="768" stamp_init="1723712830.340043" stamp_final="1723712895.677852" username="apac4" allocationname="unknown" flags="0" pid="3087466" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53378e+01" utime="4.70167e+01" stime="9.25779e+00" mtime="3.01380e+01" gflop="0.00000e+00" gbyte="3.77258e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.01380e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf46c146e146f14ee556f146f14e5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52019e+01" utime="4.69866e+01" stime="9.24529e+00" mtime="3.01380e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.01380e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4952e+08" > 7.3339e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4943e+08" > 6.2932e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6209e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5986e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.4850e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1040e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5460e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.6314e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0477e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5453e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7353e+01 </func>
</region>
</regions>
<internal rank="150" log_i="1723712895.677852" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="151" mpi_size="768" stamp_init="1723712830.340058" stamp_final="1723712895.691853" username="apac4" allocationname="unknown" flags="0" pid="3087467" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53518e+01" utime="5.06154e+01" stime="7.80298e+00" mtime="3.07611e+01" gflop="0.00000e+00" gbyte="3.76488e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.07611e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001215121518" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52171e+01" utime="5.05840e+01" stime="7.79238e+00" mtime="3.07611e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.07611e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4974e+08" > 4.5681e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5061e+08" > 3.0760e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1443e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5789e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1369e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5465e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.6271e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0404e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5421e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8778e+01 </func>
</region>
</regions>
<internal rank="151" log_i="1723712895.691853" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="152" mpi_size="768" stamp_init="1723712830.340817" stamp_final="1723712895.686393" username="apac4" allocationname="unknown" flags="0" pid="3087468" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53456e+01" utime="4.79540e+01" stime="8.77647e+00" mtime="3.02504e+01" gflop="0.00000e+00" gbyte="3.76331e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.02504e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf46f1470147114f4567114711492" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52120e+01" utime="4.79255e+01" stime="8.76302e+00" mtime="3.02504e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.02504e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4946e+08" > 5.2580e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4876e+08" > 4.5018e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6607e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.6113e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.2990e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1006e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5468e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.5860e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0466e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5464e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7638e+01 </func>
</region>
</regions>
<internal rank="152" log_i="1723712895.686393" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="153" mpi_size="768" stamp_init="1723712830.340062" stamp_final="1723712895.676484" username="apac4" allocationname="unknown" flags="0" pid="3087469" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53364e+01" utime="5.07821e+01" stime="7.56617e+00" mtime="3.05838e+01" gflop="0.00000e+00" gbyte="3.76297e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.05838e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52008e+01" utime="5.07493e+01" stime="7.55664e+00" mtime="3.05838e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.05838e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4865e+08" > 3.6130e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4954e+08" > 2.7478e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0097e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.6043e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1240e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5456e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.7142e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0370e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5455e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8810e+01 </func>
</region>
</regions>
<internal rank="153" log_i="1723712895.676484" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="154" mpi_size="768" stamp_init="1723712830.340032" stamp_final="1723712895.680188" username="apac4" allocationname="unknown" flags="0" pid="3087470" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53402e+01" utime="4.90159e+01" stime="8.19066e+00" mtime="3.03587e+01" gflop="0.00000e+00" gbyte="3.74359e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.03587e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d414d514d714b255d714d6146f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52048e+01" utime="4.89828e+01" stime="8.18145e+00" mtime="3.03587e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.03587e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4828e+08" > 5.0834e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4852e+08" > 3.9364e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4143e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.6004e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.0068e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1005e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5458e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.6623e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0463e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5453e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8028e+01 </func>
</region>
</regions>
<internal rank="154" log_i="1723712895.680188" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="155" mpi_size="768" stamp_init="1723712830.340047" stamp_final="1723712895.688995" username="apac4" allocationname="unknown" flags="0" pid="3087471" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53489e+01" utime="5.07173e+01" stime="7.74880e+00" mtime="3.10165e+01" gflop="0.00000e+00" gbyte="3.77403e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.10165e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c515c615c815af55c815c7153c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52117e+01" utime="5.06893e+01" stime="7.73430e+00" mtime="3.10165e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.10165e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4894e+08" > 3.5799e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4803e+08" > 2.5707e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3757e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.6021e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1370e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5449e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.7468e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0482e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5417e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8883e+01 </func>
</region>
</regions>
<internal rank="155" log_i="1723712895.688995" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="156" mpi_size="768" stamp_init="1723712830.341513" stamp_final="1723712895.679912" username="apac4" allocationname="unknown" flags="0" pid="3087472" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53384e+01" utime="4.79402e+01" stime="8.94810e+00" mtime="3.07798e+01" gflop="0.00000e+00" gbyte="3.76686e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.07798e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52028e+01" utime="4.79118e+01" stime="8.93421e+00" mtime="3.07798e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.07798e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4911e+08" > 5.3402e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4833e+08" > 3.7434e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0282e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.6062e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.5592e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1005e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5452e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.7322e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0463e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5419e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7805e+01 </func>
</region>
</regions>
<internal rank="156" log_i="1723712895.679912" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="157" mpi_size="768" stamp_init="1723712830.340061" stamp_final="1723712895.681053" username="apac4" allocationname="unknown" flags="0" pid="3087473" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53410e+01" utime="5.05397e+01" stime="7.87042e+00" mtime="3.11433e+01" gflop="0.00000e+00" gbyte="3.77186e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.11433e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52042e+01" utime="5.05094e+01" stime="7.85881e+00" mtime="3.11433e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.11433e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4736e+08" > 3.5518e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4894e+08" > 2.7330e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7883e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.6003e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7895e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1305e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5442e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.8077e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0469e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5423e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8601e+01 </func>
</region>
</regions>
<internal rank="157" log_i="1723712895.681053" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="158" mpi_size="768" stamp_init="1723712830.340045" stamp_final="1723712895.686407" username="apac4" allocationname="unknown" flags="0" pid="3087474" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53464e+01" utime="4.85935e+01" stime="8.58572e+00" mtime="3.01170e+01" gflop="0.00000e+00" gbyte="3.77144e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.01170e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4fa14fb14fc14ed55fc14fc14e1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52142e+01" utime="4.85577e+01" stime="8.57903e+00" mtime="3.01170e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.01170e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4938e+08" > 5.0131e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4926e+08" > 3.8536e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1805e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5708e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0293e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1169e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5443e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.8367e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0440e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5437e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7056e+01 </func>
</region>
</regions>
<internal rank="158" log_i="1723712895.686407" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="159" mpi_size="768" stamp_init="1723712830.341635" stamp_final="1723712895.676813" username="apac4" allocationname="unknown" flags="0" pid="3087475" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53352e+01" utime="5.06480e+01" stime="7.69472e+00" mtime="3.06980e+01" gflop="0.00000e+00" gbyte="3.77499e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.06980e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e415e515e6153c55e615e6154c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52008e+01" utime="5.06111e+01" stime="7.68945e+00" mtime="3.06980e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.06980e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4921e+08" > 3.4314e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4913e+08" > 2.7750e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3198e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.6056e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1293e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5441e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.8244e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0475e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5424e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8630e+01 </func>
</region>
</regions>
<internal rank="159" log_i="1723712895.676813" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="160" mpi_size="768" stamp_init="1723712830.340064" stamp_final="1723712895.683692" username="apac4" allocationname="unknown" flags="0" pid="3087476" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53436e+01" utime="4.68048e+01" stime="9.52802e+00" mtime="3.02368e+01" gflop="0.00000e+00" gbyte="3.76984e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.02368e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b1149955b114b01457" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52075e+01" utime="4.67724e+01" stime="9.51771e+00" mtime="3.02368e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.02368e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.0599e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4825e+08" > 7.5329e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4909e+08" > 5.1784e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8754e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5159e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1927e+00 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.3022e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5430e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.9247e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0462e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5460e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7181e+01 </func>
</region>
</regions>
<internal rank="160" log_i="1723712895.683692" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="161" mpi_size="768" stamp_init="1723712830.340157" stamp_final="1723712895.683085" username="apac4" allocationname="unknown" flags="0" pid="3087477" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53429e+01" utime="5.03263e+01" stime="8.10319e+00" mtime="3.10223e+01" gflop="0.00000e+00" gbyte="3.77861e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.10223e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004d144d14bf" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52048e+01" utime="5.02928e+01" stime="8.09402e+00" mtime="3.10223e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.10223e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4815e+08" > 4.8855e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4948e+08" > 2.9671e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8013e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.6002e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1274e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5431e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.9631e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0442e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5419e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8331e+01 </func>
</region>
</regions>
<internal rank="161" log_i="1723712895.683085" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="162" mpi_size="768" stamp_init="1723712830.340290" stamp_final="1723712895.686626" username="apac4" allocationname="unknown" flags="0" pid="3087478" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53463e+01" utime="4.91314e+01" stime="8.19846e+00" mtime="3.00494e+01" gflop="0.00000e+00" gbyte="3.77640e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.00494e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004d14a0554d144c14a9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52101e+01" utime="4.91036e+01" stime="8.18347e+00" mtime="3.00494e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.00494e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4936e+08" > 6.2415e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4815e+08" > 4.6874e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8305e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.6070e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8950e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1004e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5422e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.9861e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0452e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5443e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8169e+01 </func>
</region>
</regions>
<internal rank="162" log_i="1723712895.686626" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="163" mpi_size="768" stamp_init="1723712830.340054" stamp_final="1723712895.682313" username="apac4" allocationname="unknown" flags="0" pid="3087479" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53423e+01" utime="5.07576e+01" stime="7.65207e+00" mtime="3.04868e+01" gflop="0.00000e+00" gbyte="3.76766e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.04868e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008f148f1469" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52086e+01" utime="5.07235e+01" stime="7.64377e+00" mtime="3.04868e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.04868e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5038e+08" > 4.6222e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4841e+08" > 3.1499e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2246e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5885e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1004e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5429e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 4.9887e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0469e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5413e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8411e+01 </func>
</region>
</regions>
<internal rank="163" log_i="1723712895.682313" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="164" mpi_size="768" stamp_init="1723712830.340286" stamp_final="1723712895.681052" username="apac4" allocationname="unknown" flags="0" pid="3087480" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53408e+01" utime="4.89417e+01" stime="8.34828e+00" mtime="3.00713e+01" gflop="0.00000e+00" gbyte="3.77834e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.00713e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000961495148e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52101e+01" utime="4.89084e+01" stime="8.33965e+00" mtime="3.00713e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.00713e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4808e+08" > 6.1808e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5021e+08" > 4.0982e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1766e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.6095e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.0483e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1004e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5424e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.0299e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0442e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5431e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7853e+01 </func>
</region>
</regions>
<internal rank="164" log_i="1723712895.681052" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="165" mpi_size="768" stamp_init="1723712830.340083" stamp_final="1723712895.687767" username="apac4" allocationname="unknown" flags="0" pid="3087481" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53477e+01" utime="5.04077e+01" stime="7.99316e+00" mtime="3.06696e+01" gflop="0.00000e+00" gbyte="3.76747e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.06696e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004d145c554d144d1491" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52142e+01" utime="5.03728e+01" stime="7.98577e+00" mtime="3.06696e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.06696e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5021e+08" > 4.6442e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4865e+08" > 3.3038e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8561e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5853e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4080e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1007e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5424e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.0296e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0473e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5405e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7962e+01 </func>
</region>
</regions>
<internal rank="165" log_i="1723712895.687767" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="166" mpi_size="768" stamp_init="1723712830.341724" stamp_final="1723712895.687862" username="apac4" allocationname="unknown" flags="0" pid="3087482" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53461e+01" utime="4.85319e+01" stime="8.56614e+00" mtime="3.01677e+01" gflop="0.00000e+00" gbyte="3.76980e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.01677e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a115a315a415e656a415a41544" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52122e+01" utime="4.85007e+01" stime="8.55462e+00" mtime="3.01677e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.01677e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4926e+08" > 5.9921e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4989e+08" > 3.7331e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9364e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.6030e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8310e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1010e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5406e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.1764e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0483e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5031e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7221e+01 </func>
</region>
</regions>
<internal rank="166" log_i="1723712895.687862" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="167" mpi_size="768" stamp_init="1723712830.341596" stamp_final="1723712895.695947" username="apac4" allocationname="unknown" flags="0" pid="3087483" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u15b</host>
<perf wtime="6.53544e+01" utime="5.00526e+01" stime="8.06333e+00" mtime="3.03712e+01" gflop="0.00000e+00" gbyte="3.76415e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="3.03712e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf414151615171570561715171514" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52214e+01" utime="5.00239e+01" stime="8.04898e+00" mtime="3.03712e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="3.03712e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4925e+08" > 4.7318e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4835e+08" > 2.9460e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2758e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 3.5960e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1008e-01 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5414e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.1399e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0451e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5410e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8228e+01 </func>
</region>
</regions>
<internal rank="167" log_i="1723712895.695947" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="168" mpi_size="768" stamp_init="1723712830.540078" stamp_final="1723712895.689496" username="apac4" allocationname="unknown" flags="0" pid="542397" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51494e+01" utime="4.15848e+01" stime="1.27082e+01" mtime="2.74311e+01" gflop="0.00000e+00" gbyte="3.86593e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.74311e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf45e14601461147c566114611487" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50229e+01" utime="4.15474e+01" stime="1.27032e+01" mtime="2.74311e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.74311e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4935e+08" > 7.4916e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4920e+08" > 4.7938e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0620e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5274e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0917e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.3356e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5412e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.1635e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0458e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5368e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7377e+01 </func>
</region>
</regions>
<internal rank="168" log_i="1723712895.689496" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="169" mpi_size="768" stamp_init="1723712830.540044" stamp_final="1723712895.689362" username="apac4" allocationname="unknown" flags="0" pid="542398" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51493e+01" utime="4.87095e+01" stime="7.47300e+00" mtime="2.84841e+01" gflop="0.00000e+00" gbyte="3.76450e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84841e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4481449144a14f8554a144a148d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50166e+01" utime="4.86836e+01" stime="7.45493e+00" mtime="2.84841e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84841e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4761e+08" > 4.6313e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5003e+08" > 3.1277e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3910e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5291e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.1840e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5416e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.0898e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0503e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5240e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8403e+01 </func>
</region>
</regions>
<internal rank="169" log_i="1723712895.689362" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="170" mpi_size="768" stamp_init="1723712830.540058" stamp_final="1723712895.682254" username="apac4" allocationname="unknown" flags="0" pid="542399" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51422e+01" utime="4.60168e+01" stime="8.39840e+00" mtime="2.81571e+01" gflop="0.00000e+00" gbyte="3.76850e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81571e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b715b915ba15fb55ba15ba1515" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50124e+01" utime="4.59878e+01" stime="8.38502e+00" mtime="2.81571e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81571e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4869e+08" > 7.0358e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4798e+08" > 5.7622e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4936e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5484e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.0109e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.4059e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5404e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.2031e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0465e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5365e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7687e+01 </func>
</region>
</regions>
<internal rank="170" log_i="1723712895.682254" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="171" mpi_size="768" stamp_init="1723712830.540064" stamp_final="1723712895.686788" username="apac4" allocationname="unknown" flags="0" pid="542400" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51467e+01" utime="4.87915e+01" stime="7.45078e+00" mtime="2.87598e+01" gflop="0.00000e+00" gbyte="3.76675e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87598e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf436144f1461145e5561145c1472" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50190e+01" utime="4.87541e+01" stime="7.44540e+00" mtime="2.87598e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87598e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4950e+08" > 4.5158e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4868e+08" > 3.1545e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1516e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5528e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1522e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5418e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.1120e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0499e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5221e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8908e+01 </func>
</region>
</regions>
<internal rank="171" log_i="1723712895.686788" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="172" mpi_size="768" stamp_init="1723712830.540069" stamp_final="1723712895.689609" username="apac4" allocationname="unknown" flags="0" pid="542401" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51495e+01" utime="4.63499e+01" stime="8.30473e+00" mtime="2.80840e+01" gflop="0.00000e+00" gbyte="3.77697e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80840e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50223e+01" utime="4.63156e+01" stime="8.29618e+00" mtime="2.80840e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80840e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4933e+08" > 6.2707e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4882e+08" > 4.1667e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4107e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5362e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.7605e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.6651e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5409e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.1555e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0068e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4867e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7801e+01 </func>
</region>
</regions>
<internal rank="172" log_i="1723712895.689609" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="173" mpi_size="768" stamp_init="1723712830.540088" stamp_final="1723712895.680092" username="apac4" allocationname="unknown" flags="0" pid="542402" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51400e+01" utime="4.89053e+01" stime="7.31379e+00" mtime="2.86462e+01" gflop="0.00000e+00" gbyte="3.77499e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86462e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf47515771578157e557815781534" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50109e+01" utime="4.88761e+01" stime="7.30068e+00" mtime="2.86462e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86462e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4849e+08" > 4.5198e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4873e+08" > 3.2661e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1017e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5277e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.2959e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5403e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.2239e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0517e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5235e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8865e+01 </func>
</region>
</regions>
<internal rank="173" log_i="1723712895.680092" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="174" mpi_size="768" stamp_init="1723712830.540035" stamp_final="1723712895.689141" username="apac4" allocationname="unknown" flags="0" pid="542403" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51491e+01" utime="4.62236e+01" stime="8.45630e+00" mtime="2.82689e+01" gflop="0.00000e+00" gbyte="3.76446e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82689e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d114d014a8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50219e+01" utime="4.61934e+01" stime="8.44475e+00" mtime="2.82689e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82689e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4877e+08" > 6.1466e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4932e+08" > 4.1757e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9506e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5331e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8624e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.6730e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5409e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.1994e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0468e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5334e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7461e+01 </func>
</region>
</regions>
<internal rank="174" log_i="1723712895.689141" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="175" mpi_size="768" stamp_init="1723712830.540099" stamp_final="1723712895.686810" username="apac4" allocationname="unknown" flags="0" pid="542404" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51467e+01" utime="4.86817e+01" stime="7.55272e+00" mtime="2.87670e+01" gflop="0.00000e+00" gbyte="3.77995e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87670e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f8157856f815f81526" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50191e+01" utime="4.86460e+01" stime="7.54560e+00" mtime="2.87670e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87670e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4990e+08" > 4.4926e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4900e+08" > 3.1146e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9340e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5188e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0191e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5398e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.2784e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0494e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5220e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8165e+01 </func>
</region>
</regions>
<internal rank="175" log_i="1723712895.686810" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="176" mpi_size="768" stamp_init="1723712830.540055" stamp_final="1723712895.689688" username="apac4" allocationname="unknown" flags="0" pid="542405" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51496e+01" utime="4.55462e+01" stime="8.58773e+00" mtime="2.81448e+01" gflop="0.00000e+00" gbyte="3.77892e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81448e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4861488148914bb56891488147a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50235e+01" utime="4.55169e+01" stime="8.57437e+00" mtime="2.81448e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81448e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4904e+08" > 6.4754e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4939e+08" > 5.5538e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2559e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5355e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8411e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.5640e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5395e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.3450e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0474e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5341e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6981e+01 </func>
</region>
</regions>
<internal rank="176" log_i="1723712895.689688" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="177" mpi_size="768" stamp_init="1723712830.540050" stamp_final="1723712895.690718" username="apac4" allocationname="unknown" flags="0" pid="542406" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51507e+01" utime="4.87444e+01" stime="7.49634e+00" mtime="2.87194e+01" gflop="0.00000e+00" gbyte="3.76957e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87194e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000029141355291429148d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50215e+01" utime="4.87133e+01" stime="7.48550e+00" mtime="2.87194e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87194e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5082e+08" > 3.6220e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4729e+08" > 2.6248e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6847e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5404e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2927e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5392e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.3842e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0524e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5125e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8446e+01 </func>
</region>
</regions>
<internal rank="177" log_i="1723712895.690718" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="178" mpi_size="768" stamp_init="1723712830.540079" stamp_final="1723712895.690307" username="apac4" allocationname="unknown" flags="0" pid="542407" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51502e+01" utime="4.76461e+01" stime="7.68835e+00" mtime="2.81719e+01" gflop="0.00000e+00" gbyte="3.77434e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81719e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50231e+01" utime="4.76145e+01" stime="7.67754e+00" mtime="2.81719e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81719e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4915e+08" > 4.4110e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5075e+08" > 3.3203e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4955e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5309e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.2680e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.5332e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5389e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.3641e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0515e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5328e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8004e+01 </func>
</region>
</regions>
<internal rank="178" log_i="1723712895.690307" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="179" mpi_size="768" stamp_init="1723712830.540061" stamp_final="1723712895.676550" username="apac4" allocationname="unknown" flags="0" pid="542408" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51365e+01" utime="4.87654e+01" stime="7.48525e+00" mtime="2.90789e+01" gflop="0.00000e+00" gbyte="3.76431e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90789e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50084e+01" utime="4.87376e+01" stime="7.47147e+00" mtime="2.90789e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90789e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4998e+08" > 3.5514e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5016e+08" > 2.7287e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6728e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5179e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.5661e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5379e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.4746e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0463e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4625e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8839e+01 </func>
</region>
</regions>
<internal rank="179" log_i="1723712895.676550" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="180" mpi_size="768" stamp_init="1723712830.540076" stamp_final="1723712895.694877" username="apac4" allocationname="unknown" flags="0" pid="542409" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51548e+01" utime="4.61042e+01" stime="8.42124e+00" mtime="2.82372e+01" gflop="0.00000e+00" gbyte="3.76541e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82372e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b614b6146f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50293e+01" utime="4.60683e+01" stime="8.41519e+00" mtime="2.82372e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82372e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4891e+08" > 5.8725e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4953e+08" > 4.4214e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5836e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5352e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5821e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.5948e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5377e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.4692e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0477e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5274e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7818e+01 </func>
</region>
</regions>
<internal rank="180" log_i="1723712895.694877" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="181" mpi_size="768" stamp_init="1723712830.540083" stamp_final="1723712895.678713" username="apac4" allocationname="unknown" flags="0" pid="542410" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51386e+01" utime="4.87418e+01" stime="7.49389e+00" mtime="2.88441e+01" gflop="0.00000e+00" gbyte="3.77377e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88441e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c814c7145e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50101e+01" utime="4.87109e+01" stime="7.48288e+00" mtime="2.88441e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88441e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.3447e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4832e+08" > 3.5169e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4850e+08" > 2.6699e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0622e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5198e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.5280e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5380e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.5030e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0490e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5112e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8217e+01 </func>
</region>
</regions>
<internal rank="181" log_i="1723712895.678713" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="182" mpi_size="768" stamp_init="1723712830.540030" stamp_final="1723712895.690353" username="apac4" allocationname="unknown" flags="0" pid="542411" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51503e+01" utime="4.72058e+01" stime="8.07634e+00" mtime="2.86430e+01" gflop="0.00000e+00" gbyte="3.76949e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86430e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50251e+01" utime="4.71755e+01" stime="8.06434e+00" mtime="2.86430e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86430e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5092e+08" > 4.5585e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4841e+08" > 3.6368e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.3279e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5306e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.0347e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.1110e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5376e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.4994e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0467e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5278e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7633e+01 </func>
</region>
</regions>
<internal rank="182" log_i="1723712895.690353" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="183" mpi_size="768" stamp_init="1723712830.540028" stamp_final="1723712895.677146" username="apac4" allocationname="unknown" flags="0" pid="542412" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51371e+01" utime="4.84359e+01" stime="7.27547e+00" mtime="2.85056e+01" gflop="0.00000e+00" gbyte="3.78136e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85056e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003c143c149d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50092e+01" utime="4.84064e+01" stime="7.26236e+00" mtime="2.85056e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85056e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4851e+08" > 3.5790e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5049e+08" > 2.8382e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6361e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5488e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0967e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0960e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5378e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.5286e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0488e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5108e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8275e+01 </func>
</region>
</regions>
<internal rank="183" log_i="1723712895.677146" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="184" mpi_size="768" stamp_init="1723712830.540079" stamp_final="1723712895.683042" username="apac4" allocationname="unknown" flags="0" pid="542413" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51430e+01" utime="4.61309e+01" stime="8.41775e+00" mtime="2.76933e+01" gflop="0.00000e+00" gbyte="3.77518e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.76933e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf469156b156c1593556c156c154a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50144e+01" utime="4.61002e+01" stime="8.40601e+00" mtime="2.76933e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.76933e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4744e+08" > 6.6258e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4870e+08" > 5.1706e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1280e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5210e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.2459e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.5578e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5374e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.5569e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0506e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5288e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6657e+01 </func>
</region>
</regions>
<internal rank="184" log_i="1723712895.683042" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="185" mpi_size="768" stamp_init="1723712830.540112" stamp_final="1723712895.683022" username="apac4" allocationname="unknown" flags="0" pid="542414" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51429e+01" utime="4.87056e+01" stime="7.51980e+00" mtime="2.86349e+01" gflop="0.00000e+00" gbyte="3.76938e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86349e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b214b314b514dc56b514b4146b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50144e+01" utime="4.86707e+01" stime="7.51193e+00" mtime="2.86349e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86349e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4903e+08" > 4.8901e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4886e+08" > 2.6039e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0828e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5395e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2398e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8681e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5368e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.6201e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0467e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5111e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7831e+01 </func>
</region>
</regions>
<internal rank="185" log_i="1723712895.683022" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="186" mpi_size="768" stamp_init="1723712830.540014" stamp_final="1723712895.682491" username="apac4" allocationname="unknown" flags="0" pid="542415" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51425e+01" utime="4.72774e+01" stime="7.96737e+00" mtime="2.81326e+01" gflop="0.00000e+00" gbyte="3.77018e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81326e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c6141755c614c614c2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50138e+01" utime="4.72418e+01" stime="7.96056e+00" mtime="2.81326e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81326e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4846e+08" > 5.7376e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4989e+08" > 4.7069e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7019e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5348e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.8930e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.4860e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5363e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.6354e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0496e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5264e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7608e+01 </func>
</region>
</regions>
<internal rank="186" log_i="1723712895.682491" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="187" mpi_size="768" stamp_init="1723712830.540053" stamp_final="1723712895.686371" username="apac4" allocationname="unknown" flags="0" pid="542416" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51463e+01" utime="4.86024e+01" stime="7.62977e+00" mtime="2.87921e+01" gflop="0.00000e+00" gbyte="3.75469e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87921e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50179e+01" utime="4.85661e+01" stime="7.62439e+00" mtime="2.87921e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87921e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4904e+08" > 4.9064e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4911e+08" > 2.6282e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7362e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5530e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9802e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.1742e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5362e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.6471e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0503e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5092e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8320e+01 </func>
</region>
</regions>
<internal rank="187" log_i="1723712895.686371" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="188" mpi_size="768" stamp_init="1723712830.540026" stamp_final="1723712895.689045" username="apac4" allocationname="unknown" flags="0" pid="542417" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51490e+01" utime="4.70622e+01" stime="7.95417e+00" mtime="2.79134e+01" gflop="0.00000e+00" gbyte="3.76301e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79134e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c414c31489" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50215e+01" utime="4.70300e+01" stime="7.94402e+00" mtime="2.79134e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79134e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4779e+08" > 5.8081e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4802e+08" > 4.6002e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9492e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5295e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5172e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.6038e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5358e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.6855e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0495e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5264e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7138e+01 </func>
</region>
</regions>
<internal rank="188" log_i="1723712895.689045" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="189" mpi_size="768" stamp_init="1723712830.540051" stamp_final="1723712895.700525" username="apac4" allocationname="unknown" flags="0" pid="542418" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51605e+01" utime="4.84847e+01" stime="7.49582e+00" mtime="2.84777e+01" gflop="0.00000e+00" gbyte="3.77056e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84777e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50315e+01" utime="4.84525e+01" stime="7.48588e+00" mtime="2.84777e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84777e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4787e+08" > 4.6754e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4765e+08" > 2.6484e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6544e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5352e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.5721e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5354e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.7295e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0508e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5100e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8127e+01 </func>
</region>
</regions>
<internal rank="189" log_i="1723712895.700525" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="190" mpi_size="768" stamp_init="1723712830.540027" stamp_final="1723712895.683611" username="apac4" allocationname="unknown" flags="0" pid="542419" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51436e+01" utime="4.70464e+01" stime="8.08632e+00" mtime="2.78167e+01" gflop="0.00000e+00" gbyte="3.76808e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78167e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ff15fc55ff15ff1507" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50164e+01" utime="4.70145e+01" stime="8.07570e+00" mtime="2.78167e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78167e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4749e+08" > 5.5976e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4847e+08" > 3.6215e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0948e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5379e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.1962e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.8849e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5356e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.7247e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0498e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5251e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6920e+01 </func>
</region>
</regions>
<internal rank="190" log_i="1723712895.683611" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="191" mpi_size="768" stamp_init="1723712830.540079" stamp_final="1723712895.686473" username="apac4" allocationname="unknown" flags="0" pid="542420" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u07b</host>
<perf wtime="6.51464e+01" utime="4.86379e+01" stime="7.64215e+00" mtime="2.83434e+01" gflop="0.00000e+00" gbyte="3.76637e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83434e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50159e+01" utime="4.86039e+01" stime="7.63393e+00" mtime="2.83434e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83434e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4896e+08" > 4.7141e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4701e+08" > 2.6779e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1889e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5461e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0753e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5350e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.7960e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0470e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5086e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7451e+01 </func>
</region>
</regions>
<internal rank="191" log_i="1723712895.686473" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="192" mpi_size="768" stamp_init="1723712830.419095" stamp_final="1723712895.678295" username="apac4" allocationname="unknown" flags="0" pid="687859" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52592e+01" utime="4.30164e+01" stime="1.25190e+01" mtime="2.84356e+01" gflop="0.00000e+00" gbyte="3.85143e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84356e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ef14b756ef14ef14e8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51343e+01" utime="4.29919e+01" stime="1.25020e+01" mtime="2.84356e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84356e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.2875e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4839e+08" > 6.0529e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4759e+08" > 4.8312e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.7753e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3655e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3439e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8597e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5351e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.7956e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0401e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4549e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6649e+01 </func>
</region>
</regions>
<internal rank="192" log_i="1723712895.678295" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="193" mpi_size="768" stamp_init="1723712830.419643" stamp_final="1723712895.688248" username="apac4" allocationname="unknown" flags="0" pid="687860" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52686e+01" utime="4.90544e+01" stime="7.49957e+00" mtime="2.85438e+01" gflop="0.00000e+00" gbyte="3.78120e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85438e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51351e+01" utime="4.90212e+01" stime="7.48922e+00" mtime="2.85438e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85438e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4849e+08" > 4.5705e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4924e+08" > 3.8086e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4616e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6929e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.4926e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5353e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.7622e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0411e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4975e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8226e+01 </func>
</region>
</regions>
<internal rank="193" log_i="1723712895.688248" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="194" mpi_size="768" stamp_init="1723712830.418966" stamp_final="1723712895.683250" username="apac4" allocationname="unknown" flags="0" pid="687861" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52643e+01" utime="4.71227e+01" stime="8.26762e+00" mtime="2.82657e+01" gflop="0.00000e+00" gbyte="3.77369e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82657e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4881589158a155f558a158a1548" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51374e+01" utime="4.70869e+01" stime="8.26056e+00" mtime="2.82657e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82657e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4893e+08" > 6.0217e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4773e+08" > 4.5729e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9119e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6862e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9081e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5943e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5349e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.7602e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0405e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5074e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7354e+01 </func>
</region>
</regions>
<internal rank="194" log_i="1723712895.683250" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="195" mpi_size="768" stamp_init="1723712830.418883" stamp_final="1723712895.688337" username="apac4" allocationname="unknown" flags="0" pid="687862" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52695e+01" utime="4.95407e+01" stime="7.07007e+00" mtime="2.84824e+01" gflop="0.00000e+00" gbyte="3.77441e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84824e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d914a555d914d914d1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51411e+01" utime="4.95112e+01" stime="7.05756e+00" mtime="2.84824e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84824e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4721e+08" > 4.6310e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4753e+08" > 3.7189e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7379e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6892e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6121e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5354e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.7885e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0394e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4935e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8891e+01 </func>
</region>
</regions>
<internal rank="195" log_i="1723712895.688337" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="196" mpi_size="768" stamp_init="1723712830.418982" stamp_final="1723712895.695769" username="apac4" allocationname="unknown" flags="0" pid="687863" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52768e+01" utime="4.71467e+01" stime="8.28427e+00" mtime="2.84797e+01" gflop="0.00000e+00" gbyte="3.77968e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84797e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004d154c1514" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51488e+01" utime="4.71144e+01" stime="8.27459e+00" mtime="2.84797e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84797e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4826e+08" > 6.0854e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4778e+08" > 6.1922e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1468e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7091e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.6139e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7573e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5348e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.8121e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0402e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5063e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7282e+01 </func>
</region>
</regions>
<internal rank="196" log_i="1723712895.695769" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="197" mpi_size="768" stamp_init="1723712830.418876" stamp_final="1723712895.696421" username="apac4" allocationname="unknown" flags="0" pid="687864" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52775e+01" utime="4.94329e+01" stime="7.30940e+00" mtime="2.87224e+01" gflop="0.00000e+00" gbyte="3.76194e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87224e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf46b156c156d15a3566d156d1525" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51507e+01" utime="4.94108e+01" stime="7.29000e+00" mtime="2.87224e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87224e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4945e+08" > 4.3962e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4926e+08" > 3.3985e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3741e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6952e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.4590e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5349e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.8428e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0415e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4983e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8514e+01 </func>
</region>
</regions>
<internal rank="197" log_i="1723712895.696421" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="198" mpi_size="768" stamp_init="1723712830.418992" stamp_final="1723712895.695785" username="apac4" allocationname="unknown" flags="0" pid="687865" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52768e+01" utime="4.77488e+01" stime="7.90620e+00" mtime="2.81442e+01" gflop="0.00000e+00" gbyte="3.77132e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81442e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004e154e153b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51504e+01" utime="4.77171e+01" stime="7.89575e+00" mtime="2.81442e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81442e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.0599e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4991e+08" > 5.8525e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4980e+08" > 5.2928e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6835e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6836e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.0347e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7340e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5339e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.8822e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0409e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5045e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7475e+01 </func>
</region>
</regions>
<internal rank="198" log_i="1723712895.695785" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="199" mpi_size="768" stamp_init="1723712830.418880" stamp_final="1723712895.684035" username="apac4" allocationname="unknown" flags="0" pid="687866" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52652e+01" utime="4.92578e+01" stime="7.56184e+00" mtime="2.86480e+01" gflop="0.00000e+00" gbyte="3.76904e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86480e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b614b61490" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51376e+01" utime="4.92296e+01" stime="7.54816e+00" mtime="2.86480e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86480e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4926e+08" > 4.6304e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4941e+08" > 3.2406e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4158e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7015e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7309e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5336e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.9201e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0438e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4939e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8371e+01 </func>
</region>
</regions>
<internal rank="199" log_i="1723712895.684035" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="200" mpi_size="768" stamp_init="1723712830.418971" stamp_final="1723712895.677134" username="apac4" allocationname="unknown" flags="0" pid="687867" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52582e+01" utime="4.73876e+01" stime="8.23274e+00" mtime="2.83044e+01" gflop="0.00000e+00" gbyte="3.77144e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83044e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b315b415b5156c55b515b51550" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51336e+01" utime="4.73563e+01" stime="8.22176e+00" mtime="2.83044e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83044e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4900e+08" > 4.8764e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4898e+08" > 3.8971e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.3934e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7017e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.7519e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6772e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5336e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.9611e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0422e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5028e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7010e+01 </func>
</region>
</regions>
<internal rank="200" log_i="1723712895.677134" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="201" mpi_size="768" stamp_init="1723712830.418903" stamp_final="1723712895.688326" username="apac4" allocationname="unknown" flags="0" pid="687868" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52694e+01" utime="4.93315e+01" stime="7.53938e+00" mtime="2.92938e+01" gflop="0.00000e+00" gbyte="3.78204e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92938e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003e143d1488" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51398e+01" utime="4.93005e+01" stime="7.52848e+00" mtime="2.92938e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92938e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4850e+08" > 3.5850e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4838e+08" > 2.9327e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9749e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6597e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9903e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5338e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.9599e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0440e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5014e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8606e+01 </func>
</region>
</regions>
<internal rank="201" log_i="1723712895.688326" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="202" mpi_size="768" stamp_init="1723712830.418959" stamp_final="1723712895.680692" username="apac4" allocationname="unknown" flags="0" pid="687869" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52617e+01" utime="4.59171e+01" stime="8.67421e+00" mtime="2.87768e+01" gflop="0.00000e+00" gbyte="3.76629e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87768e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51356e+01" utime="4.58766e+01" stime="8.67270e+00" mtime="2.87768e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87768e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4920e+08" > 6.1866e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4807e+08" > 4.2930e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.3934e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7049e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1531e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7161e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5329e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.9413e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0439e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5026e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7350e+01 </func>
</region>
</regions>
<internal rank="202" log_i="1723712895.680692" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="203" mpi_size="768" stamp_init="1723712830.419363" stamp_final="1723712895.677422" username="apac4" allocationname="unknown" flags="0" pid="687870" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52581e+01" utime="4.96142e+01" stime="7.19624e+00" mtime="2.88107e+01" gflop="0.00000e+00" gbyte="3.78105e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88107e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a8148c55a814a7147a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51294e+01" utime="4.95886e+01" stime="7.17956e+00" mtime="2.88107e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88107e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4945e+08" > 3.6165e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4874e+08" > 2.6725e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0650e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6934e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6810e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5335e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 5.9768e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0404e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4935e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8999e+01 </func>
</region>
</regions>
<internal rank="203" log_i="1723712895.677422" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="204" mpi_size="768" stamp_init="1723712830.418973" stamp_final="1723712895.683227" username="apac4" allocationname="unknown" flags="0" pid="687871" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52643e+01" utime="4.72755e+01" stime="8.08968e+00" mtime="2.86662e+01" gflop="0.00000e+00" gbyte="3.77422e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86662e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51364e+01" utime="4.72452e+01" stime="8.07807e+00" mtime="2.86662e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86662e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4769e+08" > 5.1895e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4860e+08" > 3.7493e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.4216e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7073e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.4322e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7425e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5329e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.0437e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0425e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5025e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7316e+01 </func>
</region>
</regions>
<internal rank="204" log_i="1723712895.683227" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="205" mpi_size="768" stamp_init="1723712830.418894" stamp_final="1723712895.683120" username="apac4" allocationname="unknown" flags="0" pid="687872" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52642e+01" utime="4.94043e+01" stime="7.45728e+00" mtime="2.90213e+01" gflop="0.00000e+00" gbyte="3.75488e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90213e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ec15ed15ef157d55ef15ee153f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51358e+01" utime="4.93701e+01" stime="7.44930e+00" mtime="2.90213e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90213e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4974e+08" > 3.6151e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4879e+08" > 2.9549e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7660e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7031e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.7203e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5325e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.0973e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0433e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4456e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8496e+01 </func>
</region>
</regions>
<internal rank="205" log_i="1723712895.683120" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="206" mpi_size="768" stamp_init="1723712830.419467" stamp_final="1723712895.688266" username="apac4" allocationname="unknown" flags="0" pid="687873" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52688e+01" utime="4.77681e+01" stime="7.92118e+00" mtime="2.81872e+01" gflop="0.00000e+00" gbyte="3.76743e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81872e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000037156355371537153b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51425e+01" utime="4.77375e+01" stime="7.91017e+00" mtime="2.81872e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81872e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4949e+08" > 4.7930e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5027e+08" > 4.5594e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7438e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6989e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.7193e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7662e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5320e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.1425e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0404e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5017e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7555e+01 </func>
</region>
</regions>
<internal rank="206" log_i="1723712895.688266" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="207" mpi_size="768" stamp_init="1723712830.418917" stamp_final="1723712895.683120" username="apac4" allocationname="unknown" flags="0" pid="687874" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52642e+01" utime="4.91025e+01" stime="7.76378e+00" mtime="2.93965e+01" gflop="0.00000e+00" gbyte="3.77163e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.93965e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d114e255d114d114f7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51359e+01" utime="4.90728e+01" stime="7.75159e+00" mtime="2.93965e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.93965e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4915e+08" > 3.5295e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5057e+08" > 2.4090e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2338e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7221e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8597e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.6471e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5320e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.1426e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0436e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4954e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8401e+01 </func>
</region>
</regions>
<internal rank="207" log_i="1723712895.683120" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="208" mpi_size="768" stamp_init="1723712830.418986" stamp_final="1723712895.677212" username="apac4" allocationname="unknown" flags="0" pid="687875" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52582e+01" utime="4.66712e+01" stime="8.23603e+00" mtime="2.76775e+01" gflop="0.00000e+00" gbyte="3.75107e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.76775e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d014d114d2144556d214d214c4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51323e+01" utime="4.66409e+01" stime="8.22426e+00" mtime="2.76775e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.76775e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4783e+08" > 6.5849e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4920e+08" > 4.1318e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3205e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4432e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4063e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.5272e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5319e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.1437e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0393e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5021e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7305e+01 </func>
</region>
</regions>
<internal rank="208" log_i="1723712895.677212" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="209" mpi_size="768" stamp_init="1723712830.418906" stamp_final="1723712895.693293" username="apac4" allocationname="unknown" flags="0" pid="687876" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52744e+01" utime="4.92791e+01" stime="7.54451e+00" mtime="2.87575e+01" gflop="0.00000e+00" gbyte="3.77819e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87575e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001014fb146f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51457e+01" utime="4.92479e+01" stime="7.53362e+00" mtime="2.87575e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87575e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4965e+08" > 4.7180e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4779e+08" > 3.3288e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1532e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7000e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7490e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5315e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.1586e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0412e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4970e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8735e+01 </func>
</region>
</regions>
<internal rank="209" log_i="1723712895.693293" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="210" mpi_size="768" stamp_init="1723712830.418959" stamp_final="1723712895.676991" username="apac4" allocationname="unknown" flags="0" pid="687877" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52580e+01" utime="4.70306e+01" stime="8.50770e+00" mtime="2.85438e+01" gflop="0.00000e+00" gbyte="3.76682e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85438e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000461533554615451503" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51306e+01" utime="4.69922e+01" stime="8.50416e+00" mtime="2.85438e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85438e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4888e+08" > 6.2551e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4911e+08" > 5.0302e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7214e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6985e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.5088e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.6640e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5309e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.2521e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0402e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5001e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7783e+01 </func>
</region>
</regions>
<internal rank="210" log_i="1723712895.676991" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="211" mpi_size="768" stamp_init="1723712830.418911" stamp_final="1723712895.677486" username="apac4" allocationname="unknown" flags="0" pid="687878" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52586e+01" utime="4.93212e+01" stime="7.37029e+00" mtime="2.87296e+01" gflop="0.00000e+00" gbyte="3.77632e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87296e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51310e+01" utime="4.92939e+01" stime="7.35412e+00" mtime="2.87296e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87296e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4885e+08" > 4.8422e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4999e+08" > 2.6590e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9861e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6902e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4305e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.6458e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5316e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.1835e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0426e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4909e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8876e+01 </func>
</region>
</regions>
<internal rank="211" log_i="1723712895.677486" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="212" mpi_size="768" stamp_init="1723712830.419512" stamp_final="1723712895.683129" username="apac4" allocationname="unknown" flags="0" pid="687879" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52636e+01" utime="4.68840e+01" stime="8.26007e+00" mtime="2.82070e+01" gflop="0.00000e+00" gbyte="3.77064e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82070e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51387e+01" utime="4.68540e+01" stime="8.24797e+00" mtime="2.82070e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82070e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4862e+08" > 6.3093e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5002e+08" > 4.0822e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3463e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7014e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.4376e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8022e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5305e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.2557e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0408e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.5010e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7814e+01 </func>
</region>
</regions>
<internal rank="212" log_i="1723712895.683129" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="213" mpi_size="768" stamp_init="1723712830.418919" stamp_final="1723712895.691278" username="apac4" allocationname="unknown" flags="0" pid="687880" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52724e+01" utime="4.90395e+01" stime="7.62404e+00" mtime="2.90110e+01" gflop="0.00000e+00" gbyte="3.77953e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90110e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000fa14f914ef" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51434e+01" utime="4.90104e+01" stime="7.61069e+00" mtime="2.90110e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90110e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4933e+08" > 4.6879e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4935e+08" > 2.8933e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7901e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7123e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0980e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5296e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.3861e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0412e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4941e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8341e+01 </func>
</region>
</regions>
<internal rank="213" log_i="1723712895.691278" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="214" mpi_size="768" stamp_init="1723712830.419004" stamp_final="1723712895.694954" username="apac4" allocationname="unknown" flags="0" pid="687881" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52759e+01" utime="4.69918e+01" stime="8.30817e+00" mtime="2.78587e+01" gflop="0.00000e+00" gbyte="3.76389e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78587e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51507e+01" utime="4.69577e+01" stime="8.30020e+00" mtime="2.78587e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78587e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4972e+08" > 5.9897e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4924e+08" > 4.6860e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6115e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7019e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7311e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7690e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5302e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.3304e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0431e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4996e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7232e+01 </func>
</region>
</regions>
<internal rank="214" log_i="1723712895.694954" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="215" mpi_size="768" stamp_init="1723712830.418897" stamp_final="1723712895.692214" username="apac4" allocationname="unknown" flags="0" pid="687882" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u09a</host>
<perf wtime="6.52733e+01" utime="4.95256e+01" stime="7.32047e+00" mtime="2.86931e+01" gflop="0.00000e+00" gbyte="3.77178e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86931e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c615c715c8158055c815c8152d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51456e+01" utime="4.94940e+01" stime="7.30965e+00" mtime="2.86931e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86931e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4933e+08" > 4.6983e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4982e+08" > 3.0566e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2794e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6883e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7218e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5290e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.4234e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0404e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4979e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8561e+01 </func>
</region>
</regions>
<internal rank="215" log_i="1723712895.692214" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="216" mpi_size="768" stamp_init="1723712830.578853" stamp_final="1723712895.685123" username="apac4" allocationname="unknown" flags="0" pid="774597" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51063e+01" utime="4.27242e+01" stime="1.22199e+01" mtime="2.77403e+01" gflop="0.00000e+00" gbyte="3.86879e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77403e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf46515671568154256681568150f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49757e+01" utime="4.26964e+01" stime="1.22048e+01" mtime="2.77403e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77403e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4809e+08" > 6.3364e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4891e+08" > 4.0497e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1708e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4147e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.7459e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.5520e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5299e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.3514e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0472e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4930e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7807e+01 </func>
</region>
</regions>
<internal rank="216" log_i="1723712895.685123" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="217" mpi_size="768" stamp_init="1723712830.578682" stamp_final="1723712895.692858" username="apac4" allocationname="unknown" flags="0" pid="774598" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51142e+01" utime="4.89951e+01" stime="7.31956e+00" mtime="2.83915e+01" gflop="0.00000e+00" gbyte="3.77151e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83915e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b815b7154e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49795e+01" utime="4.89612e+01" stime="7.30988e+00" mtime="2.83915e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83915e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4858e+08" > 4.5064e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4905e+08" > 2.9343e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1670e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3930e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7334e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5304e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.3091e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0444e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4698e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8683e+01 </func>
</region>
</regions>
<internal rank="217" log_i="1723712895.692858" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="218" mpi_size="768" stamp_init="1723712830.578619" stamp_final="1723712895.683943" username="apac4" allocationname="unknown" flags="0" pid="774599" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51053e+01" utime="4.58027e+01" stime="8.54188e+00" mtime="2.77186e+01" gflop="0.00000e+00" gbyte="3.76888e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77186e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49736e+01" utime="4.57712e+01" stime="8.53049e+00" mtime="2.77186e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77186e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4804e+08" > 6.7837e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4941e+08" > 3.9574e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7977e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4022e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.7990e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7377e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5294e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.3341e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0500e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4869e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7133e+01 </func>
</region>
</regions>
<internal rank="218" log_i="1723712895.683943" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="219" mpi_size="768" stamp_init="1723712830.578743" stamp_final="1723712895.685656" username="apac4" allocationname="unknown" flags="0" pid="774600" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51069e+01" utime="4.85975e+01" stime="7.71771e+00" mtime="2.89006e+01" gflop="0.00000e+00" gbyte="3.76888e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89006e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006b1487556b146b14d9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49749e+01" utime="4.85639e+01" stime="7.70913e+00" mtime="2.89006e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89006e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4964e+08" > 4.5144e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4796e+08" > 2.8387e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4990e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3966e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1079e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5303e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.3252e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0487e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4664e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8855e+01 </func>
</region>
</regions>
<internal rank="219" log_i="1723712895.685656" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="220" mpi_size="768" stamp_init="1723712830.578680" stamp_final="1723712895.690492" username="apac4" allocationname="unknown" flags="0" pid="774601" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51118e+01" utime="4.64110e+01" stime="8.40217e+00" mtime="2.77604e+01" gflop="0.00000e+00" gbyte="3.77312e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77604e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4381539153a1594553a153a1546" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49798e+01" utime="4.63790e+01" stime="8.39119e+00" mtime="2.77604e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77604e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4958e+08" > 5.9796e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4849e+08" > 3.6122e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0757e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3897e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7819e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6042e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5288e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.4409e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0485e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4860e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6990e+01 </func>
</region>
</regions>
<internal rank="220" log_i="1723712895.690492" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="221" mpi_size="768" stamp_init="1723712830.578736" stamp_final="1723712895.687536" username="apac4" allocationname="unknown" flags="0" pid="774602" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51088e+01" utime="4.85819e+01" stime="7.64522e+00" mtime="2.86577e+01" gflop="0.00000e+00" gbyte="3.76438e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86577e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b814b7149d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49780e+01" utime="4.85441e+01" stime="7.64098e+00" mtime="2.86577e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86577e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4944e+08" > 4.7942e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4913e+08" > 2.7680e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8419e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4032e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.5897e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5292e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.4034e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0473e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4632e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8238e+01 </func>
</region>
</regions>
<internal rank="221" log_i="1723712895.687536" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="222" mpi_size="768" stamp_init="1723712830.578657" stamp_final="1723712895.680015" username="apac4" allocationname="unknown" flags="0" pid="774603" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51014e+01" utime="4.52224e+01" stime="8.82518e+00" mtime="2.73958e+01" gflop="0.00000e+00" gbyte="3.76442e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.73958e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49713e+01" utime="4.51906e+01" stime="8.81484e+00" mtime="2.73958e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.73958e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4996e+08" > 8.4974e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5145e+08" > 5.5299e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4158e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4151e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.7217e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.8378e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5286e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.4496e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0444e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4861e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6997e+01 </func>
</region>
</regions>
<internal rank="222" log_i="1723712895.680015" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="223" mpi_size="768" stamp_init="1723712830.578709" stamp_final="1723712895.690623" username="apac4" allocationname="unknown" flags="0" pid="774604" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51119e+01" utime="4.88472e+01" stime="7.46029e+00" mtime="2.87959e+01" gflop="0.00000e+00" gbyte="3.77831e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87959e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49801e+01" utime="4.88194e+01" stime="7.44489e+00" mtime="2.87959e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87959e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5010e+08" > 4.3907e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4944e+08" > 2.7445e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5369e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4145e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.9380e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5275e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.5735e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0469e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4633e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8719e+01 </func>
</region>
</regions>
<internal rank="223" log_i="1723712895.690623" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="224" mpi_size="768" stamp_init="1723712830.578689" stamp_final="1723712895.685130" username="apac4" allocationname="unknown" flags="0" pid="774605" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51064e+01" utime="4.70692e+01" stime="8.12117e+00" mtime="2.81648e+01" gflop="0.00000e+00" gbyte="3.76617e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81648e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c615c715c9155f55c915c81520" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49760e+01" utime="4.70327e+01" stime="8.11413e+00" mtime="2.81648e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81648e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4987e+08" > 4.9279e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4881e+08" > 4.0322e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2066e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3161e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.4267e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1952e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5270e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.5829e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0466e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4824e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7352e+01 </func>
</region>
</regions>
<internal rank="224" log_i="1723712895.685130" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="225" mpi_size="768" stamp_init="1723712830.578712" stamp_final="1723712895.681020" username="apac4" allocationname="unknown" flags="0" pid="774606" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51023e+01" utime="4.90235e+01" stime="7.23982e+00" mtime="2.83572e+01" gflop="0.00000e+00" gbyte="3.76316e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83572e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d914d414de" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49709e+01" utime="4.89899e+01" stime="7.23128e+00" mtime="2.83572e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83572e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4919e+08" > 3.5133e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4931e+08" > 2.8146e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3396e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4134e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3351e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6161e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5276e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.6039e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0499e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4644e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8556e+01 </func>
</region>
</regions>
<internal rank="225" log_i="1723712895.681020" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="226" mpi_size="768" stamp_init="1723712830.578644" stamp_final="1723712895.692986" username="apac4" allocationname="unknown" flags="0" pid="774607" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51143e+01" utime="4.68554e+01" stime="8.08154e+00" mtime="2.77913e+01" gflop="0.00000e+00" gbyte="3.76270e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77913e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000dd15dd1523" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49843e+01" utime="4.68254e+01" stime="8.06951e+00" mtime="2.77913e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77913e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.0967e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4836e+08" > 5.4970e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4913e+08" > 4.5111e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0703e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3936e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3638e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8367e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5260e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.6743e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0472e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4826e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8063e+01 </func>
</region>
</regions>
<internal rank="226" log_i="1723712895.692986" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="227" mpi_size="768" stamp_init="1723712830.578757" stamp_final="1723712895.679215" username="apac4" allocationname="unknown" flags="0" pid="774608" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51005e+01" utime="4.90630e+01" stime="7.20723e+00" mtime="2.84244e+01" gflop="0.00000e+00" gbyte="3.74935e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84244e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c214c21463" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49695e+01" utime="4.90345e+01" stime="7.19277e+00" mtime="2.84244e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84244e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4949e+08" > 3.5073e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4885e+08" > 2.7787e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3086e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3926e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8379e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5268e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.6776e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0488e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4631e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8676e+01 </func>
</region>
</regions>
<internal rank="227" log_i="1723712895.679215" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="228" mpi_size="768" stamp_init="1723712830.578734" stamp_final="1723712895.690015" username="apac4" allocationname="unknown" flags="0" pid="774609" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51113e+01" utime="4.60029e+01" stime="8.54196e+00" mtime="2.81661e+01" gflop="0.00000e+00" gbyte="3.76442e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81661e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000eb152655eb15eb1534" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49795e+01" utime="4.59639e+01" stime="8.53891e+00" mtime="2.81661e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81661e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4931e+08" > 5.8084e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4989e+08" > 4.3190e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5797e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3725e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3396e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.5456e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5252e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.8149e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0486e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4792e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7888e+01 </func>
</region>
</regions>
<internal rank="228" log_i="1723712895.690015" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="229" mpi_size="768" stamp_init="1723712830.578708" stamp_final="1723712895.686391" username="apac4" allocationname="unknown" flags="0" pid="774610" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51077e+01" utime="4.89033e+01" stime="7.40978e+00" mtime="2.89047e+01" gflop="0.00000e+00" gbyte="3.77964e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89047e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d014cf147c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49775e+01" utime="4.88734e+01" stime="7.39766e+00" mtime="2.89047e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89047e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4907e+08" > 3.8320e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4935e+08" > 2.9420e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.4751e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3674e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.5367e-06 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.7370e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5267e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.6912e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0492e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4630e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7981e+01 </func>
</region>
</regions>
<internal rank="229" log_i="1723712895.686391" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="230" mpi_size="768" stamp_init="1723712830.578633" stamp_final="1723712895.689714" username="apac4" allocationname="unknown" flags="0" pid="774611" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51111e+01" utime="4.58916e+01" stime="8.59172e+00" mtime="2.78562e+01" gflop="0.00000e+00" gbyte="3.76865e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78562e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49802e+01" utime="4.58528e+01" stime="8.58847e+00" mtime="2.78562e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78562e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4949e+08" > 5.3747e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4869e+08" > 4.3079e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1632e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3970e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.3879e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6247e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5246e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.9034e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0492e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4795e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7042e+01 </func>
</region>
</regions>
<internal rank="230" log_i="1723712895.689714" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="231" mpi_size="768" stamp_init="1723712830.578762" stamp_final="1723712895.682188" username="apac4" allocationname="unknown" flags="0" pid="774612" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51034e+01" utime="4.90130e+01" stime="7.28681e+00" mtime="2.88874e+01" gflop="0.00000e+00" gbyte="3.76690e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88874e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003315d7553315331551" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49712e+01" utime="4.89877e+01" stime="7.26983e+00" mtime="2.88874e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88874e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4905e+08" > 3.5601e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4926e+08" > 2.5109e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7727e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4084e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.5448e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5261e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.7177e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0494e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4633e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8657e+01 </func>
</region>
</regions>
<internal rank="231" log_i="1723712895.682188" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="232" mpi_size="768" stamp_init="1723712830.578702" stamp_final="1723712895.690053" username="apac4" allocationname="unknown" flags="0" pid="774613" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51114e+01" utime="4.60814e+01" stime="8.40547e+00" mtime="2.74039e+01" gflop="0.00000e+00" gbyte="3.77823e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.74039e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a314a514a6144155a614a614e0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49804e+01" utime="4.60468e+01" stime="8.39746e+00" mtime="2.74039e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.74039e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4888e+08" > 6.9265e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4938e+08" > 4.8491e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5266e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3589e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.1065e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.5711e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5254e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.7777e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0495e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4782e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7102e+01 </func>
</region>
</regions>
<internal rank="232" log_i="1723712895.690053" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="233" mpi_size="768" stamp_init="1723712830.578679" stamp_final="1723712895.684623" username="apac4" allocationname="unknown" flags="0" pid="774614" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51059e+01" utime="4.90180e+01" stime="7.27456e+00" mtime="2.84568e+01" gflop="0.00000e+00" gbyte="3.76858e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84568e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000079144b5679147914e2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49733e+01" utime="4.89915e+01" stime="7.25839e+00" mtime="2.84568e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84568e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.0014e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4894e+08" > 4.8228e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4901e+08" > 3.2455e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7813e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4094e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.2410e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5252e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.8357e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0487e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4619e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8088e+01 </func>
</region>
</regions>
<internal rank="233" log_i="1723712895.684623" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="234" mpi_size="768" stamp_init="1723712830.578665" stamp_final="1723712895.685310" username="apac4" allocationname="unknown" flags="0" pid="774615" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51066e+01" utime="4.72078e+01" stime="7.98074e+00" mtime="2.80219e+01" gflop="0.00000e+00" gbyte="3.75118e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80219e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf46c156e156f15b7556f156e1505" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49752e+01" utime="4.71752e+01" stime="7.96993e+00" mtime="2.80219e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80219e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5066e+08" > 6.3898e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4783e+08" > 5.0026e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9959e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4098e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.7590e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6040e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5251e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.8478e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0483e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4793e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7253e+01 </func>
</region>
</regions>
<internal rank="234" log_i="1723712895.685310" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="235" mpi_size="768" stamp_init="1723712830.579030" stamp_final="1723712895.679356" username="apac4" allocationname="unknown" flags="0" pid="774616" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51003e+01" utime="4.89979e+01" stime="7.30564e+00" mtime="2.86072e+01" gflop="0.00000e+00" gbyte="3.76965e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86072e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49677e+01" utime="4.89640e+01" stime="7.29735e+00" mtime="2.86072e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86072e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4892e+08" > 4.7672e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5079e+08" > 3.0411e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4468e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4011e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6271e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5246e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.9025e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0456e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4641e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8584e+01 </func>
</region>
</regions>
<internal rank="235" log_i="1723712895.679356" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="236" mpi_size="768" stamp_init="1723712830.578623" stamp_final="1723712895.678313" username="apac4" allocationname="unknown" flags="0" pid="774617" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.50997e+01" utime="4.64888e+01" stime="8.10799e+00" mtime="2.75132e+01" gflop="0.00000e+00" gbyte="3.74756e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.75132e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008e158e152c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49682e+01" utime="4.64571e+01" stime="8.09704e+00" mtime="2.75132e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.75132e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4950e+08" > 6.5108e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4947e+08" > 4.8421e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2883e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4155e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.9308e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.5668e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5244e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.8735e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0471e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4748e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7441e+01 </func>
</region>
</regions>
<internal rank="236" log_i="1723712895.678313" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="237" mpi_size="768" stamp_init="1723712830.578677" stamp_final="1723712895.679907" username="apac4" allocationname="unknown" flags="0" pid="774618" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51012e+01" utime="4.88879e+01" stime="7.39807e+00" mtime="2.84393e+01" gflop="0.00000e+00" gbyte="3.76049e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84393e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ee14f014f114bf55f114f114ea" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49684e+01" utime="4.88555e+01" stime="7.38849e+00" mtime="2.84393e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84393e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4924e+08" > 4.7683e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4975e+08" > 3.2848e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5304e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3987e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.5620e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5242e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.9489e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0490e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4613e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8332e+01 </func>
</region>
</regions>
<internal rank="237" log_i="1723712895.679907" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="238" mpi_size="768" stamp_init="1723712830.578675" stamp_final="1723712895.684774" username="apac4" allocationname="unknown" flags="0" pid="774619" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51061e+01" utime="4.67398e+01" stime="8.04051e+00" mtime="2.80530e+01" gflop="0.00000e+00" gbyte="3.76171e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80530e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49749e+01" utime="4.67088e+01" stime="8.02892e+00" mtime="2.80530e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80530e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4904e+08" > 6.2205e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4885e+08" > 5.4025e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2006e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3959e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4138e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1267e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5221e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.1261e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0492e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4733e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7110e+01 </func>
</region>
</regions>
<internal rank="238" log_i="1723712895.684774" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="239" mpi_size="768" stamp_init="1723712830.578775" stamp_final="1723712895.682162" username="apac4" allocationname="unknown" flags="0" pid="774620" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u15a</host>
<perf wtime="6.51034e+01" utime="4.84361e+01" stime="7.68724e+00" mtime="2.85282e+01" gflop="0.00000e+00" gbyte="3.77415e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85282e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49710e+01" utime="4.84028e+01" stime="7.67743e+00" mtime="2.85282e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85282e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4866e+08" > 4.9333e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4875e+08" > 3.1007e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8748e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3776e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1270e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5230e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.0302e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0506e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4106e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8082e+01 </func>
</region>
</regions>
<internal rank="239" log_i="1723712895.682162" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="240" mpi_size="768" stamp_init="1723712830.275937" stamp_final="1723712895.694466" username="apac4" allocationname="unknown" flags="0" pid="980708" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54185e+01" utime="4.25906e+01" stime="1.21941e+01" mtime="2.71561e+01" gflop="0.00000e+00" gbyte="3.86803e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.71561e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf45e155f1560157955601560150e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52896e+01" utime="4.25582e+01" stime="1.21832e+01" mtime="2.71561e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.71561e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5013e+08" > 6.5455e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4986e+08" > 4.4219e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8126e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3837e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.1750e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.2479e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5231e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.9962e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0459e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4580e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6598e+01 </func>
</region>
</regions>
<internal rank="240" log_i="1723712895.694466" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="241" mpi_size="768" stamp_init="1723712830.277088" stamp_final="1723712895.681103" username="apac4" allocationname="unknown" flags="0" pid="980709" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54040e+01" utime="4.90503e+01" stime="7.36795e+00" mtime="2.79828e+01" gflop="0.00000e+00" gbyte="3.76968e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79828e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b114b01471" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52668e+01" utime="4.90130e+01" stime="7.36196e+00" mtime="2.79828e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79828e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5006e+08" > 5.3870e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4980e+08" > 3.1608e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1674e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3912e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.0000e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5242e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.9340e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0476e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4546e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8189e+01 </func>
</region>
</regions>
<internal rank="241" log_i="1723712895.681103" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="242" mpi_size="768" stamp_init="1723712830.275942" stamp_final="1723712895.697691" username="apac4" allocationname="unknown" flags="0" pid="980710" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54217e+01" utime="4.67967e+01" stime="8.12666e+00" mtime="2.74603e+01" gflop="0.00000e+00" gbyte="3.76076e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.74603e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006b146b1462" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52892e+01" utime="4.67629e+01" stime="8.11862e+00" mtime="2.74603e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.74603e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4851e+08" > 6.8198e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4901e+08" > 5.3641e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2023e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3905e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.7275e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.2877e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5243e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.9172e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0485e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4576e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7469e+01 </func>
</region>
</regions>
<internal rank="242" log_i="1723712895.697691" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="243" mpi_size="768" stamp_init="1723712830.275916" stamp_final="1723712895.696085" username="apac4" allocationname="unknown" flags="0" pid="980711" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54202e+01" utime="4.92000e+01" stime="7.29223e+00" mtime="2.78694e+01" gflop="0.00000e+00" gbyte="3.77670e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78694e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44b144d144e14d6554e144d14c2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52867e+01" utime="4.91624e+01" stime="7.28719e+00" mtime="2.78694e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78694e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4876e+08" > 4.6756e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4808e+08" > 3.3722e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8106e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3941e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3842e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5141e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5234e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.9754e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0455e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4000e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8502e+01 </func>
</region>
</regions>
<internal rank="243" log_i="1723712895.696085" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="244" mpi_size="768" stamp_init="1723712830.277010" stamp_final="1723712895.692639" username="apac4" allocationname="unknown" flags="0" pid="980712" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54156e+01" utime="4.67644e+01" stime="8.12078e+00" mtime="2.73735e+01" gflop="0.00000e+00" gbyte="3.76945e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.73735e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009714a955971497147b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52881e+01" utime="4.67317e+01" stime="8.11139e+00" mtime="2.73735e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.73735e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4928e+08" > 6.3147e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4907e+08" > 5.0251e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2624e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3917e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0524e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4744e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5233e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 6.9658e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0469e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4077e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7365e+01 </func>
</region>
</regions>
<internal rank="244" log_i="1723712895.692639" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="245" mpi_size="768" stamp_init="1723712830.275914" stamp_final="1723712895.690272" username="apac4" allocationname="unknown" flags="0" pid="980713" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54144e+01" utime="4.91276e+01" stime="7.29810e+00" mtime="2.79552e+01" gflop="0.00000e+00" gbyte="3.76225e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79552e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52810e+01" utime="4.90978e+01" stime="7.28592e+00" mtime="2.79552e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79552e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4800e+08" > 4.6848e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4925e+08" > 2.8665e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5042e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3750e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1921e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.5109e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5230e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.0493e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0451e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4519e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7916e+01 </func>
</region>
</regions>
<internal rank="245" log_i="1723712895.690272" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="246" mpi_size="768" stamp_init="1723712830.275887" stamp_final="1723712895.689678" username="apac4" allocationname="unknown" flags="0" pid="980714" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54138e+01" utime="4.63515e+01" stime="8.46890e+00" mtime="2.72794e+01" gflop="0.00000e+00" gbyte="3.74195e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.72794e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003f143a563f143e148c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52807e+01" utime="4.63234e+01" stime="8.45497e+00" mtime="2.72794e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.72794e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5005e+08" > 7.5371e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5038e+08" > 5.2224e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5933e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3711e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.6519e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.5177e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5229e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.0463e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0450e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4558e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6841e+01 </func>
</region>
</regions>
<internal rank="246" log_i="1723712895.689678" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="247" mpi_size="768" stamp_init="1723712830.275932" stamp_final="1723712895.689866" username="apac4" allocationname="unknown" flags="0" pid="980715" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54139e+01" utime="4.90209e+01" stime="7.42311e+00" mtime="2.78959e+01" gflop="0.00000e+00" gbyte="3.77567e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78959e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008914425589148814d8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52822e+01" utime="4.89884e+01" stime="7.41228e+00" mtime="2.78959e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78959e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4940e+08" > 4.7924e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4996e+08" > 2.8987e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4485e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3663e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.5291e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5227e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.0819e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0486e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4508e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7912e+01 </func>
</region>
</regions>
<internal rank="247" log_i="1723712895.689866" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="248" mpi_size="768" stamp_init="1723712830.275870" stamp_final="1723712895.682503" username="apac4" allocationname="unknown" flags="0" pid="980716" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54066e+01" utime="4.62083e+01" stime="8.58640e+00" mtime="2.77579e+01" gflop="0.00000e+00" gbyte="3.74195e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77579e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000033142e1477" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52732e+01" utime="4.61793e+01" stime="8.57342e+00" mtime="2.77579e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77579e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4883e+08" > 6.9962e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4713e+08" > 4.1388e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.5565e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3973e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1362e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3542e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5225e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.0756e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0472e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4535e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6400e+01 </func>
</region>
</regions>
<internal rank="248" log_i="1723712895.682503" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="249" mpi_size="768" stamp_init="1723712830.275937" stamp_final="1723712895.691834" username="apac4" allocationname="unknown" flags="0" pid="980717" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54159e+01" utime="4.88741e+01" stime="7.29871e+00" mtime="2.80647e+01" gflop="0.00000e+00" gbyte="3.75801e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80647e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4861487148914e85589148814de" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52822e+01" utime="4.88371e+01" stime="7.29303e+00" mtime="2.80647e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80647e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4953e+08" > 4.8763e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4866e+08" > 2.6399e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5884e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3796e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2561e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5226e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.0975e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0486e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4506e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7918e+01 </func>
</region>
</regions>
<internal rank="249" log_i="1723712895.691834" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="250" mpi_size="768" stamp_init="1723712830.275917" stamp_final="1723712895.689788" username="apac4" allocationname="unknown" flags="0" pid="980718" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54139e+01" utime="4.66081e+01" stime="8.27243e+00" mtime="2.73753e+01" gflop="0.00000e+00" gbyte="3.77609e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.73753e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000e1142755e114e114ba" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52797e+01" utime="4.65750e+01" stime="8.26290e+00" mtime="2.73753e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.73753e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4873e+08" > 6.7219e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4860e+08" > 4.6995e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4607e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3951e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.3631e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3561e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5223e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.1203e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0465e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4533e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7137e+01 </func>
</region>
</regions>
<internal rank="250" log_i="1723712895.689788" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="251" mpi_size="768" stamp_init="1723712830.275934" stamp_final="1723712895.693838" username="apac4" allocationname="unknown" flags="0" pid="980719" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54179e+01" utime="4.90990e+01" stime="7.34173e+00" mtime="2.84422e+01" gflop="0.00000e+00" gbyte="3.76255e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84422e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000023141e145a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52848e+01" utime="4.90653e+01" stime="7.33354e+00" mtime="2.84422e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84422e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4842e+08" > 4.9577e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4894e+08" > 3.4222e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6186e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3898e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4470e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5218e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.1414e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0460e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3962e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8242e+01 </func>
</region>
</regions>
<internal rank="251" log_i="1723712895.693838" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="252" mpi_size="768" stamp_init="1723712830.275911" stamp_final="1723712895.695496" username="apac4" allocationname="unknown" flags="0" pid="980720" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54196e+01" utime="4.66197e+01" stime="8.17626e+00" mtime="2.73139e+01" gflop="0.00000e+00" gbyte="3.77403e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.73139e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f614a555f614f614b9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52890e+01" utime="4.65855e+01" stime="8.16759e+00" mtime="2.73139e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.73139e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4917e+08" > 7.2684e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4860e+08" > 6.7521e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7389e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3826e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.7898e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3561e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5217e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.1648e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0487e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4539e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6735e+01 </func>
</region>
</regions>
<internal rank="252" log_i="1723712895.695496" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="253" mpi_size="768" stamp_init="1723712830.277048" stamp_final="1723712895.691612" username="apac4" allocationname="unknown" flags="0" pid="980721" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54146e+01" utime="4.88499e+01" stime="7.41790e+00" mtime="2.84502e+01" gflop="0.00000e+00" gbyte="3.75999e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84502e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f814f714a1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52813e+01" utime="4.88220e+01" stime="7.40413e+00" mtime="2.84502e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84502e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4809e+08" > 4.9148e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5101e+08" > 2.9823e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6918e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3833e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0266e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3783e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5207e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.2852e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0493e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4500e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8191e+01 </func>
</region>
</regions>
<internal rank="253" log_i="1723712895.691612" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="254" mpi_size="768" stamp_init="1723712830.275942" stamp_final="1723712895.684515" username="apac4" allocationname="unknown" flags="0" pid="980722" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54086e+01" utime="4.45457e+01" stime="8.97984e+00" mtime="2.71591e+01" gflop="0.00000e+00" gbyte="3.75458e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.71591e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000221583562215221533" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52763e+01" utime="4.45131e+01" stime="8.97036e+00" mtime="2.71591e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.71591e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4923e+08" > 9.1566e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4892e+08" > 7.3795e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5621e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3957e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3561e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7142e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5209e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.2586e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0478e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4548e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6546e+01 </func>
</region>
</regions>
<internal rank="254" log_i="1723712895.684515" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="255" mpi_size="768" stamp_init="1723712830.276783" stamp_final="1723712895.684602" username="apac4" allocationname="unknown" flags="0" pid="980723" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54078e+01" utime="4.89782e+01" stime="7.34980e+00" mtime="2.81832e+01" gflop="0.00000e+00" gbyte="3.78075e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81832e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000fa14ee55fa14fa14ad" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52741e+01" utime="4.89459e+01" stime="7.33999e+00" mtime="2.81832e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81832e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4843e+08" > 4.9652e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4923e+08" > 3.0416e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0809e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3761e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7812e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5194e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.4166e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0473e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4500e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7535e+01 </func>
</region>
</regions>
<internal rank="255" log_i="1723712895.684602" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="256" mpi_size="768" stamp_init="1723712830.276565" stamp_final="1723712895.684865" username="apac4" allocationname="unknown" flags="0" pid="980724" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54083e+01" utime="4.36162e+01" stime="9.42610e+00" mtime="2.71244e+01" gflop="0.00000e+00" gbyte="3.75805e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.71244e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000891488149c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52754e+01" utime="4.35904e+01" stime="9.40911e+00" mtime="2.71244e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.71244e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4868e+08" > 1.4373e+00 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4841e+08" > 9.0390e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1998e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3276e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.1158e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5160e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.7247e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0482e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4564e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6340e+01 </func>
</region>
</regions>
<internal rank="256" log_i="1723712895.684865" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="257" mpi_size="768" stamp_init="1723712830.275907" stamp_final="1723712895.686609" username="apac4" allocationname="unknown" flags="0" pid="980725" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54107e+01" utime="4.89676e+01" stime="7.47325e+00" mtime="2.80944e+01" gflop="0.00000e+00" gbyte="3.76175e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80944e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000077147714d8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52776e+01" utime="4.89361e+01" stime="7.46265e+00" mtime="2.80944e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80944e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4899e+08" > 6.1447e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4845e+08" > 3.0913e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2472e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3761e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2411e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5148e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5162e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.7322e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0470e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4501e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8164e+01 </func>
</region>
</regions>
<internal rank="257" log_i="1723712895.686609" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="258" mpi_size="768" stamp_init="1723712830.276181" stamp_final="1723712895.690925" username="apac4" allocationname="unknown" flags="0" pid="980726" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54147e+01" utime="4.54254e+01" stime="8.69787e+00" mtime="2.75387e+01" gflop="0.00000e+00" gbyte="3.76408e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.75387e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52807e+01" utime="4.53964e+01" stime="8.68464e+00" mtime="2.75387e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.75387e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4749e+08" > 1.0294e+00 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4827e+08" > 7.1617e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0774e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3982e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.8917e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.9870e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5160e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.7103e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0464e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4549e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7299e+01 </func>
</region>
</regions>
<internal rank="258" log_i="1723712895.690925" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="259" mpi_size="768" stamp_init="1723712830.275929" stamp_final="1723712895.683884" username="apac4" allocationname="unknown" flags="0" pid="980727" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54080e+01" utime="4.90719e+01" stime="7.33009e+00" mtime="2.79891e+01" gflop="0.00000e+00" gbyte="3.76850e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79891e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52759e+01" utime="4.90447e+01" stime="7.31487e+00" mtime="2.79891e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79891e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4719e+08" > 5.9802e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4809e+08" > 3.1908e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.6499e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3859e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2602e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5144e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.9338e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0477e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4502e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8658e+01 </func>
</region>
</regions>
<internal rank="259" log_i="1723712895.683884" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="260" mpi_size="768" stamp_init="1723712830.275899" stamp_final="1723712895.691916" username="apac4" allocationname="unknown" flags="0" pid="980728" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54160e+01" utime="4.66729e+01" stime="8.24731e+00" mtime="2.72173e+01" gflop="0.00000e+00" gbyte="3.74607e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.72173e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c6156655c615c61508" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52856e+01" utime="4.66433e+01" stime="8.23425e+00" mtime="2.72173e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.72173e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4809e+08" > 7.7529e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4823e+08" > 4.6451e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1348e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3820e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.9311e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3159e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5152e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.8204e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0486e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4557e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7210e+01 </func>
</region>
</regions>
<internal rank="260" log_i="1723712895.691916" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="261" mpi_size="768" stamp_init="1723712830.275936" stamp_final="1723712895.695147" username="apac4" allocationname="unknown" flags="0" pid="980729" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54192e+01" utime="4.90757e+01" stime="7.29154e+00" mtime="2.79245e+01" gflop="0.00000e+00" gbyte="3.76225e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79245e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d415d41531" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52871e+01" utime="4.90468e+01" stime="7.27814e+00" mtime="2.79245e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79245e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5000e+08" > 6.0504e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4914e+08" > 3.2703e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8369e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3884e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4581e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5150e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.8676e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0452e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4478e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8397e+01 </func>
</region>
</regions>
<internal rank="261" log_i="1723712895.695147" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="262" mpi_size="768" stamp_init="1723712830.275927" stamp_final="1723712895.698836" username="apac4" allocationname="unknown" flags="0" pid="980730" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54229e+01" utime="4.68833e+01" stime="8.16449e+00" mtime="2.72977e+01" gflop="0.00000e+00" gbyte="3.75896e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.72977e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52939e+01" utime="4.68574e+01" stime="8.14828e+00" mtime="2.72977e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.72977e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.3113e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 8.4965e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5038e+08" > 5.3622e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0511e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3779e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.5790e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3788e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5144e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.9250e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0445e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4560e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7302e+01 </func>
</region>
</regions>
<internal rank="262" log_i="1723712895.698836" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="263" mpi_size="768" stamp_init="1723712830.275952" stamp_final="1723712895.693775" username="apac4" allocationname="unknown" flags="0" pid="980731" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u10b</host>
<perf wtime="6.54178e+01" utime="4.88654e+01" stime="7.57040e+00" mtime="2.82285e+01" gflop="0.00000e+00" gbyte="3.74348e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82285e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52831e+01" utime="4.88366e+01" stime="7.55728e+00" mtime="2.82285e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82285e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4965e+08" > 6.1459e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4984e+08" > 3.1597e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5877e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4029e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.6768e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5147e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.8965e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0451e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4487e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7931e+01 </func>
</region>
</regions>
<internal rank="263" log_i="1723712895.693775" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="264" mpi_size="768" stamp_init="1723712830.304156" stamp_final="1723712895.691406" username="apac4" allocationname="unknown" flags="0" pid="718103" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53872e+01" utime="4.27533e+01" stime="1.26743e+01" mtime="2.81909e+01" gflop="0.00000e+00" gbyte="3.86559e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81909e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52606e+01" utime="4.27242e+01" stime="1.26604e+01" mtime="2.81909e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81909e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4899e+08" > 5.8717e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4849e+08" > 6.1816e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1946e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5980e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0647e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7974e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5136e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.9578e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0349e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4476e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7085e+01 </func>
</region>
</regions>
<internal rank="264" log_i="1723712895.691406" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="265" mpi_size="768" stamp_init="1723712830.304204" stamp_final="1723712895.688733" username="apac4" allocationname="unknown" flags="0" pid="718104" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53845e+01" utime="4.84638e+01" stime="7.87936e+00" mtime="2.89628e+01" gflop="0.00000e+00" gbyte="3.76919e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89628e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4cb14cc14cd149855cd14cd1478" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52522e+01" utime="4.84326e+01" stime="7.86747e+00" mtime="2.89628e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89628e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4765e+08" > 4.7018e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4811e+08" > 3.7371e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1195e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5862e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7989e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5139e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.9451e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0352e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3846e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8084e+01 </func>
</region>
</regions>
<internal rank="265" log_i="1723712895.688733" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="266" mpi_size="768" stamp_init="1723712830.304182" stamp_final="1723712895.694498" username="apac4" allocationname="unknown" flags="0" pid="718105" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53903e+01" utime="4.71315e+01" stime="8.16976e+00" mtime="2.79306e+01" gflop="0.00000e+00" gbyte="3.77930e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79306e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4871489148a14a6558a148914e2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52626e+01" utime="4.71007e+01" stime="8.15762e+00" mtime="2.79306e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79306e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4714e+08" > 5.4259e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4681e+08" > 4.2448e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5658e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5865e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.8917e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9311e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5141e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.9222e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0351e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3910e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7529e+01 </func>
</region>
</regions>
<internal rank="266" log_i="1723712895.694498" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="267" mpi_size="768" stamp_init="1723712830.304202" stamp_final="1723712895.688751" username="apac4" allocationname="unknown" flags="0" pid="718106" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53845e+01" utime="4.86908e+01" stime="7.73324e+00" mtime="2.91112e+01" gflop="0.00000e+00" gbyte="3.75103e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91112e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000bb14ba14bc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52549e+01" utime="4.86588e+01" stime="7.72336e+00" mtime="2.91112e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91112e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.2159e-05 </func>
<func name="MPI_Isend" count="127164" bytes="8.4946e+08" > 4.6184e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4862e+08" > 3.6833e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7163e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5931e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9159e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5138e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.9512e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0327e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4311e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8639e+01 </func>
</region>
</regions>
<internal rank="267" log_i="1723712895.688751" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="268" mpi_size="768" stamp_init="1723712830.304807" stamp_final="1723712895.680397" username="apac4" allocationname="unknown" flags="0" pid="718107" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53756e+01" utime="4.57767e+01" stime="8.77405e+00" mtime="2.79241e+01" gflop="0.00000e+00" gbyte="3.76194e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79241e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000da15d91551" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52500e+01" utime="4.57405e+01" stime="8.76882e+00" mtime="2.79241e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79241e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4847e+08" > 6.9231e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4998e+08" > 5.6776e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4608e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5961e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1811e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8029e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5142e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.9197e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0375e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4447e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7453e+01 </func>
</region>
</regions>
<internal rank="268" log_i="1723712895.680397" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="269" mpi_size="768" stamp_init="1723712830.304242" stamp_final="1723712895.679523" username="apac4" allocationname="unknown" flags="0" pid="718108" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53753e+01" utime="4.87127e+01" stime="7.72462e+00" mtime="2.91987e+01" gflop="0.00000e+00" gbyte="3.75786e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91987e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f014f114f3147a56f314f2147f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52470e+01" utime="4.86787e+01" stime="7.71610e+00" mtime="2.91987e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91987e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5026e+08" > 4.5855e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4987e+08" > 3.5661e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8159e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6010e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8031e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5135e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.9943e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0353e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4304e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8624e+01 </func>
</region>
</regions>
<internal rank="269" log_i="1723712895.679523" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="270" mpi_size="768" stamp_init="1723712830.304937" stamp_final="1723712895.685006" username="apac4" allocationname="unknown" flags="0" pid="718109" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53801e+01" utime="4.72355e+01" stime="7.98122e+00" mtime="2.78834e+01" gflop="0.00000e+00" gbyte="3.75427e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78834e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52537e+01" utime="4.72049e+01" stime="7.97002e+00" mtime="2.78834e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78834e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.6689e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5047e+08" > 5.7467e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5049e+08" > 6.3966e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3855e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5970e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.9393e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.6189e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5133e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.0365e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0378e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4436e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7596e+01 </func>
</region>
</regions>
<internal rank="270" log_i="1723712895.685006" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="271" mpi_size="768" stamp_init="1723712830.304243" stamp_final="1723712895.694802" username="apac4" allocationname="unknown" flags="0" pid="718110" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53906e+01" utime="4.86987e+01" stime="7.75920e+00" mtime="2.88695e+01" gflop="0.00000e+00" gbyte="3.76556e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88695e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c514c714c814d056c814c814e6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52632e+01" utime="4.86670e+01" stime="7.74890e+00" mtime="2.88695e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88695e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4948e+08" > 4.4343e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4949e+08" > 3.4747e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8018e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6129e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2411e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9209e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5133e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.0474e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0372e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4306e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8316e+01 </func>
</region>
</regions>
<internal rank="271" log_i="1723712895.694802" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="272" mpi_size="768" stamp_init="1723712830.304176" stamp_final="1723712895.685837" username="apac4" allocationname="unknown" flags="0" pid="718111" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53817e+01" utime="4.63819e+01" stime="8.56808e+00" mtime="2.83233e+01" gflop="0.00000e+00" gbyte="3.76755e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83233e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf429142b142c1420552c142b14fe" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52531e+01" utime="4.63472e+01" stime="8.56110e+00" mtime="2.83233e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83233e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5101e+08" > 4.9008e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4949e+08" > 4.3635e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9721e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6030e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9612e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6958e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5104e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.3220e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0360e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4409e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7550e+01 </func>
</region>
</regions>
<internal rank="272" log_i="1723712895.685837" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="273" mpi_size="768" stamp_init="1723712830.304226" stamp_final="1723712895.686803" username="apac4" allocationname="unknown" flags="0" pid="718112" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53826e+01" utime="4.90813e+01" stime="7.36693e+00" mtime="2.88537e+01" gflop="0.00000e+00" gbyte="3.77785e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88537e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52545e+01" utime="4.90477e+01" stime="7.35852e+00" mtime="2.88537e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88537e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4887e+08" > 3.6084e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5044e+08" > 2.4815e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3364e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6114e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.2850e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5119e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.1876e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0335e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4274e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8854e+01 </func>
</region>
</regions>
<internal rank="273" log_i="1723712895.686803" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="274" mpi_size="768" stamp_init="1723712830.304159" stamp_final="1723712895.680020" username="apac4" allocationname="unknown" flags="0" pid="718113" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53759e+01" utime="4.70246e+01" stime="8.09613e+00" mtime="2.84114e+01" gflop="0.00000e+00" gbyte="3.76492e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84114e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004214411459" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52494e+01" utime="4.69935e+01" stime="8.08527e+00" mtime="2.84114e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84114e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4974e+08" > 5.2339e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4849e+08" > 3.9831e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5764e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5988e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5048e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7690e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5109e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.2398e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0326e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3870e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8010e+01 </func>
</region>
</regions>
<internal rank="274" log_i="1723712895.680020" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="275" mpi_size="768" stamp_init="1723712830.304203" stamp_final="1723712895.688251" username="apac4" allocationname="unknown" flags="0" pid="718114" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53840e+01" utime="4.88660e+01" stime="7.36949e+00" mtime="2.93488e+01" gflop="0.00000e+00" gbyte="3.77491e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.93488e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a114a314a4149a55a414a31495" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52581e+01" utime="4.88324e+01" stime="7.36122e+00" mtime="2.93488e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.93488e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4894e+08" > 3.5923e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4866e+08" > 3.0524e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6038e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5968e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.8642e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5113e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.2477e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0330e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4257e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9091e+01 </func>
</region>
</regions>
<internal rank="275" log_i="1723712895.688251" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="276" mpi_size="768" stamp_init="1723712830.304168" stamp_final="1723712895.680574" username="apac4" allocationname="unknown" flags="0" pid="718115" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53764e+01" utime="4.70626e+01" stime="8.22576e+00" mtime="2.83089e+01" gflop="0.00000e+00" gbyte="3.77769e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83089e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf489148b148c1465558c148c14be" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52489e+01" utime="4.70341e+01" stime="8.21203e+00" mtime="2.83089e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83089e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4852e+08" > 4.8320e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4865e+08" > 3.7044e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7329e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5980e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9503e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7607e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5092e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.4180e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0374e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4396e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7794e+01 </func>
</region>
</regions>
<internal rank="276" log_i="1723712895.680574" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="277" mpi_size="768" stamp_init="1723712830.305265" stamp_final="1723712895.692457" username="apac4" allocationname="unknown" flags="0" pid="718116" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53872e+01" utime="4.87312e+01" stime="7.72893e+00" mtime="2.89177e+01" gflop="0.00000e+00" gbyte="3.76579e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89177e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4531454145614f65556145514c1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52606e+01" utime="4.87023e+01" stime="7.71601e+00" mtime="2.89177e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89177e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4915e+08" > 3.6037e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4934e+08" > 2.9407e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8161e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6073e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.5448e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5095e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.4308e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0373e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4273e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8439e+01 </func>
</region>
</regions>
<internal rank="277" log_i="1723712895.692457" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="278" mpi_size="768" stamp_init="1723712830.304173" stamp_final="1723712895.690294" username="apac4" allocationname="unknown" flags="0" pid="718117" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53861e+01" utime="4.77566e+01" stime="7.81882e+00" mtime="2.84986e+01" gflop="0.00000e+00" gbyte="3.76324e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84986e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006c156b150c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52590e+01" utime="4.77274e+01" stime="7.80642e+00" mtime="2.84986e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84986e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4837e+08" > 4.4346e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4928e+08" > 3.6793e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0390e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5867e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8133e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7929e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3096e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.8414e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0335e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4383e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7728e+01 </func>
</region>
</regions>
<internal rank="278" log_i="1723712895.690294" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="279" mpi_size="768" stamp_init="1723712830.304227" stamp_final="1723712895.678118" username="apac4" allocationname="unknown" flags="0" pid="718118" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53739e+01" utime="4.88518e+01" stime="7.61065e+00" mtime="2.88279e+01" gflop="0.00000e+00" gbyte="3.76869e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88279e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c614c814c914c755c914c914e5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52470e+01" utime="4.88166e+01" stime="7.60364e+00" mtime="2.88279e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88279e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4991e+08" > 3.6332e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4968e+08" > 2.9290e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3826e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5848e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1683e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.7948e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5174e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 7.6464e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0380e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4263e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8801e+01 </func>
</region>
</regions>
<internal rank="279" log_i="1723712895.678118" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="280" mpi_size="768" stamp_init="1723712830.304659" stamp_final="1723712895.686755" username="apac4" allocationname="unknown" flags="0" pid="718119" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53821e+01" utime="4.72296e+01" stime="8.14176e+00" mtime="2.83981e+01" gflop="0.00000e+00" gbyte="3.76507e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83981e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52552e+01" utime="4.71950e+01" stime="8.13436e+00" mtime="2.83981e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83981e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4780e+08" > 6.6846e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4748e+08" > 4.0484e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6037e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5844e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.0137e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9421e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5095e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.4365e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0342e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4376e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7829e+01 </func>
</region>
</regions>
<internal rank="280" log_i="1723712895.686755" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="281" mpi_size="768" stamp_init="1723712830.304218" stamp_final="1723712895.679303" username="apac4" allocationname="unknown" flags="0" pid="718120" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53751e+01" utime="4.89680e+01" stime="7.41889e+00" mtime="2.85212e+01" gflop="0.00000e+00" gbyte="3.76606e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85212e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f715f815f915ff56f915f91523" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52484e+01" utime="4.89377e+01" stime="7.40685e+00" mtime="2.85212e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85212e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4784e+08" > 4.8792e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4912e+08" > 2.8607e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3975e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5894e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1921e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.2132e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5092e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.4678e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0356e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4267e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8350e+01 </func>
</region>
</regions>
<internal rank="281" log_i="1723712895.679303" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="282" mpi_size="768" stamp_init="1723712830.304191" stamp_final="1723712895.680338" username="apac4" allocationname="unknown" flags="0" pid="718121" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53761e+01" utime="4.46113e+01" stime="9.08898e+00" mtime="2.77185e+01" gflop="0.00000e+00" gbyte="3.77243e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77185e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4481449144a14d1554a144a1462" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52489e+01" utime="4.45802e+01" stime="9.07797e+00" mtime="2.77185e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77185e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4962e+08" > 9.3070e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4875e+08" > 7.4745e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3345e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5933e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.5987e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9330e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5087e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.4950e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0346e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4370e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7120e+01 </func>
</region>
</regions>
<internal rank="282" log_i="1723712895.680338" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="283" mpi_size="768" stamp_init="1723712830.304226" stamp_final="1723712895.683598" username="apac4" allocationname="unknown" flags="0" pid="718122" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53794e+01" utime="4.88281e+01" stime="7.59072e+00" mtime="2.87679e+01" gflop="0.00000e+00" gbyte="3.74680e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87679e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000068146814fa" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52519e+01" utime="4.87970e+01" stime="7.57964e+00" mtime="2.87679e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87679e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4853e+08" > 4.8269e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4984e+08" > 3.3798e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5442e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5911e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9221e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5086e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.5271e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0353e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4238e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8452e+01 </func>
</region>
</regions>
<internal rank="283" log_i="1723712895.683598" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="284" mpi_size="768" stamp_init="1723712830.305146" stamp_final="1723712895.687884" username="apac4" allocationname="unknown" flags="0" pid="718123" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53827e+01" utime="4.41505e+01" stime="9.16282e+00" mtime="2.73309e+01" gflop="0.00000e+00" gbyte="3.77808e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.73309e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d715d915da151a55da15da1510" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52570e+01" utime="4.41187e+01" stime="9.15222e+00" mtime="2.73309e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.73309e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4900e+08" > 8.6907e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4848e+08" > 5.8447e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1445e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6008e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3729e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2568e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5074e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.6459e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0342e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4361e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6990e+01 </func>
</region>
</regions>
<internal rank="284" log_i="1723712895.687884" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="285" mpi_size="768" stamp_init="1723712830.304237" stamp_final="1723712895.680968" username="apac4" allocationname="unknown" flags="0" pid="718124" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53767e+01" utime="4.89684e+01" stime="7.46656e+00" mtime="2.87134e+01" gflop="0.00000e+00" gbyte="3.77865e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87134e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d3142755d314d314aa" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52476e+01" utime="4.89341e+01" stime="7.45862e+00" mtime="2.87134e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87134e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4818e+08" > 4.8807e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5029e+08" > 3.4035e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7481e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5867e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2470e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5080e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.5852e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0384e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4252e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8192e+01 </func>
</region>
</regions>
<internal rank="285" log_i="1723712895.680968" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="286" mpi_size="768" stamp_init="1723712830.304167" stamp_final="1723712895.685857" username="apac4" allocationname="unknown" flags="0" pid="718125" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53817e+01" utime="4.72910e+01" stime="8.26619e+00" mtime="2.81809e+01" gflop="0.00000e+00" gbyte="3.74416e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81809e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52547e+01" utime="4.72610e+01" stime="8.25438e+00" mtime="2.81809e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81809e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5006e+08" > 5.3519e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4950e+08" > 3.9821e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7947e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5920e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.6560e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.5145e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5072e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.6672e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0343e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4341e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7554e+01 </func>
</region>
</regions>
<internal rank="286" log_i="1723712895.685857" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="287" mpi_size="768" stamp_init="1723712830.304223" stamp_final="1723712895.685138" username="apac4" allocationname="unknown" flags="0" pid="718126" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u02a</host>
<perf wtime="6.53809e+01" utime="4.89712e+01" stime="7.43655e+00" mtime="2.85143e+01" gflop="0.00000e+00" gbyte="3.76865e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85143e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e814e814f5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52516e+01" utime="4.89372e+01" stime="7.42817e+00" mtime="2.85143e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85143e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4972e+08" > 4.9653e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4825e+08" > 2.8261e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2819e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5835e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8358e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0681e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5070e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.6604e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0370e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4256e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8458e+01 </func>
</region>
</regions>
<internal rank="287" log_i="1723712895.685138" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="288" mpi_size="768" stamp_init="1723712830.256047" stamp_final="1723712895.678977" username="apac4" allocationname="unknown" flags="0" pid="1947612" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54229e+01" utime="4.15418e+01" stime="1.21705e+01" mtime="2.80773e+01" gflop="0.00000e+00" gbyte="3.85571e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80773e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4151416141714fc5517141714c6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52997e+01" utime="4.15140e+01" stime="1.21568e+01" mtime="2.80773e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80773e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4899e+08" > 5.9919e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4815e+08" > 3.8529e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6025e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3758e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3190e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.0915e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5074e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.6549e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0436e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4236e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7792e+01 </func>
</region>
</regions>
<internal rank="288" log_i="1723712895.678977" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="289" mpi_size="768" stamp_init="1723712830.254852" stamp_final="1723712895.686659" username="apac4" allocationname="unknown" flags="0" pid="1947613" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54318e+01" utime="4.83236e+01" stime="6.83332e+00" mtime="2.81931e+01" gflop="0.00000e+00" gbyte="3.77949e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81931e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000026142614f8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53014e+01" utime="4.82954e+01" stime="6.81837e+00" mtime="2.81931e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81931e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4846e+08" > 4.7500e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4885e+08" > 2.8739e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9219e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3819e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1948e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.4178e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5086e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.5155e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0404e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4091e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8719e+01 </func>
</region>
</regions>
<internal rank="289" log_i="1723712895.686659" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="290" mpi_size="768" stamp_init="1723712830.254595" stamp_final="1723712895.680496" username="apac4" allocationname="unknown" flags="0" pid="1947614" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54259e+01" utime="4.68256e+01" stime="8.08075e+00" mtime="2.76094e+01" gflop="0.00000e+00" gbyte="3.77785e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.76094e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf47a147c147d146d567d147c14ef" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53023e+01" utime="4.67949e+01" stime="8.06921e+00" mtime="2.76094e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.76094e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4893e+08" > 6.2711e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4887e+08" > 4.0358e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9598e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3501e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.8410e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2603e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5083e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.5794e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0422e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4211e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7957e+01 </func>
</region>
</regions>
<internal rank="290" log_i="1723712895.680496" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="291" mpi_size="768" stamp_init="1723712830.254714" stamp_final="1723712895.685475" username="apac4" allocationname="unknown" flags="0" pid="1947615" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54308e+01" utime="4.90061e+01" stime="7.15916e+00" mtime="2.83864e+01" gflop="0.00000e+00" gbyte="3.77712e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83864e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53055e+01" utime="4.89779e+01" stime="7.14530e+00" mtime="2.83864e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83864e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 4.5638e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4851e+08" > 2.8906e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8725e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3878e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2360e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5082e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.5900e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0403e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4090e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8975e+01 </func>
</region>
</regions>
<internal rank="291" log_i="1723712895.685475" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="292" mpi_size="768" stamp_init="1723712830.254737" stamp_final="1723712895.695166" username="apac4" allocationname="unknown" flags="0" pid="1947616" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54404e+01" utime="4.59846e+01" stime="7.75119e+00" mtime="2.74527e+01" gflop="0.00000e+00" gbyte="3.77846e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.74527e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c514c714c8144155c814c714db" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53163e+01" utime="4.59485e+01" stime="7.74509e+00" mtime="2.74527e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.74527e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4823e+08" > 6.9365e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4906e+08" > 3.2709e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4925e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3711e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5378e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2250e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5067e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.7448e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0410e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4211e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7197e+01 </func>
</region>
</regions>
<internal rank="292" log_i="1723712895.695166" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="293" mpi_size="768" stamp_init="1723712830.256160" stamp_final="1723712895.681877" username="apac4" allocationname="unknown" flags="0" pid="1947617" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54257e+01" utime="4.79389e+01" stime="7.15250e+00" mtime="2.82742e+01" gflop="0.00000e+00" gbyte="3.77598e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82742e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000097149714cf" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53024e+01" utime="4.79119e+01" stime="7.13741e+00" mtime="2.82742e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82742e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4943e+08" > 4.5472e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4945e+08" > 3.2250e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4879e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3894e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1948e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4163e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5075e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.6690e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0416e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4083e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8248e+01 </func>
</region>
</regions>
<internal rank="293" log_i="1723712895.681877" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="294" mpi_size="768" stamp_init="1723712830.254570" stamp_final="1723712895.678411" username="apac4" allocationname="unknown" flags="0" pid="1947618" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54238e+01" utime="4.71355e+01" stime="7.98193e+00" mtime="2.77038e+01" gflop="0.00000e+00" gbyte="3.76053e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77038e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000131513150e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52989e+01" utime="4.71027e+01" stime="7.97309e+00" mtime="2.77038e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77038e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4880e+08" > 5.6393e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5008e+08" > 4.0181e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5804e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3818e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3852e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.1671e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5068e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.7136e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0435e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4222e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7472e+01 </func>
</region>
</regions>
<internal rank="294" log_i="1723712895.678411" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="295" mpi_size="768" stamp_init="1723712830.254571" stamp_final="1723712895.691035" username="apac4" allocationname="unknown" flags="0" pid="1947619" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54365e+01" utime="4.88569e+01" stime="7.10248e+00" mtime="2.80949e+01" gflop="0.00000e+00" gbyte="3.77842e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80949e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf419141a141b1493561b141b14ef" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53119e+01" utime="4.88225e+01" stime="7.09430e+00" mtime="2.80949e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80949e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4976e+08" > 4.5832e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4767e+08" > 2.4453e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9507e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3719e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.4923e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5062e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.7633e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0420e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4057e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8624e+01 </func>
</region>
</regions>
<internal rank="295" log_i="1723712895.691035" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="296" mpi_size="768" stamp_init="1723712830.254574" stamp_final="1723712895.677089" username="apac4" allocationname="unknown" flags="0" pid="1947620" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54225e+01" utime="4.72102e+01" stime="8.04541e+00" mtime="2.83382e+01" gflop="0.00000e+00" gbyte="3.77171e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83382e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53000e+01" utime="4.71781e+01" stime="8.03530e+00" mtime="2.83382e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83382e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5098e+08" > 4.7386e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4819e+08" > 3.8211e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8875e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3643e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7305e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.3013e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5064e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.7434e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0447e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4217e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7891e+01 </func>
</region>
</regions>
<internal rank="296" log_i="1723712895.677089" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="297" mpi_size="768" stamp_init="1723712830.254574" stamp_final="1723712895.695177" username="apac4" allocationname="unknown" flags="0" pid="1947621" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54406e+01" utime="4.86847e+01" stime="7.52467e+00" mtime="2.86323e+01" gflop="0.00000e+00" gbyte="3.75656e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86323e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53139e+01" utime="4.86490e+01" stime="7.51833e+00" mtime="2.86323e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86323e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4871e+08" > 3.6375e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4940e+08" > 2.9969e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5265e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3903e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.3826e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5059e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.7895e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0443e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3486e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8657e+01 </func>
</region>
</regions>
<internal rank="297" log_i="1723712895.695177" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="298" mpi_size="768" stamp_init="1723712830.255067" stamp_final="1723712895.687064" username="apac4" allocationname="unknown" flags="0" pid="1947622" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54320e+01" utime="4.53818e+01" stime="8.64693e+00" mtime="2.81815e+01" gflop="0.00000e+00" gbyte="3.76972e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81815e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000471547554715471513" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53088e+01" utime="4.53498e+01" stime="8.63718e+00" mtime="2.81815e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81815e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4892e+08" > 6.9993e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4963e+08" > 5.5979e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8509e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3803e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.1049e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.2870e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5060e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.7983e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0429e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4200e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7528e+01 </func>
</region>
</regions>
<internal rank="298" log_i="1723712895.687064" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="299" mpi_size="768" stamp_init="1723712830.254556" stamp_final="1723712895.684579" username="apac4" allocationname="unknown" flags="0" pid="1947623" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54300e+01" utime="4.91079e+01" stime="6.79069e+00" mtime="2.83067e+01" gflop="0.00000e+00" gbyte="3.76602e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83067e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48b148c148e14fa568e148d148e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53050e+01" utime="4.90736e+01" stime="6.78314e+00" mtime="2.83067e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83067e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4956e+08" > 3.6183e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4838e+08" > 3.0619e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8623e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3999e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.5950e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5061e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.8138e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0434e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4036e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8992e+01 </func>
</region>
</regions>
<internal rank="299" log_i="1723712895.684579" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="300" mpi_size="768" stamp_init="1723712830.254664" stamp_final="1723712895.688493" username="apac4" allocationname="unknown" flags="0" pid="1947624" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54338e+01" utime="4.46876e+01" stime="8.34839e+00" mtime="2.80960e+01" gflop="0.00000e+00" gbyte="3.76122e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80960e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53098e+01" utime="4.46525e+01" stime="8.34171e+00" mtime="2.80960e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80960e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5055e+08" > 5.6574e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4886e+08" > 4.6131e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0626e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3817e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2058e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2372e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5051e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.8567e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0441e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4187e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7372e+01 </func>
</region>
</regions>
<internal rank="300" log_i="1723712895.688493" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="301" mpi_size="768" stamp_init="1723712830.255158" stamp_final="1723712895.683362" username="apac4" allocationname="unknown" flags="0" pid="1947625" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54282e+01" utime="4.89032e+01" stime="7.28194e+00" mtime="2.85872e+01" gflop="0.00000e+00" gbyte="3.76911e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85872e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000027142714f3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53059e+01" utime="4.88757e+01" stime="7.26741e+00" mtime="2.85872e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85872e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4843e+08" > 3.6974e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4972e+08" > 2.4443e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5228e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3757e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.0137e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5053e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.8551e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0401e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3505e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8628e+01 </func>
</region>
</regions>
<internal rank="301" log_i="1723712895.683362" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="302" mpi_size="768" stamp_init="1723712830.254577" stamp_final="1723712895.692214" username="apac4" allocationname="unknown" flags="0" pid="1947626" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54376e+01" utime="4.49726e+01" stime="8.20750e+00" mtime="2.82191e+01" gflop="0.00000e+00" gbyte="3.75378e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82191e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf460156115621534566215621542" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53136e+01" utime="4.49427e+01" stime="8.19554e+00" mtime="2.82191e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82191e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4913e+08" > 6.0230e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4961e+08" > 5.4862e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9547e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3855e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0591e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.3197e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5050e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.8995e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0442e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4178e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7553e+01 </func>
</region>
</regions>
<internal rank="302" log_i="1723712895.692214" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="303" mpi_size="768" stamp_init="1723712830.254596" stamp_final="1723712895.691024" username="apac4" allocationname="unknown" flags="0" pid="1947627" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54364e+01" utime="4.79822e+01" stime="7.15334e+00" mtime="2.85043e+01" gflop="0.00000e+00" gbyte="3.75988e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85043e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf410151115131513561315121553" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53110e+01" utime="4.79558e+01" stime="7.13760e+00" mtime="2.85043e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85043e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4928e+08" > 3.5062e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5019e+08" > 3.1585e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4518e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3875e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.3392e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5043e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.9872e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0428e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4029e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8617e+01 </func>
</region>
</regions>
<internal rank="303" log_i="1723712895.691024" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="304" mpi_size="768" stamp_init="1723712830.254843" stamp_final="1723712895.689417" username="apac4" allocationname="unknown" flags="0" pid="1947628" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54346e+01" utime="4.55285e+01" stime="8.69862e+00" mtime="2.75570e+01" gflop="0.00000e+00" gbyte="3.75549e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.75570e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53122e+01" utime="4.54970e+01" stime="8.68823e+00" mtime="2.75570e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.75570e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4879e+08" > 7.0525e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5014e+08" > 8.4742e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8117e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3758e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.2221e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2649e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5044e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.9853e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0426e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4144e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6908e+01 </func>
</region>
</regions>
<internal rank="304" log_i="1723712895.689417" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="305" mpi_size="768" stamp_init="1723712830.254585" stamp_final="1723712895.691803" username="apac4" allocationname="unknown" flags="0" pid="1947629" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54372e+01" utime="4.88621e+01" stime="7.33683e+00" mtime="2.79790e+01" gflop="0.00000e+00" gbyte="3.77113e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79790e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002c15c9552c152c1514" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53122e+01" utime="4.88335e+01" stime="7.32317e+00" mtime="2.79790e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79790e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5011e+08" > 4.8617e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4882e+08" > 3.0322e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0684e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3756e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.6420e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5041e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 8.9871e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0414e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4015e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8352e+01 </func>
</region>
</regions>
<internal rank="305" log_i="1723712895.691803" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="306" mpi_size="768" stamp_init="1723712830.255933" stamp_final="1723712895.692326" username="apac4" allocationname="unknown" flags="0" pid="1947630" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54364e+01" utime="4.60248e+01" stime="7.93262e+00" mtime="2.81431e+01" gflop="0.00000e+00" gbyte="3.77651e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81431e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c4142055c414c414cb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53126e+01" utime="4.59987e+01" stime="7.91658e+00" mtime="2.81431e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81431e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4911e+08" > 5.6782e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4784e+08" > 3.6098e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9080e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3717e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0276e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2632e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5030e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.1292e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0410e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4121e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7593e+01 </func>
</region>
</regions>
<internal rank="306" log_i="1723712895.692326" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="307" mpi_size="768" stamp_init="1723712830.256138" stamp_final="1723712895.685910" username="apac4" allocationname="unknown" flags="0" pid="1947631" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54298e+01" utime="4.76626e+01" stime="7.44578e+00" mtime="2.87717e+01" gflop="0.00000e+00" gbyte="3.76434e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87717e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53055e+01" utime="4.76318e+01" stime="7.43541e+00" mtime="2.87717e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87717e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4700e+08" > 4.9105e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4851e+08" > 2.9196e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4743e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3616e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1074e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5039e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.0449e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0440e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4003e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8746e+01 </func>
</region>
</regions>
<internal rank="307" log_i="1723712895.685910" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="308" mpi_size="768" stamp_init="1723712830.254638" stamp_final="1723712895.678255" username="apac4" allocationname="unknown" flags="0" pid="1947632" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54236e+01" utime="4.60289e+01" stime="7.79124e+00" mtime="2.78087e+01" gflop="0.00000e+00" gbyte="3.74546e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78087e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53000e+01" utime="4.59920e+01" stime="7.78580e+00" mtime="2.78087e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78087e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4885e+08" > 6.2079e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4831e+08" > 4.6597e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8068e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3710e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8253e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.6172e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5032e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.0496e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0399e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4098e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7297e+01 </func>
</region>
</regions>
<internal rank="308" log_i="1723712895.678255" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="309" mpi_size="768" stamp_init="1723712830.254818" stamp_final="1723712895.688186" username="apac4" allocationname="unknown" flags="0" pid="1947633" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54334e+01" utime="4.82996e+01" stime="6.85769e+00" mtime="2.80206e+01" gflop="0.00000e+00" gbyte="3.76041e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80206e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53100e+01" utime="4.82735e+01" stime="6.84138e+00" mtime="2.80206e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80206e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4916e+08" > 4.7981e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4682e+08" > 2.8744e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2168e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3935e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9087e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5927e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5021e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.1943e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0413e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3991e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8240e+01 </func>
</region>
</regions>
<internal rank="309" log_i="1723712895.688186" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="310" mpi_size="768" stamp_init="1723712830.255939" stamp_final="1723712895.686074" username="apac4" allocationname="unknown" flags="0" pid="1947634" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54301e+01" utime="4.66456e+01" stime="8.39356e+00" mtime="2.81302e+01" gflop="0.00000e+00" gbyte="3.77258e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81302e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000da147955da14da14d1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53074e+01" utime="4.66173e+01" stime="8.37980e+00" mtime="2.81302e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81302e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4839e+08" > 6.0353e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5015e+08" > 4.5307e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9190e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3781e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5763e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.6332e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5029e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.1053e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0449e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.4092e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7518e+01 </func>
</region>
</regions>
<internal rank="310" log_i="1723712895.686074" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="311" mpi_size="768" stamp_init="1723712830.254589" stamp_final="1723712895.685796" username="apac4" allocationname="unknown" flags="0" pid="1947635" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u16a</host>
<perf wtime="6.54312e+01" utime="4.80857e+01" stime="7.00161e+00" mtime="2.81896e+01" gflop="0.00000e+00" gbyte="3.77644e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81896e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f314e755f314f214c9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53067e+01" utime="4.80506e+01" stime="6.99464e+00" mtime="2.81896e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81896e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4948e+08" > 4.9515e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4894e+08" > 3.1954e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3463e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3819e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0279e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0436e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5030e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.1319e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0420e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 5.3986e-02 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8265e+01 </func>
</region>
</regions>
<internal rank="311" log_i="1723712895.685796" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="312" mpi_size="768" stamp_init="1723712830.426176" stamp_final="1723712895.680107" username="apac4" allocationname="unknown" flags="0" pid="919211" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52539e+01" utime="4.20634e+01" stime="1.29317e+01" mtime="2.84540e+01" gflop="0.00000e+00" gbyte="3.85841e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84540e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b2148155b214b21465" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51288e+01" utime="4.20287e+01" stime="1.29244e+01" mtime="2.84540e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84540e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4617e+08" > 7.0772e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4799e+08" > 4.8266e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.6793e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8649e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.9292e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4200e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5022e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.2320e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0410e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1254e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6428e+01 </func>
</region>
</regions>
<internal rank="312" log_i="1723712895.680107" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="313" mpi_size="768" stamp_init="1723712830.424669" stamp_final="1723712895.685306" username="apac4" allocationname="unknown" flags="0" pid="919212" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52606e+01" utime="4.87570e+01" stime="7.70744e+00" mtime="2.88658e+01" gflop="0.00000e+00" gbyte="3.76560e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88658e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4911592159315f755931593154d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51284e+01" utime="4.87262e+01" stime="7.69513e+00" mtime="2.88658e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88658e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4718e+08" > 5.4439e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4818e+08" > 2.8170e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7620e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8695e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4319e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.3709e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5033e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.0649e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 7.8650e-03 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1261e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7940e+01 </func>
</region>
</regions>
<internal rank="313" log_i="1723712895.685306" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="314" mpi_size="768" stamp_init="1723712830.425780" stamp_final="1723712895.690404" username="apac4" allocationname="unknown" flags="0" pid="919213" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52646e+01" utime="4.67373e+01" stime="8.66545e+00" mtime="2.84947e+01" gflop="0.00000e+00" gbyte="3.77506e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84947e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51384e+01" utime="4.67085e+01" stime="8.65201e+00" mtime="2.84947e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84947e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4814e+08" > 7.2026e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4681e+08" > 4.5367e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0364e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8598e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.5061e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3329e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5021e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.1841e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0432e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1263e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7114e+01 </func>
</region>
</regions>
<internal rank="314" log_i="1723712895.690404" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="315" mpi_size="768" stamp_init="1723712830.424399" stamp_final="1723712895.689589" username="apac4" allocationname="unknown" flags="0" pid="919214" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52652e+01" utime="4.90479e+01" stime="7.59695e+00" mtime="2.87566e+01" gflop="0.00000e+00" gbyte="3.76678e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87566e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4371439143a14a1563a143a1475" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51361e+01" utime="4.90125e+01" stime="7.59049e+00" mtime="2.87566e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87566e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4838e+08" > 5.2953e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4805e+08" > 2.8546e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2983e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8804e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.8119e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5029e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.1462e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0435e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1255e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8300e+01 </func>
</region>
</regions>
<internal rank="315" log_i="1723712895.689589" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="316" mpi_size="768" stamp_init="1723712830.425382" stamp_final="1723712895.679911" username="apac4" allocationname="unknown" flags="0" pid="919215" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52545e+01" utime="4.54820e+01" stime="8.49320e+00" mtime="2.87856e+01" gflop="0.00000e+00" gbyte="3.76431e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87856e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4171518151915fc551915191555" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51259e+01" utime="4.54525e+01" stime="8.48073e+00" mtime="2.87856e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87856e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4820e+08" > 7.7036e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4936e+08" > 4.7052e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.2087e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8259e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6320e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4563e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5022e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.1651e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0414e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1255e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7214e+01 </func>
</region>
</regions>
<internal rank="316" log_i="1723712895.679911" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="317" mpi_size="768" stamp_init="1723712830.424407" stamp_final="1723712895.695934" username="apac4" allocationname="unknown" flags="0" pid="919216" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52715e+01" utime="4.88908e+01" stime="7.87319e+00" mtime="2.93234e+01" gflop="0.00000e+00" gbyte="3.77884e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.93234e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000058146f5558145814c5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51421e+01" utime="4.88650e+01" stime="7.85647e+00" mtime="2.93234e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.93234e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4868e+08" > 5.4221e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4924e+08" > 2.4717e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9656e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8679e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.7486e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5023e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.2082e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0436e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1260e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8204e+01 </func>
</region>
</regions>
<internal rank="317" log_i="1723712895.695934" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="318" mpi_size="768" stamp_init="1723712830.424395" stamp_final="1723712895.680540" username="apac4" allocationname="unknown" flags="0" pid="919217" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52561e+01" utime="4.60988e+01" stime="8.22829e+00" mtime="2.82083e+01" gflop="0.00000e+00" gbyte="3.74359e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82083e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49c159d159e1582559e159e151c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51307e+01" utime="4.60656e+01" stime="8.21866e+00" mtime="2.82083e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82083e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4997e+08" > 7.2907e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4822e+08" > 4.2668e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9298e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8470e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.1485e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.7529e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5021e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.2353e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0433e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1256e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6941e+01 </func>
</region>
</regions>
<internal rank="318" log_i="1723712895.680540" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="319" mpi_size="768" stamp_init="1723712830.424977" stamp_final="1723712895.688632" username="apac4" allocationname="unknown" flags="0" pid="919218" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52637e+01" utime="4.81355e+01" stime="7.42157e+00" mtime="2.84774e+01" gflop="0.00000e+00" gbyte="3.77499e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84774e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf469156b156c15be556c156b1521" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51351e+01" utime="4.81016e+01" stime="7.41291e+00" mtime="2.84774e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84774e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4896e+08" > 5.3910e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4953e+08" > 3.0008e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8105e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8672e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1696e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8992e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.5016e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.2825e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 8.4162e-05 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1261e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7522e+01 </func>
</region>
</regions>
<internal rank="319" log_i="1723712895.688632" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="320" mpi_size="768" stamp_init="1723712830.424612" stamp_final="1723712895.680579" username="apac4" allocationname="unknown" flags="0" pid="919219" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52560e+01" utime="4.49350e+01" stime="8.54395e+00" mtime="2.83724e+01" gflop="0.00000e+00" gbyte="3.76301e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83724e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000092141d55921492146d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51311e+01" utime="4.49082e+01" stime="8.52904e+00" mtime="2.83724e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83724e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.7166e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4786e+08" > 7.3108e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4725e+08" > 5.0115e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8811e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3596e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.0055e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.2614e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4979e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.9626e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0411e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1258e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7127e+01 </func>
</region>
</regions>
<internal rank="320" log_i="1723712895.680579" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="321" mpi_size="768" stamp_init="1723712830.424684" stamp_final="1723712895.676594" username="apac4" allocationname="unknown" flags="0" pid="919220" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52519e+01" utime="4.84027e+01" stime="7.22785e+00" mtime="2.91546e+01" gflop="0.00000e+00" gbyte="3.76957e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91546e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a515a51530" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51225e+01" utime="4.83731e+01" stime="7.21580e+00" mtime="2.91546e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91546e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4820e+08" > 4.1646e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4971e+08" > 3.0735e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1744e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8634e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.0413e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4978e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.6684e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0434e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1261e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8949e+01 </func>
</region>
</regions>
<internal rank="321" log_i="1723712895.676594" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="322" mpi_size="768" stamp_init="1723712830.426116" stamp_final="1723712895.693043" username="apac4" allocationname="unknown" flags="0" pid="919221" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52669e+01" utime="4.59918e+01" stime="8.66011e+00" mtime="2.83375e+01" gflop="0.00000e+00" gbyte="3.76263e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83375e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51426e+01" utime="4.59591e+01" stime="8.65119e+00" mtime="2.83375e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83375e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4806e+08" > 6.5950e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4729e+08" > 4.5546e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0384e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8823e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4004e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.4397e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4968e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.6798e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0433e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1256e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7990e+01 </func>
</region>
</regions>
<internal rank="322" log_i="1723712895.693043" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="323" mpi_size="768" stamp_init="1723712830.425262" stamp_final="1723712895.676656" username="apac4" allocationname="unknown" flags="0" pid="919222" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52514e+01" utime="4.93168e+01" stime="7.42178e+00" mtime="2.96434e+01" gflop="0.00000e+00" gbyte="3.77117e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.96434e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007c15bb557c157c1511" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51231e+01" utime="4.92852e+01" stime="7.41124e+00" mtime="2.96434e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.96434e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4907e+08" > 4.4087e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4853e+08" > 3.3323e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4371e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8764e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3909e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4976e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.6421e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0435e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1221e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9137e+01 </func>
</region>
</regions>
<internal rank="323" log_i="1723712895.676656" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="324" mpi_size="768" stamp_init="1723712830.424383" stamp_final="1723712895.680521" username="apac4" allocationname="unknown" flags="0" pid="919223" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52561e+01" utime="4.56131e+01" stime="7.98183e+00" mtime="2.81391e+01" gflop="0.00000e+00" gbyte="3.77537e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81391e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008714871499" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51275e+01" utime="4.55768e+01" stime="7.97545e+00" mtime="2.81391e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81391e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4921e+08" > 6.2555e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4914e+08" > 4.3432e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1640e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8636e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1233e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.4359e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4960e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.6127e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0420e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1257e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7725e+01 </func>
</region>
</regions>
<internal rank="324" log_i="1723712895.680521" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="325" mpi_size="768" stamp_init="1723712830.424505" stamp_final="1723712895.681045" username="apac4" allocationname="unknown" flags="0" pid="919224" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52565e+01" utime="4.92571e+01" stime="7.47956e+00" mtime="2.89588e+01" gflop="0.00000e+00" gbyte="3.76255e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89588e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e414e414dd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51271e+01" utime="4.92265e+01" stime="7.46843e+00" mtime="2.89588e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89588e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4866e+08" > 4.2139e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5037e+08" > 2.9645e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1746e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8799e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1710e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4140e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4981e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.6076e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0440e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1254e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8731e+01 </func>
</region>
</regions>
<internal rank="325" log_i="1723712895.681045" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="326" mpi_size="768" stamp_init="1723712830.424390" stamp_final="1723712895.690103" username="apac4" allocationname="unknown" flags="0" pid="919225" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52657e+01" utime="4.67137e+01" stime="8.20079e+00" mtime="2.82760e+01" gflop="0.00000e+00" gbyte="3.77144e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82760e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51403e+01" utime="4.66854e+01" stime="8.18707e+00" mtime="2.82760e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82760e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4923e+08" > 6.1102e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4762e+08" > 5.0298e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2869e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8711e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0994e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.4910e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4963e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.6266e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0424e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1263e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7739e+01 </func>
</region>
</regions>
<internal rank="326" log_i="1723712895.690103" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="327" mpi_size="768" stamp_init="1723712830.424394" stamp_final="1723712895.688557" username="apac4" allocationname="unknown" flags="0" pid="919226" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52642e+01" utime="4.94565e+01" stime="7.23099e+00" mtime="2.91599e+01" gflop="0.00000e+00" gbyte="3.76560e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91599e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000020141f14ea" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51354e+01" utime="4.94263e+01" stime="7.21873e+00" mtime="2.91599e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91599e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4880e+08" > 4.2380e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4859e+08" > 3.3702e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2321e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8714e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.7990e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4978e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.6668e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0410e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1262e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8878e+01 </func>
</region>
</regions>
<internal rank="327" log_i="1723712895.688557" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="328" mpi_size="768" stamp_init="1723712830.424403" stamp_final="1723712895.683242" username="apac4" allocationname="unknown" flags="0" pid="919227" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52588e+01" utime="4.54128e+01" stime="8.82388e+00" mtime="2.82040e+01" gflop="0.00000e+00" gbyte="3.75069e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82040e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000001b15f3551b151a1551" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51337e+01" utime="4.53810e+01" stime="8.81335e+00" mtime="2.82040e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82040e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4769e+08" > 9.2094e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4884e+08" > 6.1015e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5765e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7438e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3189e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8551e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4962e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.8232e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0417e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1265e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7169e+01 </func>
</region>
</regions>
<internal rank="328" log_i="1723712895.683242" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="329" mpi_size="768" stamp_init="1723712830.425710" stamp_final="1723712895.689785" username="apac4" allocationname="unknown" flags="0" pid="919228" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52641e+01" utime="4.92122e+01" stime="7.47846e+00" mtime="2.89333e+01" gflop="0.00000e+00" gbyte="3.77495e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89333e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006014475660146014e3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51364e+01" utime="4.91782e+01" stime="7.47024e+00" mtime="2.89333e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89333e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4828e+08" > 4.7982e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4910e+08" > 3.1835e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4872e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8577e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.8813e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4958e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.8672e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0421e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1263e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8356e+01 </func>
</region>
</regions>
<internal rank="329" log_i="1723712895.689785" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="330" mpi_size="768" stamp_init="1723712830.426173" stamp_final="1723712895.684878" username="apac4" allocationname="unknown" flags="0" pid="919229" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52587e+01" utime="4.66958e+01" stime="8.35407e+00" mtime="2.85654e+01" gflop="0.00000e+00" gbyte="3.77110e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85654e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002814271499" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51332e+01" utime="4.66612e+01" stime="8.34657e+00" mtime="2.85654e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85654e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4752e+08" > 6.1215e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4596e+08" > 4.6779e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6855e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8513e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8610e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.7545e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4957e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.8337e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0420e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1254e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7651e+01 </func>
</region>
</regions>
<internal rank="330" log_i="1723712895.684878" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="331" mpi_size="768" stamp_init="1723712830.426037" stamp_final="1723712895.681629" username="apac4" allocationname="unknown" flags="0" pid="919230" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52556e+01" utime="4.94546e+01" stime="7.25913e+00" mtime="2.86395e+01" gflop="0.00000e+00" gbyte="3.76633e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86395e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c514c51485" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51271e+01" utime="4.94233e+01" stime="7.24871e+00" mtime="2.86395e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86395e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1206e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4910e+08" > 4.9188e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4778e+08" > 3.0077e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7456e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8637e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8725e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4952e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.8917e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0436e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1255e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8789e+01 </func>
</region>
</regions>
<internal rank="331" log_i="1723712895.681629" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="332" mpi_size="768" stamp_init="1723712830.424720" stamp_final="1723712895.689993" username="apac4" allocationname="unknown" flags="0" pid="919231" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52653e+01" utime="4.69508e+01" stime="8.24527e+00" mtime="2.80385e+01" gflop="0.00000e+00" gbyte="3.74905e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80385e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4341435143614f35536143614ab" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51384e+01" utime="4.69181e+01" stime="8.23561e+00" mtime="2.80385e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80385e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4831e+08" > 6.7903e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4985e+08" > 4.2538e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3450e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8467e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3851e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.4767e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4949e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.9523e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0411e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1264e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7391e+01 </func>
</region>
</regions>
<internal rank="332" log_i="1723712895.689993" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="333" mpi_size="768" stamp_init="1723712830.424517" stamp_final="1723712895.679136" username="apac4" allocationname="unknown" flags="0" pid="919232" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52546e+01" utime="4.93370e+01" stime="7.25887e+00" mtime="2.86519e+01" gflop="0.00000e+00" gbyte="3.76404e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86519e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51257e+01" utime="4.93077e+01" stime="7.24605e+00" mtime="2.86519e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86519e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4934e+08" > 4.6603e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5001e+08" > 2.8042e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9131e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8706e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4305e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.1669e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4949e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.9566e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0427e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1262e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8649e+01 </func>
</region>
</regions>
<internal rank="333" log_i="1723712895.679136" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="334" mpi_size="768" stamp_init="1723712830.424383" stamp_final="1723712895.692731" username="apac4" allocationname="unknown" flags="0" pid="919233" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52683e+01" utime="4.65883e+01" stime="8.41142e+00" mtime="2.85548e+01" gflop="0.00000e+00" gbyte="3.75629e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85548e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a014a114a2149956a214a214ba" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51413e+01" utime="4.65569e+01" stime="8.40033e+00" mtime="2.85548e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85548e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5018e+08" > 6.9348e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4838e+08" > 4.6407e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6455e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8519e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2781e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.4500e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4944e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0011e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0403e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1264e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7585e+01 </func>
</region>
</regions>
<internal rank="334" log_i="1723712895.692731" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="335" mpi_size="768" stamp_init="1723712830.424402" stamp_final="1723712895.695505" username="apac4" allocationname="unknown" flags="0" pid="919234" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u14b</host>
<perf wtime="6.52711e+01" utime="4.92243e+01" stime="7.47933e+00" mtime="2.89459e+01" gflop="0.00000e+00" gbyte="3.76514e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89459e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51411e+01" utime="4.91964e+01" stime="7.46516e+00" mtime="2.89459e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89459e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.5831e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4950e+08" > 4.6370e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4820e+08" > 2.6986e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3129e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8585e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3623e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4950e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.9161e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0381e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1281e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8562e+01 </func>
</region>
</regions>
<internal rank="335" log_i="1723712895.695505" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="336" mpi_size="768" stamp_init="1723712830.099167" stamp_final="1723712895.696719" username="apac4" allocationname="unknown" flags="0" pid="818999" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55976e+01" utime="4.26686e+01" stime="1.26888e+01" mtime="2.79710e+01" gflop="0.00000e+00" gbyte="3.85288e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79710e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009f14f9559f149f1468" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54624e+01" utime="4.26390e+01" stime="1.26753e+01" mtime="2.79710e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79710e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4943e+08" > 7.2854e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4914e+08" > 6.4576e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7987e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7950e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.2093e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3101e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4932e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0104e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0465e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1257e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7858e+01 </func>
</region>
</regions>
<internal rank="336" log_i="1723712895.696719" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="337" mpi_size="768" stamp_init="1723712830.099113" stamp_final="1723712895.682708" username="apac4" allocationname="unknown" flags="0" pid="819000" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55836e+01" utime="4.91814e+01" stime="7.25638e+00" mtime="2.89803e+01" gflop="0.00000e+00" gbyte="3.78197e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89803e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a0155655a0159b150f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54437e+01" utime="4.91491e+01" stime="7.24487e+00" mtime="2.89803e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89803e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.0252e-05 </func>
<func name="MPI_Isend" count="127164" bytes="8.4946e+08" > 4.5300e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4909e+08" > 3.1259e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2547e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8025e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7657e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4954e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4944e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.9647e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0471e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1259e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8717e+01 </func>
</region>
</regions>
<internal rank="337" log_i="1723712895.682708" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="338" mpi_size="768" stamp_init="1723712830.099136" stamp_final="1723712895.683377" username="apac4" allocationname="unknown" flags="0" pid="819001" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55842e+01" utime="4.67620e+01" stime="8.35679e+00" mtime="2.85728e+01" gflop="0.00000e+00" gbyte="3.76640e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85728e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4671468146914535569146914dc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54463e+01" utime="4.67292e+01" stime="8.34697e+00" mtime="2.85728e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85728e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4811e+08" > 6.5201e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4855e+08" > 4.5987e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3072e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7820e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8191e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5170e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4941e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 9.9964e-02 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0453e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1257e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8063e+01 </func>
</region>
</regions>
<internal rank="338" log_i="1723712895.683377" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="339" mpi_size="768" stamp_init="1723712830.099119" stamp_final="1723712895.677757" username="apac4" allocationname="unknown" flags="0" pid="819002" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55786e+01" utime="4.93931e+01" stime="7.24576e+00" mtime="2.90332e+01" gflop="0.00000e+00" gbyte="3.77380e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90332e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006d14a1556d146d14a4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54407e+01" utime="4.93578e+01" stime="7.23810e+00" mtime="2.90332e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90332e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4779e+08" > 4.3884e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4846e+08" > 2.7555e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9223e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7926e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5263e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4938e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0032e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0495e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1262e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9129e+01 </func>
</region>
</regions>
<internal rank="339" log_i="1723712895.677757" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="340" mpi_size="768" stamp_init="1723712830.099099" stamp_final="1723712895.683740" username="apac4" allocationname="unknown" flags="0" pid="819003" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55846e+01" utime="4.66370e+01" stime="8.27102e+00" mtime="2.78435e+01" gflop="0.00000e+00" gbyte="3.76373e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78435e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf46514671468142f5568146814af" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54460e+01" utime="4.66054e+01" stime="8.26013e+00" mtime="2.78435e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78435e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4999e+08" > 6.5535e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4934e+08" > 5.0284e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7412e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7972e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4186e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3321e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4933e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0012e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0452e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1262e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7878e+01 </func>
</region>
</regions>
<internal rank="340" log_i="1723712895.683740" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="341" mpi_size="768" stamp_init="1723712830.099063" stamp_final="1723712895.680741" username="apac4" allocationname="unknown" flags="0" pid="819004" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55817e+01" utime="4.92395e+01" stime="7.49686e+00" mtime="2.91550e+01" gflop="0.00000e+00" gbyte="3.76926e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91550e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006e146e14ef" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54453e+01" utime="4.92075e+01" stime="7.48583e+00" mtime="2.91550e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91550e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5086e+08" > 4.6924e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4968e+08" > 3.0170e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3534e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8019e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.5688e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4935e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0025e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0474e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1264e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8779e+01 </func>
</region>
</regions>
<internal rank="341" log_i="1723712895.680741" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="342" mpi_size="768" stamp_init="1723712830.099139" stamp_final="1723712895.688505" username="apac4" allocationname="unknown" flags="0" pid="819005" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55894e+01" utime="4.65659e+01" stime="8.50488e+00" mtime="2.81080e+01" gflop="0.00000e+00" gbyte="3.77430e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81080e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000050151e565015501525" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54534e+01" utime="4.65386e+01" stime="8.48864e+00" mtime="2.81080e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81080e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4953e+08" > 6.9677e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4870e+08" > 6.6487e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0808e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8092e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.8876e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1930e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4930e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0073e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0457e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1263e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7733e+01 </func>
</region>
</regions>
<internal rank="342" log_i="1723712895.688505" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="343" mpi_size="768" stamp_init="1723712830.099107" stamp_final="1723712895.689409" username="apac4" allocationname="unknown" flags="0" pid="819006" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55903e+01" utime="4.91757e+01" stime="7.59251e+00" mtime="2.90347e+01" gflop="0.00000e+00" gbyte="3.77617e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90347e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44d154e154f15de564f154f1543" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54516e+01" utime="4.91436e+01" stime="7.58164e+00" mtime="2.90347e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90347e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4924e+08" > 4.4137e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4908e+08" > 3.1231e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2193e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8026e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7895e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9979e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4935e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0073e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0463e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1264e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8817e+01 </func>
</region>
</regions>
<internal rank="343" log_i="1723712895.689409" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="344" mpi_size="768" stamp_init="1723712830.099154" stamp_final="1723712895.677617" username="apac4" allocationname="unknown" flags="0" pid="819007" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55785e+01" utime="4.75394e+01" stime="7.96393e+00" mtime="2.85527e+01" gflop="0.00000e+00" gbyte="3.76957e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85527e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b314b514b6141455b614b614d3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54402e+01" utime="4.75070e+01" stime="7.95368e+00" mtime="2.85527e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85527e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4940e+08" > 4.6516e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4679e+08" > 3.8762e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5336e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8019e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.4908e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4863e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4923e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0133e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0478e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1264e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7991e+01 </func>
</region>
</regions>
<internal rank="344" log_i="1723712895.677617" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="345" mpi_size="768" stamp_init="1723712830.099090" stamp_final="1723712895.686024" username="apac4" allocationname="unknown" flags="0" pid="819008" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55869e+01" utime="4.94266e+01" stime="7.44633e+00" mtime="2.92119e+01" gflop="0.00000e+00" gbyte="3.77266e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92119e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003e153d151d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54489e+01" utime="4.93972e+01" stime="7.43325e+00" mtime="2.92119e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92119e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4743e+08" > 3.6838e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4802e+08" > 2.5895e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7446e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8194e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.0447e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4924e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0145e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0489e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1263e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8532e+01 </func>
</region>
</regions>
<internal rank="345" log_i="1723712895.686024" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="346" mpi_size="768" stamp_init="1723712830.099140" stamp_final="1723712895.690583" username="apac4" allocationname="unknown" flags="0" pid="819009" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55914e+01" utime="4.72951e+01" stime="8.00390e+00" mtime="2.84401e+01" gflop="0.00000e+00" gbyte="3.76503e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84401e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54545e+01" utime="4.72564e+01" stime="8.00008e+00" mtime="2.84401e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84401e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4825e+08" > 5.0695e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4829e+08" > 6.0807e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5431e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8012e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.9182e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.1350e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4915e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0186e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0479e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1216e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7806e+01 </func>
</region>
</regions>
<internal rank="346" log_i="1723712895.690583" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="347" mpi_size="768" stamp_init="1723712830.099084" stamp_final="1723712895.687429" username="apac4" allocationname="unknown" flags="0" pid="819010" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55883e+01" utime="4.93763e+01" stime="7.14882e+00" mtime="2.91093e+01" gflop="0.00000e+00" gbyte="3.76839e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91093e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4bd14bf14c014e855c014bf147f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54472e+01" utime="4.93399e+01" stime="7.14278e+00" mtime="2.91093e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91093e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4906e+08" > 3.4242e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4818e+08" > 2.8089e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1544e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8108e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8358e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7989e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4921e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0210e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0504e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1262e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9048e+01 </func>
</region>
</regions>
<internal rank="347" log_i="1723712895.687429" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="348" mpi_size="768" stamp_init="1723712830.099181" stamp_final="1723712895.678225" username="apac4" allocationname="unknown" flags="0" pid="819011" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55790e+01" utime="4.69015e+01" stime="8.27725e+00" mtime="2.86102e+01" gflop="0.00000e+00" gbyte="3.76053e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86102e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54442e+01" utime="4.68692e+01" stime="8.26682e+00" mtime="2.86102e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86102e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4945e+08" > 5.1506e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5025e+08" > 3.9072e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0143e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7969e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6139e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4940e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4916e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0243e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0452e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1263e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7520e+01 </func>
</region>
</regions>
<internal rank="348" log_i="1723712895.678225" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="349" mpi_size="768" stamp_init="1723712830.099099" stamp_final="1723712895.691467" username="apac4" allocationname="unknown" flags="0" pid="819012" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55924e+01" utime="4.91595e+01" stime="7.66562e+00" mtime="2.94346e+01" gflop="0.00000e+00" gbyte="3.77888e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.94346e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003514351475" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54537e+01" utime="4.91196e+01" stime="7.66274e+00" mtime="2.94346e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.94346e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4890e+08" > 3.5726e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4872e+08" > 3.0554e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8929e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8068e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.0881e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4909e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0301e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0502e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1264e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8625e+01 </func>
</region>
</regions>
<internal rank="349" log_i="1723712895.691467" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="350" mpi_size="768" stamp_init="1723712830.099171" stamp_final="1723712895.677630" username="apac4" allocationname="unknown" flags="0" pid="819013" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55785e+01" utime="4.67572e+01" stime="8.53052e+00" mtime="2.85531e+01" gflop="0.00000e+00" gbyte="3.76797e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85531e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54438e+01" utime="4.67297e+01" stime="8.51557e+00" mtime="2.85531e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85531e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4825e+08" > 5.5143e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5005e+08" > 5.7226e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5316e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7931e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.5075e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.5071e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4909e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0332e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0459e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1263e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7898e+01 </func>
</region>
</regions>
<internal rank="350" log_i="1723712895.677630" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="351" mpi_size="768" stamp_init="1723712830.099881" stamp_final="1723712895.687383" username="apac4" allocationname="unknown" flags="0" pid="819014" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55875e+01" utime="4.95858e+01" stime="7.16483e+00" mtime="2.89737e+01" gflop="0.00000e+00" gbyte="3.76911e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89737e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000291532562915291530" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54513e+01" utime="4.95525e+01" stime="7.15542e+00" mtime="2.89737e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89737e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4908e+08" > 3.4835e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4898e+08" > 2.3681e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1665e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7857e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1206e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4779e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4907e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0352e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0499e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1261e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8927e+01 </func>
</region>
</regions>
<internal rank="351" log_i="1723712895.687383" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="352" mpi_size="768" stamp_init="1723712830.099155" stamp_final="1723712895.692049" username="apac4" allocationname="unknown" flags="0" pid="819015" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55929e+01" utime="4.69173e+01" stime="8.48757e+00" mtime="2.80575e+01" gflop="0.00000e+00" gbyte="3.76713e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80575e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d014d0147a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54556e+01" utime="4.68865e+01" stime="8.47555e+00" mtime="2.80575e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80575e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5009e+08" > 6.5606e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5000e+08" > 6.4652e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3232e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3739e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.3328e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.2776e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4892e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0460e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0448e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1259e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7487e+01 </func>
</region>
</regions>
<internal rank="352" log_i="1723712895.692049" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="353" mpi_size="768" stamp_init="1723712830.099082" stamp_final="1723712895.681374" username="apac4" allocationname="unknown" flags="0" pid="819016" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55823e+01" utime="4.94888e+01" stime="7.31103e+00" mtime="2.85424e+01" gflop="0.00000e+00" gbyte="3.75404e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85424e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000041144014d0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54446e+01" utime="4.94532e+01" stime="7.30368e+00" mtime="2.85424e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85424e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4845e+08" > 4.8585e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4856e+08" > 3.2381e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7766e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8017e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.5741e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4888e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0505e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0485e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1207e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8722e+01 </func>
</region>
</regions>
<internal rank="353" log_i="1723712895.681374" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="354" mpi_size="768" stamp_init="1723712830.099151" stamp_final="1723712895.683280" username="apac4" allocationname="unknown" flags="0" pid="819017" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55841e+01" utime="4.72818e+01" stime="8.39274e+00" mtime="2.84571e+01" gflop="0.00000e+00" gbyte="3.76305e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84571e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000eb145655eb14eb1465" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54510e+01" utime="4.72551e+01" stime="8.37643e+00" mtime="2.84571e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84571e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4921e+08" > 6.0154e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4928e+08" > 4.2514e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3020e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8032e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0362e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3885e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4891e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0522e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0493e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1262e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7984e+01 </func>
</region>
</regions>
<internal rank="354" log_i="1723712895.683280" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="355" mpi_size="768" stamp_init="1723712830.099052" stamp_final="1723712895.688279" username="apac4" allocationname="unknown" flags="0" pid="819018" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55892e+01" utime="4.93262e+01" stime="7.50375e+00" mtime="2.94794e+01" gflop="0.00000e+00" gbyte="3.76171e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.94794e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000be14be14bf" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54503e+01" utime="4.92922e+01" stime="7.49546e+00" mtime="2.94794e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.94794e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4936e+08" > 4.7438e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4927e+08" > 2.8225e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6020e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8009e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3840e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4885e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0538e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0484e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1264e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8851e+01 </func>
</region>
</regions>
<internal rank="355" log_i="1723712895.688279" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="356" mpi_size="768" stamp_init="1723712830.099126" stamp_final="1723712895.683981" username="apac4" allocationname="unknown" flags="0" pid="819019" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55849e+01" utime="4.58400e+01" stime="9.00104e+00" mtime="2.80594e+01" gflop="0.00000e+00" gbyte="3.77884e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80594e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003e143e14cf" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54463e+01" utime="4.58081e+01" stime="8.98976e+00" mtime="2.80594e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80594e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4899e+08" > 6.6283e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4970e+08" > 6.3086e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6901e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7868e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3617e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.5900e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4883e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0565e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0468e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1258e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7135e+01 </func>
</region>
</regions>
<internal rank="356" log_i="1723712895.683981" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="357" mpi_size="768" stamp_init="1723712830.099070" stamp_final="1723712895.681305" username="apac4" allocationname="unknown" flags="0" pid="819020" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55822e+01" utime="4.92935e+01" stime="7.52433e+00" mtime="2.89567e+01" gflop="0.00000e+00" gbyte="3.75771e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89567e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000051145114aa" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54452e+01" utime="4.92602e+01" stime="7.51529e+00" mtime="2.89567e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89567e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4990e+08" > 4.8202e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4937e+08" > 2.9764e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5354e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8022e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3961e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4878e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0624e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0472e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1256e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8384e+01 </func>
</region>
</regions>
<internal rank="357" log_i="1723712895.681305" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="358" mpi_size="768" stamp_init="1723712830.099103" stamp_final="1723712895.685423" username="apac4" allocationname="unknown" flags="0" pid="819021" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55863e+01" utime="4.71719e+01" stime="8.37185e+00" mtime="2.85003e+01" gflop="0.00000e+00" gbyte="3.77769e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85003e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4531455145614f75656145514bf" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54512e+01" utime="4.71403e+01" stime="8.36022e+00" mtime="2.85003e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85003e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4974e+08" > 6.6057e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4885e+08" > 5.0177e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4240e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7890e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5187e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1322e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4875e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0680e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0478e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1260e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7854e+01 </func>
</region>
</regions>
<internal rank="358" log_i="1723712895.685423" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="359" mpi_size="768" stamp_init="1723712830.099054" stamp_final="1723712895.687686" username="apac4" allocationname="unknown" flags="0" pid="819022" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u09b</host>
<perf wtime="6.55886e+01" utime="4.93108e+01" stime="7.39241e+00" mtime="2.89476e+01" gflop="0.00000e+00" gbyte="3.75328e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89476e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54510e+01" utime="4.92732e+01" stime="7.38699e+00" mtime="2.89476e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89476e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4887e+08" > 4.8994e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4973e+08" > 2.8999e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5691e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7912e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.0919e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4876e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0672e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0494e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1260e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8346e+01 </func>
</region>
</regions>
<internal rank="359" log_i="1723712895.687686" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="360" mpi_size="768" stamp_init="1723712830.179932" stamp_final="1723712895.693130" username="apac4" allocationname="unknown" flags="0" pid="3142310" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55132e+01" utime="4.28558e+01" stime="1.25706e+01" mtime="2.82392e+01" gflop="0.00000e+00" gbyte="3.86471e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82392e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b2144855b214b21468" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53856e+01" utime="4.28236e+01" stime="1.25604e+01" mtime="2.82392e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82392e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4931e+08" > 5.8579e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4778e+08" > 3.4344e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4624e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3851e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5916e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3308e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4870e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0690e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0486e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1276e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8052e+01 </func>
</region>
</regions>
<internal rank="360" log_i="1723712895.693130" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="361" mpi_size="768" stamp_init="1723712830.179957" stamp_final="1723712895.682571" username="apac4" allocationname="unknown" flags="0" pid="3142311" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55026e+01" utime="4.90173e+01" stime="7.49550e+00" mtime="2.84787e+01" gflop="0.00000e+00" gbyte="3.77953e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84787e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d8148a55d814d814e3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53699e+01" utime="4.89798e+01" stime="7.48952e+00" mtime="2.84787e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84787e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4965e+08" > 4.6357e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4880e+08" > 3.4122e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1336e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3921e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6662e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4883e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0598e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0503e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1272e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8735e+01 </func>
</region>
</regions>
<internal rank="361" log_i="1723712895.682571" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="362" mpi_size="768" stamp_init="1723712830.180103" stamp_final="1723712895.690492" username="apac4" allocationname="unknown" flags="0" pid="3142312" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55104e+01" utime="4.67590e+01" stime="8.34762e+00" mtime="2.79068e+01" gflop="0.00000e+00" gbyte="3.76438e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79068e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53832e+01" utime="4.67264e+01" stime="8.33824e+00" mtime="2.79068e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79068e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4784e+08" > 5.9275e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5085e+08" > 4.2894e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1805e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3776e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.7023e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.4204e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4863e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0754e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0478e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1224e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7994e+01 </func>
</region>
</regions>
<internal rank="362" log_i="1723712895.690492" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="363" mpi_size="768" stamp_init="1723712830.179921" stamp_final="1723712895.688261" username="apac4" allocationname="unknown" flags="0" pid="3142313" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55083e+01" utime="4.92300e+01" stime="7.29728e+00" mtime="2.83046e+01" gflop="0.00000e+00" gbyte="3.77796e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83046e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53787e+01" utime="4.91968e+01" stime="7.28802e+00" mtime="2.83046e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83046e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5067e+08" > 4.4969e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4717e+08" > 2.6168e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7875e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4000e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1921e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.5191e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4876e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0674e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0488e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1271e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8921e+01 </func>
</region>
</regions>
<internal rank="363" log_i="1723712895.688261" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="364" mpi_size="768" stamp_init="1723712830.179899" stamp_final="1723712895.685037" username="apac4" allocationname="unknown" flags="0" pid="3142314" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55051e+01" utime="4.43248e+01" stime="9.30180e+00" mtime="2.75963e+01" gflop="0.00000e+00" gbyte="3.76823e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.75963e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ba14d455ba14ba14eb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53755e+01" utime="4.42998e+01" stime="9.28456e+00" mtime="2.75963e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.75963e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4953e+08" > 8.7744e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4829e+08" > 6.4821e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3746e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3849e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0994e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.1028e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4855e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0764e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0472e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1277e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7172e+01 </func>
</region>
</regions>
<internal rank="364" log_i="1723712895.685037" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="365" mpi_size="768" stamp_init="1723712830.179875" stamp_final="1723712895.692877" username="apac4" allocationname="unknown" flags="0" pid="3142315" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55130e+01" utime="4.91437e+01" stime="7.33889e+00" mtime="2.83118e+01" gflop="0.00000e+00" gbyte="3.76492e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83118e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ba149e55ba14b914a7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53869e+01" utime="4.91097e+01" stime="7.33076e+00" mtime="2.83118e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83118e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4735e+08" > 4.6971e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5112e+08" > 3.3535e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9354e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3945e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.0430e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4867e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0733e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0503e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1220e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8757e+01 </func>
</region>
</regions>
<internal rank="365" log_i="1723712895.692877" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="366" mpi_size="768" stamp_init="1723712830.179922" stamp_final="1723712895.690818" username="apac4" allocationname="unknown" flags="0" pid="3142316" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55109e+01" utime="4.54195e+01" stime="8.96640e+00" mtime="2.79492e+01" gflop="0.00000e+00" gbyte="3.77853e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79492e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53824e+01" utime="4.53833e+01" stime="8.96004e+00" mtime="2.79492e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79492e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.5163e+08" > 7.9012e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4845e+08" > 6.1711e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8246e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3932e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.7641e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.0508e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4870e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0745e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0500e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1275e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7154e+01 </func>
</region>
</regions>
<internal rank="366" log_i="1723712895.690818" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="367" mpi_size="768" stamp_init="1723712830.179950" stamp_final="1723712895.684703" username="apac4" allocationname="unknown" flags="0" pid="3142317" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55048e+01" utime="4.88089e+01" stime="7.76825e+00" mtime="2.89693e+01" gflop="0.00000e+00" gbyte="3.76888e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89693e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c514c714c8148d56c814c81456" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53748e+01" utime="4.87775e+01" stime="7.75777e+00" mtime="2.89693e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89693e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4856e+08" > 4.4518e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4918e+08" > 2.9151e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6133e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3829e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.0720e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4862e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0824e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0462e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1267e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8778e+01 </func>
</region>
</regions>
<internal rank="367" log_i="1723712895.684703" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="368" mpi_size="768" stamp_init="1723712830.179868" stamp_final="1723712895.690819" username="apac4" allocationname="unknown" flags="0" pid="3142318" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55110e+01" utime="4.55970e+01" stime="8.76546e+00" mtime="2.84249e+01" gflop="0.00000e+00" gbyte="3.77682e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84249e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000096149614c3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53808e+01" utime="4.55705e+01" stime="8.74979e+00" mtime="2.84249e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84249e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4856e+08" > 7.2074e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4861e+08" > 4.6411e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9593e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3929e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2209e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3012e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4847e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0924e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0507e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1276e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7584e+01 </func>
</region>
</regions>
<internal rank="368" log_i="1723712895.690819" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="369" mpi_size="768" stamp_init="1723712830.179923" stamp_final="1723712895.687016" username="apac4" allocationname="unknown" flags="0" pid="3142319" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55071e+01" utime="4.91562e+01" stime="7.41611e+00" mtime="2.88982e+01" gflop="0.00000e+00" gbyte="3.77247e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88982e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53772e+01" utime="4.91224e+01" stime="7.40777e+00" mtime="2.88982e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88982e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4809e+08" > 3.5475e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4889e+08" > 2.8692e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2825e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3919e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.0038e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4843e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0981e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0455e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1226e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9116e+01 </func>
</region>
</regions>
<internal rank="369" log_i="1723712895.687016" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="370" mpi_size="768" stamp_init="1723712830.179854" stamp_final="1723712895.684215" username="apac4" allocationname="unknown" flags="0" pid="3142320" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55044e+01" utime="4.66156e+01" stime="8.26700e+00" mtime="2.79585e+01" gflop="0.00000e+00" gbyte="3.75492e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79585e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ae148e55ae14ae149c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53772e+01" utime="4.65827e+01" stime="8.25769e+00" mtime="2.79585e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79585e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.2159e-05 </func>
<func name="MPI_Isend" count="127164" bytes="8.4954e+08" > 5.3173e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4988e+08" > 4.1343e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0848e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3846e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6001e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6050e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4840e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.0999e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0477e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1276e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8196e+01 </func>
</region>
</regions>
<internal rank="370" log_i="1723712895.684215" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="371" mpi_size="768" stamp_init="1723712830.179901" stamp_final="1723712895.678720" username="apac4" allocationname="unknown" flags="0" pid="3142321" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.54988e+01" utime="4.95070e+01" stime="7.07210e+00" mtime="2.84717e+01" gflop="0.00000e+00" gbyte="3.77869e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84717e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009815971503" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53716e+01" utime="4.94734e+01" stime="7.06258e+00" mtime="2.84717e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84717e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4944e+08" > 3.4759e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4963e+08" > 2.7225e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8720e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3948e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.1378e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4835e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1115e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0469e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1269e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9106e+01 </func>
</region>
</regions>
<internal rank="371" log_i="1723712895.678720" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="372" mpi_size="768" stamp_init="1723712830.179884" stamp_final="1723712895.684031" username="apac4" allocationname="unknown" flags="0" pid="3142322" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55041e+01" utime="4.71786e+01" stime="8.27469e+00" mtime="2.80800e+01" gflop="0.00000e+00" gbyte="3.75950e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80800e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005315e555531552153c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53751e+01" utime="4.71462e+01" stime="8.26465e+00" mtime="2.80800e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80800e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4911e+08" > 4.6127e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4848e+08" > 4.6834e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9628e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3940e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.6130e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.2559e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4840e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1014e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0496e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1276e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7495e+01 </func>
</region>
</regions>
<internal rank="372" log_i="1723712895.684031" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="373" mpi_size="768" stamp_init="1723712830.179921" stamp_final="1723712895.685425" username="apac4" allocationname="unknown" flags="0" pid="3142323" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55055e+01" utime="4.93254e+01" stime="7.23252e+00" mtime="2.82561e+01" gflop="0.00000e+00" gbyte="3.76396e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82561e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a5143456a514a514fc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53778e+01" utime="4.92923e+01" stime="7.22371e+00" mtime="2.82561e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82561e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4901e+08" > 3.5006e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4944e+08" > 2.8254e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9422e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3783e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4544e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.6550e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4835e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1098e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0486e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1271e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8836e+01 </func>
</region>
</regions>
<internal rank="373" log_i="1723712895.685425" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="374" mpi_size="768" stamp_init="1723712830.179856" stamp_final="1723712895.694972" username="apac4" allocationname="unknown" flags="0" pid="3142324" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55151e+01" utime="4.73024e+01" stime="8.17494e+00" mtime="2.80347e+01" gflop="0.00000e+00" gbyte="3.77228e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80347e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ce145656ce14ce1478" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53859e+01" utime="4.72671e+01" stime="8.16809e+00" mtime="2.80347e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80347e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4925e+08" > 4.6331e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4889e+08" > 3.5276e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9261e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3698e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.9353e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.2671e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4836e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1068e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0481e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1275e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7520e+01 </func>
</region>
</regions>
<internal rank="374" log_i="1723712895.694972" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="375" mpi_size="768" stamp_init="1723712830.179865" stamp_final="1723712895.684979" username="apac4" allocationname="unknown" flags="0" pid="3142325" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55051e+01" utime="4.87647e+01" stime="7.56624e+00" mtime="2.90324e+01" gflop="0.00000e+00" gbyte="3.75275e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90324e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e314e214f2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53775e+01" utime="4.87268e+01" stime="7.56172e+00" mtime="2.90324e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90324e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4880e+08" > 3.5849e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4908e+08" > 2.3883e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7818e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3942e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.7561e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4827e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1178e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0475e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1268e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8754e+01 </func>
</region>
</regions>
<internal rank="375" log_i="1723712895.684979" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="376" mpi_size="768" stamp_init="1723712830.179870" stamp_final="1723712895.691009" username="apac4" allocationname="unknown" flags="0" pid="3142326" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55111e+01" utime="4.62983e+01" stime="8.03065e+00" mtime="2.82784e+01" gflop="0.00000e+00" gbyte="3.77869e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82784e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000034142f14c8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53816e+01" utime="4.62735e+01" stime="8.01298e+00" mtime="2.82784e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82784e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4696e+08" > 5.6849e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4523e+08" > 5.7412e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.5027e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3764e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.8321e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3990e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4815e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1274e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0464e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1275e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7050e+01 </func>
</region>
</regions>
<internal rank="376" log_i="1723712895.691009" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="377" mpi_size="768" stamp_init="1723712830.179886" stamp_final="1723712895.687119" username="apac4" allocationname="unknown" flags="0" pid="3142327" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55072e+01" utime="4.88018e+01" stime="7.48190e+00" mtime="2.84483e+01" gflop="0.00000e+00" gbyte="3.77628e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84483e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006a147d556a146a1487" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53789e+01" utime="4.87661e+01" stime="7.47511e+00" mtime="2.84483e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84483e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4780e+08" > 4.6866e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4808e+08" > 2.5805e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3032e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3849e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.8391e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4820e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1253e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0472e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1271e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8545e+01 </func>
</region>
</regions>
<internal rank="377" log_i="1723712895.687119" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="378" mpi_size="768" stamp_init="1723712830.179911" stamp_final="1723712895.691224" username="apac4" allocationname="unknown" flags="0" pid="3142328" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55113e+01" utime="4.78024e+01" stime="7.84768e+00" mtime="2.76901e+01" gflop="0.00000e+00" gbyte="3.77357e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.76901e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006015305560155f1505" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53816e+01" utime="4.77689e+01" stime="7.83846e+00" mtime="2.76901e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.76901e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4847e+08" > 5.6056e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4929e+08" > 3.8686e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1899e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3892e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.4298e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.4219e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4826e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1182e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0451e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1274e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7791e+01 </func>
</region>
</regions>
<internal rank="378" log_i="1723712895.691224" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="379" mpi_size="768" stamp_init="1723712830.179916" stamp_final="1723712895.674065" username="apac4" allocationname="unknown" flags="0" pid="3142329" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.54941e+01" utime="4.90750e+01" stime="7.45871e+00" mtime="2.84558e+01" gflop="0.00000e+00" gbyte="3.78113e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84558e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b2142055b214b21477" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53678e+01" utime="4.90384e+01" stime="7.45237e+00" mtime="2.84558e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84558e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4922e+08" > 4.7688e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4951e+08" > 3.0899e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2938e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3674e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1921e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.4393e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4822e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1232e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0506e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1270e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8567e+01 </func>
</region>
</regions>
<internal rank="379" log_i="1723712895.674065" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="380" mpi_size="768" stamp_init="1723712830.179934" stamp_final="1723712895.683911" username="apac4" allocationname="unknown" flags="0" pid="3142330" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55040e+01" utime="4.65126e+01" stime="7.88235e+00" mtime="2.79003e+01" gflop="0.00000e+00" gbyte="3.77026e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79003e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c314c314d0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53751e+01" utime="4.64750e+01" stime="7.87761e+00" mtime="2.79003e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79003e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4965e+08" > 6.3612e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4777e+08" > 3.8913e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7240e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4042e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.7971e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3963e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4810e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1313e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0460e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1274e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7373e+01 </func>
</region>
</regions>
<internal rank="380" log_i="1723712895.683911" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="381" mpi_size="768" stamp_init="1723712830.180001" stamp_final="1723712895.673949" username="apac4" allocationname="unknown" flags="0" pid="3142331" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.54939e+01" utime="4.92013e+01" stime="7.32211e+00" mtime="2.83403e+01" gflop="0.00000e+00" gbyte="3.77781e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83403e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000033143214dd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53639e+01" utime="4.91694e+01" stime="7.31175e+00" mtime="2.83403e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83403e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4913e+08" > 5.1683e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4937e+08" > 2.5562e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1875e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3909e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4180e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4810e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1367e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0462e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1270e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8498e+01 </func>
</region>
</regions>
<internal rank="381" log_i="1723712895.673949" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="382" mpi_size="768" stamp_init="1723712830.179890" stamp_final="1723712895.686024" username="apac4" allocationname="unknown" flags="0" pid="3142332" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55061e+01" utime="4.64171e+01" stime="7.89888e+00" mtime="2.74657e+01" gflop="0.00000e+00" gbyte="3.77129e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.74657e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4341436143714e75637143614f2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53775e+01" utime="4.63866e+01" stime="7.88683e+00" mtime="2.74657e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.74657e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4885e+08" > 6.1375e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4892e+08" > 4.2982e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5070e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3857e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.3208e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.4128e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4810e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1361e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0458e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1273e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7196e+01 </func>
</region>
</regions>
<internal rank="382" log_i="1723712895.686024" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="383" mpi_size="768" stamp_init="1723712830.179945" stamp_final="1723712895.693064" username="apac4" allocationname="unknown" flags="0" pid="3142333" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u19b</host>
<perf wtime="6.55131e+01" utime="4.91327e+01" stime="7.40794e+00" mtime="2.85160e+01" gflop="0.00000e+00" gbyte="3.76976e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85160e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005c145c14a3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53868e+01" utime="4.91025e+01" stime="7.39591e+00" mtime="2.85160e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85160e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4836e+08" > 4.6950e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4889e+08" > 2.8899e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7317e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3814e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8410e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4814e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1316e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0482e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1267e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8181e+01 </func>
</region>
</regions>
<internal rank="383" log_i="1723712895.693064" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="384" mpi_size="768" stamp_init="1723712830.433546" stamp_final="1723712895.687918" username="apac4" allocationname="unknown" flags="0" pid="691101" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52544e+01" utime="4.29343e+01" stime="1.20558e+01" mtime="2.81381e+01" gflop="0.00000e+00" gbyte="3.85098e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81381e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b014af14fd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51212e+01" utime="4.29012e+01" stime="1.20465e+01" mtime="2.81381e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81381e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4776e+08" > 5.9594e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4811e+08" > 5.4029e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6555e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3057e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6905e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.1948e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4808e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1441e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0354e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1275e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7640e+01 </func>
</region>
</regions>
<internal rank="384" log_i="1723712895.687918" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="385" mpi_size="768" stamp_init="1723712830.433781" stamp_final="1723712895.680048" username="apac4" allocationname="unknown" flags="0" pid="691102" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52463e+01" utime="4.91790e+01" stime="7.16391e+00" mtime="2.84733e+01" gflop="0.00000e+00" gbyte="3.75385e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84733e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b015b115b2155755b215b2153c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51095e+01" utime="4.91449e+01" stime="7.15438e+00" mtime="2.84733e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84733e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4957e+08" > 4.6042e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4856e+08" > 3.2500e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1733e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4578e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1696e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.0692e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4819e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1302e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0453e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1214e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8629e+01 </func>
</region>
</regions>
<internal rank="385" log_i="1723712895.680048" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="386" mpi_size="768" stamp_init="1723712830.433549" stamp_final="1723712895.685366" username="apac4" allocationname="unknown" flags="0" pid="691103" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52518e+01" utime="4.69403e+01" stime="8.00942e+00" mtime="2.79640e+01" gflop="0.00000e+00" gbyte="3.77407e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79640e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51163e+01" utime="4.69086e+01" stime="7.99830e+00" mtime="2.79640e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79640e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4802e+08" > 6.6333e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4757e+08" > 4.3659e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0591e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4664e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.6253e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.2635e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4812e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1367e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0412e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1275e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8011e+01 </func>
</region>
</regions>
<internal rank="386" log_i="1723712895.685366" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="387" mpi_size="768" stamp_init="1723712830.433468" stamp_final="1723712895.691061" username="apac4" allocationname="unknown" flags="0" pid="691104" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52576e+01" utime="4.91915e+01" stime="7.23432e+00" mtime="2.85344e+01" gflop="0.00000e+00" gbyte="3.76259e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85344e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f115f11515" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51240e+01" utime="4.91578e+01" stime="7.22574e+00" mtime="2.85344e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85344e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4863e+08" > 4.5248e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4856e+08" > 2.9390e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0212e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4549e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.2063e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4814e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1354e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0428e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1213e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8856e+01 </func>
</region>
</regions>
<internal rank="387" log_i="1723712895.691061" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="388" mpi_size="768" stamp_init="1723712830.433575" stamp_final="1723712895.679021" username="apac4" allocationname="unknown" flags="0" pid="691105" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52454e+01" utime="4.71291e+01" stime="7.98867e+00" mtime="2.79402e+01" gflop="0.00000e+00" gbyte="3.76617e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79402e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000027142714b8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51114e+01" utime="4.70953e+01" stime="7.97991e+00" mtime="2.79402e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79402e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4939e+08" > 6.3764e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4832e+08" > 5.4524e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2862e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4700e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9407e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.5630e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4806e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1404e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0428e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1275e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7771e+01 </func>
</region>
</regions>
<internal rank="388" log_i="1723712895.679021" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="389" mpi_size="768" stamp_init="1723712830.433479" stamp_final="1723712895.686765" username="apac4" allocationname="unknown" flags="0" pid="691106" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52533e+01" utime="4.93367e+01" stime="7.05469e+00" mtime="2.84913e+01" gflop="0.00000e+00" gbyte="3.77350e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84913e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005d155c1530" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51181e+01" utime="4.93018e+01" stime="7.04674e+00" mtime="2.84913e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84913e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4811e+08" > 4.5608e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4994e+08" > 3.0200e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1089e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4677e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6941e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.6541e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4798e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1517e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0443e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1215e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8709e+01 </func>
</region>
</regions>
<internal rank="389" log_i="1723712895.686765" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="390" mpi_size="768" stamp_init="1723712830.433587" stamp_final="1723712895.685623" username="apac4" allocationname="unknown" flags="0" pid="691107" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52520e+01" utime="4.68288e+01" stime="8.00933e+00" mtime="2.75515e+01" gflop="0.00000e+00" gbyte="3.77254e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.75515e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ff14c255ff14ff1475" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51171e+01" utime="4.67910e+01" stime="8.00388e+00" mtime="2.75515e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.75515e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4981e+08" > 6.3902e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4843e+08" > 5.8360e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8968e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4574e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.1468e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4159e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4805e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1410e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0419e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1276e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7770e+01 </func>
</region>
</regions>
<internal rank="390" log_i="1723712895.685623" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="391" mpi_size="768" stamp_init="1723712830.433445" stamp_final="1723712895.680525" username="apac4" allocationname="unknown" flags="0" pid="691108" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52471e+01" utime="4.91289e+01" stime="7.19641e+00" mtime="2.89414e+01" gflop="0.00000e+00" gbyte="3.77754e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89414e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51141e+01" utime="4.90955e+01" stime="7.18646e+00" mtime="2.89414e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89414e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4947e+08" > 4.4841e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4942e+08" > 3.1221e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7221e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4671e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9802e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4183e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4804e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1501e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0423e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1216e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8549e+01 </func>
</region>
</regions>
<internal rank="391" log_i="1723712895.680525" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="392" mpi_size="768" stamp_init="1723712830.433567" stamp_final="1723712895.687811" username="apac4" allocationname="unknown" flags="0" pid="691109" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52542e+01" utime="4.66162e+01" stime="8.30520e+00" mtime="2.80081e+01" gflop="0.00000e+00" gbyte="3.77296e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80081e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51195e+01" utime="4.65811e+01" stime="8.29793e+00" mtime="2.80081e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80081e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4784e+08" > 4.5735e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4761e+08" > 4.4229e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6521e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4655e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9884e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3501e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4789e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1568e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0428e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1277e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7669e+01 </func>
</region>
</regions>
<internal rank="392" log_i="1723712895.687811" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="393" mpi_size="768" stamp_init="1723712830.433448" stamp_final="1723712895.690469" username="apac4" allocationname="unknown" flags="0" pid="691110" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52570e+01" utime="4.90121e+01" stime="7.42450e+00" mtime="2.85640e+01" gflop="0.00000e+00" gbyte="3.77922e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85640e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000981597154b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51264e+01" utime="4.89780e+01" stime="7.41665e+00" mtime="2.85640e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85640e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4765e+08" > 3.5066e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4787e+08" > 2.5711e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4816e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4667e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4461e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4797e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1536e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0435e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1215e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8518e+01 </func>
</region>
</regions>
<internal rank="393" log_i="1723712895.690469" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="394" mpi_size="768" stamp_init="1723712830.433550" stamp_final="1723712895.690362" username="apac4" allocationname="unknown" flags="0" pid="691111" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52568e+01" utime="4.57710e+01" stime="8.28097e+00" mtime="2.72384e+01" gflop="0.00000e+00" gbyte="3.76820e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.72384e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51208e+01" utime="4.57309e+01" stime="8.27840e+00" mtime="2.72384e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.72384e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4748e+08" > 6.1930e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4767e+08" > 5.4867e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.6162e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4551e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3757e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3468e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4787e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1557e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0428e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1235e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7771e+01 </func>
</region>
</regions>
<internal rank="394" log_i="1723712895.690362" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="395" mpi_size="768" stamp_init="1723712830.433455" stamp_final="1723712895.684921" username="apac4" allocationname="unknown" flags="0" pid="691112" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52515e+01" utime="4.93464e+01" stime="7.09343e+00" mtime="2.86891e+01" gflop="0.00000e+00" gbyte="3.76633e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86891e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51194e+01" utime="4.93122e+01" stime="7.08420e+00" mtime="2.86891e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86891e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4914e+08" > 3.5553e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4961e+08" > 2.5772e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0848e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4681e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9325e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1377e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4784e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1664e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0436e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1214e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9032e+01 </func>
</region>
</regions>
<internal rank="395" log_i="1723712895.684921" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="396" mpi_size="768" stamp_init="1723712830.433609" stamp_final="1723712895.684375" username="apac4" allocationname="unknown" flags="0" pid="691113" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52508e+01" utime="4.51748e+01" stime="8.86303e+00" mtime="2.77809e+01" gflop="0.00000e+00" gbyte="3.76549e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77809e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48a158c158d151c568d158d1519" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51165e+01" utime="4.51412e+01" stime="8.85381e+00" mtime="2.77809e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77809e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4929e+08" > 6.7564e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4894e+08" > 5.6311e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2182e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4500e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9948e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.3040e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4779e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1648e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0423e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1275e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7657e+01 </func>
</region>
</regions>
<internal rank="396" log_i="1723712895.684375" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="397" mpi_size="768" stamp_init="1723712830.433469" stamp_final="1723712895.686317" username="apac4" allocationname="unknown" flags="0" pid="691114" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52528e+01" utime="4.92796e+01" stime="7.11260e+00" mtime="2.84834e+01" gflop="0.00000e+00" gbyte="3.77003e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84834e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000070146f14c1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51196e+01" utime="4.92497e+01" stime="7.09971e+00" mtime="2.84834e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84834e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4884e+08" > 3.8123e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4886e+08" > 2.6302e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0320e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4609e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7895e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.2950e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4787e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1601e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0428e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1213e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8860e+01 </func>
</region>
</regions>
<internal rank="397" log_i="1723712895.686317" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="398" mpi_size="768" stamp_init="1723712830.433577" stamp_final="1723712895.685101" username="apac4" allocationname="unknown" flags="0" pid="691115" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52515e+01" utime="4.64875e+01" stime="8.27432e+00" mtime="2.82090e+01" gflop="0.00000e+00" gbyte="3.77876e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82090e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f8157155f815f8150e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51157e+01" utime="4.64561e+01" stime="8.26306e+00" mtime="2.82090e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82090e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 5.0661e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4878e+08" > 4.0870e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6050e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4478e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.4379e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.2749e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4778e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1674e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0451e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1254e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7880e+01 </func>
</region>
</regions>
<internal rank="398" log_i="1723712895.685101" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="399" mpi_size="768" stamp_init="1723712830.433494" stamp_final="1723712895.680486" username="apac4" allocationname="unknown" flags="0" pid="691116" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52470e+01" utime="4.88749e+01" stime="7.48984e+00" mtime="2.85133e+01" gflop="0.00000e+00" gbyte="3.77823e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85133e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4fc15fe15ff153c55ff15fe152f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51135e+01" utime="4.88434e+01" stime="7.47806e+00" mtime="2.85133e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85133e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4876e+08" > 3.6852e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4932e+08" > 2.4615e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4018e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4636e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6226e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.2978e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4772e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1752e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0468e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1214e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8534e+01 </func>
</region>
</regions>
<internal rank="399" log_i="1723712895.680486" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="400" mpi_size="768" stamp_init="1723712830.433586" stamp_final="1723712895.679779" username="apac4" allocationname="unknown" flags="0" pid="691117" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52462e+01" utime="4.71754e+01" stime="8.10649e+00" mtime="2.78481e+01" gflop="0.00000e+00" gbyte="3.76369e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78481e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000fc15fc1550" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51116e+01" utime="4.71372e+01" stime="8.10218e+00" mtime="2.78481e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78481e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4957e+08" > 5.9609e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4908e+08" > 3.9890e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4842e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4310e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8277e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.2776e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4771e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1817e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0455e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1216e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7575e+01 </func>
</region>
</regions>
<internal rank="400" log_i="1723712895.679779" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="401" mpi_size="768" stamp_init="1723712830.433465" stamp_final="1723712895.694731" username="apac4" allocationname="unknown" flags="0" pid="691118" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52613e+01" utime="4.88103e+01" stime="7.61526e+00" mtime="2.85594e+01" gflop="0.00000e+00" gbyte="3.77510e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85594e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf46314641465141c5565146514c6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51269e+01" utime="4.87713e+01" stime="7.61105e+00" mtime="2.85594e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85594e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5003e+08" > 4.8001e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5027e+08" > 3.0459e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8058e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4517e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8849e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4827e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4775e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1764e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0452e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1214e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8072e+01 </func>
</region>
</regions>
<internal rank="401" log_i="1723712895.694731" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="402" mpi_size="768" stamp_init="1723712830.433540" stamp_final="1723712895.695364" username="apac4" allocationname="unknown" flags="0" pid="691119" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52618e+01" utime="4.68265e+01" stime="8.36161e+00" mtime="2.80771e+01" gflop="0.00000e+00" gbyte="3.77094e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80771e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51259e+01" utime="4.67891e+01" stime="8.35669e+00" mtime="2.80771e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80771e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4886e+08" > 5.7777e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4931e+08" > 5.0460e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1946e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4641e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7309e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.2995e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4771e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1786e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0425e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1216e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8070e+01 </func>
</region>
</regions>
<internal rank="402" log_i="1723712895.695364" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="403" mpi_size="768" stamp_init="1723712830.433494" stamp_final="1723712895.690451" username="apac4" allocationname="unknown" flags="0" pid="691120" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52570e+01" utime="4.87313e+01" stime="7.68855e+00" mtime="2.86935e+01" gflop="0.00000e+00" gbyte="3.75862e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86935e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009515bd559515951524" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51218e+01" utime="4.86902e+01" stime="7.68753e+00" mtime="2.86935e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86935e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4929e+08" > 4.8193e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4876e+08" > 3.3047e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3506e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4686e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3203e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4771e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1797e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0435e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1215e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8639e+01 </func>
</region>
</regions>
<internal rank="403" log_i="1723712895.690451" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="404" mpi_size="768" stamp_init="1723712830.433590" stamp_final="1723712895.695424" username="apac4" allocationname="unknown" flags="0" pid="691121" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52618e+01" utime="4.69389e+01" stime="8.16500e+00" mtime="2.77882e+01" gflop="0.00000e+00" gbyte="3.77663e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77882e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f214f11485" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51267e+01" utime="4.69045e+01" stime="8.15645e+00" mtime="2.77882e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77882e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4911e+08" > 5.8803e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4988e+08" > 3.8611e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1498e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4479e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0838e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6081e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4760e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1910e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0458e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1213e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7843e+01 </func>
</region>
</regions>
<internal rank="404" log_i="1723712895.695424" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="405" mpi_size="768" stamp_init="1723712830.433482" stamp_final="1723712895.685750" username="apac4" allocationname="unknown" flags="0" pid="691122" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52523e+01" utime="4.88958e+01" stime="7.52536e+00" mtime="2.86111e+01" gflop="0.00000e+00" gbyte="3.76419e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86111e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c615c715c915b355c915c81511" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51196e+01" utime="4.88591e+01" stime="7.51937e+00" mtime="2.86111e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86111e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4955e+08" > 4.8134e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4881e+08" > 3.1038e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4052e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4580e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3140e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.7931e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4765e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1865e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0453e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1213e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8516e+01 </func>
</region>
</regions>
<internal rank="405" log_i="1723712895.685750" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="406" mpi_size="768" stamp_init="1723712830.433564" stamp_final="1723712895.690124" username="apac4" allocationname="unknown" flags="0" pid="691123" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52566e+01" utime="4.70895e+01" stime="7.99993e+00" mtime="2.75835e+01" gflop="0.00000e+00" gbyte="3.75896e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.75835e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51216e+01" utime="4.70537e+01" stime="7.99263e+00" mtime="2.75835e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.75835e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4862e+08" > 5.7706e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4788e+08" > 4.5356e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2657e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4288e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5259e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.5291e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4762e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1893e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0466e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1216e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7545e+01 </func>
</region>
</regions>
<internal rank="406" log_i="1723712895.690124" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="407" mpi_size="768" stamp_init="1723712830.433490" stamp_final="1723712895.680410" username="apac4" allocationname="unknown" flags="0" pid="691124" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13b</host>
<perf wtime="6.52469e+01" utime="4.87235e+01" stime="7.66918e+00" mtime="2.83937e+01" gflop="0.00000e+00" gbyte="3.77575e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83937e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4121514151515b2551515141524" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51104e+01" utime="4.86888e+01" stime="7.66127e+00" mtime="2.83937e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83937e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4763e+08" > 4.8017e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4975e+08" > 3.2113e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4450e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4594e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8849e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.8120e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4750e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2032e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0422e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1212e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8258e+01 </func>
</region>
</regions>
<internal rank="407" log_i="1723712895.680410" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="408" mpi_size="768" stamp_init="1723712830.447068" stamp_final="1723712895.684545" username="apac4" allocationname="unknown" flags="0" pid="1090888" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52375e+01" utime="4.32879e+01" stime="1.25210e+01" mtime="2.88634e+01" gflop="0.00000e+00" gbyte="3.87100e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88634e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000951595152a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50996e+01" utime="4.32596e+01" stime="1.25062e+01" mtime="2.88634e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88634e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4859e+08" > 5.1660e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4857e+08" > 3.9520e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1714e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8633e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5214e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.4731e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4752e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1975e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0503e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1224e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7549e+01 </func>
</region>
</regions>
<internal rank="408" log_i="1723712895.684545" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="409" mpi_size="768" stamp_init="1723712830.447065" stamp_final="1723712895.682278" username="apac4" allocationname="unknown" flags="0" pid="1090889" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52352e+01" utime="4.90610e+01" stime="7.70204e+00" mtime="2.92870e+01" gflop="0.00000e+00" gbyte="3.77369e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92870e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50920e+01" utime="4.90238e+01" stime="7.69522e+00" mtime="2.92870e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92870e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4837e+08" > 4.6401e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4794e+08" > 3.1483e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7522e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8910e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0266e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.1727e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4760e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1927e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0463e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1225e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8425e+01 </func>
</region>
</regions>
<internal rank="409" log_i="1723712895.682278" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="410" mpi_size="768" stamp_init="1723712830.446932" stamp_final="1723712895.694533" username="apac4" allocationname="unknown" flags="0" pid="1090890" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52476e+01" utime="4.76813e+01" stime="8.14367e+00" mtime="2.88754e+01" gflop="0.00000e+00" gbyte="3.77293e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88754e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51085e+01" utime="4.76563e+01" stime="8.12579e+00" mtime="2.88754e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88754e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4850e+08" > 5.5573e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4922e+08" > 3.6829e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3953e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8736e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1386e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5282e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4759e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1935e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0445e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1229e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8290e+01 </func>
</region>
</regions>
<internal rank="410" log_i="1723712895.694533" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="411" mpi_size="768" stamp_init="1723712830.446862" stamp_final="1723712895.683280" username="apac4" allocationname="unknown" flags="0" pid="1090891" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52364e+01" utime="4.90575e+01" stime="7.70769e+00" mtime="2.92195e+01" gflop="0.00000e+00" gbyte="3.74012e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92195e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006e146e148e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50994e+01" utime="4.90235e+01" stime="7.69894e+00" mtime="2.92195e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92195e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4906e+08" > 4.6327e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4768e+08" > 3.3406e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1714e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8513e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3855e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5373e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4752e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2008e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0460e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1182e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8977e+01 </func>
</region>
</regions>
<internal rank="411" log_i="1723712895.683280" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="412" mpi_size="768" stamp_init="1723712830.446463" stamp_final="1723712895.695172" username="apac4" allocationname="unknown" flags="0" pid="1090892" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52487e+01" utime="4.50246e+01" stime="9.22759e+00" mtime="2.83704e+01" gflop="0.00000e+00" gbyte="3.77342e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83704e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51117e+01" utime="4.49926e+01" stime="9.21621e+00" mtime="2.83704e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83704e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4768e+08" > 8.5095e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4897e+08" > 8.8742e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1966e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8725e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.0819e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.4709e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4752e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.1956e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0463e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1223e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7633e+01 </func>
</region>
</regions>
<internal rank="412" log_i="1723712895.695172" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="413" mpi_size="768" stamp_init="1723712830.447009" stamp_final="1723712895.689006" username="apac4" allocationname="unknown" flags="0" pid="1090893" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52420e+01" utime="4.92925e+01" stime="7.40648e+00" mtime="2.89762e+01" gflop="0.00000e+00" gbyte="3.76221e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89762e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51040e+01" utime="4.92588e+01" stime="7.39787e+00" mtime="2.89762e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89762e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4797e+08" > 4.5533e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4677e+08" > 2.9958e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3953e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8474e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9789e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.8232e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4753e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2007e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0484e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1226e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8521e+01 </func>
</region>
</regions>
<internal rank="413" log_i="1723712895.689006" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="414" mpi_size="768" stamp_init="1723712830.446926" stamp_final="1723712895.677384" username="apac4" allocationname="unknown" flags="0" pid="1090894" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52305e+01" utime="4.65269e+01" stime="8.53609e+00" mtime="2.78736e+01" gflop="0.00000e+00" gbyte="3.77842e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78736e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50923e+01" utime="4.64958e+01" stime="8.52407e+00" mtime="2.78736e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78736e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4855e+08" > 6.5001e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4806e+08" > 4.7112e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7610e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8783e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.4850e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.4802e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4742e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2075e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0501e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1228e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7812e+01 </func>
</region>
</regions>
<internal rank="414" log_i="1723712895.677384" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="415" mpi_size="768" stamp_init="1723712830.448659" stamp_final="1723712895.685351" username="apac4" allocationname="unknown" flags="0" pid="1090895" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52367e+01" utime="4.89800e+01" stime="7.76263e+00" mtime="2.91754e+01" gflop="0.00000e+00" gbyte="3.75340e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91754e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000012144655121412149b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51020e+01" utime="4.89541e+01" stime="7.74570e+00" mtime="2.91754e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91754e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4809e+08" > 4.5665e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4982e+08" > 2.6377e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6588e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8573e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4305e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.4683e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4742e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2076e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0066e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1229e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8452e+01 </func>
</region>
</regions>
<internal rank="415" log_i="1723712895.685351" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="416" mpi_size="768" stamp_init="1723712830.446318" stamp_final="1723712895.694362" username="apac4" allocationname="unknown" flags="0" pid="1090896" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52480e+01" utime="4.55746e+01" stime="8.99360e+00" mtime="2.83490e+01" gflop="0.00000e+00" gbyte="3.76686e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83490e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000121466561214121492" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51093e+01" utime="4.55395e+01" stime="8.98627e+00" mtime="2.83490e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83490e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4867e+08" > 5.8746e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4932e+08" > 5.4732e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5081e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7147e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6805e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.3248e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4734e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2180e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0454e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1229e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7589e+01 </func>
</region>
</regions>
<internal rank="416" log_i="1723712895.694362" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="417" mpi_size="768" stamp_init="1723712830.446329" stamp_final="1723712895.684126" username="apac4" allocationname="unknown" flags="0" pid="1090897" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52378e+01" utime="4.93267e+01" stime="7.38705e+00" mtime="2.90795e+01" gflop="0.00000e+00" gbyte="3.75134e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90795e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51015e+01" utime="4.92973e+01" stime="7.37361e+00" mtime="2.90795e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90795e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4822e+08" > 3.5540e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4905e+08" > 2.8782e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2273e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8709e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2187e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.0353e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4722e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2319e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0486e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1226e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8873e+01 </func>
</region>
</regions>
<internal rank="417" log_i="1723712895.684126" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="418" mpi_size="768" stamp_init="1723712830.446615" stamp_final="1723712895.694067" username="apac4" allocationname="unknown" flags="0" pid="1090898" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52475e+01" utime="4.75250e+01" stime="8.24496e+00" mtime="2.88668e+01" gflop="0.00000e+00" gbyte="3.76324e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88668e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003c1410553c143c14a2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51099e+01" utime="4.74825e+01" stime="8.24493e+00" mtime="2.88668e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88668e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4976e+08" > 5.1588e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5017e+08" > 3.5123e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7153e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8737e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3503e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6188e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4729e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2253e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0442e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1229e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8001e+01 </func>
</region>
</regions>
<internal rank="418" log_i="1723712895.694067" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="419" mpi_size="768" stamp_init="1723712830.448346" stamp_final="1723712895.681935" username="apac4" allocationname="unknown" flags="0" pid="1090899" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52336e+01" utime="4.94672e+01" stime="7.27481e+00" mtime="2.90364e+01" gflop="0.00000e+00" gbyte="3.77785e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90364e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50948e+01" utime="4.94331e+01" stime="7.26594e+00" mtime="2.90364e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90364e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5028e+08" > 3.6520e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5047e+08" > 2.6606e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2360e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8777e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5659e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4722e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2321e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0478e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1229e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8807e+01 </func>
</region>
</regions>
<internal rank="419" log_i="1723712895.681935" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="420" mpi_size="768" stamp_init="1723712830.446989" stamp_final="1723712895.682192" username="apac4" allocationname="unknown" flags="0" pid="1090900" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52352e+01" utime="4.70158e+01" stime="8.39579e+00" mtime="2.85940e+01" gflop="0.00000e+00" gbyte="3.78117e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85940e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50997e+01" utime="4.69820e+01" stime="8.38674e+00" mtime="2.85940e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85940e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4982e+08" > 4.5106e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4877e+08" > 4.3293e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8672e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8660e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6022e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.5695e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4720e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2325e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0502e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1230e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7640e+01 </func>
</region>
</regions>
<internal rank="420" log_i="1723712895.682192" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="421" mpi_size="768" stamp_init="1723712830.447555" stamp_final="1723712895.686813" username="apac4" allocationname="unknown" flags="0" pid="1090901" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52393e+01" utime="4.91875e+01" stime="7.61302e+00" mtime="2.93278e+01" gflop="0.00000e+00" gbyte="3.77213e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.93278e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000dc143856dc14db14b5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51025e+01" utime="4.91537e+01" stime="7.60396e+00" mtime="2.93278e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.93278e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4832e+08" > 3.6742e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4838e+08" > 2.6038e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7387e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8737e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.6601e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4722e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2318e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0493e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1226e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8602e+01 </func>
</region>
</regions>
<internal rank="421" log_i="1723712895.686813" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="422" mpi_size="768" stamp_init="1723712830.448247" stamp_final="1723712895.688261" username="apac4" allocationname="unknown" flags="0" pid="1090902" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52400e+01" utime="4.80253e+01" stime="8.00900e+00" mtime="2.87521e+01" gflop="0.00000e+00" gbyte="3.77544e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87521e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44e14501451148a565114501493" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51048e+01" utime="4.79936e+01" stime="7.99811e+00" mtime="2.87521e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87521e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4883e+08" > 4.2973e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4828e+08" > 3.5526e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7293e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8728e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.4598e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.1910e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4717e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2329e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0396e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1228e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7958e+01 </func>
</region>
</regions>
<internal rank="422" log_i="1723712895.688261" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="423" mpi_size="768" stamp_init="1723712830.446323" stamp_final="1723712895.688559" username="apac4" allocationname="unknown" flags="0" pid="1090903" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52422e+01" utime="4.92374e+01" stime="7.58201e+00" mtime="2.89803e+01" gflop="0.00000e+00" gbyte="3.76873e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89803e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a814a814a9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51027e+01" utime="4.92057e+01" stime="7.57108e+00" mtime="2.89803e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89803e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4926e+08" > 3.5348e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4830e+08" > 2.6766e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4920e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8862e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9802e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.0201e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4712e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2395e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0503e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1222e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8503e+01 </func>
</region>
</regions>
<internal rank="423" log_i="1723712895.688559" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="424" mpi_size="768" stamp_init="1723712830.446335" stamp_final="1723712895.694416" username="apac4" allocationname="unknown" flags="0" pid="1090904" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52481e+01" utime="4.69624e+01" stime="8.37490e+00" mtime="2.83271e+01" gflop="0.00000e+00" gbyte="3.76793e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83271e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004c15a4564c154b1522" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51118e+01" utime="4.69375e+01" stime="8.35690e+00" mtime="2.83271e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83271e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4845e+08" > 6.5938e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4891e+08" > 4.7712e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4669e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8642e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2784e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6748e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4704e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2438e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0474e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1230e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7564e+01 </func>
</region>
</regions>
<internal rank="424" log_i="1723712895.694416" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="425" mpi_size="768" stamp_init="1723712830.447021" stamp_final="1723712895.680468" username="apac4" allocationname="unknown" flags="0" pid="1090905" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52334e+01" utime="4.93869e+01" stime="7.37911e+00" mtime="2.86221e+01" gflop="0.00000e+00" gbyte="3.76671e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86221e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50962e+01" utime="4.93506e+01" stime="7.37216e+00" mtime="2.86221e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86221e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4762e+08" > 4.9029e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4777e+08" > 3.4638e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9952e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8596e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5988e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.8033e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4703e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2517e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0441e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1221e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8518e+01 </func>
</region>
</regions>
<internal rank="425" log_i="1723712895.680468" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="426" mpi_size="768" stamp_init="1723712830.448610" stamp_final="1723712895.689222" username="apac4" allocationname="unknown" flags="0" pid="1090906" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52406e+01" utime="4.70833e+01" stime="8.19862e+00" mtime="2.81590e+01" gflop="0.00000e+00" gbyte="3.77842e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81590e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51023e+01" utime="4.70538e+01" stime="8.18535e+00" mtime="2.81590e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81590e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4767e+08" > 6.6318e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4903e+08" > 4.6939e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1925e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8660e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0463e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7557e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4705e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2480e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0444e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1230e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7664e+01 </func>
</region>
</regions>
<internal rank="426" log_i="1723712895.689222" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="427" mpi_size="768" stamp_init="1723712830.446968" stamp_final="1723712895.686553" username="apac4" allocationname="unknown" flags="0" pid="1090907" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52396e+01" utime="4.92513e+01" stime="7.50532e+00" mtime="2.92620e+01" gflop="0.00000e+00" gbyte="3.76389e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92620e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf419141b141c14e5551c141c1461" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51028e+01" utime="4.92217e+01" stime="7.49230e+00" mtime="2.92620e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92620e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4844e+08" > 4.6070e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4879e+08" > 3.1921e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4001e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8702e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6891e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4705e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2486e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0471e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1221e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8775e+01 </func>
</region>
</regions>
<internal rank="427" log_i="1723712895.686553" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="428" mpi_size="768" stamp_init="1723712830.446327" stamp_final="1723712895.683875" username="apac4" allocationname="unknown" flags="0" pid="1090908" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52375e+01" utime="4.65694e+01" stime="8.55961e+00" mtime="2.81316e+01" gflop="0.00000e+00" gbyte="3.74596e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81316e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51002e+01" utime="4.65405e+01" stime="8.54523e+00" mtime="2.81316e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81316e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4787e+08" > 6.5043e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4860e+08" > 4.4113e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3750e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8698e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.4618e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7178e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4703e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2507e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0441e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1231e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7468e+01 </func>
</region>
</regions>
<internal rank="428" log_i="1723712895.683875" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="429" mpi_size="768" stamp_init="1723712830.446335" stamp_final="1723712895.680485" username="apac4" allocationname="unknown" flags="0" pid="1090909" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52341e+01" utime="4.88175e+01" stime="7.74717e+00" mtime="2.90039e+01" gflop="0.00000e+00" gbyte="3.76308e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90039e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50977e+01" utime="4.87861e+01" stime="7.73561e+00" mtime="2.90039e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90039e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4940e+08" > 4.8092e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4804e+08" > 3.0345e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3853e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8645e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2187e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7549e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4696e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2551e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0451e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1222e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8519e+01 </func>
</region>
</regions>
<internal rank="429" log_i="1723712895.680485" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="430" mpi_size="768" stamp_init="1723712830.448092" stamp_final="1723712895.688221" username="apac4" allocationname="unknown" flags="0" pid="1090910" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52401e+01" utime="4.72497e+01" stime="8.27640e+00" mtime="2.83022e+01" gflop="0.00000e+00" gbyte="3.76076e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83022e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51031e+01" utime="4.72149e+01" stime="8.26892e+00" mtime="2.83022e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83022e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4889e+08" > 5.8367e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5008e+08" > 5.0027e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7337e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8604e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7101e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9209e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4695e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2551e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0471e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1229e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7348e+01 </func>
</region>
</regions>
<internal rank="430" log_i="1723712895.688221" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="431" mpi_size="768" stamp_init="1723712830.446627" stamp_final="1723712895.680334" username="apac4" allocationname="unknown" flags="0" pid="1090911" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u06a</host>
<perf wtime="6.52337e+01" utime="4.88439e+01" stime="7.59155e+00" mtime="2.88633e+01" gflop="0.00000e+00" gbyte="3.74149e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88633e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009e159e151e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50951e+01" utime="4.88133e+01" stime="7.58000e+00" mtime="2.88633e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88633e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4909e+08" > 4.7392e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4985e+08" > 2.7925e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2404e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8701e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9402e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4691e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2639e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0447e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1225e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8527e+01 </func>
</region>
</regions>
<internal rank="431" log_i="1723712895.680334" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="432" mpi_size="768" stamp_init="1723712830.473254" stamp_final="1723712895.684786" username="apac4" allocationname="unknown" flags="0" pid="843832" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52115e+01" utime="4.19015e+01" stime="1.28716e+01" mtime="2.80356e+01" gflop="0.00000e+00" gbyte="3.84834e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80356e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50841e+01" utime="4.18658e+01" stime="1.28657e+01" mtime="2.80356e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80356e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4853e+08" > 6.6155e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4849e+08" > 6.0476e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2770e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7013e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8411e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.4151e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4664e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2847e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0498e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1236e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7606e+01 </func>
</region>
</regions>
<internal rank="432" log_i="1723712895.684786" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="433" mpi_size="768" stamp_init="1723712830.474047" stamp_final="1723712895.681237" username="apac4" allocationname="unknown" flags="0" pid="843833" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52072e+01" utime="4.93698e+01" stime="7.29480e+00" mtime="2.82742e+01" gflop="0.00000e+00" gbyte="3.76625e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82742e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000931493149a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50732e+01" utime="4.93406e+01" stime="7.28063e+00" mtime="2.82742e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82742e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4921e+08" > 4.7077e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4945e+08" > 3.1945e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.3641e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7171e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9851e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4694e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2578e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0489e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1232e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8970e+01 </func>
</region>
</regions>
<internal rank="433" log_i="1723712895.681237" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="434" mpi_size="768" stamp_init="1723712830.473465" stamp_final="1723712895.679582" username="apac4" allocationname="unknown" flags="0" pid="843834" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52061e+01" utime="4.73012e+01" stime="8.11633e+00" mtime="2.83325e+01" gflop="0.00000e+00" gbyte="3.77720e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83325e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf473157515761552557615751540" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50782e+01" utime="4.72696e+01" stime="8.10529e+00" mtime="2.83325e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83325e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5104e+08" > 5.9400e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4959e+08" > 4.4411e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1047e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7081e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.2932e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8732e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4693e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2592e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0515e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1236e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8153e+01 </func>
</region>
</regions>
<internal rank="434" log_i="1723712895.679582" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="435" mpi_size="768" stamp_init="1723712830.474399" stamp_final="1723712895.692310" username="apac4" allocationname="unknown" flags="0" pid="843835" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52179e+01" utime="4.92834e+01" stime="7.39527e+00" mtime="2.85944e+01" gflop="0.00000e+00" gbyte="3.76968e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85944e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006215611520" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50880e+01" utime="4.92545e+01" stime="7.38201e+00" mtime="2.85944e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85944e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4949e+08" > 4.6309e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5063e+08" > 3.1115e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7175e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6888e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8908e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4683e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2669e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0501e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1229e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8966e+01 </func>
</region>
</regions>
<internal rank="435" log_i="1723712895.692310" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="436" mpi_size="768" stamp_init="1723712830.473101" stamp_final="1723712895.689667" username="apac4" allocationname="unknown" flags="0" pid="843836" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52166e+01" utime="4.71382e+01" stime="8.37144e+00" mtime="2.80034e+01" gflop="0.00000e+00" gbyte="3.74771e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80034e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4581459145a1463555a145a148e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50875e+01" utime="4.71039e+01" stime="8.36330e+00" mtime="2.80034e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80034e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4926e+08" > 5.5809e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5037e+08" > 3.7439e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3866e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7154e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6302e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.4061e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4682e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2717e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0496e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1235e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7575e+01 </func>
</region>
</regions>
<internal rank="436" log_i="1723712895.689667" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="437" mpi_size="768" stamp_init="1723712830.474792" stamp_final="1723712895.679980" username="apac4" allocationname="unknown" flags="0" pid="843837" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52052e+01" utime="4.92670e+01" stime="7.46958e+00" mtime="2.86331e+01" gflop="0.00000e+00" gbyte="3.76217e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86331e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000097143c5597149714d0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50776e+01" utime="4.92352e+01" stime="7.45981e+00" mtime="2.86331e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86331e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5082e+08" > 4.5522e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4775e+08" > 3.2298e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9024e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7302e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.5034e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.7341e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4681e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2681e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0490e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1231e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8791e+01 </func>
</region>
</regions>
<internal rank="437" log_i="1723712895.679980" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="438" mpi_size="768" stamp_init="1723712830.475059" stamp_final="1723712895.690021" username="apac4" allocationname="unknown" flags="0" pid="843838" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52150e+01" utime="4.48854e+01" stime="9.15432e+00" mtime="2.77446e+01" gflop="0.00000e+00" gbyte="3.75740e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77446e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4741475147714a75677147614db" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50859e+01" utime="4.48474e+01" stime="9.15006e+00" mtime="2.77446e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77446e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4802e+08" > 8.5574e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5002e+08" > 5.6432e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0594e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7072e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.6968e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.4199e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4678e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2719e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0503e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1184e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7339e+01 </func>
</region>
</regions>
<internal rank="438" log_i="1723712895.690021" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="439" mpi_size="768" stamp_init="1723712830.473025" stamp_final="1723712895.683127" username="apac4" allocationname="unknown" flags="0" pid="843839" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52101e+01" utime="4.87901e+01" stime="7.60100e+00" mtime="2.85337e+01" gflop="0.00000e+00" gbyte="3.77491e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85337e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005615d855561555154a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50796e+01" utime="4.87518e+01" stime="7.59657e+00" mtime="2.85337e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85337e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4997e+08" > 4.4491e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4897e+08" > 3.1028e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1664e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7165e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1013e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4676e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2772e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0483e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1230e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8446e+01 </func>
</region>
</regions>
<internal rank="439" log_i="1723712895.683127" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="440" mpi_size="768" stamp_init="1723712830.473113" stamp_final="1723712895.686519" username="apac4" allocationname="unknown" flags="0" pid="843840" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52134e+01" utime="4.72000e+01" stime="8.45230e+00" mtime="2.86474e+01" gflop="0.00000e+00" gbyte="3.76423e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86474e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c3149055c314c314fa" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50837e+01" utime="4.71708e+01" stime="8.43955e+00" mtime="2.86474e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86474e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4542e+08" > 5.4570e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4892e+08" > 3.9819e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0303e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7023e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3881e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.4151e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4671e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2842e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0514e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1235e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7600e+01 </func>
</region>
</regions>
<internal rank="440" log_i="1723712895.686519" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="441" mpi_size="768" stamp_init="1723712830.473114" stamp_final="1723712895.680296" username="apac4" allocationname="unknown" flags="0" pid="843841" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52072e+01" utime="4.90852e+01" stime="7.42332e+00" mtime="2.87363e+01" gflop="0.00000e+00" gbyte="3.76942e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87363e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000e914c555e914e914bc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50770e+01" utime="4.90597e+01" stime="7.40606e+00" mtime="2.87363e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87363e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4820e+08" > 4.5576e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4711e+08" > 2.8956e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9915e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7270e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1206e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8881e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4671e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2830e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0497e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1230e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8811e+01 </func>
</region>
</regions>
<internal rank="441" log_i="1723712895.680296" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="442" mpi_size="768" stamp_init="1723712830.473276" stamp_final="1723712895.679050" username="apac4" allocationname="unknown" flags="0" pid="843842" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52058e+01" utime="4.71204e+01" stime="8.33090e+00" mtime="2.84453e+01" gflop="0.00000e+00" gbyte="3.76350e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84453e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b414b414f7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50766e+01" utime="4.70872e+01" stime="8.32187e+00" mtime="2.84453e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84453e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4769e+08" > 5.9024e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4901e+08" > 4.1730e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3353e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7008e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.3181e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.4478e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4669e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2814e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0486e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1232e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8050e+01 </func>
</region>
</regions>
<internal rank="442" log_i="1723712895.679050" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="443" mpi_size="768" stamp_init="1723712830.473038" stamp_final="1723712895.685907" username="apac4" allocationname="unknown" flags="0" pid="843843" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52129e+01" utime="4.92119e+01" stime="7.61212e+00" mtime="2.90427e+01" gflop="0.00000e+00" gbyte="3.76385e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90427e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003915391505" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50813e+01" utime="4.91795e+01" stime="7.60268e+00" mtime="2.90427e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90427e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4871e+08" > 4.5067e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4776e+08" > 2.7658e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4065e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7235e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7820e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4672e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2818e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0480e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1229e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8716e+01 </func>
</region>
</regions>
<internal rank="443" log_i="1723712895.685907" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="444" mpi_size="768" stamp_init="1723712830.473389" stamp_final="1723712895.679406" username="apac4" allocationname="unknown" flags="0" pid="843844" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52060e+01" utime="4.45524e+01" stime="9.26363e+00" mtime="2.80569e+01" gflop="0.00000e+00" gbyte="3.77781e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80569e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50760e+01" utime="4.45182e+01" stime="9.25436e+00" mtime="2.80569e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80569e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4812e+08" > 1.0047e+00 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4774e+08" > 7.6719e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2333e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6937e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.2030e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.5592e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4657e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2905e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0498e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1232e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7321e+01 </func>
</region>
</regions>
<internal rank="444" log_i="1723712895.679406" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="445" mpi_size="768" stamp_init="1723712830.473032" stamp_final="1723712895.678382" username="apac4" allocationname="unknown" flags="0" pid="843845" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52053e+01" utime="4.88300e+01" stime="7.94447e+00" mtime="2.92051e+01" gflop="0.00000e+00" gbyte="3.77941e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92051e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4171418141a14f5551a141914f3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50760e+01" utime="4.87970e+01" stime="7.93415e+00" mtime="2.92051e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92051e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4787e+08" > 4.6401e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4902e+08" > 3.1008e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7432e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7257e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7180e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.1396e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4657e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2942e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0479e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1229e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8523e+01 </func>
</region>
</regions>
<internal rank="445" log_i="1723712895.678382" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="446" mpi_size="768" stamp_init="1723712830.473192" stamp_final="1723712895.682589" username="apac4" allocationname="unknown" flags="0" pid="843846" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52094e+01" utime="4.65869e+01" stime="8.81154e+00" mtime="2.86514e+01" gflop="0.00000e+00" gbyte="3.76923e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86514e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006e146e1459" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50801e+01" utime="4.65571e+01" stime="8.79859e+00" mtime="2.86514e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86514e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.4782e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4813e+08" > 6.4007e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4878e+08" > 5.3989e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.1313e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7227e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.1008e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.6345e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4664e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2891e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0508e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1234e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7375e+01 </func>
</region>
</regions>
<internal rank="446" log_i="1723712895.682589" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="447" mpi_size="768" stamp_init="1723712830.473714" stamp_final="1723712895.688915" username="apac4" allocationname="unknown" flags="0" pid="843847" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52152e+01" utime="4.91504e+01" stime="7.65510e+00" mtime="2.88737e+01" gflop="0.00000e+00" gbyte="3.77392e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88737e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf439143b143c14c3553c143c14f4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50846e+01" utime="4.91156e+01" stime="7.64770e+00" mtime="2.88737e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88737e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4901e+08" > 4.6832e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4784e+08" > 2.7948e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6865e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6962e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2159e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0087e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4658e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.2962e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0494e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1235e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8275e+01 </func>
</region>
</regions>
<internal rank="447" log_i="1723712895.688915" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="448" mpi_size="768" stamp_init="1723712830.473112" stamp_final="1723712895.694711" username="apac4" allocationname="unknown" flags="0" pid="843848" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52216e+01" utime="4.70057e+01" stime="8.50540e+00" mtime="2.81627e+01" gflop="0.00000e+00" gbyte="3.77129e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81627e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005e145d14ff" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50928e+01" utime="4.69699e+01" stime="8.49873e+00" mtime="2.81627e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81627e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4869e+08" > 7.7950e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4898e+08" > 4.4961e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0333e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3281e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.7924e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9789e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4643e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3085e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0481e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1232e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7878e+01 </func>
</region>
</regions>
<internal rank="448" log_i="1723712895.694711" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="449" mpi_size="768" stamp_init="1723712830.474928" stamp_final="1723712895.687494" username="apac4" allocationname="unknown" flags="0" pid="843849" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52126e+01" utime="4.90706e+01" stime="7.78452e+00" mtime="2.86097e+01" gflop="0.00000e+00" gbyte="3.75690e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86097e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50842e+01" utime="4.90358e+01" stime="7.77739e+00" mtime="2.86097e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86097e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5030e+08" > 5.5187e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4869e+08" > 2.7735e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8720e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6897e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0233e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4641e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3095e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0490e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1228e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8740e+01 </func>
</region>
</regions>
<internal rank="449" log_i="1723712895.687494" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="450" mpi_size="768" stamp_init="1723712830.473092" stamp_final="1723712895.684804" username="apac4" allocationname="unknown" flags="0" pid="843850" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52117e+01" utime="4.69152e+01" stime="8.56989e+00" mtime="2.86042e+01" gflop="0.00000e+00" gbyte="3.74245e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86042e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c415c4150b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50825e+01" utime="4.68843e+01" stime="8.55792e+00" mtime="2.86042e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86042e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4842e+08" > 7.2147e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4779e+08" > 3.7746e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4859e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7093e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1229e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.4600e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4634e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3174e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0515e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1234e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7920e+01 </func>
</region>
</regions>
<internal rank="450" log_i="1723712895.684804" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="451" mpi_size="768" stamp_init="1723712830.473034" stamp_final="1723712895.687431" username="apac4" allocationname="unknown" flags="0" pid="843851" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52144e+01" utime="4.93111e+01" stime="7.49369e+00" mtime="2.86957e+01" gflop="0.00000e+00" gbyte="3.76324e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86957e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43d153e153f152a563f153f1524" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50821e+01" utime="4.92769e+01" stime="7.48564e+00" mtime="2.86957e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86957e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4733e+08" > 5.5332e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5054e+08" > 2.9130e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8847e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7128e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.4712e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4625e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3262e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0479e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1228e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8788e+01 </func>
</region>
</regions>
<internal rank="451" log_i="1723712895.687431" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="452" mpi_size="768" stamp_init="1723712830.473089" stamp_final="1723712895.679522" username="apac4" allocationname="unknown" flags="0" pid="843852" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52064e+01" utime="4.58953e+01" stime="8.78847e+00" mtime="2.80448e+01" gflop="0.00000e+00" gbyte="3.76854e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80448e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4fa14fc14fd14f155fd14fd14ef" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50777e+01" utime="4.58602e+01" stime="8.78096e+00" mtime="2.80448e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80448e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5106e+08" > 9.0321e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4884e+08" > 5.4113e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0196e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7063e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4329e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.4702e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4625e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3248e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0486e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1235e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7634e+01 </func>
</region>
</regions>
<internal rank="452" log_i="1723712895.679522" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="453" mpi_size="768" stamp_init="1723712830.473472" stamp_final="1723712895.695801" username="apac4" allocationname="unknown" flags="0" pid="843853" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52223e+01" utime="4.91500e+01" stime="7.64541e+00" mtime="2.84672e+01" gflop="0.00000e+00" gbyte="3.75458e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84672e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50912e+01" utime="4.91182e+01" stime="7.63473e+00" mtime="2.84672e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84672e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4817e+08" > 5.5879e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4790e+08" > 3.1294e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8752e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7101e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.1203e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4622e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3293e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0478e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1228e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8572e+01 </func>
</region>
</regions>
<internal rank="453" log_i="1723712895.695801" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="454" mpi_size="768" stamp_init="1723712830.473272" stamp_final="1723712895.696892" username="apac4" allocationname="unknown" flags="0" pid="843854" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52236e+01" utime="4.62800e+01" stime="8.71937e+00" mtime="2.79816e+01" gflop="0.00000e+00" gbyte="3.76675e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79816e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008a15891502" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50940e+01" utime="4.62495e+01" stime="8.70743e+00" mtime="2.79816e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79816e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4784e+08" > 8.4954e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4816e+08" > 5.1805e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7592e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7001e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7173e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8310e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4622e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3318e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0490e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1233e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7890e+01 </func>
</region>
</regions>
<internal rank="454" log_i="1723712895.696892" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="455" mpi_size="768" stamp_init="1723712830.473022" stamp_final="1723712895.685785" username="apac4" allocationname="unknown" flags="0" pid="843855" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u12a</host>
<perf wtime="6.52128e+01" utime="4.93196e+01" stime="7.53912e+00" mtime="2.81981e+01" gflop="0.00000e+00" gbyte="3.76545e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81981e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a2157455a215a21545" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50819e+01" utime="4.92897e+01" stime="7.52647e+00" mtime="2.81981e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81981e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4906e+08" > 5.7093e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4978e+08" > 2.8569e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.6577e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7079e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8171e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4614e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3405e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0499e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1227e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8505e+01 </func>
</region>
</regions>
<internal rank="455" log_i="1723712895.685785" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="456" mpi_size="768" stamp_init="1723712830.391065" stamp_final="1723712895.684396" username="apac4" allocationname="unknown" flags="0" pid="1046732" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52933e+01" utime="4.28146e+01" stime="1.25582e+01" mtime="2.77922e+01" gflop="0.00000e+00" gbyte="3.85952e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77922e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf428142a142b142f552b142b1476" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51654e+01" utime="4.27811e+01" stime="1.25492e+01" mtime="2.77922e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77922e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4827e+08" > 5.7858e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4870e+08" > 3.4676e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4777e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2836e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6661e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6441e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4616e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3313e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0422e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1257e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7696e+01 </func>
</region>
</regions>
<internal rank="456" log_i="1723712895.684396" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="457" mpi_size="768" stamp_init="1723712830.391700" stamp_final="1723712895.684289" username="apac4" allocationname="unknown" flags="0" pid="1046733" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52926e+01" utime="4.88630e+01" stime="7.47691e+00" mtime="2.81930e+01" gflop="0.00000e+00" gbyte="3.77567e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81930e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007f144a557f147f14b1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51605e+01" utime="4.88370e+01" stime="7.45982e+00" mtime="2.81930e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81930e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4776e+08" > 4.6396e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4834e+08" > 3.2203e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1330e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2843e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2629e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4627e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3236e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0435e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1259e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8554e+01 </func>
</region>
</regions>
<internal rank="457" log_i="1723712895.684289" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="458" mpi_size="768" stamp_init="1723712830.391559" stamp_final="1723712895.687886" username="apac4" allocationname="unknown" flags="0" pid="1046734" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52963e+01" utime="4.73800e+01" stime="7.94591e+00" mtime="2.78449e+01" gflop="0.00000e+00" gbyte="3.78075e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78449e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44e154f155015fb555015501545" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51680e+01" utime="4.73496e+01" stime="7.93380e+00" mtime="2.78449e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78449e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4851e+08" > 5.3393e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4819e+08" > 3.9004e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1539e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2807e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.2025e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.1756e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4625e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3253e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0401e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1259e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8105e+01 </func>
</region>
</regions>
<internal rank="458" log_i="1723712895.687886" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="459" mpi_size="768" stamp_init="1723712830.391628" stamp_final="1723712895.685166" username="apac4" allocationname="unknown" flags="0" pid="1046735" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52935e+01" utime="4.86902e+01" stime="7.54419e+00" mtime="2.82715e+01" gflop="0.00000e+00" gbyte="3.77213e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82715e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf42a142b142c142e552c142c14d8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51655e+01" utime="4.86570e+01" stime="7.53531e+00" mtime="2.82715e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82715e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4925e+08" > 4.4071e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4920e+08" > 2.6323e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.6755e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2939e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7405e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.1780e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4612e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3393e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0419e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1260e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9109e+01 </func>
</region>
</regions>
<internal rank="459" log_i="1723712895.685166" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="460" mpi_size="768" stamp_init="1723712830.392519" stamp_final="1723712895.686994" username="apac4" allocationname="unknown" flags="0" pid="1046736" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52945e+01" utime="4.64970e+01" stime="8.49104e+00" mtime="2.79195e+01" gflop="0.00000e+00" gbyte="3.76709e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79195e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51675e+01" utime="4.64664e+01" stime="8.47938e+00" mtime="2.79195e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79195e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4965e+08" > 6.6967e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4902e+08" > 5.1322e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5406e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2637e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.9846e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.7130e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4614e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3314e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0432e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1260e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7673e+01 </func>
</region>
</regions>
<internal rank="460" log_i="1723712895.686994" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="461" mpi_size="768" stamp_init="1723712830.392754" stamp_final="1723712895.680816" username="apac4" allocationname="unknown" flags="0" pid="1046737" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52881e+01" utime="4.88174e+01" stime="7.53773e+00" mtime="2.81685e+01" gflop="0.00000e+00" gbyte="3.76526e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81685e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ea14b656ea14ea14aa" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51598e+01" utime="4.87824e+01" stime="7.53101e+00" mtime="2.81685e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81685e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4898e+08" > 4.4833e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4992e+08" > 2.9022e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9459e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2838e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.9072e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4610e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3416e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0410e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1256e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8735e+01 </func>
</region>
</regions>
<internal rank="461" log_i="1723712895.680816" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="462" mpi_size="768" stamp_init="1723712830.391612" stamp_final="1723712895.682759" username="apac4" allocationname="unknown" flags="0" pid="1046738" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52911e+01" utime="4.59245e+01" stime="8.55399e+00" mtime="2.77247e+01" gflop="0.00000e+00" gbyte="3.77907e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77247e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005c145c14e6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51612e+01" utime="4.58877e+01" stime="8.54798e+00" mtime="2.77247e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77247e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4987e+08" > 7.3333e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4838e+08" > 4.8465e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2094e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2975e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.4660e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6894e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4601e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3449e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0433e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1256e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7710e+01 </func>
</region>
</regions>
<internal rank="462" log_i="1723712895.682759" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="463" mpi_size="768" stamp_init="1723712830.392935" stamp_final="1723712895.691172" username="apac4" allocationname="unknown" flags="0" pid="1046739" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52982e+01" utime="4.86539e+01" stime="7.72977e+00" mtime="2.86344e+01" gflop="0.00000e+00" gbyte="3.76202e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86344e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51710e+01" utime="4.86241e+01" stime="7.71785e+00" mtime="2.86344e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86344e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4833e+08" > 4.4650e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4885e+08" > 3.0515e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8442e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2905e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.7089e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4609e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3411e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0415e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1255e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8302e+01 </func>
</region>
</regions>
<internal rank="463" log_i="1723712895.691172" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="464" mpi_size="768" stamp_init="1723712830.391065" stamp_final="1723712895.694300" username="apac4" allocationname="unknown" flags="0" pid="1046740" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.53032e+01" utime="4.59474e+01" stime="8.71578e+00" mtime="2.81664e+01" gflop="0.00000e+00" gbyte="3.76644e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81664e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51747e+01" utime="4.59175e+01" stime="8.70345e+00" mtime="2.81664e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81664e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4722e+08" > 6.2814e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4802e+08" > 4.7074e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7979e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2898e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5191e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.0201e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4594e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3581e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0466e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1259e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7683e+01 </func>
</region>
</regions>
<internal rank="464" log_i="1723712895.694300" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="465" mpi_size="768" stamp_init="1723712830.391343" stamp_final="1723712895.684141" username="apac4" allocationname="unknown" flags="0" pid="1046741" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52928e+01" utime="4.88227e+01" stime="7.07883e+00" mtime="2.82522e+01" gflop="0.00000e+00" gbyte="3.77495e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82522e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000f814ba56f814f314dc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51637e+01" utime="4.87872e+01" stime="7.07244e+00" mtime="2.82522e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82522e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4793e+08" > 3.5470e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4889e+08" > 2.8984e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0962e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2869e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6941e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.1563e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4586e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3635e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0464e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1257e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8765e+01 </func>
</region>
</regions>
<internal rank="465" log_i="1723712895.684141" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="466" mpi_size="768" stamp_init="1723712830.391552" stamp_final="1723712895.680838" username="apac4" allocationname="unknown" flags="0" pid="1046742" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52893e+01" utime="4.77606e+01" stime="7.81508e+00" mtime="2.77260e+01" gflop="0.00000e+00" gbyte="3.76740e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77260e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008414a15584148314f7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51619e+01" utime="4.77276e+01" stime="7.80520e+00" mtime="2.77260e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77260e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4906e+08" > 4.6718e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4766e+08" > 3.9188e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7480e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2551e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2374e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3181e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4581e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3663e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0434e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1210e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8497e+01 </func>
</region>
</regions>
<internal rank="466" log_i="1723712895.680838" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="467" mpi_size="768" stamp_init="1723712830.391368" stamp_final="1723712895.689575" username="apac4" allocationname="unknown" flags="0" pid="1046743" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52982e+01" utime="4.87035e+01" stime="7.26590e+00" mtime="2.83910e+01" gflop="0.00000e+00" gbyte="3.77277e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83910e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002a1429552a142514af" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51709e+01" utime="4.86784e+01" stime="7.24900e+00" mtime="2.83910e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83910e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4761e+08" > 3.7900e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4903e+08" > 2.7211e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0074e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2954e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8849e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.4462e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4567e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3830e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0430e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1259e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8961e+01 </func>
</region>
</regions>
<internal rank="467" log_i="1723712895.689575" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="468" mpi_size="768" stamp_init="1723712830.391052" stamp_final="1723712895.680955" username="apac4" allocationname="unknown" flags="0" pid="1046744" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52899e+01" utime="4.65450e+01" stime="8.47433e+00" mtime="2.83558e+01" gflop="0.00000e+00" gbyte="3.77171e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83558e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c5147156c514c514a6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51607e+01" utime="4.65119e+01" stime="8.46469e+00" mtime="2.83558e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83558e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4942e+08" > 5.3708e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4839e+08" > 4.1872e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5906e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2991e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3344e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.1350e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4576e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3680e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0483e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1259e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8166e+01 </func>
</region>
</regions>
<internal rank="468" log_i="1723712895.680955" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="469" mpi_size="768" stamp_init="1723712830.391346" stamp_final="1723712895.691464" username="apac4" allocationname="unknown" flags="0" pid="1046745" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.53001e+01" utime="4.85997e+01" stime="7.15541e+00" mtime="2.86266e+01" gflop="0.00000e+00" gbyte="3.77563e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86266e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49315941595152555951595150c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51717e+01" utime="4.85696e+01" stime="7.14278e+00" mtime="2.86266e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86266e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4985e+08" > 3.4532e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4927e+08" > 2.6612e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2074e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2905e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.0086e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4577e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3740e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0443e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1254e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9036e+01 </func>
</region>
</regions>
<internal rank="469" log_i="1723712895.691464" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="470" mpi_size="768" stamp_init="1723712830.391508" stamp_final="1723712895.681364" username="apac4" allocationname="unknown" flags="0" pid="1046746" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52899e+01" utime="4.62032e+01" stime="8.35523e+00" mtime="2.75900e+01" gflop="0.00000e+00" gbyte="3.78010e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.75900e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51605e+01" utime="4.61696e+01" stime="8.34615e+00" mtime="2.75900e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.75900e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4936e+08" > 6.3698e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4935e+08" > 4.6268e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0831e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2884e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7180e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.2194e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4561e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3905e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0453e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1260e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7815e+01 </func>
</region>
</regions>
<internal rank="470" log_i="1723712895.681364" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="471" mpi_size="768" stamp_init="1723712830.391606" stamp_final="1723712895.687055" username="apac4" allocationname="unknown" flags="0" pid="1046747" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52954e+01" utime="4.86806e+01" stime="7.25020e+00" mtime="2.89714e+01" gflop="0.00000e+00" gbyte="3.76091e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89714e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d915db15dc159b55dc15dc152e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51657e+01" utime="4.86488e+01" stime="7.24057e+00" mtime="2.89714e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89714e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4829e+08" > 3.4178e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4723e+08" > 2.6698e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 7.0362e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2969e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.2271e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4558e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3958e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0435e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1256e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8549e+01 </func>
</region>
</regions>
<internal rank="471" log_i="1723712895.687055" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="472" mpi_size="768" stamp_init="1723712830.391063" stamp_final="1723712895.687316" username="apac4" allocationname="unknown" flags="0" pid="1046748" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52963e+01" utime="4.49892e+01" stime="8.82399e+00" mtime="2.77460e+01" gflop="0.00000e+00" gbyte="3.76038e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77460e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51684e+01" utime="4.49592e+01" stime="8.81218e+00" mtime="2.77460e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77460e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4847e+08" > 8.0691e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4765e+08" > 7.2501e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7783e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2844e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.0802e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.1060e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4562e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3848e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0404e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1257e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7079e+01 </func>
</region>
</regions>
<internal rank="472" log_i="1723712895.687316" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="473" mpi_size="768" stamp_init="1723712830.393003" stamp_final="1723712895.691405" username="apac4" allocationname="unknown" flags="0" pid="1046749" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52984e+01" utime="4.93821e+01" stime="7.13951e+00" mtime="2.82171e+01" gflop="0.00000e+00" gbyte="3.76602e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82171e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51715e+01" utime="4.93524e+01" stime="7.12702e+00" mtime="2.82171e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82171e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4885e+08" > 4.6852e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4747e+08" > 3.5004e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1698e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2822e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3842e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1528e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4570e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3816e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0427e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1256e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8532e+01 </func>
</region>
</regions>
<internal rank="473" log_i="1723712895.691405" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="474" mpi_size="768" stamp_init="1723712830.391054" stamp_final="1723712895.681737" username="apac4" allocationname="unknown" flags="0" pid="1046750" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52907e+01" utime="4.73356e+01" stime="7.97040e+00" mtime="2.75295e+01" gflop="0.00000e+00" gbyte="3.76644e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.75295e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d314d314f7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51622e+01" utime="4.72981e+01" stime="7.96482e+00" mtime="2.75295e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.75295e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4810e+08" > 5.8142e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4869e+08" > 5.4435e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.5420e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2691e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.5075e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.0959e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4546e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4031e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0422e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1257e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8363e+01 </func>
</region>
</regions>
<internal rank="474" log_i="1723712895.681737" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="475" mpi_size="768" stamp_init="1723712830.391346" stamp_final="1723712895.691439" username="apac4" allocationname="unknown" flags="0" pid="1046751" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.53001e+01" utime="4.92253e+01" stime="7.24768e+00" mtime="2.81603e+01" gflop="0.00000e+00" gbyte="3.76240e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81603e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004015401505" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51731e+01" utime="4.91955e+01" stime="7.23556e+00" mtime="2.81603e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81603e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4863e+08" > 4.7640e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4917e+08" > 3.5118e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9209e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2925e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4305e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.0519e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4564e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3872e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0431e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1254e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8712e+01 </func>
</region>
</regions>
<internal rank="475" log_i="1723712895.691439" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="476" mpi_size="768" stamp_init="1723712830.392398" stamp_final="1723712895.680903" username="apac4" allocationname="unknown" flags="0" pid="1046752" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52885e+01" utime="4.69984e+01" stime="8.12166e+00" mtime="2.75443e+01" gflop="0.00000e+00" gbyte="3.76659e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.75443e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000061157f566115611508" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51608e+01" utime="4.69667e+01" stime="8.11105e+00" mtime="2.75443e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.75443e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4881e+08" > 5.9819e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4849e+08" > 5.8472e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9138e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2716e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0790e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.5542e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4550e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4023e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0450e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1260e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7979e+01 </func>
</region>
</regions>
<internal rank="476" log_i="1723712895.680903" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="477" mpi_size="768" stamp_init="1723712830.392156" stamp_final="1723712895.680807" username="apac4" allocationname="unknown" flags="0" pid="1046753" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52887e+01" utime="4.90373e+01" stime="7.49449e+00" mtime="2.87668e+01" gflop="0.00000e+00" gbyte="3.76343e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87668e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c215c2153e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51607e+01" utime="4.90031e+01" stime="7.48713e+00" mtime="2.87668e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87668e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4873e+08" > 4.6752e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4870e+08" > 3.0286e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8962e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2967e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.3553e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4559e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3923e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0412e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1255e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8355e+01 </func>
</region>
</regions>
<internal rank="477" log_i="1723712895.680807" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="478" mpi_size="768" stamp_init="1723712830.392666" stamp_final="1723712895.687576" username="apac4" allocationname="unknown" flags="0" pid="1046754" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52949e+01" utime="4.70853e+01" stime="8.20570e+00" mtime="2.78964e+01" gflop="0.00000e+00" gbyte="3.76331e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78964e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003115311553" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51686e+01" utime="4.70561e+01" stime="8.19303e+00" mtime="2.78964e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78964e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5007e+08" > 6.1794e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4893e+08" > 4.5820e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4009e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2926e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.9632e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.4371e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4552e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4000e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0419e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1258e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7818e+01 </func>
</region>
</regions>
<internal rank="478" log_i="1723712895.687576" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="479" mpi_size="768" stamp_init="1723712830.392555" stamp_final="1723712895.685279" username="apac4" allocationname="unknown" flags="0" pid="1046755" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09b</host>
<perf wtime="6.52927e+01" utime="4.89048e+01" stime="7.27276e+00" mtime="2.82778e+01" gflop="0.00000e+00" gbyte="3.77613e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82778e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51645e+01" utime="4.88721e+01" stime="7.26295e+00" mtime="2.82778e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82778e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4830e+08" > 4.9008e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5058e+08" > 3.0759e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5583e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2956e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.5752e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4550e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4014e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0406e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1259e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8182e+01 </func>
</region>
</regions>
<internal rank="479" log_i="1723712895.685279" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="480" mpi_size="768" stamp_init="1723712830.374311" stamp_final="1723712895.682625" username="apac4" allocationname="unknown" flags="0" pid="554489" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53083e+01" utime="4.27885e+01" stime="1.22548e+01" mtime="2.79283e+01" gflop="0.00000e+00" gbyte="3.83480e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79283e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51718e+01" utime="4.27592e+01" stime="1.22415e+01" mtime="2.79283e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79283e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4940e+08" > 5.8316e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4899e+08" > 3.3926e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7363e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3809e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.7462e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.8142e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4548e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4083e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0449e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1298e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7431e+01 </func>
</region>
</regions>
<internal rank="480" log_i="1723712895.682625" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="481" mpi_size="768" stamp_init="1723712830.374349" stamp_final="1723712895.683483" username="apac4" allocationname="unknown" flags="0" pid="554490" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53091e+01" utime="4.89586e+01" stime="7.30026e+00" mtime="2.84651e+01" gflop="0.00000e+00" gbyte="3.77728e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84651e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51687e+01" utime="4.89267e+01" stime="7.28810e+00" mtime="2.84651e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84651e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4920e+08" > 4.7202e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4916e+08" > 2.9223e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1279e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4213e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0803e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4612e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3443e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0502e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1296e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8685e+01 </func>
</region>
</regions>
<internal rank="481" log_i="1723712895.683483" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="482" mpi_size="768" stamp_init="1723712830.374230" stamp_final="1723712895.688836" username="apac4" allocationname="unknown" flags="0" pid="554491" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53146e+01" utime="4.66751e+01" stime="8.20807e+00" mtime="2.80898e+01" gflop="0.00000e+00" gbyte="3.77773e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80898e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e115e1153f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51769e+01" utime="4.66460e+01" stime="8.19389e+00" mtime="2.80898e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80898e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.8678e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4823e+08" > 5.8955e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4920e+08" > 3.7131e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4991e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4222e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0557e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4510e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4608e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3487e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0449e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1297e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7820e+01 </func>
</region>
</regions>
<internal rank="482" log_i="1723712895.688836" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="483" mpi_size="768" stamp_init="1723712830.374471" stamp_final="1723712895.689834" username="apac4" allocationname="unknown" flags="0" pid="554492" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53154e+01" utime="4.89216e+01" stime="7.35755e+00" mtime="2.85891e+01" gflop="0.00000e+00" gbyte="3.76209e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85891e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002c142c14e5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51792e+01" utime="4.88915e+01" stime="7.34535e+00" mtime="2.85891e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85891e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4835e+08" > 4.3979e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4823e+08" > 2.7972e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1763e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4181e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4632e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4605e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.3524e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0472e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1297e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8806e+01 </func>
</region>
</regions>
<internal rank="483" log_i="1723712895.689834" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="484" mpi_size="768" stamp_init="1723712830.374185" stamp_final="1723712895.681763" username="apac4" allocationname="unknown" flags="0" pid="554493" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53076e+01" utime="4.69566e+01" stime="8.07299e+00" mtime="2.80260e+01" gflop="0.00000e+00" gbyte="3.77617e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80260e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4fe1422141414a7561414f11476" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51727e+01" utime="4.69268e+01" stime="8.05991e+00" mtime="2.80260e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80260e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4847e+08" > 6.4765e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4862e+08" > 4.0229e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1248e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4100e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4019e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0547e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4548e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4095e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0479e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1298e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8073e+01 </func>
</region>
</regions>
<internal rank="484" log_i="1723712895.681763" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="485" mpi_size="768" stamp_init="1723712830.374317" stamp_final="1723712895.682221" username="apac4" allocationname="unknown" flags="0" pid="554494" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53079e+01" utime="4.88426e+01" stime="7.34261e+00" mtime="2.86479e+01" gflop="0.00000e+00" gbyte="3.77678e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86479e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51702e+01" utime="4.88133e+01" stime="7.32903e+00" mtime="2.86479e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86479e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4933e+08" > 4.5228e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4852e+08" > 2.6612e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2612e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4253e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.6716e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4539e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4143e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0490e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1296e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8763e+01 </func>
</region>
</regions>
<internal rank="485" log_i="1723712895.682221" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="486" mpi_size="768" stamp_init="1723712830.374203" stamp_final="1723712895.691817" username="apac4" allocationname="unknown" flags="0" pid="554495" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53176e+01" utime="4.62116e+01" stime="8.25964e+00" mtime="2.77774e+01" gflop="0.00000e+00" gbyte="3.77235e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77774e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51801e+01" utime="4.61809e+01" stime="8.24781e+00" mtime="2.77774e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77774e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4666e+08" > 6.5615e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4928e+08" > 4.5835e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2768e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4052e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.6982e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0603e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4538e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4164e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0495e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1298e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7663e+01 </func>
</region>
</regions>
<internal rank="486" log_i="1723712895.691817" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="487" mpi_size="768" stamp_init="1723712830.374295" stamp_final="1723712895.692227" username="apac4" allocationname="unknown" flags="0" pid="554496" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53179e+01" utime="4.88145e+01" stime="7.29331e+00" mtime="2.81484e+01" gflop="0.00000e+00" gbyte="3.76564e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81484e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004b144b14e0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51807e+01" utime="4.87865e+01" stime="7.27883e+00" mtime="2.81484e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81484e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4932e+08" > 4.5318e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4792e+08" > 3.1020e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9041e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4110e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2888e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0599e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4546e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4114e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0469e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1294e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8619e+01 </func>
</region>
</regions>
<internal rank="487" log_i="1723712895.692227" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="488" mpi_size="768" stamp_init="1723712830.374169" stamp_final="1723712895.692385" username="apac4" allocationname="unknown" flags="0" pid="554497" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53182e+01" utime="4.61682e+01" stime="8.21217e+00" mtime="2.80161e+01" gflop="0.00000e+00" gbyte="3.77628e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80161e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ef14ef14d0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51811e+01" utime="4.61344e+01" stime="8.20292e+00" mtime="2.80161e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80161e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4733e+08" > 6.0114e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4955e+08" > 5.5701e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2315e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4144e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.7799e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0616e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4526e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4259e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0497e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1294e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7980e+01 </func>
</region>
</regions>
<internal rank="488" log_i="1723712895.692385" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="489" mpi_size="768" stamp_init="1723712830.374323" stamp_final="1723712895.686938" username="apac4" allocationname="unknown" flags="0" pid="554498" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53126e+01" utime="4.89716e+01" stime="7.31806e+00" mtime="2.87182e+01" gflop="0.00000e+00" gbyte="3.77514e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87182e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c614c814c9144656c914c91493" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51769e+01" utime="4.89439e+01" stime="7.30333e+00" mtime="2.87182e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87182e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4900e+08" > 3.8835e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4805e+08" > 3.0007e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4657e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4217e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8597e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.3748e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4534e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4230e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0515e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1295e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8686e+01 </func>
</region>
</regions>
<internal rank="489" log_i="1723712895.686938" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="490" mpi_size="768" stamp_init="1723712830.375648" stamp_final="1723712895.687846" username="apac4" allocationname="unknown" flags="0" pid="554499" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53122e+01" utime="4.46849e+01" stime="8.93234e+00" mtime="2.76477e+01" gflop="0.00000e+00" gbyte="3.76221e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.76477e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51769e+01" utime="4.46560e+01" stime="8.91937e+00" mtime="2.76477e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.76477e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4759e+08" > 8.0830e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4867e+08" > 5.2429e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0141e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4221e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6500e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.3881e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4526e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4281e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0497e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1298e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7628e+01 </func>
</region>
</regions>
<internal rank="490" log_i="1723712895.687846" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="491" mpi_size="768" stamp_init="1723712830.374331" stamp_final="1723712895.694987" username="apac4" allocationname="unknown" flags="0" pid="554500" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53207e+01" utime="4.90179e+01" stime="7.23748e+00" mtime="2.90789e+01" gflop="0.00000e+00" gbyte="3.77911e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90789e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf449144a144c1444554c144b14c5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51831e+01" utime="4.89841e+01" stime="7.22706e+00" mtime="2.90789e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90789e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4898e+08" > 3.5572e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4734e+08" > 2.9293e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3742e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4236e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6689e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.3385e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4538e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4161e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0477e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1294e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9176e+01 </func>
</region>
</regions>
<internal rank="491" log_i="1723712895.694987" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="492" mpi_size="768" stamp_init="1723712830.374361" stamp_final="1723712895.688742" username="apac4" allocationname="unknown" flags="0" pid="554501" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53144e+01" utime="4.72160e+01" stime="7.84488e+00" mtime="2.82078e+01" gflop="0.00000e+00" gbyte="3.77720e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82078e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008d14f3558d148c1480" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51783e+01" utime="4.71835e+01" stime="7.83429e+00" mtime="2.82078e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82078e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.8358e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4858e+08" > 4.6619e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4830e+08" > 3.8539e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5890e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4233e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5402e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0591e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4518e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4373e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0466e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1296e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7960e+01 </func>
</region>
</regions>
<internal rank="492" log_i="1723712895.688742" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="493" mpi_size="768" stamp_init="1723712830.374305" stamp_final="1723712895.686975" username="apac4" allocationname="unknown" flags="0" pid="554502" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53127e+01" utime="4.90166e+01" stime="7.20130e+00" mtime="2.85120e+01" gflop="0.00000e+00" gbyte="3.74783e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85120e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c414dd14ef14ce55ef14e9146f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51762e+01" utime="4.89883e+01" stime="7.18711e+00" mtime="2.85120e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85120e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4875e+08" > 3.6055e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4858e+08" > 2.8061e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9775e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4096e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1117e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4519e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4357e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0503e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1295e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9006e+01 </func>
</region>
</regions>
<internal rank="493" log_i="1723712895.686975" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="494" mpi_size="768" stamp_init="1723712830.375337" stamp_final="1723712895.687474" username="apac4" allocationname="unknown" flags="0" pid="554503" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53121e+01" utime="4.69724e+01" stime="8.08687e+00" mtime="2.83649e+01" gflop="0.00000e+00" gbyte="3.76514e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83649e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f014ef14fc" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51770e+01" utime="4.69413e+01" stime="8.07557e+00" mtime="2.83649e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83649e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4858e+08" > 5.0489e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5035e+08" > 4.3880e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6330e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4155e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0180e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0594e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4520e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4380e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0494e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1296e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8038e+01 </func>
</region>
</regions>
<internal rank="494" log_i="1723712895.687474" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="495" mpi_size="768" stamp_init="1723712830.374398" stamp_final="1723712895.680291" username="apac4" allocationname="unknown" flags="0" pid="554504" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53059e+01" utime="4.89273e+01" stime="7.35568e+00" mtime="2.85251e+01" gflop="0.00000e+00" gbyte="3.77827e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85251e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007c144a557c14761456" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51687e+01" utime="4.88946e+01" stime="7.34588e+00" mtime="2.85251e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85251e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4788e+08" > 3.6751e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4870e+08" > 2.4523e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3022e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4061e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.0328e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4512e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4454e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0504e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1295e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8703e+01 </func>
</region>
</regions>
<internal rank="495" log_i="1723712895.680291" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="496" mpi_size="768" stamp_init="1723712830.374324" stamp_final="1723712895.676421" username="apac4" allocationname="unknown" flags="0" pid="554505" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53021e+01" utime="4.67773e+01" stime="8.16437e+00" mtime="2.79222e+01" gflop="0.00000e+00" gbyte="3.77087e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79222e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51658e+01" utime="4.67468e+01" stime="8.15212e+00" mtime="2.79222e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79222e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4943e+08" > 6.5064e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4869e+08" > 4.2160e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2688e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3825e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.4114e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.8271e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4520e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4362e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0484e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1298e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7808e+01 </func>
</region>
</regions>
<internal rank="496" log_i="1723712895.676421" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="497" mpi_size="768" stamp_init="1723712830.374343" stamp_final="1723712895.689830" username="apac4" allocationname="unknown" flags="0" pid="554506" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53155e+01" utime="4.87251e+01" stime="7.46324e+00" mtime="2.84734e+01" gflop="0.00000e+00" gbyte="3.76282e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84734e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b614b714b8149a56b814b8147f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51793e+01" utime="4.86933e+01" stime="7.45263e+00" mtime="2.84734e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84734e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4913e+08" > 5.1094e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4939e+08" > 3.0676e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2979e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4265e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.7431e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4521e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4374e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0486e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1295e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8486e+01 </func>
</region>
</regions>
<internal rank="497" log_i="1723712895.689830" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="498" mpi_size="768" stamp_init="1723712830.374169" stamp_final="1723712895.682581" username="apac4" allocationname="unknown" flags="0" pid="554507" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53084e+01" utime="4.64925e+01" stime="8.35615e+00" mtime="2.82748e+01" gflop="0.00000e+00" gbyte="3.77106e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82748e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51713e+01" utime="4.64618e+01" stime="8.34362e+00" mtime="2.82748e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82748e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5085e+08" > 6.1709e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5007e+08" > 4.1284e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4086e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3996e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.9101e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0953e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4515e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4420e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0465e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1296e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8078e+01 </func>
</region>
</regions>
<internal rank="498" log_i="1723712895.682581" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="499" mpi_size="768" stamp_init="1723712830.375338" stamp_final="1723712895.680154" username="apac4" allocationname="unknown" flags="0" pid="554508" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53048e+01" utime="4.88678e+01" stime="7.41714e+00" mtime="2.84350e+01" gflop="0.00000e+00" gbyte="3.76942e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84350e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51682e+01" utime="4.88343e+01" stime="7.40822e+00" mtime="2.84350e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84350e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4883e+08" > 4.9086e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5003e+08" > 3.0139e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8793e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4143e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3127e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.1043e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4512e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4429e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0502e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1245e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8900e+01 </func>
</region>
</regions>
<internal rank="499" log_i="1723712895.680154" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="500" mpi_size="768" stamp_init="1723712830.375638" stamp_final="1723712895.681362" username="apac4" allocationname="unknown" flags="0" pid="554509" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53057e+01" utime="4.66218e+01" stime="8.38207e+00" mtime="2.83016e+01" gflop="0.00000e+00" gbyte="3.76541e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83016e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4651567156815b0566815671528" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51734e+01" utime="4.65898e+01" stime="8.37090e+00" mtime="2.83016e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83016e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4921e+08" > 6.5646e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4851e+08" > 4.4988e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6324e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4116e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5800e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0729e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4545e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4109e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0459e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1299e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7825e+01 </func>
</region>
</regions>
<internal rank="500" log_i="1723712895.681362" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="501" mpi_size="768" stamp_init="1723712830.374354" stamp_final="1723712895.687028" username="apac4" allocationname="unknown" flags="0" pid="554510" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53127e+01" utime="4.89058e+01" stime="7.37462e+00" mtime="2.86498e+01" gflop="0.00000e+00" gbyte="3.77567e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86498e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c914cb14cc14a355cc14cc14f4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51747e+01" utime="4.88691e+01" stime="7.36865e+00" mtime="2.86498e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86498e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 5.1578e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5024e+08" > 3.1293e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4002e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4116e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0739e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4556e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4022e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0484e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1293e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8561e+01 </func>
</region>
</regions>
<internal rank="501" log_i="1723712895.687028" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="502" mpi_size="768" stamp_init="1723712830.374168" stamp_final="1723712895.682495" username="apac4" allocationname="unknown" flags="0" pid="554511" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53083e+01" utime="4.68733e+01" stime="8.01724e+00" mtime="2.77918e+01" gflop="0.00000e+00" gbyte="3.76442e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77918e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d514a455d514d514ef" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51708e+01" utime="4.68393e+01" stime="8.00838e+00" mtime="2.77918e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77918e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4930e+08" > 5.6233e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4900e+08" > 4.4402e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2598e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4295e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.4851e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.9237e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4554e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4045e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0458e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1296e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7768e+01 </func>
</region>
</regions>
<internal rank="502" log_i="1723712895.682495" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="503" mpi_size="768" stamp_init="1723712830.374337" stamp_final="1723712895.688137" username="apac4" allocationname="unknown" flags="0" pid="554512" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u09a</host>
<perf wtime="6.53138e+01" utime="4.89170e+01" stime="7.36209e+00" mtime="2.82336e+01" gflop="0.00000e+00" gbyte="3.75690e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82336e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51787e+01" utime="4.88918e+01" stime="7.34426e+00" mtime="2.82336e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82336e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4945e+08" > 5.1123e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4831e+08" > 3.0397e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8357e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4029e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.8989e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4548e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4066e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0467e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1293e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8732e+01 </func>
</region>
</regions>
<internal rank="503" log_i="1723712895.688137" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="504" mpi_size="768" stamp_init="1723712830.490340" stamp_final="1723712895.684256" username="apac4" allocationname="unknown" flags="0" pid="1820026" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51939e+01" utime="4.11217e+01" stime="1.27215e+01" mtime="2.74568e+01" gflop="0.00000e+00" gbyte="3.85170e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.74568e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000097146d5697149714ad" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50601e+01" utime="4.10870e+01" stime="1.27143e+01" mtime="2.74568e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.74568e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4752e+08" > 8.2908e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4711e+08" > 5.6085e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0294e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3716e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4612e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.8316e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4500e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4534e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0411e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1337e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7442e+01 </func>
</region>
</regions>
<internal rank="504" log_i="1723712895.684256" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="505" mpi_size="768" stamp_init="1723712830.490385" stamp_final="1723712895.679704" username="apac4" allocationname="unknown" flags="0" pid="1820027" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51893e+01" utime="4.84577e+01" stime="7.53175e+00" mtime="2.83284e+01" gflop="0.00000e+00" gbyte="3.78029e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83284e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50550e+01" utime="4.84198e+01" stime="7.52602e+00" mtime="2.83284e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83284e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4892e+08" > 5.4371e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4667e+08" > 3.0161e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2176e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3841e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2131e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4499e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4559e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0378e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1337e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8432e+01 </func>
</region>
</regions>
<internal rank="505" log_i="1723712895.679704" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="506" mpi_size="768" stamp_init="1723712830.490347" stamp_final="1723712895.691791" username="apac4" allocationname="unknown" flags="0" pid="1820028" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.52014e+01" utime="4.63325e+01" stime="8.36968e+00" mtime="2.81223e+01" gflop="0.00000e+00" gbyte="3.76156e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81223e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009a159a153b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50712e+01" utime="4.63028e+01" stime="8.35642e+00" mtime="2.81223e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81223e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4781e+08" > 6.8427e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4827e+08" > 4.2554e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4078e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3710e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.1962e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0878e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4502e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4572e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0387e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1338e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7886e+01 </func>
</region>
</regions>
<internal rank="506" log_i="1723712895.691791" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="507" mpi_size="768" stamp_init="1723712830.490388" stamp_final="1723712895.690171" username="apac4" allocationname="unknown" flags="0" pid="1820029" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51998e+01" utime="4.82851e+01" stime="7.49683e+00" mtime="2.85668e+01" gflop="0.00000e+00" gbyte="3.76877e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85668e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50696e+01" utime="4.82565e+01" stime="7.48313e+00" mtime="2.85668e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85668e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4838e+08" > 5.9556e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4791e+08" > 2.5744e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1121e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3887e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7356e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4504e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4552e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0389e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1333e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8719e+01 </func>
</region>
</regions>
<internal rank="507" log_i="1723712895.690171" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="508" mpi_size="768" stamp_init="1723712830.490386" stamp_final="1723712895.684096" username="apac4" allocationname="unknown" flags="0" pid="1820030" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51937e+01" utime="4.64695e+01" stime="8.21244e+00" mtime="2.79547e+01" gflop="0.00000e+00" gbyte="3.73726e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79547e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000af15aa1505" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50626e+01" utime="4.64399e+01" stime="8.19926e+00" mtime="2.79547e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79547e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4887e+08" > 6.7900e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4932e+08" > 3.9243e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7565e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3707e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0209e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.8669e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4492e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4656e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0438e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1337e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7382e+01 </func>
</region>
</regions>
<internal rank="508" log_i="1723712895.684096" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="509" mpi_size="768" stamp_init="1723712830.490414" stamp_final="1723712895.679782" username="apac4" allocationname="unknown" flags="0" pid="1820031" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51894e+01" utime="4.87533e+01" stime="7.26222e+00" mtime="2.83142e+01" gflop="0.00000e+00" gbyte="3.76770e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83142e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000070156f1546" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50581e+01" utime="4.87127e+01" stime="7.26068e+00" mtime="2.83142e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83142e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4828e+08" > 5.2786e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4829e+08" > 2.9501e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9901e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3816e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2159e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.6990e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4500e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4601e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0378e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1335e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8664e+01 </func>
</region>
</regions>
<internal rank="509" log_i="1723712895.679782" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="510" mpi_size="768" stamp_init="1723712830.490318" stamp_final="1723712895.684136" username="apac4" allocationname="unknown" flags="0" pid="1820032" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51938e+01" utime="4.53625e+01" stime="8.60285e+00" mtime="2.75786e+01" gflop="0.00000e+00" gbyte="3.76469e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.75786e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4141516151715bc551715171525" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50624e+01" utime="4.53264e+01" stime="8.59670e+00" mtime="2.75786e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.75786e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4821e+08" > 7.8728e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4756e+08" > 5.2515e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0205e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3657e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.8705e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.8712e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4495e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4625e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0442e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1337e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7626e+01 </func>
</region>
</regions>
<internal rank="510" log_i="1723712895.684136" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="511" mpi_size="768" stamp_init="1723712830.490379" stamp_final="1723712895.689920" username="apac4" allocationname="unknown" flags="0" pid="1820033" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51995e+01" utime="4.84533e+01" stime="7.58359e+00" mtime="2.86080e+01" gflop="0.00000e+00" gbyte="3.74599e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86080e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005214521496" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50684e+01" utime="4.84205e+01" stime="7.57412e+00" mtime="2.86080e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86080e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4769e+08" > 5.3771e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4954e+08" > 2.6877e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4077e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3880e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8820e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4485e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.4749e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0434e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1332e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8522e+01 </func>
</region>
</regions>
<internal rank="511" log_i="1723712895.689920" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="512" mpi_size="768" stamp_init="1723712830.490388" stamp_final="1723712895.691872" username="apac4" allocationname="unknown" flags="0" pid="1820034" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.52015e+01" utime="4.60388e+01" stime="8.34442e+00" mtime="2.80162e+01" gflop="0.00000e+00" gbyte="3.76457e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80162e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c5146955c514c5148d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50677e+01" utime="4.60066e+01" stime="8.33512e+00" mtime="2.80162e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80162e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4824e+08" > 5.9912e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4901e+08" > 4.7004e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0846e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2937e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.5574e-02 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4437e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5224e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0444e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1339e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8196e+01 </func>
</region>
</regions>
<internal rank="512" log_i="1723712895.691872" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="513" mpi_size="768" stamp_init="1723712830.490387" stamp_final="1723712895.687008" username="apac4" allocationname="unknown" flags="0" pid="1820035" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51966e+01" utime="4.88722e+01" stime="7.17397e+00" mtime="2.85049e+01" gflop="0.00000e+00" gbyte="3.76759e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85049e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50667e+01" utime="4.88429e+01" stime="7.16116e+00" mtime="2.85049e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85049e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4926e+08" > 4.3934e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4941e+08" > 3.3385e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1042e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3833e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.1456e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4436e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5241e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0409e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1332e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8818e+01 </func>
</region>
</regions>
<internal rank="513" log_i="1723712895.687008" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="514" mpi_size="768" stamp_init="1723712830.490318" stamp_final="1723712895.682140" username="apac4" allocationname="unknown" flags="0" pid="1820036" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51918e+01" utime="4.58361e+01" stime="8.22095e+00" mtime="2.78013e+01" gflop="0.00000e+00" gbyte="3.76743e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78013e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002c1582552c152c1513" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50590e+01" utime="4.58017e+01" stime="8.21324e+00" mtime="2.78013e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78013e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4821e+08" > 7.1836e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4860e+08" > 5.3684e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9487e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3649e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1110e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.5419e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4429e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5295e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0405e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1337e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7988e+01 </func>
</region>
</regions>
<internal rank="514" log_i="1723712895.682140" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="515" mpi_size="768" stamp_init="1723712830.490416" stamp_final="1723712895.691504" username="apac4" allocationname="unknown" flags="0" pid="1820037" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.52011e+01" utime="4.85666e+01" stime="7.15302e+00" mtime="2.82172e+01" gflop="0.00000e+00" gbyte="3.76987e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82172e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49f14a014a2148f56a214a114f7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50694e+01" utime="4.85302e+01" stime="7.14724e+00" mtime="2.82172e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82172e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4835e+08" > 4.5563e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4887e+08" > 3.1320e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7568e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3914e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3129e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4427e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5319e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0395e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1328e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8860e+01 </func>
</region>
</regions>
<internal rank="515" log_i="1723712895.691504" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="516" mpi_size="768" stamp_init="1723712830.490329" stamp_final="1723712895.689702" username="apac4" allocationname="unknown" flags="0" pid="1820038" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51994e+01" utime="4.52236e+01" stime="8.53038e+00" mtime="2.79225e+01" gflop="0.00000e+00" gbyte="3.76759e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79225e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b415b31500" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50658e+01" utime="4.51897e+01" stime="8.52222e+00" mtime="2.79225e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79225e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5138e+08" > 7.3760e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4940e+08" > 4.3923e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5426e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3736e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.9884e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.8080e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4422e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5322e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0381e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1337e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7499e+01 </func>
</region>
</regions>
<internal rank="516" log_i="1723712895.689702" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="517" mpi_size="768" stamp_init="1723712830.490433" stamp_final="1723712895.683966" username="apac4" allocationname="unknown" flags="0" pid="1820039" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51935e+01" utime="4.87384e+01" stime="7.31236e+00" mtime="2.90434e+01" gflop="0.00000e+00" gbyte="3.77411e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90434e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006f1495566f146e14a5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50630e+01" utime="4.87065e+01" stime="7.30099e+00" mtime="2.90434e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90434e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4938e+08" > 4.1963e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4973e+08" > 3.6149e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5013e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3728e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.9299e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4419e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5370e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0418e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1332e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8987e+01 </func>
</region>
</regions>
<internal rank="517" log_i="1723712895.683966" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="518" mpi_size="768" stamp_init="1723712830.490368" stamp_final="1723712895.681731" username="apac4" allocationname="unknown" flags="0" pid="1820040" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51914e+01" utime="4.69975e+01" stime="7.93175e+00" mtime="2.83784e+01" gflop="0.00000e+00" gbyte="3.76999e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83784e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000004014401497" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50602e+01" utime="4.69657e+01" stime="7.92132e+00" mtime="2.83784e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83784e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4964e+08" > 5.6523e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4822e+08" > 4.3741e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4843e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3688e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.3770e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.8939e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4425e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5345e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0440e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1337e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8180e+01 </func>
</region>
</regions>
<internal rank="518" log_i="1723712895.681731" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="519" mpi_size="768" stamp_init="1723712830.490440" stamp_final="1723712895.693431" username="apac4" allocationname="unknown" flags="0" pid="1820041" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.52030e+01" utime="4.87015e+01" stime="7.42610e+00" mtime="2.88794e+01" gflop="0.00000e+00" gbyte="3.76259e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88794e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a515a715a8157555a815a81532" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50727e+01" utime="4.86634e+01" stime="7.42204e+00" mtime="2.88794e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88794e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4800e+08" > 4.2660e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4930e+08" > 3.2891e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3278e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3770e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1683e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.9222e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4421e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5380e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0394e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1329e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8989e+01 </func>
</region>
</regions>
<internal rank="519" log_i="1723712895.693431" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="520" mpi_size="768" stamp_init="1723712830.490374" stamp_final="1723712895.681056" username="apac4" allocationname="unknown" flags="0" pid="1820042" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51907e+01" utime="4.62418e+01" stime="8.16930e+00" mtime="2.74631e+01" gflop="0.00000e+00" gbyte="3.75942e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.74631e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000093149314e8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50579e+01" utime="4.62074e+01" stime="8.16207e+00" mtime="2.74631e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.74631e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4827e+08" > 6.5664e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4961e+08" > 5.7630e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1627e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3783e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9610e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.9189e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4409e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5512e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0425e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1334e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7479e+01 </func>
</region>
</regions>
<internal rank="520" log_i="1723712895.681056" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="521" mpi_size="768" stamp_init="1723712830.490391" stamp_final="1723712895.679543" username="apac4" allocationname="unknown" flags="0" pid="1820043" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51892e+01" utime="4.87166e+01" stime="7.28388e+00" mtime="2.82615e+01" gflop="0.00000e+00" gbyte="3.76984e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82615e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000054155e565415541547" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50604e+01" utime="4.86804e+01" stime="7.27754e+00" mtime="2.82615e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82615e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4989e+08" > 4.6819e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4808e+08" > 2.8336e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2048e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3708e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.9218e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4412e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5479e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0385e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1328e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8463e+01 </func>
</region>
</regions>
<internal rank="521" log_i="1723712895.679543" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="522" mpi_size="768" stamp_init="1723712830.490324" stamp_final="1723712895.691722" username="apac4" allocationname="unknown" flags="0" pid="1820044" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.52014e+01" utime="4.62821e+01" stime="8.27756e+00" mtime="2.79056e+01" gflop="0.00000e+00" gbyte="3.76110e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79056e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000cf15cf1536" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50705e+01" utime="4.62475e+01" stime="8.27004e+00" mtime="2.79056e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79056e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4928e+08" > 6.4156e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5003e+08" > 5.5098e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2173e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3743e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.2731e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.8631e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4407e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5474e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0397e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1332e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7891e+01 </func>
</region>
</regions>
<internal rank="522" log_i="1723712895.691722" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="523" mpi_size="768" stamp_init="1723712830.490429" stamp_final="1723712895.684218" username="apac4" allocationname="unknown" flags="0" pid="1820045" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51938e+01" utime="4.88009e+01" stime="7.15151e+00" mtime="2.88491e+01" gflop="0.00000e+00" gbyte="3.76915e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88491e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002c148c552c142c14cd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50627e+01" utime="4.87623e+01" stime="7.14766e+00" mtime="2.88491e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88491e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4992e+08" > 4.9723e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4786e+08" > 3.3521e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3055e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3836e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.2795e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4405e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5514e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0407e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1330e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8901e+01 </func>
</region>
</regions>
<internal rank="523" log_i="1723712895.684218" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="524" mpi_size="768" stamp_init="1723712830.490366" stamp_final="1723712895.682974" username="apac4" allocationname="unknown" flags="0" pid="1820046" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51926e+01" utime="4.54393e+01" stime="8.58600e+00" mtime="2.76976e+01" gflop="0.00000e+00" gbyte="3.77819e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.76976e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001f141f14c0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50627e+01" utime="4.54056e+01" stime="8.57784e+00" mtime="2.76976e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.76976e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4728e+08" > 7.6094e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5062e+08" > 5.5049e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2508e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3754e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5272e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.9349e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4392e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5658e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0402e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1337e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7525e+01 </func>
</region>
</regions>
<internal rank="524" log_i="1723712895.682974" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="525" mpi_size="768" stamp_init="1723712830.490398" stamp_final="1723712895.686781" username="apac4" allocationname="unknown" flags="0" pid="1820047" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51964e+01" utime="4.83842e+01" stime="7.63616e+00" mtime="2.81755e+01" gflop="0.00000e+00" gbyte="3.77060e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81755e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4ee14f014f1149955f114f114bb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50673e+01" utime="4.83451e+01" stime="7.63323e+00" mtime="2.81755e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81755e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4776e+08" > 4.5768e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4904e+08" > 2.8418e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0029e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3437e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.0599e-06 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.9442e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4405e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5560e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0409e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1329e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8616e+01 </func>
</region>
</regions>
<internal rank="525" log_i="1723712895.686781" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="526" mpi_size="768" stamp_init="1723712830.490355" stamp_final="1723712895.682760" username="apac4" allocationname="unknown" flags="0" pid="1820048" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51924e+01" utime="4.64664e+01" stime="8.21609e+00" mtime="2.78254e+01" gflop="0.00000e+00" gbyte="3.77159e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78254e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c515c51544" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50600e+01" utime="4.64297e+01" stime="8.21040e+00" mtime="2.78254e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78254e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5021e+08" > 6.4784e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4847e+08" > 5.1311e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4656e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3731e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8362e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.1547e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4405e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5555e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0447e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1335e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7558e+01 </func>
</region>
</regions>
<internal rank="526" log_i="1723712895.682760" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="527" mpi_size="768" stamp_init="1723712830.490401" stamp_final="1723712895.685518" username="apac4" allocationname="unknown" flags="0" pid="1820049" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u17b</host>
<perf wtime="6.51951e+01" utime="4.86378e+01" stime="7.37272e+00" mtime="2.85467e+01" gflop="0.00000e+00" gbyte="3.75893e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85467e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004c1415554c144c1498" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50659e+01" utime="4.86059e+01" stime="7.36269e+00" mtime="2.85467e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85467e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4986e+08" > 4.6381e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4985e+08" > 3.2964e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6495e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3767e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.1526e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4398e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5584e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0430e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1333e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8297e+01 </func>
</region>
</regions>
<internal rank="527" log_i="1723712895.685518" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="528" mpi_size="768" stamp_init="1723712830.489393" stamp_final="1723712895.683402" username="apac4" allocationname="unknown" flags="0" pid="863728" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51940e+01" utime="4.33176e+01" stime="1.24798e+01" mtime="2.87380e+01" gflop="0.00000e+00" gbyte="3.84956e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87380e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4651466146814cf5568146714f6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50660e+01" utime="4.32893e+01" stime="1.24661e+01" mtime="2.87380e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87380e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4741e+08" > 5.3782e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4698e+08" > 4.0202e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3484e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9925e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4023e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3406e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4388e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5682e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0395e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1390e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8091e+01 </func>
</region>
</regions>
<internal rank="528" log_i="1723712895.683402" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="529" mpi_size="768" stamp_init="1723712830.490272" stamp_final="1723712895.687622" username="apac4" allocationname="unknown" flags="0" pid="863729" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51974e+01" utime="4.91670e+01" stime="7.65479e+00" mtime="2.92887e+01" gflop="0.00000e+00" gbyte="3.78078e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92887e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b514b614b814dd56b814b71468" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50632e+01" utime="4.91290e+01" stime="7.64897e+00" mtime="2.92887e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92887e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4816e+08" > 4.5172e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4850e+08" > 2.9375e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2347e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9997e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0027e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9381e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4395e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5581e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0444e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1389e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8848e+01 </func>
</region>
</regions>
<internal rank="529" log_i="1723712895.687622" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="530" mpi_size="768" stamp_init="1723712830.489157" stamp_final="1723712895.684116" username="apac4" allocationname="unknown" flags="0" pid="863730" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51950e+01" utime="4.73459e+01" stime="8.14774e+00" mtime="2.83240e+01" gflop="0.00000e+00" gbyte="3.75912e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83240e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000af14af14a5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50633e+01" utime="4.73125e+01" stime="8.13905e+00" mtime="2.83240e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83240e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5104e+08" > 6.1145e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4741e+08" > 4.2157e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.4506e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9870e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.9843e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8022e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4391e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5615e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0412e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1385e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8507e+01 </func>
</region>
</regions>
<internal rank="530" log_i="1723712895.684116" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="531" mpi_size="768" stamp_init="1723712830.489166" stamp_final="1723712895.692123" username="apac4" allocationname="unknown" flags="0" pid="863731" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.52030e+01" utime="4.94408e+01" stime="7.43463e+00" mtime="2.95233e+01" gflop="0.00000e+00" gbyte="3.76972e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.95233e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50705e+01" utime="4.94139e+01" stime="7.41957e+00" mtime="2.95233e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.95233e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4720e+08" > 4.6046e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5059e+08" > 2.6994e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1327e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0001e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4305e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9066e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4393e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5637e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0446e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1383e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9177e+01 </func>
</region>
</regions>
<internal rank="531" log_i="1723712895.692123" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="532" mpi_size="768" stamp_init="1723712830.489159" stamp_final="1723712895.690999" username="apac4" allocationname="unknown" flags="0" pid="863732" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.52018e+01" utime="4.56567e+01" stime="8.93135e+00" mtime="2.82973e+01" gflop="0.00000e+00" gbyte="3.76186e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82973e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf46e1470147114c35571147114fe" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50739e+01" utime="4.56255e+01" stime="8.91994e+00" mtime="2.82973e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82973e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4993e+08" > 7.5262e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4937e+08" > 5.3812e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7386e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0015e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4302e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6779e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4393e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5625e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0405e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1383e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8023e+01 </func>
</region>
</regions>
<internal rank="532" log_i="1723712895.690999" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="533" mpi_size="768" stamp_init="1723712830.490902" stamp_final="1723712895.688313" username="apac4" allocationname="unknown" flags="0" pid="863733" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51974e+01" utime="4.88735e+01" stime="7.95911e+00" mtime="2.97274e+01" gflop="0.00000e+00" gbyte="3.76156e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.97274e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43a153b153c152e553c153c1506" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50675e+01" utime="4.88463e+01" stime="7.94472e+00" mtime="2.97274e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.97274e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4841e+08" > 4.4693e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5062e+08" > 2.7339e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6936e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9979e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6925e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4387e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5664e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0418e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1389e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8836e+01 </func>
</region>
</regions>
<internal rank="533" log_i="1723712895.688313" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="534" mpi_size="768" stamp_init="1723712830.491097" stamp_final="1723712895.683886" username="apac4" allocationname="unknown" flags="0" pid="863734" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51928e+01" utime="4.74816e+01" stime="8.21805e+00" mtime="2.87156e+01" gflop="0.00000e+00" gbyte="3.77689e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87156e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4451447144814bc564814481486" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50631e+01" utime="4.74510e+01" stime="8.20632e+00" mtime="2.87156e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87156e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127164" bytes="8.4692e+08" > 5.5184e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4909e+08" > 4.3156e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2667e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9902e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.8399e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2780e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4376e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5780e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0419e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1381e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8138e+01 </func>
</region>
</regions>
<internal rank="534" log_i="1723712895.683886" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="535" mpi_size="768" stamp_init="1723712830.489396" stamp_final="1723712895.687673" username="apac4" allocationname="unknown" flags="0" pid="863735" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51983e+01" utime="4.88747e+01" stime="7.65041e+00" mtime="2.91523e+01" gflop="0.00000e+00" gbyte="3.77525e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91523e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50664e+01" utime="4.88437e+01" stime="7.63910e+00" mtime="2.91523e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91523e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4917e+08" > 4.5365e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4611e+08" > 2.6965e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0242e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9778e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0967e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.5134e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4379e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5790e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0468e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1380e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8943e+01 </func>
</region>
</regions>
<internal rank="535" log_i="1723712895.687673" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="536" mpi_size="768" stamp_init="1723712830.489162" stamp_final="1723712895.686215" username="apac4" allocationname="unknown" flags="0" pid="863736" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51971e+01" utime="4.69506e+01" stime="8.50867e+00" mtime="2.88712e+01" gflop="0.00000e+00" gbyte="3.75824e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88712e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50664e+01" utime="4.69240e+01" stime="8.49352e+00" mtime="2.88712e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88712e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4974e+08" > 5.0798e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4839e+08" > 4.5457e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5670e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9988e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7271e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3566e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4374e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5798e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0414e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1387e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8025e+01 </func>
</region>
</regions>
<internal rank="536" log_i="1723712895.686215" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="537" mpi_size="768" stamp_init="1723712830.490440" stamp_final="1723712895.681698" username="apac4" allocationname="unknown" flags="0" pid="863737" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51913e+01" utime="4.89775e+01" stime="7.82139e+00" mtime="2.94606e+01" gflop="0.00000e+00" gbyte="3.75889e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.94606e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43b153c153d15c8563d153d1524" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50605e+01" utime="4.89466e+01" stime="7.81076e+00" mtime="2.94606e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.94606e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4984e+08" > 3.4985e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5032e+08" > 2.7997e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4369e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0003e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9789e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2877e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4372e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5858e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0404e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1382e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8918e+01 </func>
</region>
</regions>
<internal rank="537" log_i="1723712895.681698" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="538" mpi_size="768" stamp_init="1723712830.491083" stamp_final="1723712895.689416" username="apac4" allocationname="unknown" flags="0" pid="863738" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51983e+01" utime="4.73111e+01" stime="8.30498e+00" mtime="2.91756e+01" gflop="0.00000e+00" gbyte="3.77697e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91756e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf41d141e142014465520141f148b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50725e+01" utime="4.72760e+01" stime="8.29787e+00" mtime="2.91756e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91756e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4876e+08" > 5.3751e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4842e+08" > 3.7928e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4293e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.0016e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.8889e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3802e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4371e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5859e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0399e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1388e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8444e+01 </func>
</region>
</regions>
<internal rank="538" log_i="1723712895.689416" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="539" mpi_size="768" stamp_init="1723712830.489164" stamp_final="1723712895.682926" username="apac4" allocationname="unknown" flags="0" pid="863739" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51938e+01" utime="4.95498e+01" stime="7.30938e+00" mtime="2.92456e+01" gflop="0.00000e+00" gbyte="3.77403e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92456e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004b15a0564b154b153f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50626e+01" utime="4.95175e+01" stime="7.29924e+00" mtime="2.92456e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92456e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4882e+08" > 3.6841e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4855e+08" > 3.1968e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8773e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9863e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9312e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7068e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4374e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5834e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0434e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1383e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9256e+01 </func>
</region>
</regions>
<internal rank="539" log_i="1723712895.682926" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="540" mpi_size="768" stamp_init="1723712830.491078" stamp_final="1723712895.679150" username="apac4" allocationname="unknown" flags="0" pid="863740" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51881e+01" utime="4.70957e+01" stime="8.11147e+00" mtime="2.82191e+01" gflop="0.00000e+00" gbyte="3.76369e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82191e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000029142914fd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50580e+01" utime="4.70681e+01" stime="8.09707e+00" mtime="2.82191e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82191e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4834e+08" > 5.2236e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4961e+08" > 4.0078e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.5930e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9885e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.9601e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3785e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4366e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5895e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0407e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1387e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8350e+01 </func>
</region>
</regions>
<internal rank="540" log_i="1723712895.679150" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="541" mpi_size="768" stamp_init="1723712830.489295" stamp_final="1723712895.678427" username="apac4" allocationname="unknown" flags="0" pid="863741" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51891e+01" utime="4.92882e+01" stime="7.56142e+00" mtime="2.95210e+01" gflop="0.00000e+00" gbyte="3.78044e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.95210e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000010141014c5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50588e+01" utime="4.92564e+01" stime="7.55136e+00" mtime="2.95210e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.95210e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4951e+08" > 3.5308e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4896e+08" > 2.9420e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2959e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9943e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3349e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4365e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5926e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0404e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1383e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9123e+01 </func>
</region>
</regions>
<internal rank="541" log_i="1723712895.678427" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="542" mpi_size="768" stamp_init="1723712830.489840" stamp_final="1723712895.690385" username="apac4" allocationname="unknown" flags="0" pid="863742" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.52005e+01" utime="4.75317e+01" stime="8.27317e+00" mtime="2.87693e+01" gflop="0.00000e+00" gbyte="3.77785e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87693e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002314b85623142314a4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50721e+01" utime="4.75019e+01" stime="8.26026e+00" mtime="2.87693e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87693e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4938e+08" > 4.7043e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4760e+08" > 3.5349e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3494e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9976e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.1287e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6961e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4362e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5955e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0458e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1386e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8191e+01 </func>
</region>
</regions>
<internal rank="542" log_i="1723712895.690385" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="543" mpi_size="768" stamp_init="1723712830.489171" stamp_final="1723712895.681591" username="apac4" allocationname="unknown" flags="0" pid="863743" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51924e+01" utime="4.91357e+01" stime="7.43582e+00" mtime="2.91289e+01" gflop="0.00000e+00" gbyte="3.77438e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91289e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000073155c557315731519" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50609e+01" utime="4.91021e+01" stime="7.42706e+00" mtime="2.91289e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91289e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4647e+08" > 3.7128e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5008e+08" > 2.4417e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3284e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9984e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9630e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4359e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.5952e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0407e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1388e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8677e+01 </func>
</region>
</regions>
<internal rank="543" log_i="1723712895.681591" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="544" mpi_size="768" stamp_init="1723712830.489178" stamp_final="1723712895.689934" username="apac4" allocationname="unknown" flags="0" pid="863744" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.52008e+01" utime="4.57124e+01" stime="8.75014e+00" mtime="2.78462e+01" gflop="0.00000e+00" gbyte="3.76835e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78462e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50706e+01" utime="4.56794e+01" stime="8.74072e+00" mtime="2.78462e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78462e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4864e+08" > 7.7190e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4928e+08" > 5.1628e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7966e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9796e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1121e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4781e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4348e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6092e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0437e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1385e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7520e+01 </func>
</region>
</regions>
<internal rank="544" log_i="1723712895.689934" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="545" mpi_size="768" stamp_init="1723712830.491073" stamp_final="1723712895.678781" username="apac4" allocationname="unknown" flags="0" pid="863745" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51877e+01" utime="4.91131e+01" stime="7.74235e+00" mtime="2.93085e+01" gflop="0.00000e+00" gbyte="3.77544e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.93085e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000db14f756db14da14b0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50575e+01" utime="4.90838e+01" stime="7.72890e+00" mtime="2.93085e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.93085e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4955e+08" > 4.8445e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4967e+08" > 2.8734e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4293e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9970e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.3646e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4345e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6097e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0448e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1332e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8640e+01 </func>
</region>
</regions>
<internal rank="545" log_i="1723712895.678781" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="546" mpi_size="768" stamp_init="1723712830.489185" stamp_final="1723712895.677474" username="apac4" allocationname="unknown" flags="0" pid="863746" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51883e+01" utime="4.63344e+01" stime="8.56715e+00" mtime="2.81567e+01" gflop="0.00000e+00" gbyte="3.76453e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81567e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50593e+01" utime="4.63114e+01" stime="8.54776e+00" mtime="2.81567e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81567e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4880e+08" > 7.8343e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4828e+08" > 5.5913e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8772e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9804e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.9353e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.3941e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4340e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6153e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0455e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1382e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7733e+01 </func>
</region>
</regions>
<internal rank="546" log_i="1723712895.677474" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="547" mpi_size="768" stamp_init="1723712830.489395" stamp_final="1723712895.687334" username="apac4" allocationname="unknown" flags="0" pid="863747" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51979e+01" utime="4.92933e+01" stime="7.53400e+00" mtime="2.91246e+01" gflop="0.00000e+00" gbyte="3.77193e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91246e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d414d614d714e455d714d71458" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50674e+01" utime="4.92668e+01" stime="7.51809e+00" mtime="2.91246e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91246e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4843e+08" > 4.8186e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4867e+08" > 3.3120e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1487e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9897e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1696e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.5374e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4331e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6235e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0429e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1334e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8744e+01 </func>
</region>
</regions>
<internal rank="547" log_i="1723712895.687334" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="548" mpi_size="768" stamp_init="1723712830.489173" stamp_final="1723712895.683701" username="apac4" allocationname="unknown" flags="0" pid="863748" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51945e+01" utime="4.71082e+01" stime="8.44735e+00" mtime="2.88755e+01" gflop="0.00000e+00" gbyte="3.74058e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88755e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004c15d0554c154c1551" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50656e+01" utime="4.70765e+01" stime="8.43629e+00" mtime="2.88755e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88755e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4825e+08" > 5.9646e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4777e+08" > 4.8511e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3892e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9903e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.9986e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3570e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4338e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6153e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0435e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1390e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8125e+01 </func>
</region>
</regions>
<internal rank="548" log_i="1723712895.683701" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="549" mpi_size="768" stamp_init="1723712830.489750" stamp_final="1723712895.695355" username="apac4" allocationname="unknown" flags="0" pid="863749" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.52056e+01" utime="4.92014e+01" stime="7.58013e+00" mtime="2.91576e+01" gflop="0.00000e+00" gbyte="3.76896e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91576e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f814fa14fb14af55fb14fa14ee" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50751e+01" utime="4.91711e+01" stime="7.56812e+00" mtime="2.91576e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91576e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4876e+08" > 5.0221e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4746e+08" > 3.0934e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1260e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9957e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3623e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4330e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6246e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0441e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1331e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8778e+01 </func>
</region>
</regions>
<internal rank="549" log_i="1723712895.695355" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="550" mpi_size="768" stamp_init="1723712830.489182" stamp_final="1723712895.690168" username="apac4" allocationname="unknown" flags="0" pid="863750" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.52010e+01" utime="4.71693e+01" stime="8.39286e+00" mtime="2.85910e+01" gflop="0.00000e+00" gbyte="3.76865e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85910e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008b158b1544" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50728e+01" utime="4.71373e+01" stime="8.38264e+00" mtime="2.85910e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85910e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4599e+08" > 5.8496e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4704e+08" > 4.4616e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6799e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9768e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.7591e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3787e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4336e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6228e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0406e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1386e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7574e+01 </func>
</region>
</regions>
<internal rank="550" log_i="1723712895.690168" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="551" mpi_size="768" stamp_init="1723712830.489450" stamp_final="1723712895.688781" username="apac4" allocationname="unknown" flags="0" pid="863751" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u21b</host>
<perf wtime="6.51993e+01" utime="4.89646e+01" stime="7.85784e+00" mtime="2.94687e+01" gflop="0.00000e+00" gbyte="3.76949e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.94687e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006615661501" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.50685e+01" utime="4.89356e+01" stime="7.84439e+00" mtime="2.94687e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.94687e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4879e+08" > 4.7877e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4745e+08" > 2.7568e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6411e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9941e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3275e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4322e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6375e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0435e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1384e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8602e+01 </func>
</region>
</regions>
<internal rank="551" log_i="1723712895.688781" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="552" mpi_size="768" stamp_init="1723712830.382132" stamp_final="1723712895.684355" username="apac4" allocationname="unknown" flags="0" pid="1712907" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.53022e+01" utime="4.28390e+01" stime="1.28744e+01" mtime="2.86000e+01" gflop="0.00000e+00" gbyte="3.86536e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86000e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002b142b1499" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51776e+01" utime="4.28044e+01" stime="1.28666e+01" mtime="2.86000e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86000e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4859e+08" > 5.4180e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4855e+08" > 3.5943e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4067e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.1885e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.1208e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.4860e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4310e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6485e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0481e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1416e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7694e+01 </func>
</region>
</regions>
<internal rank="552" log_i="1723712895.684355" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="553" mpi_size="768" stamp_init="1723712830.383799" stamp_final="1723712895.690219" username="apac4" allocationname="unknown" flags="0" pid="1712908" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.53064e+01" utime="4.94858e+01" stime="7.22589e+00" mtime="2.88222e+01" gflop="0.00000e+00" gbyte="3.77811e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88222e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51742e+01" utime="4.94570e+01" stime="7.21110e+00" mtime="2.88222e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88222e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4935e+08" > 4.7383e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4862e+08" > 3.2901e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8600e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.1945e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.7852e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4331e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6234e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0472e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1414e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8532e+01 </func>
</region>
</regions>
<internal rank="553" log_i="1723712895.690219" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="554" mpi_size="768" stamp_init="1723712830.383537" stamp_final="1723712895.693539" username="apac4" allocationname="unknown" flags="0" pid="1712909" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.53100e+01" utime="4.76756e+01" stime="8.04586e+00" mtime="2.88519e+01" gflop="0.00000e+00" gbyte="3.76888e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88519e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51861e+01" utime="4.76374e+01" stime="8.04208e+00" mtime="2.88519e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88519e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.5763e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4686e+08" > 5.5433e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4949e+08" > 4.9525e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2710e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.1985e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.6001e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.4090e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4329e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6260e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0457e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1415e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8050e+01 </func>
</region>
</regions>
<internal rank="554" log_i="1723712895.693539" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="555" mpi_size="768" stamp_init="1723712830.382124" stamp_final="1723712895.682479" username="apac4" allocationname="unknown" flags="0" pid="1712910" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.53004e+01" utime="4.97479e+01" stime="7.13039e+00" mtime="2.91822e+01" gflop="0.00000e+00" gbyte="3.76881e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91822e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ac15cc55ac15ab1546" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51722e+01" utime="4.97111e+01" stime="7.12508e+00" mtime="2.91822e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91822e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4898e+08" > 4.3823e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4844e+08" > 2.9850e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.6101e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.2029e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.8798e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4306e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6519e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0475e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1423e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9171e+01 </func>
</region>
</regions>
<internal rank="555" log_i="1723712895.682479" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="556" mpi_size="768" stamp_init="1723712830.382145" stamp_final="1723712895.681025" username="apac4" allocationname="unknown" flags="0" pid="1712911" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.52989e+01" utime="4.66126e+01" stime="8.39991e+00" mtime="2.88218e+01" gflop="0.00000e+00" gbyte="3.76045e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88218e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000002115211532" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51712e+01" utime="4.65852e+01" stime="8.38567e+00" mtime="2.88218e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88218e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4886e+08" > 7.4807e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4912e+08" > 6.2113e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3310e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.1919e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.7350e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.9540e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4315e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6371e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0487e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1415e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7759e+01 </func>
</region>
</regions>
<internal rank="556" log_i="1723712895.681025" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="557" mpi_size="768" stamp_init="1723712830.382807" stamp_final="1723712895.683499" username="apac4" allocationname="unknown" flags="0" pid="1712912" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.53007e+01" utime="4.94454e+01" stime="7.46431e+00" mtime="2.93391e+01" gflop="0.00000e+00" gbyte="3.77155e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.93391e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51751e+01" utime="4.94068e+01" stime="7.46104e+00" mtime="2.93391e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.93391e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4949e+08" > 4.2455e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4890e+08" > 2.7415e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9317e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.2012e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0729e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0774e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4323e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6356e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0489e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1421e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9023e+01 </func>
</region>
</regions>
<internal rank="557" log_i="1723712895.683499" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="558" mpi_size="768" stamp_init="1723712830.382156" stamp_final="1723712895.686713" username="apac4" allocationname="unknown" flags="0" pid="1712913" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.53046e+01" utime="4.73226e+01" stime="8.16250e+00" mtime="2.88858e+01" gflop="0.00000e+00" gbyte="3.77327e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88858e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007315731549" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51788e+01" utime="4.72825e+01" stime="8.16000e+00" mtime="2.88858e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88858e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4870e+08" > 5.7788e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4877e+08" > 4.1501e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4552e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.2088e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.9155e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.9018e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4324e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6339e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0501e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1417e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7873e+01 </func>
</region>
</regions>
<internal rank="558" log_i="1723712895.686713" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="559" mpi_size="768" stamp_init="1723712830.382775" stamp_final="1723712895.695877" username="apac4" allocationname="unknown" flags="0" pid="1712914" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.53131e+01" utime="4.95552e+01" stime="7.40767e+00" mtime="2.95333e+01" gflop="0.00000e+00" gbyte="3.77880e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.95333e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000fa15fa55fa15f91515" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51861e+01" utime="4.95221e+01" stime="7.39955e+00" mtime="2.95333e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.95333e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4974e+08" > 4.4166e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4718e+08" > 2.8368e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3904e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.2021e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.2111e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4304e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6520e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0464e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1417e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8750e+01 </func>
</region>
</regions>
<internal rank="559" log_i="1723712895.695877" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="560" mpi_size="768" stamp_init="1723712830.384306" stamp_final="1723712895.679741" username="apac4" allocationname="unknown" flags="0" pid="1712915" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.52954e+01" utime="4.77725e+01" stime="8.01512e+00" mtime="2.88165e+01" gflop="0.00000e+00" gbyte="3.77609e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88165e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004014d45540144014cf" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51724e+01" utime="4.77349e+01" stime="8.01076e+00" mtime="2.88165e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88165e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4922e+08" > 4.9383e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4830e+08" > 3.2428e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2536e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.2026e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.8982e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7622e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4295e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6666e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0466e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1420e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8102e+01 </func>
</region>
</regions>
<internal rank="560" log_i="1723712895.679741" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="561" mpi_size="768" stamp_init="1723712830.382589" stamp_final="1723712895.679576" username="apac4" allocationname="unknown" flags="0" pid="1712916" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.52970e+01" utime="4.94926e+01" stime="7.40560e+00" mtime="2.94721e+01" gflop="0.00000e+00" gbyte="3.77476e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.94721e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51706e+01" utime="4.94611e+01" stime="7.39511e+00" mtime="2.94721e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.94721e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4913e+08" > 3.6030e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4933e+08" > 2.7267e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2551e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.1750e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5259e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0378e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4289e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6677e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0448e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1367e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8927e+01 </func>
</region>
</regions>
<internal rank="561" log_i="1723712895.679576" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="562" mpi_size="768" stamp_init="1723712830.384259" stamp_final="1723712895.679152" username="apac4" allocationname="unknown" flags="0" pid="1712917" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.52949e+01" utime="4.76619e+01" stime="8.17753e+00" mtime="2.88581e+01" gflop="0.00000e+00" gbyte="3.77495e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88581e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48f1590159115c155911591154a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51707e+01" utime="4.76368e+01" stime="8.16066e+00" mtime="2.88581e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88581e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4982e+08" > 4.8357e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4854e+08" > 3.8533e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3326e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.2113e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2735e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.8611e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4301e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6529e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0509e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1421e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8063e+01 </func>
</region>
</regions>
<internal rank="562" log_i="1723712895.679152" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="563" mpi_size="768" stamp_init="1723712830.382118" stamp_final="1723712895.678810" username="apac4" allocationname="unknown" flags="0" pid="1712918" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.52967e+01" utime="4.91206e+01" stime="7.52867e+00" mtime="2.95979e+01" gflop="0.00000e+00" gbyte="3.76469e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.95979e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf495149714981463559814981499" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51704e+01" utime="4.90910e+01" stime="7.51600e+00" mtime="2.95979e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.95979e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4830e+08" > 3.4201e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4957e+08" > 2.5079e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1602e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.2090e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.8680e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4304e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6559e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0503e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1415e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9132e+01 </func>
</region>
</regions>
<internal rank="563" log_i="1723712895.678810" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="564" mpi_size="768" stamp_init="1723712830.382125" stamp_final="1723712895.686161" username="apac4" allocationname="unknown" flags="0" pid="1712919" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.53040e+01" utime="4.70384e+01" stime="8.20955e+00" mtime="2.89139e+01" gflop="0.00000e+00" gbyte="3.76991e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89139e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005014f55650144f1476" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51794e+01" utime="4.70055e+01" stime="8.20000e+00" mtime="2.89139e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89139e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5077e+08" > 5.0177e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4934e+08" > 4.1840e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4734e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.1809e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4043e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7400e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4299e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6598e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0462e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1420e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7987e+01 </func>
</region>
</regions>
<internal rank="564" log_i="1723712895.686161" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="565" mpi_size="768" stamp_init="1723712830.384262" stamp_final="1723712895.679399" username="apac4" allocationname="unknown" flags="0" pid="1712920" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.52951e+01" utime="4.97008e+01" stime="7.24047e+00" mtime="2.94317e+01" gflop="0.00000e+00" gbyte="3.77361e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.94317e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000be149156be14bd14b6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51675e+01" utime="4.96670e+01" stime="7.23235e+00" mtime="2.94317e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.94317e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4902e+08" > 3.6384e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4991e+08" > 2.5568e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0859e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.2096e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.9760e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4284e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6734e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0454e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1411e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9018e+01 </func>
</region>
</regions>
<internal rank="565" log_i="1723712895.679399" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="566" mpi_size="768" stamp_init="1723712830.384275" stamp_final="1723712895.679740" username="apac4" allocationname="unknown" flags="0" pid="1712921" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.52955e+01" utime="4.74481e+01" stime="8.14223e+00" mtime="2.89883e+01" gflop="0.00000e+00" gbyte="3.77010e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89883e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a215a11507" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51717e+01" utime="4.74162e+01" stime="8.13228e+00" mtime="2.89883e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89883e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4933e+08" > 4.5407e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4837e+08" > 3.4562e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6038e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.1925e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.3923e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.3611e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4281e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6789e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0454e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1419e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7974e+01 </func>
</region>
</regions>
<internal rank="566" log_i="1723712895.679740" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="567" mpi_size="768" stamp_init="1723712830.382317" stamp_final="1723712895.683819" username="apac4" allocationname="unknown" flags="0" pid="1712922" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.53015e+01" utime="4.96118e+01" stime="7.29138e+00" mtime="2.93619e+01" gflop="0.00000e+00" gbyte="3.76705e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.93619e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000050154f1518" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51747e+01" utime="4.95788e+01" stime="7.28231e+00" mtime="2.93619e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.93619e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4855e+08" > 3.6096e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4935e+08" > 2.5131e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9851e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.2050e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.8151e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4286e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6741e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0448e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1422e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9056e+01 </func>
</region>
</regions>
<internal rank="567" log_i="1723712895.683819" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="568" mpi_size="768" stamp_init="1723712830.384049" stamp_final="1723712895.680883" username="apac4" allocationname="unknown" flags="0" pid="1712923" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.52968e+01" utime="4.64299e+01" stime="8.61430e+00" mtime="2.86959e+01" gflop="0.00000e+00" gbyte="3.77224e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86959e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51703e+01" utime="4.63966e+01" stime="8.60529e+00" mtime="2.86959e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86959e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.6294e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4841e+08" > 6.8095e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4882e+08" > 4.8347e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1281e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.1985e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9829e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7238e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4276e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6802e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0469e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1418e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7910e+01 </func>
</region>
</regions>
<internal rank="568" log_i="1723712895.680883" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="569" mpi_size="768" stamp_init="1723712830.382458" stamp_final="1723712895.685892" username="apac4" allocationname="unknown" flags="0" pid="1712924" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.53034e+01" utime="4.92066e+01" stime="7.64487e+00" mtime="2.92674e+01" gflop="0.00000e+00" gbyte="3.74123e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92674e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51766e+01" utime="4.91727e+01" stime="7.63732e+00" mtime="2.92674e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92674e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.4067e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4880e+08" > 4.7423e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4960e+08" > 3.3154e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4792e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.1954e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0169e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4278e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6785e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0455e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1418e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8355e+01 </func>
</region>
</regions>
<internal rank="569" log_i="1723712895.685892" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="570" mpi_size="768" stamp_init="1723712830.382800" stamp_final="1723712895.689805" username="apac4" allocationname="unknown" flags="0" pid="1712925" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.53070e+01" utime="4.74591e+01" stime="8.28445e+00" mtime="2.88297e+01" gflop="0.00000e+00" gbyte="3.77312e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88297e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002415f555241524154b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51800e+01" utime="4.74222e+01" stime="8.27926e+00" mtime="2.88297e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88297e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4942e+08" > 5.9344e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4901e+08" > 4.4646e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4483e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.1798e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.3910e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8406e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4281e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6786e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0475e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1419e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7835e+01 </func>
</region>
</regions>
<internal rank="570" log_i="1723712895.689805" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="571" mpi_size="768" stamp_init="1723712830.382501" stamp_final="1723712895.693795" username="apac4" allocationname="unknown" flags="0" pid="1712926" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.53113e+01" utime="4.93078e+01" stime="7.51501e+00" mtime="2.93665e+01" gflop="0.00000e+00" gbyte="3.74771e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.93665e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000099149914d4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51841e+01" utime="4.92740e+01" stime="7.50684e+00" mtime="2.93665e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.93665e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4946e+08" > 4.7540e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4979e+08" > 3.1109e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3226e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.1948e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.3020e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4283e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6775e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0470e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1416e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8614e+01 </func>
</region>
</regions>
<internal rank="571" log_i="1723712895.693795" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="572" mpi_size="768" stamp_init="1723712830.383905" stamp_final="1723712895.683132" username="apac4" allocationname="unknown" flags="0" pid="1712927" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.52992e+01" utime="4.60811e+01" stime="8.64307e+00" mtime="2.85356e+01" gflop="0.00000e+00" gbyte="3.76991e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85356e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000c115c11500" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51737e+01" utime="4.60473e+01" stime="8.63514e+00" mtime="2.85356e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85356e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4894e+08" > 7.0303e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5058e+08" > 4.4995e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4497e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.1963e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.2943e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7868e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4273e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6874e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0510e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1421e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7413e+01 </func>
</region>
</regions>
<internal rank="572" log_i="1723712895.683132" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="573" mpi_size="768" stamp_init="1723712830.384151" stamp_final="1723712895.690027" username="apac4" allocationname="unknown" flags="0" pid="1712928" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.53059e+01" utime="4.96538e+01" stime="7.26608e+00" mtime="2.90739e+01" gflop="0.00000e+00" gbyte="3.77438e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90739e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48f15901591152a559115911533" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51802e+01" utime="4.96319e+01" stime="7.24582e+00" mtime="2.90739e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90739e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4985e+08" > 4.6445e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4844e+08" > 2.5960e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7352e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.2027e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.4920e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4276e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6844e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0448e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1423e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8916e+01 </func>
</region>
</regions>
<internal rank="573" log_i="1723712895.690027" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="574" mpi_size="768" stamp_init="1723712830.382151" stamp_final="1723712895.681417" username="apac4" allocationname="unknown" flags="0" pid="1712929" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.52993e+01" utime="4.74613e+01" stime="8.25827e+00" mtime="2.89001e+01" gflop="0.00000e+00" gbyte="3.76438e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89001e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002714545527142714e5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51726e+01" utime="4.74300e+01" stime="8.24719e+00" mtime="2.89001e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89001e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4982e+08" > 5.9957e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4922e+08" > 4.1121e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6169e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.1943e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3959e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.7621e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4269e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.6873e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0486e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1413e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7717e+01 </func>
</region>
</regions>
<internal rank="574" log_i="1723712895.681417" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="575" mpi_size="768" stamp_init="1723712830.382399" stamp_final="1723712895.689933" username="apac4" allocationname="unknown" flags="0" pid="1712930" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a03u14a</host>
<perf wtime="6.53075e+01" utime="4.94031e+01" stime="7.57166e+00" mtime="2.95984e+01" gflop="0.00000e+00" gbyte="3.77460e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.95984e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf42215231524155956241524150f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51807e+01" utime="4.93684e+01" stime="7.56376e+00" mtime="2.95984e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.95984e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4938e+08" > 4.6587e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4990e+08" > 2.9570e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5307e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 2.2108e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6212e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.7788e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4258e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7039e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0469e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1411e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8632e+01 </func>
</region>
</regions>
<internal rank="575" log_i="1723712895.689933" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="576" mpi_size="768" stamp_init="1723712830.580910" stamp_final="1723712895.677838" username="apac4" allocationname="unknown" flags="0" pid="2776151" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.50969e+01" utime="4.29170e+01" stime="1.25646e+01" mtime="2.84931e+01" gflop="0.00000e+00" gbyte="3.86448e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84931e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b414af1485" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49648e+01" utime="4.28872e+01" stime="1.25519e+01" mtime="2.84931e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84931e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4785e+08" > 5.8193e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4739e+08" > 4.1935e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0095e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5109e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3612e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8408e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4254e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7070e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0437e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1409e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8390e+01 </func>
</region>
</regions>
<internal rank="576" log_i="1723712895.677838" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="577" mpi_size="768" stamp_init="1723712830.580725" stamp_final="1723712895.688922" username="apac4" allocationname="unknown" flags="0" pid="2776152" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51082e+01" utime="4.93786e+01" stime="7.28028e+00" mtime="2.85554e+01" gflop="0.00000e+00" gbyte="3.76629e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85554e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf41a14331445145f55451440148e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49730e+01" utime="4.93474e+01" stime="7.26840e+00" mtime="2.85554e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85554e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4893e+08" > 4.6698e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4848e+08" > 3.1450e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9281e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7352e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.4292e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4253e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7074e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0419e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1404e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8667e+01 </func>
</region>
</regions>
<internal rank="577" log_i="1723712895.688922" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="578" mpi_size="768" stamp_init="1723712830.580701" stamp_final="1723712895.681172" username="apac4" allocationname="unknown" flags="0" pid="2776153" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51005e+01" utime="4.77620e+01" stime="7.91068e+00" mtime="2.82791e+01" gflop="0.00000e+00" gbyte="3.76659e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82791e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000df14de14d1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49680e+01" utime="4.77390e+01" stime="7.89098e+00" mtime="2.82791e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82791e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5003e+08" > 5.4286e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4818e+08" > 4.1852e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0187e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7388e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.6560e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5206e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4260e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7001e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0426e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1410e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8211e+01 </func>
</region>
</regions>
<internal rank="578" log_i="1723712895.681172" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="579" mpi_size="768" stamp_init="1723712830.580788" stamp_final="1723712895.682298" username="apac4" allocationname="unknown" flags="0" pid="2776154" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51015e+01" utime="4.93979e+01" stime="7.22620e+00" mtime="2.89362e+01" gflop="0.00000e+00" gbyte="3.77815e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89362e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b314e655b314b3149a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49680e+01" utime="4.93685e+01" stime="7.21269e+00" mtime="2.89362e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89362e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4907e+08" > 4.5824e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4948e+08" > 2.9759e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8274e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7427e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.9971e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4261e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7031e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0449e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1403e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9150e+01 </func>
</region>
</regions>
<internal rank="579" log_i="1723712895.682298" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="580" mpi_size="768" stamp_init="1723712830.580676" stamp_final="1723712895.681215" username="apac4" allocationname="unknown" flags="0" pid="2776155" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51005e+01" utime="4.53277e+01" stime="8.58869e+00" mtime="2.75327e+01" gflop="0.00000e+00" gbyte="3.74245e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.75327e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000053145214c8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49693e+01" utime="4.52966e+01" stime="8.57753e+00" mtime="2.75327e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.75327e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.5020e-05 </func>
<func name="MPI_Isend" count="127164" bytes="8.4923e+08" > 8.7982e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4974e+08" > 5.8523e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.5734e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7422e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0082e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.4569e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4241e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7173e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0478e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1410e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7550e+01 </func>
</region>
</regions>
<internal rank="580" log_i="1723712895.681215" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="581" mpi_size="768" stamp_init="1723712830.580735" stamp_final="1723712895.691847" username="apac4" allocationname="unknown" flags="0" pid="2776156" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51111e+01" utime="4.91350e+01" stime="7.35309e+00" mtime="2.87151e+01" gflop="0.00000e+00" gbyte="3.76354e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87151e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4931494149514f756951495146c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49782e+01" utime="4.91005e+01" stime="7.34572e+00" mtime="2.87151e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87151e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4885e+08" > 4.4843e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4853e+08" > 2.8285e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7221e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7303e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2159e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7507e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4255e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7100e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0444e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1404e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9057e+01 </func>
</region>
</regions>
<internal rank="581" log_i="1723712895.691847" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="582" mpi_size="768" stamp_init="1723712830.580692" stamp_final="1723712895.689970" username="apac4" allocationname="unknown" flags="0" pid="2776157" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51093e+01" utime="4.74517e+01" stime="8.11275e+00" mtime="2.83456e+01" gflop="0.00000e+00" gbyte="3.77720e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83456e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d6154b55d615d61503" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49753e+01" utime="4.74199e+01" stime="8.10186e+00" mtime="2.83456e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83456e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4774e+08" > 6.0098e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4754e+08" > 3.5778e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2866e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7361e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.0109e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.4552e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4249e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7124e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0431e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1409e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7959e+01 </func>
</region>
</regions>
<internal rank="582" log_i="1723712895.689970" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="583" mpi_size="768" stamp_init="1723712830.580916" stamp_final="1723712895.689427" username="apac4" allocationname="unknown" flags="0" pid="2776158" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51085e+01" utime="4.92476e+01" stime="7.40192e+00" mtime="2.89095e+01" gflop="0.00000e+00" gbyte="3.76575e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89095e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49775e+01" utime="4.92140e+01" stime="7.39314e+00" mtime="2.89095e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89095e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4608e+08" > 4.4692e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4792e+08" > 2.7657e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0301e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7175e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0406e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4247e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7182e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0427e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1403e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8956e+01 </func>
</region>
</regions>
<internal rank="583" log_i="1723712895.689427" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="584" mpi_size="768" stamp_init="1723712830.580877" stamp_final="1723712895.687271" username="apac4" allocationname="unknown" flags="0" pid="2776159" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51064e+01" utime="4.76268e+01" stime="7.97456e+00" mtime="2.84615e+01" gflop="0.00000e+00" gbyte="3.77605e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84615e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49738e+01" utime="4.75925e+01" stime="7.96625e+00" mtime="2.84615e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84615e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4851e+08" > 4.5628e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4917e+08" > 4.0096e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4951e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7311e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2681e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6942e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4237e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7242e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0421e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1410e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8011e+01 </func>
</region>
</regions>
<internal rank="584" log_i="1723712895.687271" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="585" mpi_size="768" stamp_init="1723712830.580834" stamp_final="1723712895.678701" username="apac4" allocationname="unknown" flags="0" pid="2776160" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.50979e+01" utime="4.92274e+01" stime="7.28191e+00" mtime="2.88699e+01" gflop="0.00000e+00" gbyte="3.76816e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88699e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49644e+01" utime="4.91961e+01" stime="7.27130e+00" mtime="2.88699e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88699e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4790e+08" > 3.4794e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4848e+08" > 2.4904e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3164e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7404e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0266e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.7812e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4240e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7246e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0469e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1404e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8710e+01 </func>
</region>
</regions>
<internal rank="585" log_i="1723712895.678701" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="586" mpi_size="768" stamp_init="1723712830.580709" stamp_final="1723712895.676438" username="apac4" allocationname="unknown" flags="0" pid="2776161" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.50957e+01" utime="4.68255e+01" stime="7.94903e+00" mtime="2.81651e+01" gflop="0.00000e+00" gbyte="3.76656e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81651e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006c159b556c156b1513" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49633e+01" utime="4.67954e+01" stime="7.93631e+00" mtime="2.81651e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81651e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4939e+08" > 5.5620e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5029e+08" > 4.6041e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9256e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7337e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5862e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8041e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4227e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7289e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0452e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1409e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8176e+01 </func>
</region>
</regions>
<internal rank="586" log_i="1723712895.676438" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="587" mpi_size="768" stamp_init="1723712830.580698" stamp_final="1723712895.685657" username="apac4" allocationname="unknown" flags="0" pid="2776162" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51050e+01" utime="4.95833e+01" stime="7.07845e+00" mtime="2.87536e+01" gflop="0.00000e+00" gbyte="3.77872e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87536e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf49b14b414c6145f56c614c0145d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49719e+01" utime="4.95481e+01" stime="7.07094e+00" mtime="2.87536e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87536e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4965e+08" > 3.5440e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4938e+08" > 2.4648e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.6455e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7413e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0729e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7125e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4240e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7245e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0465e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1403e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9261e+01 </func>
</region>
</regions>
<internal rank="587" log_i="1723712895.685657" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="588" mpi_size="768" stamp_init="1723712830.580673" stamp_final="1723712895.694299" username="apac4" allocationname="unknown" flags="0" pid="2776163" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51136e+01" utime="4.65165e+01" stime="8.34330e+00" mtime="2.85117e+01" gflop="0.00000e+00" gbyte="3.76205e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85117e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43714381439148d5539143914ab" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49821e+01" utime="4.64880e+01" stime="8.32989e+00" mtime="2.85117e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85117e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4930e+08" > 5.3692e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4992e+08" > 4.3865e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4184e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7213e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3222e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6911e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4228e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7316e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0472e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1407e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8063e+01 </func>
</region>
</regions>
<internal rank="588" log_i="1723712895.694299" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="589" mpi_size="768" stamp_init="1723712830.580854" stamp_final="1723712895.682831" username="apac4" allocationname="unknown" flags="0" pid="2776164" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51020e+01" utime="4.95188e+01" stime="7.03775e+00" mtime="2.89843e+01" gflop="0.00000e+00" gbyte="3.77728e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89843e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49690e+01" utime="4.94827e+01" stime="7.03091e+00" mtime="2.89843e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89843e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4988e+08" > 3.6114e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4911e+08" > 2.8587e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0429e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7309e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7189e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4223e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7425e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0465e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1403e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9094e+01 </func>
</region>
</regions>
<internal rank="589" log_i="1723712895.682831" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="590" mpi_size="768" stamp_init="1723712830.580710" stamp_final="1723712895.690821" username="apac4" allocationname="unknown" flags="0" pid="2776165" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51101e+01" utime="4.72680e+01" stime="8.03621e+00" mtime="2.83780e+01" gflop="0.00000e+00" gbyte="3.76564e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83780e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ae14ae1457" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49797e+01" utime="4.72353e+01" stime="8.02673e+00" mtime="2.83780e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83780e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4861e+08" > 5.1104e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4925e+08" > 3.9730e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5291e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7441e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.4680e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7681e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4231e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7296e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0428e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1408e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7827e+01 </func>
</region>
</regions>
<internal rank="590" log_i="1723712895.690821" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="591" mpi_size="768" stamp_init="1723712830.580754" stamp_final="1723712895.682147" username="apac4" allocationname="unknown" flags="0" pid="2776166" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51014e+01" utime="4.94475e+01" stime="7.15129e+00" mtime="2.86465e+01" gflop="0.00000e+00" gbyte="3.76472e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86465e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49693e+01" utime="4.94111e+01" stime="7.14580e+00" mtime="2.86465e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86465e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4916e+08" > 3.4527e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4920e+08" > 2.8774e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9317e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7427e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1921e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7631e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4227e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7380e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0474e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1401e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8871e+01 </func>
</region>
</regions>
<internal rank="591" log_i="1723712895.682147" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="592" mpi_size="768" stamp_init="1723712830.580749" stamp_final="1723712895.686929" username="apac4" allocationname="unknown" flags="0" pid="2776167" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51062e+01" utime="4.72474e+01" stime="8.07351e+00" mtime="2.81922e+01" gflop="0.00000e+00" gbyte="3.76225e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81922e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000069143356691464148f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49747e+01" utime="4.72185e+01" stime="8.06016e+00" mtime="2.81922e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81922e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4742e+08" > 6.5211e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4830e+08" > 4.0456e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8912e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7447e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.2832e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5630e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4223e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7389e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0444e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1359e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8134e+01 </func>
</region>
</regions>
<internal rank="592" log_i="1723712895.686929" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="593" mpi_size="768" stamp_init="1723712830.580701" stamp_final="1723712895.686129" username="apac4" allocationname="unknown" flags="0" pid="2776168" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51054e+01" utime="4.93367e+01" stime="7.32600e+00" mtime="2.87196e+01" gflop="0.00000e+00" gbyte="3.77895e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87196e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000052143d555214511479" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49722e+01" utime="4.93037e+01" stime="7.31585e+00" mtime="2.87196e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87196e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4517e+08" > 4.7184e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4816e+08" > 2.8453e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2191e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7330e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0829e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4217e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7444e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0420e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1403e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8538e+01 </func>
</region>
</regions>
<internal rank="593" log_i="1723712895.686129" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="594" mpi_size="768" stamp_init="1723712830.580708" stamp_final="1723712895.686314" username="apac4" allocationname="unknown" flags="0" pid="2776169" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51056e+01" utime="4.77561e+01" stime="7.93437e+00" mtime="2.85941e+01" gflop="0.00000e+00" gbyte="3.76232e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85941e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b914bb14bc149256bc14bc14f4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49741e+01" utime="4.77290e+01" stime="7.91925e+00" mtime="2.85941e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85941e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4900e+08" > 6.0975e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4901e+08" > 5.4590e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2846e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7345e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.2943e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7020e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4218e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7469e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0432e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1407e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8184e+01 </func>
</region>
</regions>
<internal rank="594" log_i="1723712895.686314" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="595" mpi_size="768" stamp_init="1723712830.580747" stamp_final="1723712895.682289" username="apac4" allocationname="unknown" flags="0" pid="2776170" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51015e+01" utime="4.93496e+01" stime="7.34652e+00" mtime="2.86130e+01" gflop="0.00000e+00" gbyte="3.77487e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86130e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49679e+01" utime="4.93176e+01" stime="7.33570e+00" mtime="2.86130e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86130e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5085e+08" > 4.7395e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4850e+08" > 2.8011e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.4862e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7310e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2173e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.2261e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4214e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7510e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0464e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1404e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9166e+01 </func>
</region>
</regions>
<internal rank="595" log_i="1723712895.682289" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="596" mpi_size="768" stamp_init="1723712830.580741" stamp_final="1723712895.681494" username="apac4" allocationname="unknown" flags="0" pid="2776171" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51008e+01" utime="4.62129e+01" stime="8.64474e+00" mtime="2.82589e+01" gflop="0.00000e+00" gbyte="3.77750e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82589e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d315d515d615a155d615d6152e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49680e+01" utime="4.61806e+01" stime="8.63422e+00" mtime="2.82589e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82589e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4926e+08" > 7.4222e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4974e+08" > 4.7430e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4243e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7417e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3811e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5978e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4206e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7553e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0416e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1408e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7575e+01 </func>
</region>
</regions>
<internal rank="596" log_i="1723712895.681494" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="597" mpi_size="768" stamp_init="1723712830.580756" stamp_final="1723712895.698042" username="apac4" allocationname="unknown" flags="0" pid="2776172" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51173e+01" utime="4.92130e+01" stime="7.48086e+00" mtime="2.89318e+01" gflop="0.00000e+00" gbyte="3.77666e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89318e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008d1493558d148d149f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49838e+01" utime="4.91899e+01" stime="7.46096e+00" mtime="2.89318e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89318e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4855e+08" > 4.7164e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4843e+08" > 2.9304e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2417e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7248e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3351e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6531e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4200e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7672e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0457e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1402e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8738e+01 </func>
</region>
</regions>
<internal rank="597" log_i="1723712895.698042" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="598" mpi_size="768" stamp_init="1723712830.580730" stamp_final="1723712895.690341" username="apac4" allocationname="unknown" flags="0" pid="2776173" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51096e+01" utime="4.77710e+01" stime="8.02863e+00" mtime="2.78820e+01" gflop="0.00000e+00" gbyte="3.75877e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78820e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000003b143b148a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49780e+01" utime="4.77394e+01" stime="8.01808e+00" mtime="2.78820e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78820e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4890e+08" > 6.0554e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4682e+08" > 4.5179e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8975e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7166e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7571e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.5821e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4209e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7570e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0481e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1407e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7890e+01 </func>
</region>
</regions>
<internal rank="598" log_i="1723712895.690341" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="599" mpi_size="768" stamp_init="1723712830.580688" stamp_final="1723712895.683416" username="apac4" allocationname="unknown" flags="0" pid="2776174" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u13a</host>
<perf wtime="6.51027e+01" utime="4.93212e+01" stime="7.39072e+00" mtime="2.84191e+01" gflop="0.00000e+00" gbyte="3.77731e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84191e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4dd15df15e0157155e015e01552" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.49701e+01" utime="4.92925e+01" stime="7.37600e+00" mtime="2.84191e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84191e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.3447e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4827e+08" > 4.7189e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4858e+08" > 2.9864e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8083e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.7453e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.6131e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.2305e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 3.6571e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0432e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1400e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8637e+01 </func>
</region>
</regions>
<internal rank="599" log_i="1723712895.683416" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="600" mpi_size="768" stamp_init="1723712830.236296" stamp_final="1723712895.687195" username="apac4" allocationname="unknown" flags="0" pid="1940088" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54509e+01" utime="4.27056e+01" stime="1.28293e+01" mtime="2.86642e+01" gflop="0.00000e+00" gbyte="3.84300e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86642e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d715d6154f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53214e+01" utime="4.26711e+01" stime="1.28210e+01" mtime="2.86642e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86642e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4959e+08" > 5.6260e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4839e+08" > 5.2285e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3352e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8303e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1532e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0769e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4196e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7695e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0457e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1421e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8150e+01 </func>
</region>
</regions>
<internal rank="600" log_i="1723712895.687195" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="601" mpi_size="768" stamp_init="1723712830.236229" stamp_final="1723712895.683519" username="apac4" allocationname="unknown" flags="0" pid="1940089" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54473e+01" utime="4.93774e+01" stime="7.28646e+00" mtime="2.88434e+01" gflop="0.00000e+00" gbyte="3.77476e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88434e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003514385535143514c1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53131e+01" utime="4.93429e+01" stime="7.27743e+00" mtime="2.88434e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88434e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4770e+08" > 4.5672e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4939e+08" > 3.0456e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0387e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8356e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.7994e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4201e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7639e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0509e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1430e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8760e+01 </func>
</region>
</regions>
<internal rank="601" log_i="1723712895.683519" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="602" mpi_size="768" stamp_init="1723712830.236034" stamp_final="1723712895.693326" username="apac4" allocationname="unknown" flags="0" pid="1940090" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54573e+01" utime="4.70848e+01" stime="8.17054e+00" mtime="2.84388e+01" gflop="0.00000e+00" gbyte="3.77270e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84388e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf499159b159c15f5569c159b151c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53275e+01" utime="4.70532e+01" stime="8.16004e+00" mtime="2.84388e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84388e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4884e+08" > 6.0043e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4847e+08" > 4.0159e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7431e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8331e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.6744e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3893e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4205e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7583e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0489e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1429e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8501e+01 </func>
</region>
</regions>
<internal rank="602" log_i="1723712895.693326" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="603" mpi_size="768" stamp_init="1723712830.237466" stamp_final="1723712895.685279" username="apac4" allocationname="unknown" flags="0" pid="1940091" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54478e+01" utime="4.94330e+01" stime="7.15241e+00" mtime="2.86614e+01" gflop="0.00000e+00" gbyte="3.77335e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86614e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005414c55554145414be" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53191e+01" utime="4.94012e+01" stime="7.14224e+00" mtime="2.86614e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86614e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.5831e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4808e+08" > 4.3719e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4936e+08" > 2.8069e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.3048e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8474e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.2640e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4203e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7619e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0514e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1430e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9320e+01 </func>
</region>
</regions>
<internal rank="603" log_i="1723712895.685279" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="604" mpi_size="768" stamp_init="1723712830.235543" stamp_final="1723712895.677210" username="apac4" allocationname="unknown" flags="0" pid="1940092" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54417e+01" utime="4.75894e+01" stime="7.99316e+00" mtime="2.82694e+01" gflop="0.00000e+00" gbyte="3.76549e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82694e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007414731486" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53117e+01" utime="4.75608e+01" stime="7.97943e+00" mtime="2.82694e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82694e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4976e+08" > 5.9630e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4726e+08" > 4.9495e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.4847e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8388e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2920e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0807e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4195e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7642e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0464e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1376e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8568e+01 </func>
</region>
</regions>
<internal rank="604" log_i="1723712895.677210" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="605" mpi_size="768" stamp_init="1723712830.237839" stamp_final="1723712895.677083" username="apac4" allocationname="unknown" flags="0" pid="1940093" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54392e+01" utime="4.90590e+01" stime="7.52465e+00" mtime="2.92312e+01" gflop="0.00000e+00" gbyte="3.74584e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92312e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf46c146d146e14ef556e146e14e6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53135e+01" utime="4.90270e+01" stime="7.51440e+00" mtime="2.92312e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92312e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.0014e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4852e+08" > 4.4668e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5004e+08" > 3.0869e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1542e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8358e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0419e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4196e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7695e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0495e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1425e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9041e+01 </func>
</region>
</regions>
<internal rank="605" log_i="1723712895.677083" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="606" mpi_size="768" stamp_init="1723712830.235982" stamp_final="1723712895.689683" username="apac4" allocationname="unknown" flags="0" pid="1940094" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54537e+01" utime="4.70928e+01" stime="8.08258e+00" mtime="2.83250e+01" gflop="0.00000e+00" gbyte="3.76305e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83250e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf469156b156c158c556c156c154f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53245e+01" utime="4.70630e+01" stime="8.07018e+00" mtime="2.83250e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83250e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4885e+08" > 7.0276e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4892e+08" > 5.7142e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9230e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8282e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7202e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0826e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4186e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7686e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0461e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1422e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8081e+01 </func>
</region>
</regions>
<internal rank="606" log_i="1723712895.689683" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="607" mpi_size="768" stamp_init="1723712830.235571" stamp_final="1723712895.683267" username="apac4" allocationname="unknown" flags="0" pid="1940095" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54477e+01" utime="4.91293e+01" stime="7.41240e+00" mtime="2.89319e+01" gflop="0.00000e+00" gbyte="3.76316e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89319e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf47d157e157f152e557f157f1511" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53189e+01" utime="4.90953e+01" stime="7.40432e+00" mtime="2.89319e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89319e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4832e+08" > 4.4959e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4885e+08" > 2.9151e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2334e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8377e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1206e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0819e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4182e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7840e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0471e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1425e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8650e+01 </func>
</region>
</regions>
<internal rank="607" log_i="1723712895.683267" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="608" mpi_size="768" stamp_init="1723712830.237718" stamp_final="1723712895.683217" username="apac4" allocationname="unknown" flags="0" pid="1940096" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54455e+01" utime="4.69321e+01" stime="8.35652e+00" mtime="2.90272e+01" gflop="0.00000e+00" gbyte="3.77903e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90272e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d614d814d9144e56d914d914b3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53165e+01" utime="4.69042e+01" stime="8.34227e+00" mtime="2.90272e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90272e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4890e+08" > 5.0826e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4683e+08" > 3.7584e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8063e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5135e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3919e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8282e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4181e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7842e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0459e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1425e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8099e+01 </func>
</region>
</regions>
<internal rank="608" log_i="1723712895.683217" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="609" mpi_size="768" stamp_init="1723712830.236233" stamp_final="1723712895.677288" username="apac4" allocationname="unknown" flags="0" pid="1940097" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54411e+01" utime="4.94263e+01" stime="7.36108e+00" mtime="2.91975e+01" gflop="0.00000e+00" gbyte="3.76865e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91975e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4dc14de14df146a56df14de14ed" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53125e+01" utime="4.93994e+01" stime="7.34560e+00" mtime="2.91975e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91975e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4868e+08" > 3.6021e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4841e+08" > 2.5512e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4384e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8371e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3918e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4170e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7929e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0458e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1374e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8813e+01 </func>
</region>
</regions>
<internal rank="609" log_i="1723712895.677288" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="610" mpi_size="768" stamp_init="1723712830.236823" stamp_final="1723712895.679592" username="apac4" allocationname="unknown" flags="0" pid="1940098" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54428e+01" utime="4.77143e+01" stime="7.77529e+00" mtime="2.82714e+01" gflop="0.00000e+00" gbyte="3.77007e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82714e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53130e+01" utime="4.76788e+01" stime="7.76896e+00" mtime="2.82714e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82714e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4728e+08" > 4.5493e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4958e+08" > 4.0250e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7993e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8402e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.3412e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1146e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4169e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7953e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0472e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1429e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8403e+01 </func>
</region>
</regions>
<internal rank="610" log_i="1723712895.679592" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="611" mpi_size="768" stamp_init="1723712830.235854" stamp_final="1723712895.682987" username="apac4" allocationname="unknown" flags="0" pid="1940099" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54471e+01" utime="4.95072e+01" stime="7.22927e+00" mtime="2.94240e+01" gflop="0.00000e+00" gbyte="3.77560e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.94240e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53191e+01" utime="4.94759e+01" stime="7.21812e+00" mtime="2.94240e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.94240e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4916e+08" > 3.5803e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4950e+08" > 2.7811e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1651e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8445e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2159e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.9996e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4165e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8012e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0495e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1423e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9307e+01 </func>
</region>
</regions>
<internal rank="611" log_i="1723712895.682987" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="612" mpi_size="768" stamp_init="1723712830.235549" stamp_final="1723712895.689760" username="apac4" allocationname="unknown" flags="0" pid="1940100" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54542e+01" utime="4.72912e+01" stime="7.91693e+00" mtime="2.83242e+01" gflop="0.00000e+00" gbyte="3.74352e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83242e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000077147614b8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53236e+01" utime="4.72590e+01" stime="7.90728e+00" mtime="2.83242e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83242e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4954e+08" > 5.2281e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4912e+08" > 4.1997e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.6000e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8315e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1020e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0867e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4168e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7965e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0507e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1426e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8595e+01 </func>
</region>
</regions>
<internal rank="612" log_i="1723712895.689760" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="613" mpi_size="768" stamp_init="1723712830.235561" stamp_final="1723712895.685590" username="apac4" allocationname="unknown" flags="0" pid="1940101" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54500e+01" utime="4.95306e+01" stime="7.25627e+00" mtime="2.92893e+01" gflop="0.00000e+00" gbyte="3.76778e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92893e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e914e814be" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53213e+01" utime="4.94938e+01" stime="7.25075e+00" mtime="2.92893e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92893e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.5763e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4865e+08" > 3.4276e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4902e+08" > 2.6508e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1851e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8400e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1015e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4163e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8034e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0463e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1430e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9163e+01 </func>
</region>
</regions>
<internal rank="613" log_i="1723712895.685590" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="614" mpi_size="768" stamp_init="1723712830.237750" stamp_final="1723712895.688469" username="apac4" allocationname="unknown" flags="0" pid="1940102" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54507e+01" utime="4.71214e+01" stime="8.18733e+00" mtime="2.84234e+01" gflop="0.00000e+00" gbyte="3.77560e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84234e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf468146a146b1460556b146b146c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53201e+01" utime="4.70905e+01" stime="8.17631e+00" mtime="2.84234e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84234e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4594e+08" > 5.1341e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4556e+08" > 4.7784e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4211e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8333e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.7349e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0857e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4162e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.7979e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0520e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1427e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7874e+01 </func>
</region>
</regions>
<internal rank="614" log_i="1723712895.688469" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="615" mpi_size="768" stamp_init="1723712830.235555" stamp_final="1723712895.691274" username="apac4" allocationname="unknown" flags="0" pid="1940103" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54557e+01" utime="4.84152e+01" stime="7.31654e+00" mtime="2.95876e+01" gflop="0.00000e+00" gbyte="3.77544e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.95876e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e115e1154d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53258e+01" utime="4.83813e+01" stime="7.30788e+00" mtime="2.95876e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.95876e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4620e+08" > 3.9153e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4750e+08" > 2.4876e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5946e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8390e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0876e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4164e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8025e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0515e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1428e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9005e+01 </func>
</region>
</regions>
<internal rank="615" log_i="1723712895.691274" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="616" mpi_size="768" stamp_init="1723712830.235547" stamp_final="1723712895.694569" username="apac4" allocationname="unknown" flags="0" pid="1940104" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54590e+01" utime="4.56314e+01" stime="8.61615e+00" mtime="2.77300e+01" gflop="0.00000e+00" gbyte="3.76938e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77300e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005f155f1552" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53295e+01" utime="4.55961e+01" stime="8.60801e+00" mtime="2.77300e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77300e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.8215e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4791e+08" > 7.8568e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4764e+08" > 6.4797e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7718e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8316e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.2491e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0841e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4148e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8132e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0489e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1426e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7540e+01 </func>
</region>
</regions>
<internal rank="616" log_i="1723712895.694569" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="617" mpi_size="768" stamp_init="1723712830.235563" stamp_final="1723712895.688547" username="apac4" allocationname="unknown" flags="0" pid="1940105" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54530e+01" utime="4.89475e+01" stime="7.69385e+00" mtime="2.91204e+01" gflop="0.00000e+00" gbyte="3.75729e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91204e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006b144e566b1466146f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53240e+01" utime="4.89164e+01" stime="7.68265e+00" mtime="2.91204e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91204e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4997e+08" > 4.8170e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4952e+08" > 3.5198e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5909e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8479e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.5620e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4152e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8112e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0485e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1426e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8441e+01 </func>
</region>
</regions>
<internal rank="617" log_i="1723712895.688547" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="618" mpi_size="768" stamp_init="1723712830.235617" stamp_final="1723712895.675891" username="apac4" allocationname="unknown" flags="0" pid="1940106" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54403e+01" utime="4.77313e+01" stime="7.93181e+00" mtime="2.82460e+01" gflop="0.00000e+00" gbyte="3.77247e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82460e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005c1499555c145c14c0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53089e+01" utime="4.77008e+01" stime="7.92001e+00" mtime="2.82460e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82460e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4931e+08" > 5.9281e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4916e+08" > 4.0028e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7985e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8364e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.1897e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0830e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4155e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8109e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0482e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1431e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8245e+01 </func>
</region>
</regions>
<internal rank="618" log_i="1723712895.675891" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="619" mpi_size="768" stamp_init="1723712830.235570" stamp_final="1723712895.677342" username="apac4" allocationname="unknown" flags="0" pid="1940107" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54418e+01" utime="4.92655e+01" stime="7.49144e+00" mtime="2.89863e+01" gflop="0.00000e+00" gbyte="3.75286e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89863e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000030143014a8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53137e+01" utime="4.92343e+01" stime="7.47968e+00" mtime="2.89863e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89863e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4914e+08" > 4.6733e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4874e+08" > 3.0084e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9134e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8290e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0832e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4150e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8161e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0473e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1429e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9014e+01 </func>
</region>
</regions>
<internal rank="619" log_i="1723712895.677342" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="620" mpi_size="768" stamp_init="1723712830.236117" stamp_final="1723712895.684979" username="apac4" allocationname="unknown" flags="0" pid="1940108" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54489e+01" utime="4.61822e+01" stime="8.49857e+00" mtime="2.80595e+01" gflop="0.00000e+00" gbyte="3.76740e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80595e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c514c614c814de55c814c714bd" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53173e+01" utime="4.61473e+01" stime="8.49133e+00" mtime="2.80595e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80595e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4830e+08" > 7.5408e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4788e+08" > 5.5530e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7693e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8311e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1496e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.6686e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4144e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8182e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0471e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1425e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7926e+01 </func>
</region>
</regions>
<internal rank="620" log_i="1723712895.684979" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="621" mpi_size="768" stamp_init="1723712830.235557" stamp_final="1723712895.691558" username="apac4" allocationname="unknown" flags="0" pid="1940109" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54560e+01" utime="4.93328e+01" stime="7.41060e+00" mtime="2.91228e+01" gflop="0.00000e+00" gbyte="3.78506e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91228e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53264e+01" utime="4.93012e+01" stime="7.39938e+00" mtime="2.91228e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91228e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4822e+08" > 4.7244e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4694e+08" > 3.4519e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2341e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8385e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.2750e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4145e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8213e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0503e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1423e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8819e+01 </func>
</region>
</regions>
<internal rank="621" log_i="1723712895.691558" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="622" mpi_size="768" stamp_init="1723712830.235538" stamp_final="1723712895.688435" username="apac4" allocationname="unknown" flags="0" pid="1940110" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54529e+01" utime="4.71747e+01" stime="8.18129e+00" mtime="2.84843e+01" gflop="0.00000e+00" gbyte="3.77144e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84843e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53222e+01" utime="4.71413e+01" stime="8.17196e+00" mtime="2.84843e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84843e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4714e+08" > 6.3916e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4883e+08" > 5.3557e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3093e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8391e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8747e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0581e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4130e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8372e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0476e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1427e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7919e+01 </func>
</region>
</regions>
<internal rank="622" log_i="1723712895.688435" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="623" mpi_size="768" stamp_init="1723712830.237604" stamp_final="1723712895.683617" username="apac4" allocationname="unknown" flags="0" pid="1940111" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11a</host>
<perf wtime="6.54460e+01" utime="4.95849e+01" stime="7.16490e+00" mtime="2.85849e+01" gflop="0.00000e+00" gbyte="3.77178e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85849e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003f15d3563f153f152e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.53199e+01" utime="4.95491e+01" stime="7.15839e+00" mtime="2.85849e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85849e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4926e+08" > 4.6996e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4860e+08" > 2.7622e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7528e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8445e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6451e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0610e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4143e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8230e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0461e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1422e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8768e+01 </func>
</region>
</regions>
<internal rank="623" log_i="1723712895.683617" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="624" mpi_size="768" stamp_init="1723712830.288001" stamp_final="1723712895.689693" username="apac4" allocationname="unknown" flags="0" pid="1219386" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.54017e+01" utime="4.22701e+01" stime="1.25636e+01" mtime="2.79337e+01" gflop="0.00000e+00" gbyte="3.85265e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79337e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf42514261427146b5627142714c3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52696e+01" utime="4.22396e+01" stime="1.25516e+01" mtime="2.79337e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79337e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4896e+08" > 6.6459e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4642e+08" > 4.8929e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0374e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5120e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.5013e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8401e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4137e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8222e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0519e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1441e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7948e+01 </func>
</region>
</regions>
<internal rank="624" log_i="1723712895.689693" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="625" mpi_size="768" stamp_init="1723712830.286462" stamp_final="1723712895.690903" username="apac4" allocationname="unknown" flags="0" pid="1219387" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.54044e+01" utime="4.90944e+01" stime="7.45086e+00" mtime="2.85452e+01" gflop="0.00000e+00" gbyte="3.78109e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85452e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005f15bf565f155f1540" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52652e+01" utime="4.90644e+01" stime="7.43639e+00" mtime="2.85452e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85452e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.0014e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 4.5965e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4989e+08" > 3.4671e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9537e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5157e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.2650e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4139e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8257e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0545e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1440e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8858e+01 </func>
</region>
</regions>
<internal rank="625" log_i="1723712895.690903" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="626" mpi_size="768" stamp_init="1723712830.288034" stamp_final="1723712895.693288" username="apac4" allocationname="unknown" flags="0" pid="1219388" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.54053e+01" utime="4.66043e+01" stime="8.28474e+00" mtime="2.77940e+01" gflop="0.00000e+00" gbyte="3.77621e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77940e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a814a8148c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52741e+01" utime="4.65800e+01" stime="8.26624e+00" mtime="2.77940e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77940e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4894e+08" > 6.3272e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5026e+08" > 6.0425e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7874e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5065e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.4571e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.4722e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4144e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8171e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0519e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1442e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8083e+01 </func>
</region>
</regions>
<internal rank="626" log_i="1723712895.693288" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="627" mpi_size="768" stamp_init="1723712830.286277" stamp_final="1723712895.690972" username="apac4" allocationname="unknown" flags="0" pid="1219389" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.54047e+01" utime="4.93187e+01" stime="7.14682e+00" mtime="2.84279e+01" gflop="0.00000e+00" gbyte="3.77048e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84279e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000981497147f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52711e+01" utime="4.92847e+01" stime="7.13784e+00" mtime="2.84279e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84279e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5120e+08" > 4.7822e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4863e+08" > 3.3585e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.4461e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5145e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.5804e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4127e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8346e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0534e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1447e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9232e+01 </func>
</region>
</regions>
<internal rank="627" log_i="1723712895.690972" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="628" mpi_size="768" stamp_init="1723712830.286435" stamp_final="1723712895.689441" username="apac4" allocationname="unknown" flags="0" pid="1219390" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.54030e+01" utime="4.60888e+01" stime="8.57758e+00" mtime="2.80984e+01" gflop="0.00000e+00" gbyte="3.74390e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80984e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000017141614b6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52713e+01" utime="4.60595e+01" stime="8.56441e+00" mtime="2.80984e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80984e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4834e+08" > 7.1551e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5014e+08" > 5.0954e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1259e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5167e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3733e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9479e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4132e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8312e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0536e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1440e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7966e+01 </func>
</region>
</regions>
<internal rank="628" log_i="1723712895.689441" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="629" mpi_size="768" stamp_init="1723712830.286441" stamp_final="1723712895.683242" username="apac4" allocationname="unknown" flags="0" pid="1219391" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.53968e+01" utime="4.90947e+01" stime="7.35037e+00" mtime="2.84514e+01" gflop="0.00000e+00" gbyte="3.76369e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84514e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e814e814cf" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52617e+01" utime="4.90698e+01" stime="7.33307e+00" mtime="2.84514e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84514e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4882e+08" > 4.3998e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4861e+08" > 3.1819e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7593e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4850e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9469e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4131e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8347e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0510e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1444e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9012e+01 </func>
</region>
</regions>
<internal rank="629" log_i="1723712895.683242" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="630" mpi_size="768" stamp_init="1723712830.287063" stamp_final="1723712895.684056" username="apac4" allocationname="unknown" flags="0" pid="1219392" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.53970e+01" utime="4.55653e+01" stime="8.60840e+00" mtime="2.76789e+01" gflop="0.00000e+00" gbyte="3.77018e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.76789e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000401522554015401504" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52637e+01" utime="4.55307e+01" stime="8.60019e+00" mtime="2.76789e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.76789e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4917e+08" > 7.8577e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4799e+08" > 6.6117e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9547e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4995e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.4899e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9321e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4131e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8316e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0537e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1443e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7645e+01 </func>
</region>
</regions>
<internal rank="630" log_i="1723712895.684056" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="631" mpi_size="768" stamp_init="1723712830.286272" stamp_final="1723712895.690367" username="apac4" allocationname="unknown" flags="0" pid="1219393" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.54041e+01" utime="4.93972e+01" stime="7.04533e+00" mtime="2.82509e+01" gflop="0.00000e+00" gbyte="3.77171e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82509e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f414f31493" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52677e+01" utime="4.93683e+01" stime="7.03182e+00" mtime="2.82509e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82509e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4750e+08" > 4.6646e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4879e+08" > 3.4062e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.4923e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4937e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9031e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4127e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8352e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0547e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1476e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9041e+01 </func>
</region>
</regions>
<internal rank="631" log_i="1723712895.690367" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="632" mpi_size="768" stamp_init="1723712830.287961" stamp_final="1723712895.690252" username="apac4" allocationname="unknown" flags="0" pid="1219394" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.54023e+01" utime="4.65285e+01" stime="8.36235e+00" mtime="2.84049e+01" gflop="0.00000e+00" gbyte="3.77819e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84049e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf45314551456148a5556145614c7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52705e+01" utime="4.64978e+01" stime="8.35022e+00" mtime="2.84049e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84049e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4852e+08" > 7.7607e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4832e+08" > 4.9748e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1553e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5174e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.4850e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9801e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4125e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8412e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0550e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1440e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8182e+01 </func>
</region>
</regions>
<internal rank="632" log_i="1723712895.690252" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="633" mpi_size="768" stamp_init="1723712830.286246" stamp_final="1723712895.690652" username="apac4" allocationname="unknown" flags="0" pid="1219395" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.54044e+01" utime="4.88984e+01" stime="7.57614e+00" mtime="2.86378e+01" gflop="0.00000e+00" gbyte="3.76759e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86378e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000281427145b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52703e+01" utime="4.88657e+01" stime="7.56651e+00" mtime="2.86378e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86378e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5054e+08" > 4.5669e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4801e+08" > 2.9201e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3647e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5166e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.0051e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4121e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8428e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0527e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1443e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8547e+01 </func>
</region>
</regions>
<internal rank="633" log_i="1723712895.690652" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="634" mpi_size="768" stamp_init="1723712830.286366" stamp_final="1723712895.682038" username="apac4" allocationname="unknown" flags="0" pid="1219396" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.53957e+01" utime="4.72765e+01" stime="8.13148e+00" mtime="2.83592e+01" gflop="0.00000e+00" gbyte="3.76934e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83592e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002c14be552c142c1479" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52613e+01" utime="4.72462e+01" stime="8.11854e+00" mtime="2.83592e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83592e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4954e+08" > 5.6662e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5070e+08" > 3.6586e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4842e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5181e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3484e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.0032e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4117e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8462e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0502e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1445e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8030e+01 </func>
</region>
</regions>
<internal rank="634" log_i="1723712895.682038" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="635" mpi_size="768" stamp_init="1723712830.286559" stamp_final="1723712895.684192" username="apac4" allocationname="unknown" flags="0" pid="1219397" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.53976e+01" utime="4.92720e+01" stime="7.21202e+00" mtime="2.83992e+01" gflop="0.00000e+00" gbyte="3.76129e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83992e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d3145c55d314d314ec" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52639e+01" utime="4.92405e+01" stime="7.20153e+00" mtime="2.83992e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83992e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4970e+08" > 4.6065e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4905e+08" > 2.7488e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8484e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5103e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0574e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4113e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8503e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0328e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1444e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8827e+01 </func>
</region>
</regions>
<internal rank="635" log_i="1723712895.684192" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="636" mpi_size="768" stamp_init="1723712830.286490" stamp_final="1723712895.682838" username="apac4" allocationname="unknown" flags="0" pid="1219398" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.53963e+01" utime="4.57176e+01" stime="8.62924e+00" mtime="2.80524e+01" gflop="0.00000e+00" gbyte="3.76965e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80524e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003d141e563d143d14ea" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52647e+01" utime="4.56871e+01" stime="8.61727e+00" mtime="2.80524e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80524e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4957e+08" > 7.1481e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4909e+08" > 4.9404e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6774e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5187e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.9693e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.1600e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4111e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8530e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0504e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1440e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7365e+01 </func>
</region>
</regions>
<internal rank="636" log_i="1723712895.682838" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="637" mpi_size="768" stamp_init="1723712830.286253" stamp_final="1723712895.683099" username="apac4" allocationname="unknown" flags="0" pid="1219399" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.53968e+01" utime="4.91839e+01" stime="7.30636e+00" mtime="2.85048e+01" gflop="0.00000e+00" gbyte="3.77796e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85048e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006b1426556b146b14a1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52626e+01" utime="4.91499e+01" stime="7.29759e+00" mtime="2.85048e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85048e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4834e+08" > 4.5988e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4962e+08" > 2.7152e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8994e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5212e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.1583e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4112e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8520e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0488e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1445e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8873e+01 </func>
</region>
</regions>
<internal rank="637" log_i="1723712895.683099" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="638" mpi_size="768" stamp_init="1723712830.286499" stamp_final="1723712895.689336" username="apac4" allocationname="unknown" flags="0" pid="1219400" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.54028e+01" utime="4.66917e+01" stime="8.47682e+00" mtime="2.80654e+01" gflop="0.00000e+00" gbyte="3.76137e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80654e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52735e+01" utime="4.66644e+01" stime="8.46223e+00" mtime="2.80654e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80654e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.4067e-05 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4829e+08" > 5.7337e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4836e+08" > 4.3738e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6905e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5158e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.9223e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9089e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4109e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8567e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0538e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1446e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7516e+01 </func>
</region>
</regions>
<internal rank="638" log_i="1723712895.689336" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="639" mpi_size="768" stamp_init="1723712830.286259" stamp_final="1723712895.683019" username="apac4" allocationname="unknown" flags="0" pid="1219401" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.53968e+01" utime="4.92283e+01" stime="7.33312e+00" mtime="2.85693e+01" gflop="0.00000e+00" gbyte="3.75420e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85693e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005e145e14ca" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52625e+01" utime="4.92035e+01" stime="7.31498e+00" mtime="2.85693e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85693e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4813e+08" > 4.7433e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4902e+08" > 2.9537e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9378e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5145e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9170e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4105e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8549e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0417e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1429e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8888e+01 </func>
</region>
</regions>
<internal rank="639" log_i="1723712895.683019" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="640" mpi_size="768" stamp_init="1723712830.286229" stamp_final="1723712895.682882" username="apac4" allocationname="unknown" flags="0" pid="1219402" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.53967e+01" utime="4.67052e+01" stime="8.41227e+00" mtime="2.79788e+01" gflop="0.00000e+00" gbyte="3.75801e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79788e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52620e+01" utime="4.66779e+01" stime="8.39726e+00" mtime="2.79788e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79788e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4844e+08" > 7.7393e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4864e+08" > 4.1500e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5968e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.2977e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0420e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4300e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4084e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8811e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0496e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1444e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7342e+01 </func>
</region>
</regions>
<internal rank="640" log_i="1723712895.682882" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="641" mpi_size="768" stamp_init="1723712830.286278" stamp_final="1723712895.684617" username="apac4" allocationname="unknown" flags="0" pid="1219403" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.53983e+01" utime="4.86100e+01" stime="7.52612e+00" mtime="2.81951e+01" gflop="0.00000e+00" gbyte="3.77018e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81951e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ba147456ba14b5149a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52649e+01" utime="4.85875e+01" stime="7.50513e+00" mtime="2.81951e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81951e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.3910e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4884e+08" > 5.5145e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4873e+08" > 2.7107e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2660e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5023e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.4643e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4084e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8820e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0535e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1448e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8118e+01 </func>
</region>
</regions>
<internal rank="641" log_i="1723712895.684617" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="642" mpi_size="768" stamp_init="1723712830.286242" stamp_final="1723712895.681887" username="apac4" allocationname="unknown" flags="0" pid="1219404" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.53956e+01" utime="4.69335e+01" stime="8.18517e+00" mtime="2.78460e+01" gflop="0.00000e+00" gbyte="3.76591e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78460e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf44e1450145114b25551145114e3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52626e+01" utime="4.69029e+01" stime="8.17294e+00" mtime="2.78460e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78460e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5003e+08" > 7.4162e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4932e+08" > 4.7330e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0081e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5238e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.8991e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.2853e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4079e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8867e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0522e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1399e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7799e+01 </func>
</region>
</regions>
<internal rank="642" log_i="1723712895.681887" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="643" mpi_size="768" stamp_init="1723712830.286251" stamp_final="1723712895.687592" username="apac4" allocationname="unknown" flags="0" pid="1219405" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.54013e+01" utime="4.88296e+01" stime="7.69338e+00" mtime="2.89311e+01" gflop="0.00000e+00" gbyte="3.77197e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89311e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005215521509" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52675e+01" utime="4.88044e+01" stime="7.67581e+00" mtime="2.89311e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89311e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4886e+08" > 5.6260e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5082e+08" > 3.0023e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2846e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4874e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5736e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.2744e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4072e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8951e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0512e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1446e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8840e+01 </func>
</region>
</regions>
<internal rank="643" log_i="1723712895.687592" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="644" mpi_size="768" stamp_init="1723712830.286240" stamp_final="1723712895.682972" username="apac4" allocationname="unknown" flags="0" pid="1219406" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.53967e+01" utime="4.49476e+01" stime="8.89687e+00" mtime="2.75296e+01" gflop="0.00000e+00" gbyte="3.77407e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.75296e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52661e+01" utime="4.49144e+01" stime="8.88630e+00" mtime="2.75296e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.75296e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4974e+08" > 1.1065e+00 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4843e+08" > 6.8817e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7279e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5097e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6818e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.9540e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4075e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8901e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0513e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1447e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7391e+01 </func>
</region>
</regions>
<internal rank="644" log_i="1723712895.682972" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="645" mpi_size="768" stamp_init="1723712830.286312" stamp_final="1723712895.690531" username="apac4" allocationname="unknown" flags="0" pid="1219407" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.54042e+01" utime="4.89172e+01" stime="7.57570e+00" mtime="2.86925e+01" gflop="0.00000e+00" gbyte="3.77407e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86925e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48d158f15901587569015901552" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52691e+01" utime="4.88890e+01" stime="7.56117e+00" mtime="2.86925e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86925e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 5.6631e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4785e+08" > 2.9175e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2159e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5148e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.9571e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4060e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9023e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0531e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1441e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8642e+01 </func>
</region>
</regions>
<internal rank="645" log_i="1723712895.690531" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="646" mpi_size="768" stamp_init="1723712830.286234" stamp_final="1723712895.683259" username="apac4" allocationname="unknown" flags="0" pid="1219408" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.53970e+01" utime="4.69042e+01" stime="8.17977e+00" mtime="2.77369e+01" gflop="0.00000e+00" gbyte="3.77151e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77369e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a7152155a715a71541" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52645e+01" utime="4.68748e+01" stime="8.16619e+00" mtime="2.77369e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77369e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4821e+08" > 7.2821e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4855e+08" > 4.6647e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1395e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4957e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1948e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4215e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4071e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8938e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0539e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1441e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7602e+01 </func>
</region>
</regions>
<internal rank="646" log_i="1723712895.683259" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="647" mpi_size="768" stamp_init="1723712830.287387" stamp_final="1723712895.691176" username="apac4" allocationname="unknown" flags="0" pid="1219409" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u14b</host>
<perf wtime="6.54038e+01" utime="4.87150e+01" stime="7.78776e+00" mtime="2.84029e+01" gflop="0.00000e+00" gbyte="3.76518e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84029e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000cd14cc14c8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52685e+01" utime="4.86815e+01" stime="7.77909e+00" mtime="2.84029e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84029e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4866e+08" > 5.5716e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4734e+08" > 2.8520e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1497e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4985e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.3961e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4064e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9035e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0520e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1440e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8441e+01 </func>
</region>
</regions>
<internal rank="647" log_i="1723712895.691176" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="648" mpi_size="768" stamp_init="1723712830.391890" stamp_final="1723712895.680017" username="apac4" allocationname="unknown" flags="0" pid="708192" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52881e+01" utime="4.23070e+01" stime="1.25969e+01" mtime="2.80061e+01" gflop="0.00000e+00" gbyte="3.86234e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80061e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000aa14d156aa14a914de" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51542e+01" utime="4.22739e+01" stime="1.25875e+01" mtime="2.80061e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80061e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4958e+08" > 6.1823e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4848e+08" > 4.8559e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8416e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3415e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.5885e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4846e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4056e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9079e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0468e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1401e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7433e+01 </func>
</region>
</regions>
<internal rank="648" log_i="1723712895.680017" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="649" mpi_size="768" stamp_init="1723712830.392280" stamp_final="1723712895.678807" username="apac4" allocationname="unknown" flags="0" pid="708193" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52865e+01" utime="4.86977e+01" stime="7.47297e+00" mtime="2.82557e+01" gflop="0.00000e+00" gbyte="3.77075e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82557e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51520e+01" utime="4.86667e+01" stime="7.46054e+00" mtime="2.82557e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82557e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4848e+08" > 4.4849e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4864e+08" > 2.8575e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1308e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3498e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.1343e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4068e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8999e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0500e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1438e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8570e+01 </func>
</region>
</regions>
<internal rank="649" log_i="1723712895.678807" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="650" mpi_size="768" stamp_init="1723712830.391892" stamp_final="1723712895.685759" username="apac4" allocationname="unknown" flags="0" pid="708194" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52939e+01" utime="4.66659e+01" stime="8.16540e+00" mtime="2.81824e+01" gflop="0.00000e+00" gbyte="3.76007e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81824e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf46114631464144856641464148e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51590e+01" utime="4.66373e+01" stime="8.15231e+00" mtime="2.81824e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81824e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4981e+08" > 5.9676e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4810e+08" > 5.4545e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4042e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3207e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6958e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.7790e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4066e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8976e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0467e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1399e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8080e+01 </func>
</region>
</regions>
<internal rank="650" log_i="1723712895.685759" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="651" mpi_size="768" stamp_init="1723712830.391878" stamp_final="1723712895.679133" username="apac4" allocationname="unknown" flags="0" pid="708195" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52873e+01" utime="4.87341e+01" stime="7.43748e+00" mtime="2.83954e+01" gflop="0.00000e+00" gbyte="3.76320e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83954e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51566e+01" utime="4.87012e+01" stime="7.42855e+00" mtime="2.83954e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83954e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4925e+08" > 4.4289e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5121e+08" > 2.7353e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8454e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3500e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.7986e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4069e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8989e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0482e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1435e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9005e+01 </func>
</region>
</regions>
<internal rank="651" log_i="1723712895.679133" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="652" mpi_size="768" stamp_init="1723712830.392960" stamp_final="1723712895.683630" username="apac4" allocationname="unknown" flags="0" pid="708196" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52907e+01" utime="4.72158e+01" stime="7.81434e+00" mtime="2.78408e+01" gflop="0.00000e+00" gbyte="3.76759e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78408e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000016141514d1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51583e+01" utime="4.71861e+01" stime="7.80193e+00" mtime="2.78408e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78408e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5082e+08" > 5.5826e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4949e+08" > 6.2985e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1243e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3470e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3430e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.5309e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4064e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.8984e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0495e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1439e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8024e+01 </func>
</region>
</regions>
<internal rank="652" log_i="1723712895.683630" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="653" mpi_size="768" stamp_init="1723712830.392528" stamp_final="1723712895.694087" username="apac4" allocationname="unknown" flags="0" pid="708197" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.53016e+01" utime="4.87656e+01" stime="7.36823e+00" mtime="2.85302e+01" gflop="0.00000e+00" gbyte="3.76305e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85302e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000431443148e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51714e+01" utime="4.87375e+01" stime="7.35423e+00" mtime="2.85302e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85302e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4962e+08" > 4.4616e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4958e+08" > 2.8160e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1900e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3543e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9511e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4055e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9091e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0494e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1436e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8780e+01 </func>
</region>
</regions>
<internal rank="653" log_i="1723712895.694087" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="654" mpi_size="768" stamp_init="1723712830.391915" stamp_final="1723712895.686349" username="apac4" allocationname="unknown" flags="0" pid="708198" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52944e+01" utime="4.62649e+01" stime="8.25248e+00" mtime="2.75287e+01" gflop="0.00000e+00" gbyte="3.76217e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.75287e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51617e+01" utime="4.62371e+01" stime="8.23793e+00" mtime="2.75287e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.75287e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 6.6757e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.9141e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4868e+08" > 6.8343e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4983e+08" > 5.0202e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8471e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3440e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.5592e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.5762e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4046e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9169e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0473e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1439e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7881e+01 </func>
</region>
</regions>
<internal rank="654" log_i="1723712895.686349" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="655" mpi_size="768" stamp_init="1723712830.392748" stamp_final="1723712895.690040" username="apac4" allocationname="unknown" flags="0" pid="708199" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52973e+01" utime="4.89563e+01" stime="7.25505e+00" mtime="2.81740e+01" gflop="0.00000e+00" gbyte="3.75324e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81740e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4b015b115b215d856b215b21518" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51661e+01" utime="4.89269e+01" stime="7.24221e+00" mtime="2.81740e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81740e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4799e+08" > 4.4671e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4804e+08" > 3.0001e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7832e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3589e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8835e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.7158e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4053e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9116e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0501e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1435e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8827e+01 </func>
</region>
</regions>
<internal rank="655" log_i="1723712895.690040" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="656" mpi_size="768" stamp_init="1723712830.392945" stamp_final="1723712895.687014" username="apac4" allocationname="unknown" flags="0" pid="708200" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52941e+01" utime="4.68821e+01" stime="7.92248e+00" mtime="2.79316e+01" gflop="0.00000e+00" gbyte="3.77422e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79316e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51623e+01" utime="4.68546e+01" stime="7.90780e+00" mtime="2.79316e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79316e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4832e+08" > 5.3917e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4675e+08" > 3.6931e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0102e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3482e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1870e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.6999e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4042e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9260e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0515e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1439e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8272e+01 </func>
</region>
</regions>
<internal rank="656" log_i="1723712895.687014" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="657" mpi_size="768" stamp_init="1723712830.391855" stamp_final="1723712895.682998" username="apac4" allocationname="unknown" flags="0" pid="708201" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52911e+01" utime="4.86796e+01" stime="7.18788e+00" mtime="2.82684e+01" gflop="0.00000e+00" gbyte="3.76110e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82684e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e614ff1411141c551114101464" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51606e+01" utime="4.86442e+01" stime="7.18046e+00" mtime="2.82684e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82684e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4734e+08" > 3.5941e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4709e+08" > 2.4540e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1647e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3559e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1935e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9547e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4035e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9332e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0487e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1435e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8632e+01 </func>
</region>
</regions>
<internal rank="657" log_i="1723712895.682998" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="658" mpi_size="768" stamp_init="1723712830.391908" stamp_final="1723712895.682750" username="apac4" allocationname="unknown" flags="0" pid="708202" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52908e+01" utime="4.59714e+01" stime="8.27227e+00" mtime="2.75659e+01" gflop="0.00000e+00" gbyte="3.77506e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.75659e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51578e+01" utime="4.59446e+01" stime="8.25661e+00" mtime="2.75659e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.75659e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4884e+08" > 5.6800e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4833e+08" > 5.2414e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.6504e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3538e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.1989e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.7660e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4031e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9352e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0510e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1440e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8218e+01 </func>
</region>
</regions>
<internal rank="658" log_i="1723712895.682750" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="659" mpi_size="768" stamp_init="1723712830.391899" stamp_final="1723712895.688638" username="apac4" allocationname="unknown" flags="0" pid="708203" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52967e+01" utime="4.90056e+01" stime="7.10290e+00" mtime="2.83763e+01" gflop="0.00000e+00" gbyte="3.74863e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83763e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000df15bc56df15de150a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51652e+01" utime="4.89725e+01" stime="7.09363e+00" mtime="2.83763e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83763e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4941e+08" > 3.5780e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4912e+08" > 2.4317e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7604e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3672e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2159e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8332e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4029e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9397e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0522e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1433e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9143e+01 </func>
</region>
</regions>
<internal rank="659" log_i="1723712895.688638" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="660" mpi_size="768" stamp_init="1723712830.391873" stamp_final="1723712895.692677" username="apac4" allocationname="unknown" flags="0" pid="708204" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.53008e+01" utime="4.44469e+01" stime="8.85627e+00" mtime="2.79412e+01" gflop="0.00000e+00" gbyte="3.77678e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79412e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51663e+01" utime="4.44127e+01" stime="8.84823e+00" mtime="2.79412e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79412e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4878e+08" > 7.2798e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4967e+08" > 8.4400e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4597e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3516e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.3635e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9250e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4014e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9496e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0530e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1439e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7586e+01 </func>
</region>
</regions>
<internal rank="660" log_i="1723712895.692677" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="661" mpi_size="768" stamp_init="1723712830.391863" stamp_final="1723712895.693240" username="apac4" allocationname="unknown" flags="0" pid="708205" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.53014e+01" utime="4.88368e+01" stime="7.29399e+00" mtime="2.84383e+01" gflop="0.00000e+00" gbyte="3.77735e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84383e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000092141d56921491146d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51717e+01" utime="4.88091e+01" stime="7.27864e+00" mtime="2.84383e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84383e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4875e+08" > 3.6109e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4838e+08" > 2.8706e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2191e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3531e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 9.9382e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4032e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9333e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0523e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1433e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8745e+01 </func>
</region>
</regions>
<internal rank="661" log_i="1723712895.693240" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="662" mpi_size="768" stamp_init="1723712830.391860" stamp_final="1723712895.689987" username="apac4" allocationname="unknown" flags="0" pid="708206" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52981e+01" utime="4.68850e+01" stime="7.96721e+00" mtime="2.79825e+01" gflop="0.00000e+00" gbyte="3.77621e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79825e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000961496146e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51633e+01" utime="4.68503e+01" stime="7.95955e+00" mtime="2.79825e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79825e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4782e+08" > 5.2123e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4941e+08" > 4.2076e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2764e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3294e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4343e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9248e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4030e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9373e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0478e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1440e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8088e+01 </func>
</region>
</regions>
<internal rank="662" log_i="1723712895.689987" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="663" mpi_size="768" stamp_init="1723712830.391848" stamp_final="1723712895.693586" username="apac4" allocationname="unknown" flags="0" pid="708207" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.53017e+01" utime="4.89789e+01" stime="7.20661e+00" mtime="2.90380e+01" gflop="0.00000e+00" gbyte="3.76659e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90380e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e914e914e2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51713e+01" utime="4.89516e+01" stime="7.19164e+00" mtime="2.90380e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90380e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4903e+08" > 3.4511e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4646e+08" > 2.6562e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4085e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3360e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9529e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4014e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9521e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0515e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1430e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9198e+01 </func>
</region>
</regions>
<internal rank="663" log_i="1723712895.693586" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="664" mpi_size="768" stamp_init="1723712830.391889" stamp_final="1723712895.693094" username="apac4" allocationname="unknown" flags="0" pid="708208" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.53012e+01" utime="4.68915e+01" stime="7.90451e+00" mtime="2.80087e+01" gflop="0.00000e+00" gbyte="3.77502e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80087e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000006d15e8566d156d153d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51681e+01" utime="4.68574e+01" stime="7.89585e+00" mtime="2.80087e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80087e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4809e+08" > 5.8644e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4934e+08" > 4.3558e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2975e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3352e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.9056e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8311e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4021e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9438e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0490e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1441e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8017e+01 </func>
</region>
</regions>
<internal rank="664" log_i="1723712895.693094" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="665" mpi_size="768" stamp_init="1723712830.391860" stamp_final="1723712895.686828" username="apac4" allocationname="unknown" flags="0" pid="708209" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52950e+01" utime="4.84272e+01" stime="7.44835e+00" mtime="2.81493e+01" gflop="0.00000e+00" gbyte="3.74283e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81493e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a214ce56a214a214a3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51628e+01" utime="4.83977e+01" stime="7.43588e+00" mtime="2.81493e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81493e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4908e+08" > 4.9351e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4869e+08" > 3.0791e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1797e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3449e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.7789e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4023e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9459e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0473e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1429e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8375e+01 </func>
</region>
</regions>
<internal rank="665" log_i="1723712895.686828" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="666" mpi_size="768" stamp_init="1723712830.391920" stamp_final="1723712895.684091" username="apac4" allocationname="unknown" flags="0" pid="708210" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52922e+01" utime="4.65831e+01" stime="8.24797e+00" mtime="2.81391e+01" gflop="0.00000e+00" gbyte="3.74737e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81391e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e414e514e6148555e614e614f6" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51603e+01" utime="4.65489e+01" stime="8.23955e+00" mtime="2.81391e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81391e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4910e+08" > 5.9245e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4945e+08" > 3.7632e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8255e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3566e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.3739e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.6278e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4017e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9520e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0507e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1440e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8595e+01 </func>
</region>
</regions>
<internal rank="666" log_i="1723712895.684091" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="667" mpi_size="768" stamp_init="1723712830.391910" stamp_final="1723712895.693401" username="apac4" allocationname="unknown" flags="0" pid="708211" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.53015e+01" utime="4.87304e+01" stime="7.41959e+00" mtime="2.85181e+01" gflop="0.00000e+00" gbyte="3.76057e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85181e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51697e+01" utime="4.87049e+01" stime="7.40302e+00" mtime="2.85181e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85181e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4887e+08" > 4.7711e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5064e+08" > 3.0901e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0046e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3405e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.6418e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4012e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9530e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0508e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1389e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8940e+01 </func>
</region>
</regions>
<internal rank="667" log_i="1723712895.693401" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="668" mpi_size="768" stamp_init="1723712830.391906" stamp_final="1723712895.683396" username="apac4" allocationname="unknown" flags="0" pid="708212" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52915e+01" utime="4.65310e+01" stime="8.11454e+00" mtime="2.79395e+01" gflop="0.00000e+00" gbyte="3.77544e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79395e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf42e1530153115f6563115311534" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51578e+01" utime="4.64971e+01" stime="8.10543e+00" mtime="2.79395e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79395e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4902e+08" > 6.8098e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4796e+08" > 4.4218e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0490e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3448e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.3553e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8423e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4008e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9573e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0464e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1439e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8095e+01 </func>
</region>
</regions>
<internal rank="668" log_i="1723712895.683396" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="669" mpi_size="768" stamp_init="1723712830.391877" stamp_final="1723712895.689806" username="apac4" allocationname="unknown" flags="0" pid="708213" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52979e+01" utime="4.89071e+01" stime="7.26622e+00" mtime="2.79577e+01" gflop="0.00000e+00" gbyte="3.74474e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79577e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf48b1421143314e15533142e14e0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51693e+01" utime="4.88774e+01" stime="7.25346e+00" mtime="2.79577e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79577e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4751e+08" > 5.0827e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4735e+08" > 3.2107e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8028e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3486e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.5850e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4008e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9606e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0507e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1423e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8540e+01 </func>
</region>
</regions>
<internal rank="669" log_i="1723712895.689806" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="670" mpi_size="768" stamp_init="1723712830.391885" stamp_final="1723712895.685607" username="apac4" allocationname="unknown" flags="0" pid="708214" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52937e+01" utime="4.66391e+01" stime="8.27361e+00" mtime="2.84623e+01" gflop="0.00000e+00" gbyte="3.77308e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84623e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d214f755d214d214d8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51604e+01" utime="4.66049e+01" stime="8.26473e+00" mtime="2.84623e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84623e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4734e+08" > 5.7824e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4971e+08" > 3.7573e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.9701e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3572e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.1089e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8630e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4009e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9559e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0513e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1438e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7788e+01 </func>
</region>
</regions>
<internal rank="670" log_i="1723712895.685607" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="671" mpi_size="768" stamp_init="1723712830.391887" stamp_final="1723712895.683603" username="apac4" allocationname="unknown" flags="0" pid="708215" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u16a</host>
<perf wtime="6.52917e+01" utime="4.87624e+01" stime="7.37533e+00" mtime="2.85196e+01" gflop="0.00000e+00" gbyte="3.73772e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85196e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000b214b214d7" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51638e+01" utime="4.87327e+01" stime="7.36324e+00" mtime="2.85196e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85196e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4921e+08" > 4.9222e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4816e+08" > 3.3830e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3692e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.3524e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.9131e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4005e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9648e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0504e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1422e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8548e+01 </func>
</region>
</regions>
<internal rank="671" log_i="1723712895.683603" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="672" mpi_size="768" stamp_init="1723712830.292133" stamp_final="1723712895.686965" username="apac4" allocationname="unknown" flags="0" pid="530516" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53948e+01" utime="4.29484e+01" stime="1.28623e+01" mtime="2.86736e+01" gflop="0.00000e+00" gbyte="3.86417e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86736e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001215121507" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52659e+01" utime="4.29178e+01" stime="1.28506e+01" mtime="2.86736e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86736e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.1989e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4897e+08" > 5.6613e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4859e+08" > 3.9730e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3480e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4622e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.3439e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4579e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3991e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9752e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0451e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1427e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8100e+01 </func>
</region>
</regions>
<internal rank="672" log_i="1723712895.686965" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="673" mpi_size="768" stamp_init="1723712830.292115" stamp_final="1723712895.691031" username="apac4" allocationname="unknown" flags="0" pid="530517" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53989e+01" utime="4.92045e+01" stime="7.42601e+00" mtime="2.94509e+01" gflop="0.00000e+00" gbyte="3.77808e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.94509e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52635e+01" utime="4.91740e+01" stime="7.41329e+00" mtime="2.94509e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.94509e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4789e+08" > 4.6280e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4972e+08" > 3.6077e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6566e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9271e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.3447e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3998e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9719e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0484e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1422e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8639e+01 </func>
</region>
</regions>
<internal rank="673" log_i="1723712895.691031" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="674" mpi_size="768" stamp_init="1723712830.292181" stamp_final="1723712895.695779" username="apac4" allocationname="unknown" flags="0" pid="530518" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.54036e+01" utime="4.79159e+01" stime="7.99218e+00" mtime="2.88996e+01" gflop="0.00000e+00" gbyte="3.77174e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88996e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ce147b14d8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52739e+01" utime="4.78836e+01" stime="7.98244e+00" mtime="2.88996e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88996e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4953e+08" > 5.5158e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4745e+08" > 3.4184e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2038e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9271e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1015e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4661e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4001e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9679e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0456e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1422e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8457e+01 </func>
</region>
</regions>
<internal rank="674" log_i="1723712895.695779" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="675" mpi_size="768" stamp_init="1723712830.292126" stamp_final="1723712895.689676" username="apac4" allocationname="unknown" flags="0" pid="530519" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53976e+01" utime="4.95059e+01" stime="7.38687e+00" mtime="2.89708e+01" gflop="0.00000e+00" gbyte="3.74550e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89708e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf42a142b142c146f562c142c14ff" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52694e+01" utime="4.94771e+01" stime="7.37310e+00" mtime="2.89708e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89708e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4911e+08" > 4.3661e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.5029e+08" > 2.8227e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.6883e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.8812e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.8560e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.4000e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9700e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0432e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1422e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9208e+01 </func>
</region>
</regions>
<internal rank="675" log_i="1723712895.689676" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="676" mpi_size="768" stamp_init="1723712830.292144" stamp_final="1723712895.687405" username="apac4" allocationname="unknown" flags="0" pid="530520" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53953e+01" utime="4.73592e+01" stime="8.13396e+00" mtime="2.85763e+01" gflop="0.00000e+00" gbyte="3.77407e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85763e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4a714a814aa147255aa14a914e4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52663e+01" utime="4.73237e+01" stime="8.12718e+00" mtime="2.85763e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85763e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4965e+08" > 5.8500e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4812e+08" > 3.3328e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1149e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9154e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.1756e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3991e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3994e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9721e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0433e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1423e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8197e+01 </func>
</region>
</regions>
<internal rank="676" log_i="1723712895.687405" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="677" mpi_size="768" stamp_init="1723712830.292115" stamp_final="1723712895.686827" username="apac4" allocationname="unknown" flags="0" pid="530521" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53947e+01" utime="4.96414e+01" stime="7.23647e+00" mtime="2.89463e+01" gflop="0.00000e+00" gbyte="3.77480e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89463e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c914cb14cc147855cc14cb1488" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52643e+01" utime="4.96065e+01" stime="7.22919e+00" mtime="2.89463e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89463e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4668e+08" > 4.3858e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4677e+08" > 3.3053e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8252e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9375e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.3811e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3990e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9804e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0452e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1421e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8990e+01 </func>
</region>
</regions>
<internal rank="677" log_i="1723712895.686827" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="678" mpi_size="768" stamp_init="1723712830.292173" stamp_final="1723712895.681509" username="apac4" allocationname="unknown" flags="0" pid="530522" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53893e+01" utime="4.64047e+01" stime="8.44407e+00" mtime="2.83511e+01" gflop="0.00000e+00" gbyte="3.75999e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83511e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001d14181489" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52617e+01" utime="4.63775e+01" stime="8.42899e+00" mtime="2.83511e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83511e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4504e+08" > 7.1668e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4320e+08" > 4.3707e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0470e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9286e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.1301e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4060e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3977e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9853e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0435e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1427e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7890e+01 </func>
</region>
</regions>
<internal rank="678" log_i="1723712895.681509" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="679" mpi_size="768" stamp_init="1723712830.292190" stamp_final="1723712895.686355" username="apac4" allocationname="unknown" flags="0" pid="530523" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53942e+01" utime="4.95611e+01" stime="7.30074e+00" mtime="2.90465e+01" gflop="0.00000e+00" gbyte="3.77426e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90465e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000005314bf555314531465" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52655e+01" utime="4.95211e+01" stime="7.29844e+00" mtime="2.90465e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90465e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4536e+08" > 4.4001e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4640e+08" > 2.7998e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9405e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9227e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0662e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3989e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9817e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0451e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1388e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8983e+01 </func>
</region>
</regions>
<internal rank="679" log_i="1723712895.686355" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="680" mpi_size="768" stamp_init="1723712830.292181" stamp_final="1723712895.687269" username="apac4" allocationname="unknown" flags="0" pid="530524" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53951e+01" utime="4.73618e+01" stime="8.36017e+00" mtime="2.90238e+01" gflop="0.00000e+00" gbyte="3.76873e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90238e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007714725677147614b5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52679e+01" utime="4.73327e+01" stime="8.34712e+00" mtime="2.90238e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90238e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4719e+08" > 4.7796e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4858e+08" > 4.6679e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8962e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9174e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3685e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.5052e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3983e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9831e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0451e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1423e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7960e+01 </func>
</region>
</regions>
<internal rank="680" log_i="1723712895.687269" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="681" mpi_size="768" stamp_init="1723712830.292163" stamp_final="1723712895.677700" username="apac4" allocationname="unknown" flags="0" pid="530525" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53855e+01" utime="4.96321e+01" stime="7.18220e+00" mtime="2.89627e+01" gflop="0.00000e+00" gbyte="3.76442e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89627e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000d314d656d314ce145c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52565e+01" utime="4.95991e+01" stime="7.17359e+00" mtime="2.89627e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89627e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4974e+08" > 3.5225e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4805e+08" > 3.0896e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0937e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9267e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.8752e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3983e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9872e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0476e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1423e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8833e+01 </func>
</region>
</regions>
<internal rank="681" log_i="1723712895.677700" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="682" mpi_size="768" stamp_init="1723712830.292187" stamp_final="1723712895.682716" username="apac4" allocationname="unknown" flags="0" pid="530526" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53905e+01" utime="4.79640e+01" stime="7.88782e+00" mtime="2.86718e+01" gflop="0.00000e+00" gbyte="3.76873e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86718e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f914f9147e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52615e+01" utime="4.79311e+01" stime="7.87808e+00" mtime="2.86718e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86718e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4776e+08" > 4.4064e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4920e+08" > 4.4786e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0680e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9164e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.1674e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.9981e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3980e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9913e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0436e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1423e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8471e+01 </func>
</region>
</regions>
<internal rank="682" log_i="1723712895.682716" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="683" mpi_size="768" stamp_init="1723712830.292101" stamp_final="1723712895.682452" username="apac4" allocationname="unknown" flags="0" pid="530527" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53904e+01" utime="4.94209e+01" stime="7.33750e+00" mtime="2.89795e+01" gflop="0.00000e+00" gbyte="3.76450e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89795e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ea142056ea14e914e2" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52634e+01" utime="4.93931e+01" stime="7.32324e+00" mtime="2.89795e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89795e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.1921e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4863e+08" > 3.6508e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4904e+08" > 2.8041e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8644e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9266e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.0034e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3980e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9878e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0448e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1425e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9068e+01 </func>
</region>
</regions>
<internal rank="683" log_i="1723712895.682452" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="684" mpi_size="768" stamp_init="1723712830.292164" stamp_final="1723712895.686173" username="apac4" allocationname="unknown" flags="0" pid="530528" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53940e+01" utime="4.62096e+01" stime="8.51861e+00" mtime="2.85708e+01" gflop="0.00000e+00" gbyte="3.77934e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85708e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52642e+01" utime="4.61776e+01" stime="8.50809e+00" mtime="2.85708e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85708e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4891e+08" > 5.2540e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4813e+08" > 6.5870e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2251e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9165e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8456e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.8878e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3967e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 1.9975e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0487e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1430e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8110e+01 </func>
</region>
</regions>
<internal rank="684" log_i="1723712895.686173" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="685" mpi_size="768" stamp_init="1723712830.292171" stamp_final="1723712895.677638" username="apac4" allocationname="unknown" flags="0" pid="530529" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53855e+01" utime="4.93504e+01" stime="7.51621e+00" mtime="2.92976e+01" gflop="0.00000e+00" gbyte="3.76095e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92976e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52591e+01" utime="4.93208e+01" stime="7.50450e+00" mtime="2.92976e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92976e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4761e+08" > 3.3443e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4861e+08" > 2.9696e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1512e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9166e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.8718e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3962e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0095e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0488e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1424e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9139e+01 </func>
</region>
</regions>
<internal rank="685" log_i="1723712895.677638" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="686" mpi_size="768" stamp_init="1723712830.292195" stamp_final="1723712895.688670" username="apac4" allocationname="unknown" flags="0" pid="530530" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53965e+01" utime="4.66346e+01" stime="8.53199e+00" mtime="2.86603e+01" gflop="0.00000e+00" gbyte="3.76259e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86603e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4331435143614ea563614361488" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52676e+01" utime="4.66016e+01" stime="8.52353e+00" mtime="2.86603e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86603e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4613e+08" > 5.8296e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4574e+08" > 4.5066e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4718e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9275e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.7922e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.7011e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3967e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0035e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0470e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1431e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7902e+01 </func>
</region>
</regions>
<internal rank="686" log_i="1723712895.688670" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="687" mpi_size="768" stamp_init="1723712830.292175" stamp_final="1723712895.689912" username="apac4" allocationname="unknown" flags="0" pid="530531" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53977e+01" utime="4.94104e+01" stime="7.50860e+00" mtime="2.93007e+01" gflop="0.00000e+00" gbyte="3.77621e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.93007e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf42f1431143214b9553214311481" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52694e+01" utime="4.93762e+01" stime="7.50050e+00" mtime="2.93007e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.93007e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4803e+08" > 3.4812e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4874e+08" > 2.5592e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2313e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9387e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.0010e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3957e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0102e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0469e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1421e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9034e+01 </func>
</region>
</regions>
<internal rank="687" log_i="1723712895.689912" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="688" mpi_size="768" stamp_init="1723712830.292160" stamp_final="1723712895.687652" username="apac4" allocationname="unknown" flags="0" pid="530532" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53955e+01" utime="4.64521e+01" stime="8.62465e+00" mtime="2.87336e+01" gflop="0.00000e+00" gbyte="3.77380e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87336e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000081148014bb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52658e+01" utime="4.64281e+01" stime="8.60607e+00" mtime="2.87336e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87336e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4818e+08" > 7.1698e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4777e+08" > 4.7817e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8182e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4785e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.3912e-01 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.5182e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3963e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0078e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0488e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1431e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7510e+01 </func>
</region>
</regions>
<internal rank="688" log_i="1723712895.687652" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="689" mpi_size="768" stamp_init="1723712830.292129" stamp_final="1723712895.684981" username="apac4" allocationname="unknown" flags="0" pid="530533" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53929e+01" utime="4.92603e+01" stime="7.60680e+00" mtime="2.91170e+01" gflop="0.00000e+00" gbyte="3.76972e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91170e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000007d14a7557d147d146f" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52648e+01" utime="4.92251e+01" stime="7.59954e+00" mtime="2.91170e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91170e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4850e+08" > 5.0136e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4882e+08" > 3.1270e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2371e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9271e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8959e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3964e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0074e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0465e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1429e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8692e+01 </func>
</region>
</regions>
<internal rank="689" log_i="1723712895.684981" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="690" mpi_size="768" stamp_init="1723712830.292159" stamp_final="1723712895.685804" username="apac4" allocationname="unknown" flags="0" pid="530534" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53936e+01" utime="4.76151e+01" stime="8.11787e+00" mtime="2.84793e+01" gflop="0.00000e+00" gbyte="3.75874e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84793e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000661492556614661489" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52652e+01" utime="4.75817e+01" stime="8.10952e+00" mtime="2.84793e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84793e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4831e+08" > 5.6070e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4923e+08" > 4.3215e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3805e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9137e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.6321e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.2178e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3961e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0068e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0490e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1428e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7854e+01 </func>
</region>
</regions>
<internal rank="690" log_i="1723712895.685804" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="691" mpi_size="768" stamp_init="1723712830.292125" stamp_final="1723712895.679764" username="apac4" allocationname="unknown" flags="0" pid="530535" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53876e+01" utime="4.92968e+01" stime="7.56644e+00" mtime="2.94745e+01" gflop="0.00000e+00" gbyte="3.77438e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.94745e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4eb15ed15ee15f355ee15ee1517" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52570e+01" utime="4.92728e+01" stime="7.54800e+00" mtime="2.94745e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.94745e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4916e+08" > 4.6195e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4892e+08" > 2.7514e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4225e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9294e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.9000e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3959e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0125e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0491e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1429e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8903e+01 </func>
</region>
</regions>
<internal rank="691" log_i="1723712895.679764" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="692" mpi_size="768" stamp_init="1723712830.292157" stamp_final="1723712895.687632" username="apac4" allocationname="unknown" flags="0" pid="530536" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53955e+01" utime="4.66954e+01" stime="8.35870e+00" mtime="2.82994e+01" gflop="0.00000e+00" gbyte="3.77144e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82994e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000096149514f9" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52664e+01" utime="4.66581e+01" stime="8.35347e+00" mtime="2.82994e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82994e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5015e+08" > 6.8918e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4829e+08" > 5.2393e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1507e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9227e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6761e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4901e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3941e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0258e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0484e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1387e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7759e+01 </func>
</region>
</regions>
<internal rank="692" log_i="1723712895.687632" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="693" mpi_size="768" stamp_init="1723712830.292138" stamp_final="1723712895.679511" username="apac4" allocationname="unknown" flags="0" pid="530537" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53874e+01" utime="4.94347e+01" stime="7.41545e+00" mtime="2.89491e+01" gflop="0.00000e+00" gbyte="3.77552e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89491e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52591e+01" utime="4.93960e+01" stime="7.41130e+00" mtime="2.89491e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89491e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4753e+08" > 4.6133e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4876e+08" > 2.9205e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8883e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9150e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.0967e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.7212e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3952e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0191e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0475e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1428e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8929e+01 </func>
</region>
</regions>
<internal rank="693" log_i="1723712895.679511" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="694" mpi_size="768" stamp_init="1723712830.292111" stamp_final="1723712895.682962" username="apac4" allocationname="unknown" flags="0" pid="530538" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53909e+01" utime="4.71575e+01" stime="8.24599e+00" mtime="2.81163e+01" gflop="0.00000e+00" gbyte="3.77125e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81163e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000ae147655ae14ad14c8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52614e+01" utime="4.71227e+01" stime="8.23868e+00" mtime="2.81163e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81163e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.0599e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4754e+08" > 6.9384e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4729e+08" > 4.4343e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3760e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9229e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.5300e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.3828e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3942e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0252e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0477e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1422e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7353e+01 </func>
</region>
</regions>
<internal rank="694" log_i="1723712895.682962" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="695" mpi_size="768" stamp_init="1723712830.292163" stamp_final="1723712895.685138" username="apac4" allocationname="unknown" flags="0" pid="530539" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a08u11b</host>
<perf wtime="6.53930e+01" utime="4.95577e+01" stime="7.31964e+00" mtime="2.93438e+01" gflop="0.00000e+00" gbyte="3.74870e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.93438e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000af14a355af14ae14d0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52648e+01" utime="4.95294e+01" stime="7.30519e+00" mtime="2.93438e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.93438e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4907e+08" > 4.6881e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4813e+08" > 2.8840e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3246e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.9314e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.1842e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3940e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0282e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0492e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1426e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8860e+01 </func>
</region>
</regions>
<internal rank="695" log_i="1723712895.685138" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="696" mpi_size="768" stamp_init="1723712830.315422" stamp_final="1723712895.683279" username="apac4" allocationname="unknown" flags="0" pid="593407" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53679e+01" utime="4.25264e+01" stime="1.22910e+01" mtime="2.76925e+01" gflop="0.00000e+00" gbyte="3.86395e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.76925e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52408e+01" utime="4.24958e+01" stime="1.22792e+01" mtime="2.76925e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.76925e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4883e+08" > 7.0714e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4964e+08" > 4.3868e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3665e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4642e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.5919e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.5049e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3940e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0286e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0450e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1415e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7387e+01 </func>
</region>
</regions>
<internal rank="696" log_i="1723712895.683279" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="697" mpi_size="768" stamp_init="1723712830.315490" stamp_final="1723712895.682245" username="apac4" allocationname="unknown" flags="0" pid="593408" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53668e+01" utime="4.87200e+01" stime="7.39201e+00" mtime="2.82934e+01" gflop="0.00000e+00" gbyte="3.78086e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82934e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52336e+01" utime="4.86810e+01" stime="7.38722e+00" mtime="2.82934e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82934e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4987e+08" > 5.7207e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4851e+08" > 2.9098e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1299e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4690e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.6541e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3945e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0194e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0427e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1416e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8371e+01 </func>
</region>
</regions>
<internal rank="697" log_i="1723712895.682245" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="698" mpi_size="768" stamp_init="1723712830.315383" stamp_final="1723712895.679874" username="apac4" allocationname="unknown" flags="0" pid="593409" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53645e+01" utime="4.71387e+01" stime="7.70320e+00" mtime="2.74219e+01" gflop="0.00000e+00" gbyte="3.76472e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.74219e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000001e151d1544" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52369e+01" utime="4.71058e+01" stime="7.69407e+00" mtime="2.74219e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.74219e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4916e+08" > 7.5260e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4969e+08" > 4.6262e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.5034e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4648e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.6970e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.5438e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3948e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0203e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0440e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1415e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7929e+01 </func>
</region>
</regions>
<internal rank="698" log_i="1723712895.679874" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="699" mpi_size="768" stamp_init="1723712830.315505" stamp_final="1723712895.680563" username="apac4" allocationname="unknown" flags="0" pid="593410" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53651e+01" utime="4.86524e+01" stime="7.42766e+00" mtime="2.90998e+01" gflop="0.00000e+00" gbyte="3.75458e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90998e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000009f15a6559f159e150e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52404e+01" utime="4.86223e+01" stime="7.41543e+00" mtime="2.90998e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90998e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4951e+08" > 5.3904e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4877e+08" > 2.7539e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7300e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4627e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.5390e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3945e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0238e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0455e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1417e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8617e+01 </func>
</region>
</regions>
<internal rank="699" log_i="1723712895.680563" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="700" mpi_size="768" stamp_init="1723712830.315391" stamp_final="1723712895.680960" username="apac4" allocationname="unknown" flags="0" pid="593411" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53656e+01" utime="4.50171e+01" stime="8.86585e+00" mtime="2.77371e+01" gflop="0.00000e+00" gbyte="3.76579e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77371e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000301430146b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52364e+01" utime="4.49814e+01" stime="8.85989e+00" mtime="2.77371e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77371e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4804e+08" > 8.6764e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4956e+08" > 5.3477e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2681e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4608e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0590e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.5872e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3938e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0278e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0440e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1418e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7362e+01 </func>
</region>
</regions>
<internal rank="700" log_i="1723712895.680960" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="701" mpi_size="768" stamp_init="1723712830.315518" stamp_final="1723712895.683105" username="apac4" allocationname="unknown" flags="0" pid="593412" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53676e+01" utime="4.84213e+01" stime="7.41142e+00" mtime="2.88081e+01" gflop="0.00000e+00" gbyte="3.75881e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88081e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf42d152e153015715530152f150c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52410e+01" utime="4.83923e+01" stime="7.39894e+00" mtime="2.88081e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88081e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4893e+08" > 5.4251e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4768e+08" > 2.4889e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1676e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4620e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.4168e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3936e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0332e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0456e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1416e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8886e+01 </func>
</region>
</regions>
<internal rank="701" log_i="1723712895.683105" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="702" mpi_size="768" stamp_init="1723712830.315394" stamp_final="1723712895.696598" username="apac4" allocationname="unknown" flags="0" pid="593413" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53812e+01" utime="4.51569e+01" stime="8.44051e+00" mtime="2.74672e+01" gflop="0.00000e+00" gbyte="3.74012e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.74672e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4d814d914db142e55db14da14f4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52547e+01" utime="4.51224e+01" stime="8.43331e+00" mtime="2.74672e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.74672e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4937e+08" > 1.0266e+00 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4893e+08" > 6.0156e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.6594e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4617e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.7207e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.6251e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3930e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0370e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0462e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1417e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7536e+01 </func>
</region>
</regions>
<internal rank="702" log_i="1723712895.696598" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="703" mpi_size="768" stamp_init="1723712830.315476" stamp_final="1723712895.677929" username="apac4" allocationname="unknown" flags="0" pid="593414" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53625e+01" utime="4.87999e+01" stime="7.32658e+00" mtime="2.86287e+01" gflop="0.00000e+00" gbyte="3.77350e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86287e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e214e414e5143055e514e514aa" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52347e+01" utime="4.87673e+01" stime="7.31666e+00" mtime="2.86287e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86287e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4992e+08" > 5.4451e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4956e+08" > 2.9790e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1125e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4753e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9789e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.2491e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3933e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0364e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0443e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1418e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8744e+01 </func>
</region>
</regions>
<internal rank="703" log_i="1723712895.677929" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="704" mpi_size="768" stamp_init="1723712830.315420" stamp_final="1723712895.679588" username="apac4" allocationname="unknown" flags="0" pid="593415" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53642e+01" utime="4.40192e+01" stime="9.08782e+00" mtime="2.75278e+01" gflop="0.00000e+00" gbyte="3.77903e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.75278e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000004a141c554a144914c1" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52373e+01" utime="4.39840e+01" stime="9.08052e+00" mtime="2.75278e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.75278e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4864e+08" > 7.7998e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4806e+08" > 7.0017e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3794e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4648e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.2485e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.3797e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3903e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0653e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0439e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1418e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7109e+01 </func>
</region>
</regions>
<internal rank="704" log_i="1723712895.679588" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="705" mpi_size="768" stamp_init="1723712830.315510" stamp_final="1723712895.678164" username="apac4" allocationname="unknown" flags="0" pid="593416" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53627e+01" utime="4.87495e+01" stime="7.11756e+00" mtime="2.81961e+01" gflop="0.00000e+00" gbyte="3.77220e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81961e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006c156c1537" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52343e+01" utime="4.87205e+01" stime="7.10478e+00" mtime="2.81961e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81961e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4929e+08" > 4.3205e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4966e+08" > 3.2219e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9962e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4757e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.8849e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3906e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0640e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0448e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1415e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8536e+01 </func>
</region>
</regions>
<internal rank="705" log_i="1723712895.678164" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="706" mpi_size="768" stamp_init="1723712830.315409" stamp_final="1723712895.684718" username="apac4" allocationname="unknown" flags="0" pid="593417" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53693e+01" utime="4.69842e+01" stime="7.88714e+00" mtime="2.77750e+01" gflop="0.00000e+00" gbyte="3.75034e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77750e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000e115e01519" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52411e+01" utime="4.69514e+01" stime="7.87793e+00" mtime="2.77750e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77750e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4895e+08" > 5.5686e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4951e+08" > 4.7177e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9619e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4466e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.1205e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4601e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3906e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0637e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0470e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1419e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8036e+01 </func>
</region>
</regions>
<internal rank="706" log_i="1723712895.684718" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="707" mpi_size="768" stamp_init="1723712830.315485" stamp_final="1723712895.682906" username="apac4" allocationname="unknown" flags="0" pid="593418" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53674e+01" utime="4.88155e+01" stime="7.32580e+00" mtime="2.86671e+01" gflop="0.00000e+00" gbyte="3.77724e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86671e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000db15da1553" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52392e+01" utime="4.87780e+01" stime="7.32093e+00" mtime="2.86671e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86671e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4909e+08" > 4.3990e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4840e+08" > 3.4675e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1594e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4650e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.4599e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3901e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0685e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0439e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1412e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8842e+01 </func>
</region>
</regions>
<internal rank="707" log_i="1723712895.682906" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="708" mpi_size="768" stamp_init="1723712830.315375" stamp_final="1723712895.679633" username="apac4" allocationname="unknown" flags="0" pid="593419" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53643e+01" utime="4.58608e+01" stime="8.28972e+00" mtime="2.80040e+01" gflop="0.00000e+00" gbyte="3.77533e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80040e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf47f1481148214bc558214811483" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52359e+01" utime="4.58306e+01" stime="8.27740e+00" mtime="2.80040e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80040e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4927e+08" > 7.1286e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4908e+08" > 7.2514e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0579e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4696e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.9605e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4219e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3889e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0722e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0440e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1419e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7969e+01 </func>
</region>
</regions>
<internal rank="708" log_i="1723712895.679633" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="709" mpi_size="768" stamp_init="1723712830.315498" stamp_final="1723712895.687825" username="apac4" allocationname="unknown" flags="0" pid="593420" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53723e+01" utime="4.90269e+01" stime="7.08931e+00" mtime="2.86769e+01" gflop="0.00000e+00" gbyte="3.77865e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86769e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005a14541456" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52476e+01" utime="4.89970e+01" stime="7.07667e+00" mtime="2.86769e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86769e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4877e+08" > 4.3207e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4915e+08" > 3.3936e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8441e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4827e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4937e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3892e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0739e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0423e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1414e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9161e+01 </func>
</region>
</regions>
<internal rank="709" log_i="1723712895.687825" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="710" mpi_size="768" stamp_init="1723712830.315396" stamp_final="1723712895.678845" username="apac4" allocationname="unknown" flags="0" pid="593421" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53634e+01" utime="4.67676e+01" stime="7.92641e+00" mtime="2.77526e+01" gflop="0.00000e+00" gbyte="3.74794e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77526e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000f014f014d4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52374e+01" utime="4.67341e+01" stime="7.91786e+00" mtime="2.77526e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77526e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4823e+08" > 6.2431e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4866e+08" > 4.3341e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0787e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4622e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.1172e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6600e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3886e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0837e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0438e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1418e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7815e+01 </func>
</region>
</regions>
<internal rank="710" log_i="1723712895.678845" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="711" mpi_size="768" stamp_init="1723712830.315482" stamp_final="1723712895.676934" username="apac4" allocationname="unknown" flags="0" pid="593422" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53615e+01" utime="4.86734e+01" stime="7.50000e+00" mtime="2.91627e+01" gflop="0.00000e+00" gbyte="3.76545e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91627e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52352e+01" utime="4.86389e+01" stime="7.49284e+00" mtime="2.91627e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91627e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4786e+08" > 4.2131e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4757e+08" > 3.3235e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5199e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4684e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7881e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.6500e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3882e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0840e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0450e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1413e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8996e+01 </func>
</region>
</regions>
<internal rank="711" log_i="1723712895.676934" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="712" mpi_size="768" stamp_init="1723712830.315377" stamp_final="1723712895.690520" username="apac4" allocationname="unknown" flags="0" pid="593423" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53751e+01" utime="4.55034e+01" stime="8.67891e+00" mtime="2.77154e+01" gflop="0.00000e+00" gbyte="3.75393e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77154e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43415351536152b553615361500" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52465e+01" utime="4.54716e+01" stime="8.66867e+00" mtime="2.77154e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77154e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4895e+08" > 7.0046e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4886e+08" > 6.4785e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5247e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4702e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.5002e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4801e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3885e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0828e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0412e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1418e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7229e+01 </func>
</region>
</regions>
<internal rank="712" log_i="1723712895.690520" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="713" mpi_size="768" stamp_init="1723712830.315476" stamp_final="1723712895.688451" username="apac4" allocationname="unknown" flags="0" pid="593424" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53730e+01" utime="4.90543e+01" stime="7.08208e+00" mtime="2.84234e+01" gflop="0.00000e+00" gbyte="3.77312e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84234e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f3141014111452551114111471" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52460e+01" utime="4.90213e+01" stime="7.07294e+00" mtime="2.84234e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84234e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.5300e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4767e+08" > 4.8176e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4926e+08" > 2.9661e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1406e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4754e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3568e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3884e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0821e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0416e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1413e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8573e+01 </func>
</region>
</regions>
<internal rank="713" log_i="1723712895.688451" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="714" mpi_size="768" stamp_init="1723712830.315403" stamp_final="1723712895.688012" username="apac4" allocationname="unknown" flags="0" pid="593425" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53726e+01" utime="4.69562e+01" stime="8.08327e+00" mtime="2.79488e+01" gflop="0.00000e+00" gbyte="3.76308e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79488e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c8151456c815c81517" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52478e+01" utime="4.69259e+01" stime="8.07169e+00" mtime="2.79488e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79488e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.3842e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4887e+08" > 6.0924e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4879e+08" > 4.5579e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1673e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4072e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.1791e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.7910e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3883e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0874e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0425e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1418e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7996e+01 </func>
</region>
</regions>
<internal rank="714" log_i="1723712895.688012" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="715" mpi_size="768" stamp_init="1723712830.315467" stamp_final="1723712895.687587" username="apac4" allocationname="unknown" flags="0" pid="593426" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53721e+01" utime="4.87070e+01" stime="7.40593e+00" mtime="2.82346e+01" gflop="0.00000e+00" gbyte="3.77422e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82346e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000fc15fc1501" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52448e+01" utime="4.86759e+01" stime="7.39466e+00" mtime="2.82346e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82346e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4884e+08" > 4.7414e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4770e+08" > 3.1890e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9523e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4761e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0438e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3873e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0971e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0433e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1411e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8578e+01 </func>
</region>
</regions>
<internal rank="715" log_i="1723712895.687587" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="716" mpi_size="768" stamp_init="1723712830.315371" stamp_final="1723712895.682516" username="apac4" allocationname="unknown" flags="0" pid="593427" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53671e+01" utime="4.67217e+01" stime="7.96454e+00" mtime="2.75669e+01" gflop="0.00000e+00" gbyte="3.76678e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.75669e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52397e+01" utime="4.66832e+01" stime="7.96086e+00" mtime="2.75669e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.75669e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.7220e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4840e+08" > 6.3743e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4919e+08" > 4.4258e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.5283e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4739e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.4301e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4839e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3876e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0936e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0425e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1418e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8156e+01 </func>
</region>
</regions>
<internal rank="716" log_i="1723712895.682516" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="717" mpi_size="768" stamp_init="1723712830.315482" stamp_final="1723712895.677289" username="apac4" allocationname="unknown" flags="0" pid="593428" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53618e+01" utime="4.88623e+01" stime="7.29741e+00" mtime="2.87165e+01" gflop="0.00000e+00" gbyte="3.77434e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87165e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52363e+01" utime="4.88311e+01" stime="7.28653e+00" mtime="2.87165e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87165e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4864e+08" > 4.6227e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4932e+08" > 3.0334e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1297e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4772e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5020e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.6602e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3871e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0987e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0440e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1409e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8892e+01 </func>
</region>
</regions>
<internal rank="717" log_i="1723712895.677289" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="718" mpi_size="768" stamp_init="1723712830.315386" stamp_final="1723712895.679593" username="apac4" allocationname="unknown" flags="0" pid="593429" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53642e+01" utime="4.66698e+01" stime="8.06484e+00" mtime="2.77744e+01" gflop="0.00000e+00" gbyte="3.76839e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77744e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000066154c556615661522" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52398e+01" utime="4.66435e+01" stime="8.04882e+00" mtime="2.77744e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77744e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4969e+08" > 6.6972e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4828e+08" > 4.6739e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0180e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4662e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2639e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4880e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3870e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0963e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0420e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1417e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7850e+01 </func>
</region>
</regions>
<internal rank="718" log_i="1723712895.679593" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="719" mpi_size="768" stamp_init="1723712830.315467" stamp_final="1723712895.691149" username="apac4" allocationname="unknown" flags="0" pid="593430" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u03b</host>
<perf wtime="6.53757e+01" utime="4.86798e+01" stime="7.40979e+00" mtime="2.89282e+01" gflop="0.00000e+00" gbyte="3.76347e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89282e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000003f1583563f153f152e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.52480e+01" utime="4.86485e+01" stime="7.39839e+00" mtime="2.89282e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89282e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4902e+08" > 4.8135e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4932e+08" > 2.7730e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5211e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.4643e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.4772e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3865e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1044e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0465e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1408e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8711e+01 </func>
</region>
</regions>
<internal rank="719" log_i="1723712895.691149" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="720" mpi_size="768" stamp_init="1723712830.109008" stamp_final="1723712895.684806" username="apac4" allocationname="unknown" flags="0" pid="3027866" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55758e+01" utime="4.31258e+01" stime="1.25089e+01" mtime="2.82417e+01" gflop="0.00000e+00" gbyte="3.85239e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82417e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf442144414451433564514441498" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54472e+01" utime="4.30931e+01" stime="1.25003e+01" mtime="2.82417e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82417e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.4836e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4749e+08" > 5.3983e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4848e+08" > 4.3464e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.8028e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5690e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 4.1611e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8838e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3858e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1103e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0489e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1410e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7557e+01 </func>
</region>
</regions>
<internal rank="720" log_i="1723712895.684806" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="721" mpi_size="768" stamp_init="1723712830.108631" stamp_final="1723712895.690592" username="apac4" allocationname="unknown" flags="0" pid="3027867" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55820e+01" utime="4.92233e+01" stime="7.28985e+00" mtime="2.86652e+01" gflop="0.00000e+00" gbyte="3.78464e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86652e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000053143855531453147d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54522e+01" utime="4.91880e+01" stime="7.28166e+00" mtime="2.86652e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86652e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4873e+08" > 4.5699e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4985e+08" > 3.2384e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1055e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5768e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0981e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0377e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3875e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0933e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0481e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1411e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8762e+01 </func>
</region>
</regions>
<internal rank="721" log_i="1723712895.690592" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="722" mpi_size="768" stamp_init="1723712830.108668" stamp_final="1723712895.694808" username="apac4" allocationname="unknown" flags="0" pid="3027868" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55861e+01" utime="4.79392e+01" stime="7.66926e+00" mtime="2.76347e+01" gflop="0.00000e+00" gbyte="3.76652e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.76347e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54596e+01" utime="4.79064e+01" stime="7.65997e+00" mtime="2.76347e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.76347e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5039e+08" > 5.2525e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4993e+08" > 4.2743e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.5102e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5560e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.2302e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.7929e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3875e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0932e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0513e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1410e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8273e+01 </func>
</region>
</regions>
<internal rank="722" log_i="1723712895.694808" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="723" mpi_size="768" stamp_init="1723712830.109542" stamp_final="1723712895.679437" username="apac4" allocationname="unknown" flags="0" pid="3027869" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55699e+01" utime="4.91229e+01" stime="7.40073e+00" mtime="2.85882e+01" gflop="0.00000e+00" gbyte="3.77594e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85882e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000033147d553314331466" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54434e+01" utime="4.90916e+01" stime="7.39032e+00" mtime="2.85882e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85882e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4942e+08" > 4.4808e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4929e+08" > 2.8087e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9751e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5786e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.7991e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3870e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.0958e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0491e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1404e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8832e+01 </func>
</region>
</regions>
<internal rank="723" log_i="1723712895.679437" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="724" mpi_size="768" stamp_init="1723712830.108663" stamp_final="1723712895.685404" username="apac4" allocationname="unknown" flags="0" pid="3027870" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55767e+01" utime="4.69112e+01" stime="8.13606e+00" mtime="2.76654e+01" gflop="0.00000e+00" gbyte="3.77007e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.76654e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007e147e14a5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54476e+01" utime="4.68812e+01" stime="8.12440e+00" mtime="2.76654e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.76654e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4815e+08" > 5.9352e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4890e+08" > 3.9051e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1897e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5694e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.0282e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0164e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3859e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1034e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0490e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1409e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7540e+01 </func>
</region>
</regions>
<internal rank="724" log_i="1723712895.685404" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="725" mpi_size="768" stamp_init="1723712830.109574" stamp_final="1723712895.684203" username="apac4" allocationname="unknown" flags="0" pid="3027871" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55746e+01" utime="4.90531e+01" stime="7.40849e+00" mtime="2.87302e+01" gflop="0.00000e+00" gbyte="3.76476e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87302e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000e115b856e115e1154b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54515e+01" utime="4.90250e+01" stime="7.39527e+00" mtime="2.87302e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87302e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4756e+08" > 4.6990e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4875e+08" > 2.9684e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9581e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5770e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0209e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3866e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1040e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0474e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1368e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8970e+01 </func>
</region>
</regions>
<internal rank="725" log_i="1723712895.684203" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="726" mpi_size="768" stamp_init="1723712830.108648" stamp_final="1723712895.686388" username="apac4" allocationname="unknown" flags="0" pid="3027872" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55777e+01" utime="4.75660e+01" stime="8.02894e+00" mtime="2.80480e+01" gflop="0.00000e+00" gbyte="3.77304e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80480e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54505e+01" utime="4.75326e+01" stime="8.02016e+00" mtime="2.80480e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80480e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4936e+08" > 5.3220e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4743e+08" > 4.3854e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0775e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5786e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.9053e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9868e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3864e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1053e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0518e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1408e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8090e+01 </func>
</region>
</regions>
<internal rank="726" log_i="1723712895.686388" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="727" mpi_size="768" stamp_init="1723712830.108657" stamp_final="1723712895.690197" username="apac4" allocationname="unknown" flags="0" pid="3027873" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55815e+01" utime="4.91564e+01" stime="7.43929e+00" mtime="2.86227e+01" gflop="0.00000e+00" gbyte="3.77266e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86227e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54541e+01" utime="4.91245e+01" stime="7.42939e+00" mtime="2.86227e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86227e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4840e+08" > 4.4335e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4819e+08" > 3.1297e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7472e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5769e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.6928e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9949e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3858e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1111e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0478e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1404e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9098e+01 </func>
</region>
</regions>
<internal rank="727" log_i="1723712895.690197" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="728" mpi_size="768" stamp_init="1723712830.108653" stamp_final="1723712895.685421" username="apac4" allocationname="unknown" flags="0" pid="3027874" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55768e+01" utime="4.62798e+01" stime="8.41197e+00" mtime="2.76285e+01" gflop="0.00000e+00" gbyte="3.76675e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.76285e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a4143c55a414a414ab" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54490e+01" utime="4.62487e+01" stime="8.40145e+00" mtime="2.76285e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.76285e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4867e+08" > 6.6073e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4902e+08" > 4.7711e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.4522e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5884e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.5220e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8934e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3852e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1145e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0499e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1366e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8145e+01 </func>
</region>
</regions>
<internal rank="728" log_i="1723712895.685421" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="729" mpi_size="768" stamp_init="1723712830.108647" stamp_final="1723712895.690360" username="apac4" allocationname="unknown" flags="0" pid="3027875" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55817e+01" utime="4.94892e+01" stime="7.03621e+00" mtime="2.86435e+01" gflop="0.00000e+00" gbyte="3.77037e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86435e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54559e+01" utime="4.94580e+01" stime="7.02523e+00" mtime="2.86435e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86435e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.2452e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4997e+08" > 3.5830e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4829e+08" > 2.4424e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9668e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5875e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.1928e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3853e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1165e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0499e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1403e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8981e+01 </func>
</region>
</regions>
<internal rank="729" log_i="1723712895.690360" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="730" mpi_size="768" stamp_init="1723712830.108703" stamp_final="1723712895.690850" username="apac4" allocationname="unknown" flags="0" pid="3027876" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55821e+01" utime="4.79787e+01" stime="7.66046e+00" mtime="2.82429e+01" gflop="0.00000e+00" gbyte="3.77136e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.82429e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000be15bd150e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54558e+01" utime="4.79473e+01" stime="7.64971e+00" mtime="2.82429e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.82429e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Isend" count="127164" bytes="8.4837e+08" > 4.5926e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4990e+08" > 3.8770e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0265e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5905e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.1035e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9158e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3852e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1140e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0493e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1410e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8403e+01 </func>
</region>
</regions>
<internal rank="730" log_i="1723712895.690850" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="731" mpi_size="768" stamp_init="1723712830.108646" stamp_final="1723712895.679123" username="apac4" allocationname="unknown" flags="0" pid="3027877" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55705e+01" utime="4.90693e+01" stime="7.48219e+00" mtime="2.88739e+01" gflop="0.00000e+00" gbyte="3.76350e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88739e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4c114c314c414fd55c414c414c3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54461e+01" utime="4.90404e+01" stime="7.46925e+00" mtime="2.88739e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88739e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4919e+08" > 3.5981e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4896e+08" > 2.6151e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3564e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6019e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7166e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.3797e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3845e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1257e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0475e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1372e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8806e+01 </func>
</region>
</regions>
<internal rank="731" log_i="1723712895.679123" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="732" mpi_size="768" stamp_init="1723712830.108639" stamp_final="1723712895.690663" username="apac4" allocationname="unknown" flags="0" pid="3027878" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55820e+01" utime="4.56008e+01" stime="8.54198e+00" mtime="2.78751e+01" gflop="0.00000e+00" gbyte="3.77140e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78751e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54549e+01" utime="4.55676e+01" stime="8.53312e+00" mtime="2.78751e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78751e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4857e+08" > 6.6486e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4832e+08" > 5.1178e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8207e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5757e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.5844e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9961e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3837e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1243e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0476e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1409e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8037e+01 </func>
</region>
</regions>
<internal rank="732" log_i="1723712895.690663" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="733" mpi_size="768" stamp_init="1723712830.108614" stamp_final="1723712895.689461" username="apac4" allocationname="unknown" flags="0" pid="3027879" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55808e+01" utime="4.92818e+01" stime="7.26236e+00" mtime="2.89258e+01" gflop="0.00000e+00" gbyte="3.76072e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89258e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54591e+01" utime="4.92491e+01" stime="7.25357e+00" mtime="2.89258e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89258e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4843e+08" > 3.5860e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4804e+08" > 2.8248e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0957e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6007e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9789e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 2.5244e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3844e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1264e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0477e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1399e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9119e+01 </func>
</region>
</regions>
<internal rank="733" log_i="1723712895.689461" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="734" mpi_size="768" stamp_init="1723712830.108670" stamp_final="1723712895.681086" username="apac4" allocationname="unknown" flags="0" pid="3027880" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55724e+01" utime="4.56871e+01" stime="8.61217e+00" mtime="2.81155e+01" gflop="0.00000e+00" gbyte="3.76228e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81155e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d715d71535" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54444e+01" utime="4.56614e+01" stime="8.59652e+00" mtime="2.81155e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81155e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4915e+08" > 7.1712e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4867e+08" > 4.9780e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1898e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5754e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.5310e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9823e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3835e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1294e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0519e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1402e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7852e+01 </func>
</region>
</regions>
<internal rank="734" log_i="1723712895.681086" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="735" mpi_size="768" stamp_init="1723712830.108610" stamp_final="1723712895.679163" username="apac4" allocationname="unknown" flags="0" pid="3027881" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55706e+01" utime="4.92042e+01" stime="7.33182e+00" mtime="2.90147e+01" gflop="0.00000e+00" gbyte="3.76301e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90147e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000005c155b151e" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54467e+01" utime="4.91738e+01" stime="7.32092e+00" mtime="2.90147e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90147e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4855e+08" > 3.4224e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4956e+08" > 2.7470e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2493e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5899e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3828e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9792e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3834e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1359e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0478e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1408e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9080e+01 </func>
</region>
</regions>
<internal rank="735" log_i="1723712895.679163" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="736" mpi_size="768" stamp_init="1723712830.108681" stamp_final="1723712895.691321" username="apac4" allocationname="unknown" flags="0" pid="3027882" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55826e+01" utime="4.63128e+01" stime="8.49791e+00" mtime="2.77787e+01" gflop="0.00000e+00" gbyte="3.76240e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.77787e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000008014615580147f14af" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54549e+01" utime="4.62815e+01" stime="8.48749e+00" mtime="2.77787e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.77787e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5035e+08" > 6.9201e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4832e+08" > 5.0080e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0011e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5652e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.9377e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.8769e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3826e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1444e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0507e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1402e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7738e+01 </func>
</region>
</regions>
<internal rank="736" log_i="1723712895.691321" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="737" mpi_size="768" stamp_init="1723712830.108629" stamp_final="1723712895.690418" username="apac4" allocationname="unknown" flags="0" pid="3027883" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55818e+01" utime="4.90582e+01" stime="7.40031e+00" mtime="2.84877e+01" gflop="0.00000e+00" gbyte="3.77861e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84877e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4451446144714885547144714e0" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54568e+01" utime="4.90250e+01" stime="7.39066e+00" mtime="2.84877e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84877e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Isend" count="127164" bytes="8.4829e+08" > 4.7206e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4859e+08" > 2.9013e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0265e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5631e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3113e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.7156e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3821e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1493e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0520e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1407e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8670e+01 </func>
</region>
</regions>
<internal rank="737" log_i="1723712895.690418" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="738" mpi_size="768" stamp_init="1723712830.108637" stamp_final="1723712895.685652" username="apac4" allocationname="unknown" flags="0" pid="3027884" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55770e+01" utime="4.72293e+01" stime="8.20770e+00" mtime="2.80144e+01" gflop="0.00000e+00" gbyte="3.76854e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80144e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000b5143c55b514b5149d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54489e+01" utime="4.71990e+01" stime="8.19596e+00" mtime="2.80144e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80144e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 7.1526e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4744e+08" > 5.8727e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4900e+08" > 4.1225e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8186e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5746e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.3855e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9287e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3818e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1530e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0514e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1402e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8267e+01 </func>
</region>
</regions>
<internal rank="738" log_i="1723712895.685652" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="739" mpi_size="768" stamp_init="1723712830.108644" stamp_final="1723712895.679249" username="apac4" allocationname="unknown" flags="0" pid="3027885" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55706e+01" utime="4.90758e+01" stime="7.41216e+00" mtime="2.87245e+01" gflop="0.00000e+00" gbyte="3.74519e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87245e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000008e148e1475" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54475e+01" utime="4.90440e+01" stime="7.40168e+00" mtime="2.87245e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87245e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 8.1062e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4832e+08" > 4.8622e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4904e+08" > 3.4859e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3908e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5702e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.8120e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 6.5696e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3818e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1494e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0486e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1406e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8515e+01 </func>
</region>
</regions>
<internal rank="739" log_i="1723712895.679249" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="740" mpi_size="768" stamp_init="1723712830.108654" stamp_final="1723712895.679141" username="apac4" allocationname="unknown" flags="0" pid="3027886" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55705e+01" utime="4.58481e+01" stime="8.65085e+00" mtime="2.76575e+01" gflop="0.00000e+00" gbyte="3.76839e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.76575e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c115a555c115c01531" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54442e+01" utime="4.58156e+01" stime="8.64062e+00" mtime="2.76575e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.76575e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4741e+08" > 7.4413e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4833e+08" > 7.4590e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.9865e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5620e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1048e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 3.9880e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3806e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1600e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0493e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1410e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7563e+01 </func>
</region>
</regions>
<internal rank="740" log_i="1723712895.679141" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="741" mpi_size="768" stamp_init="1723712830.108623" stamp_final="1723712895.686386" username="apac4" allocationname="unknown" flags="0" pid="3027887" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55778e+01" utime="4.91130e+01" stime="7.27166e+00" mtime="2.87300e+01" gflop="0.00000e+00" gbyte="3.76743e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87300e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000009014901481" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54513e+01" utime="4.90768e+01" stime="7.26589e+00" mtime="2.87300e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87300e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4744e+08" > 4.8456e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4668e+08" > 3.0005e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0546e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5792e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.7419e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3803e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1642e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0512e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1367e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8856e+01 </func>
</region>
</regions>
<internal rank="741" log_i="1723712895.686386" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="742" mpi_size="768" stamp_init="1723712830.108660" stamp_final="1723712895.691369" username="apac4" allocationname="unknown" flags="0" pid="3027888" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55827e+01" utime="4.69406e+01" stime="8.11371e+00" mtime="2.79446e+01" gflop="0.00000e+00" gbyte="3.76286e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.79446e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54547e+01" utime="4.69107e+01" stime="8.10153e+00" mtime="2.79446e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.79446e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 1.9073e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4645e+08" > 5.7634e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4583e+08" > 6.0016e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0494e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5801e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.8201e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.0205e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3806e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1648e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0515e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1401e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7953e+01 </func>
</region>
</regions>
<internal rank="742" log_i="1723712895.691369" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="743" mpi_size="768" stamp_init="1723712830.109552" stamp_final="1723712895.690095" username="apac4" allocationname="unknown" flags="0" pid="3027889" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a07u11a</host>
<perf wtime="6.55805e+01" utime="4.89986e+01" stime="7.22326e+00" mtime="2.85853e+01" gflop="0.00000e+00" gbyte="3.76431e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85853e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000065146414a8" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.54536e+01" utime="4.89632e+01" stime="7.21680e+00" mtime="2.85853e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85853e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4682e+08" > 4.6889e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4668e+08" > 3.4176e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0319e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.5525e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0742e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.2470e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3803e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1636e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0522e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1403e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8769e+01 </func>
</region>
</regions>
<internal rank="743" log_i="1723712895.690095" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="744" mpi_size="768" stamp_init="1723712830.446966" stamp_final="1723712895.685780" username="apac4" allocationname="unknown" flags="0" pid="576867" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52388e+01" utime="4.30946e+01" stime="1.21168e+01" mtime="2.81991e+01" gflop="0.00000e+00" gbyte="3.86589e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81991e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf46b1483149614125596149014f5" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51119e+01" utime="4.30642e+01" stime="1.21045e+01" mtime="2.81991e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81991e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4879e+08" > 5.8478e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4829e+08" > 3.8892e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1791e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6381e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0258e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8263e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3808e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1635e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0381e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1353e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8027e+01 </func>
</region>
</regions>
<internal rank="744" log_i="1723712895.685780" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="745" mpi_size="768" stamp_init="1723712830.447007" stamp_final="1723712895.678401" username="apac4" allocationname="unknown" flags="0" pid="576868" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52314e+01" utime="4.92003e+01" stime="7.16629e+00" mtime="2.89957e+01" gflop="0.00000e+00" gbyte="3.76968e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89957e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf43c143e143f148f563f143f1495" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51000e+01" utime="4.91696e+01" stime="7.15389e+00" mtime="2.89957e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89957e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.3379e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4922e+08" > 4.6152e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4942e+08" > 2.9816e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.2245e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6481e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.4080e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.3561e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3810e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1615e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0362e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1396e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8902e+01 </func>
</region>
</regions>
<internal rank="745" log_i="1723712895.678401" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="746" mpi_size="768" stamp_init="1723712830.448456" stamp_final="1723712895.691273" username="apac4" allocationname="unknown" flags="0" pid="576869" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52428e+01" utime="4.71649e+01" stime="7.86259e+00" mtime="2.80712e+01" gflop="0.00000e+00" gbyte="3.77373e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80712e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000010145e5510141014f3" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51170e+01" utime="4.71335e+01" stime="7.85189e+00" mtime="2.80712e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80712e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4941e+08" > 5.8510e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4951e+08" > 3.3099e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8728e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6361e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.2792e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.8031e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3806e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1661e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0404e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1395e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8210e+01 </func>
</region>
</regions>
<internal rank="746" log_i="1723712895.691273" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="747" mpi_size="768" stamp_init="1723712830.446987" stamp_final="1723712895.686287" username="apac4" allocationname="unknown" flags="0" pid="576870" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52393e+01" utime="4.91570e+01" stime="7.20319e+00" mtime="2.87476e+01" gflop="0.00000e+00" gbyte="3.73940e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87476e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000002d14b8552d14281490" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51122e+01" utime="4.91196e+01" stime="7.19915e+00" mtime="2.87476e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87476e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5039e+08" > 4.4219e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4791e+08" > 2.6522e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7856e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6526e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.1219e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.7991e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3810e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1657e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0395e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1394e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9110e+01 </func>
</region>
</regions>
<internal rank="747" log_i="1723712895.686287" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="748" mpi_size="768" stamp_init="1723712830.448408" stamp_final="1723712895.684479" username="apac4" allocationname="unknown" flags="0" pid="576871" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52361e+01" utime="4.72797e+01" stime="8.03034e+00" mtime="2.83637e+01" gflop="0.00000e+00" gbyte="3.77449e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.83637e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000cd15e855cd15cd1529" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51095e+01" utime="4.72450e+01" stime="8.02311e+00" mtime="2.83637e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.83637e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4736e+08" > 5.5492e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4929e+08" > 4.0062e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4759e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6549e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 3.0999e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8752e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3809e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1674e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0384e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1392e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7905e+01 </func>
</region>
</regions>
<internal rank="748" log_i="1723712895.684479" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="749" mpi_size="768" stamp_init="1723712830.446981" stamp_final="1723712895.683526" username="apac4" allocationname="unknown" flags="0" pid="576872" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52365e+01" utime="4.90746e+01" stime="7.27275e+00" mtime="2.87149e+01" gflop="0.00000e+00" gbyte="3.75652e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.87149e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000007b157a150a" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51054e+01" utime="4.90425e+01" stime="7.26267e+00" mtime="2.87149e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.87149e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4855e+08" > 4.5655e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4819e+08" > 2.9285e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7157e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6569e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.9073e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8754e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3795e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1766e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0385e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1396e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9127e+01 </func>
</region>
</regions>
<internal rank="749" log_i="1723712895.683526" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="750" mpi_size="768" stamp_init="1723712830.446947" stamp_final="1723712895.688302" username="apac4" allocationname="unknown" flags="0" pid="576873" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52414e+01" utime="4.72778e+01" stime="7.95672e+00" mtime="2.80638e+01" gflop="0.00000e+00" gbyte="3.77003e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.80638e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51157e+01" utime="4.72489e+01" stime="7.94321e+00" mtime="2.80638e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.80638e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4691e+08" > 5.1883e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4783e+08" > 3.6344e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.8901e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6415e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.9531e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0922e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3798e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1789e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0389e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1396e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8238e+01 </func>
</region>
</regions>
<internal rank="750" log_i="1723712895.688302" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="751" mpi_size="768" stamp_init="1723712830.446992" stamp_final="1723712895.692241" username="apac4" allocationname="unknown" flags="0" pid="576874" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52452e+01" utime="4.89585e+01" stime="7.36074e+00" mtime="2.89946e+01" gflop="0.00000e+00" gbyte="3.77605e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.89946e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4e614e714e814b255e814e8145c" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51183e+01" utime="4.89269e+01" stime="7.35016e+00" mtime="2.89946e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.89946e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4737e+08" > 4.5512e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4935e+08" > 2.9241e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1316e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6566e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0930e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3801e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1751e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0389e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1394e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8989e+01 </func>
</region>
</regions>
<internal rank="751" log_i="1723712895.692241" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="752" mpi_size="768" stamp_init="1723712830.446965" stamp_final="1723712895.685634" username="apac4" allocationname="unknown" flags="0" pid="576875" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52387e+01" utime="4.46144e+01" stime="8.90860e+00" mtime="2.78384e+01" gflop="0.00000e+00" gbyte="3.76472e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78384e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000a4148055a414a414f4" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51132e+01" utime="4.45936e+01" stime="8.88729e+00" mtime="2.78384e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78384e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.1458e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4856e+08" > 7.5213e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4840e+08" > 5.7607e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.1443e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6652e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.6354e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7398e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3779e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1951e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0384e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1395e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7480e+01 </func>
</region>
</regions>
<internal rank="752" log_i="1723712895.685634" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="753" mpi_size="768" stamp_init="1723712830.447017" stamp_final="1723712895.688146" username="apac4" allocationname="unknown" flags="0" pid="576876" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52411e+01" utime="4.90597e+01" stime="7.26798e+00" mtime="2.86536e+01" gflop="0.00000e+00" gbyte="3.78086e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86536e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51116e+01" utime="4.90274e+01" stime="7.25824e+00" mtime="2.86536e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86536e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.0068e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4825e+08" > 3.6064e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4992e+08" > 2.7438e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.0370e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6613e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.2875e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0812e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3778e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1977e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0365e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1395e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8837e+01 </func>
</region>
</regions>
<internal rank="753" log_i="1723712895.688146" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="754" mpi_size="768" stamp_init="1723712830.446984" stamp_final="1723712895.684499" username="apac4" allocationname="unknown" flags="0" pid="576877" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52375e+01" utime="4.57802e+01" stime="8.24922e+00" mtime="2.76781e+01" gflop="0.00000e+00" gbyte="3.77327e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.76781e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51112e+01" utime="4.57463e+01" stime="8.24137e+00" mtime="2.76781e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.76781e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 6.4373e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4887e+08" > 6.3359e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4937e+08" > 5.7209e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 5.7344e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6594e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.7602e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.9579e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3768e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2001e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0380e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1396e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7863e+01 </func>
</region>
</regions>
<internal rank="754" log_i="1723712895.684499" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="755" mpi_size="768" stamp_init="1723712830.446986" stamp_final="1723712895.692099" username="apac4" allocationname="unknown" flags="0" pid="576878" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52451e+01" utime="4.90216e+01" stime="7.31250e+00" mtime="2.92475e+01" gflop="0.00000e+00" gbyte="3.77670e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.92475e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000a514a514ac" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51160e+01" utime="4.89840e+01" stime="7.30795e+00" mtime="2.92475e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.92475e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4956e+08" > 3.6745e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4903e+08" > 2.8845e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4304e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6300e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.9141e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3779e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.1968e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0414e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1396e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9061e+01 </func>
</region>
</regions>
<internal rank="755" log_i="1723712895.692099" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="756" mpi_size="768" stamp_init="1723712830.446938" stamp_final="1723712895.689276" username="apac4" allocationname="unknown" flags="0" pid="576879" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52423e+01" utime="4.74528e+01" stime="7.83075e+00" mtime="2.86587e+01" gflop="0.00000e+00" gbyte="3.75813e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86587e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000000000006e156e1516" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51160e+01" utime="4.74213e+01" stime="7.82027e+00" mtime="2.86587e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86587e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4857e+08" > 4.7817e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4843e+08" > 4.2751e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4977e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6587e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.3494e-04 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.2982e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3772e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2037e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0374e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1395e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8251e+01 </func>
</region>
</regions>
<internal rank="756" log_i="1723712895.689276" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="757" mpi_size="768" stamp_init="1723712830.447014" stamp_final="1723712895.678661" username="apac4" allocationname="unknown" flags="0" pid="576880" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52316e+01" utime="4.89419e+01" stime="7.35249e+00" mtime="2.93072e+01" gflop="0.00000e+00" gbyte="3.77537e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.93072e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4cc15e415f6154055f615f1151b" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51021e+01" utime="4.89110e+01" stime="7.34127e+00" mtime="2.93072e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.93072e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.8147e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4851e+08" > 3.7946e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4802e+08" > 2.7910e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4658e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6529e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.5974e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.0404e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3767e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2100e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0401e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1392e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9049e+01 </func>
</region>
</regions>
<internal rank="757" log_i="1723712895.678661" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="758" mpi_size="768" stamp_init="1723712830.448415" stamp_final="1723712895.676984" username="apac4" allocationname="unknown" flags="0" pid="576881" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52286e+01" utime="4.67494e+01" stime="8.03418e+00" mtime="2.85977e+01" gflop="0.00000e+00" gbyte="3.74165e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.85977e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51023e+01" utime="4.67200e+01" stime="8.02178e+00" mtime="2.85977e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.85977e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 8.3447e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4793e+08" > 4.7787e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4875e+08" > 4.1984e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7564e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6371e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 8.6069e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.5332e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3757e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2099e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0405e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1397e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7955e+01 </func>
</region>
</regions>
<internal rank="758" log_i="1723712895.676984" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="759" mpi_size="768" stamp_init="1723712830.448095" stamp_final="1723712895.682805" username="apac4" allocationname="unknown" flags="0" pid="576882" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52347e+01" utime="4.90575e+01" stime="7.29899e+00" mtime="2.91785e+01" gflop="0.00000e+00" gbyte="3.77384e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.91785e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51070e+01" utime="4.90269e+01" stime="7.28795e+00" mtime="2.91785e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.91785e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4839e+08" > 3.6111e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4740e+08" > 2.7213e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3736e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6696e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.7917e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3769e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2066e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0377e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1391e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.9020e+01 </func>
</region>
</regions>
<internal rank="759" log_i="1723712895.682805" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="760" mpi_size="768" stamp_init="1723712830.446963" stamp_final="1723712895.693413" username="apac4" allocationname="unknown" flags="0" pid="576883" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52464e+01" utime="4.46652e+01" stime="9.00989e+00" mtime="2.78445e+01" gflop="0.00000e+00" gbyte="3.78460e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.78445e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000d814d7149d" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51179e+01" utime="4.46305e+01" stime="9.00221e+00" mtime="2.78445e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.78445e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4887e+08" > 7.4739e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4982e+08" > 6.6632e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7872e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6354e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 7.7009e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.7987e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3764e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2135e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0369e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1393e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.6878e+01 </func>
</region>
</regions>
<internal rank="760" log_i="1723712895.693413" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="761" mpi_size="768" stamp_init="1723712830.448021" stamp_final="1723712895.683862" username="apac4" allocationname="unknown" flags="0" pid="576884" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52358e+01" utime="4.85474e+01" stime="7.49489e+00" mtime="2.88882e+01" gflop="0.00000e+00" gbyte="3.77384e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.88882e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000dd15dd1521" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51059e+01" utime="4.85092e+01" stime="7.49105e+00" mtime="2.88882e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.88882e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.5367e-07 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4832e+08" > 4.8264e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4979e+08" > 3.0073e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5831e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6373e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8678e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3763e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2138e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0384e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1390e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8425e+01 </func>
</region>
</regions>
<internal rank="761" log_i="1723712895.683862" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="762" mpi_size="768" stamp_init="1723712830.448311" stamp_final="1723712895.678301" username="apac4" allocationname="unknown" flags="0" pid="576885" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52300e+01" utime="4.71244e+01" stime="8.16161e+00" mtime="2.81449e+01" gflop="0.00000e+00" gbyte="3.77853e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81449e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51018e+01" utime="4.70973e+01" stime="8.14644e+00" mtime="2.81449e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81449e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.2915e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4952e+08" > 6.0482e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4922e+08" > 4.1174e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.4977e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6431e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 6.1274e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 8.0557e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3759e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2179e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0390e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1397e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7628e+01 </func>
</region>
</regions>
<internal rank="762" log_i="1723712895.678301" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="763" mpi_size="768" stamp_init="1723712830.447030" stamp_final="1723712895.696796" username="apac4" allocationname="unknown" flags="0" pid="576886" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52498e+01" utime="4.88314e+01" stime="7.52255e+00" mtime="2.86867e+01" gflop="0.00000e+00" gbyte="3.75984e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.86867e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4f515f715f8159b56f815f81515" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51232e+01" utime="4.87984e+01" stime="7.51369e+00" mtime="2.86867e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.86867e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.5011e+08" > 4.7441e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4852e+08" > 3.1119e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.3474e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6577e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 2.0266e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 5.8079e-04 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3755e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2215e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0412e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1394e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8453e+01 </func>
</region>
</regions>
<internal rank="763" log_i="1723712895.696796" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="764" mpi_size="768" stamp_init="1723712830.448145" stamp_final="1723712895.686200" username="apac4" allocationname="unknown" flags="0" pid="576887" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52381e+01" utime="4.70298e+01" stime="8.17988e+00" mtime="2.81485e+01" gflop="0.00000e+00" gbyte="3.75164e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.81485e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf400000000c4148555c414c41473" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51116e+01" utime="4.69991e+01" stime="8.16874e+00" mtime="2.81485e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.81485e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 4.0531e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4847e+08" > 5.6661e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4976e+08" > 3.6505e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5228e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6414e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 9.2690e-03 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8590e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3753e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2206e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0391e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1394e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7642e+01 </func>
</region>
</regions>
<internal rank="764" log_i="1723712895.686200" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="765" mpi_size="768" stamp_init="1723712830.446989" stamp_final="1723712895.683474" username="apac4" allocationname="unknown" flags="0" pid="576888" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52365e+01" utime="4.88288e+01" stime="7.55825e+00" mtime="2.90594e+01" gflop="0.00000e+00" gbyte="3.76919e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90594e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf4000000000000000000000000" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51080e+01" utime="4.87949e+01" stime="7.55001e+00" mtime="2.90594e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90594e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 5.9605e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4740e+08" > 4.8411e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4834e+08" > 2.6360e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.5431e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6473e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4782e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 1.1398e-02 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3750e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2267e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0378e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1392e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8625e+01 </func>
</region>
</regions>
<internal rank="765" log_i="1723712895.683474" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="766" mpi_size="768" stamp_init="1723712830.447493" stamp_final="1723712895.696460" username="apac4" allocationname="unknown" flags="0" pid="576889" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52490e+01" utime="4.70592e+01" stime="8.14735e+00" mtime="2.84223e+01" gflop="0.00000e+00" gbyte="3.77815e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.84223e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf45f146114621460566214621481" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51240e+01" utime="4.70297e+01" stime="8.13450e+00" mtime="2.84223e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.84223e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 9.2983e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 3.0994e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4924e+08" > 6.0601e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4842e+08" > 4.0490e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.7723e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6510e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 5.2929e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 7.8959e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3745e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2316e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0393e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1395e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.7622e+01 </func>
</region>
</regions>
<internal rank="766" log_i="1723712895.696460" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
<task ipm_version="2.0.6" cookie="nocookie" mpi_rank="767" mpi_size="768" stamp_init="1723712830.447016" stamp_final="1723712895.686495" username="apac4" allocationname="unknown" flags="0" pid="576890" >
<job nhosts="32" ntasks="768" start="1723712830" final="1723712895" cookie="nocookie" code="unknown" >unknown</job>
<host mach_name="x86_64" mach_info="x86_64_Linux" >a06u22a</host>
<perf wtime="6.52395e+01" utime="4.87713e+01" stime="7.57617e+00" mtime="2.90934e+01" gflop="0.00000e+00" gbyte="3.76865e-01" ></perf>
<modules nmod="1">
<module name="MPI" time="2.90934e+01" ></module>
</modules>
<switch bytes_tx="0.00000e+00" bytes_rx="0.00000e+00" ></switch>
<cmdline realpath="/work/apac4/miniforge3/envs/apac-hpc-xbx/bin/python3.10" md5sum="43e3cdf40000000000000000ea14ea14cb" >python3 -m hoomd_benchmarks.md_pair_wca --device CPU -N 2000000 --benchmark_steps 10000 -v </cmdline>
<regions n="1" >
<region label="ipm_noregion" nexits="1" wtime="6.51147e+01" utime="4.87352e+01" stime="7.56993e+00" mtime="2.90934e+01" id="0" >
<modules nmod="1">
<module name="MPI" time="2.90934e+01" ></module>
</modules>
<func name="MPI_Init" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Finalize" count="1" bytes="0.0000e+00" > 0.0000e+00 </func>
<func name="MPI_Comm_rank" count="74" bytes="0.0000e+00" > 2.8610e-06 </func>
<func name="MPI_Comm_size" count="40" bytes="0.0000e+00" > 4.7684e-06 </func>
<func name="MPI_Isend" count="127164" bytes="8.4979e+08" > 4.6876e-01 </func>
<func name="MPI_Irecv" count="127164" bytes="8.4885e+08" > 2.6089e-02 </func>
<func name="MPI_Waitall" count="106776" bytes="0.0000e+00" > 6.6025e+00 </func>
<func name="MPI_Bcast" count="91" bytes="1.5731e+04" > 1.6558e+00 </func>
<func name="MPI_Gather" count="5" bytes="2.0000e+01" > 1.4067e-05 </func>
<func name="MPI_Gatherv" count="5" bytes="7.0000e+01" > 4.7259e-03 </func>
<func name="MPI_Scatter" count="19" bytes="7.6000e+01" > 1.3742e+00 </func>
<func name="MPI_Scatterv" count="19" bytes="5.1409e+05" > 2.2343e-01 </func>
<func name="MPI_Allgather" count="1" bytes="4.0000e+00" > 1.0403e-02 </func>
<func name="MPI_Allgatherv" count="1" bytes="1.0240e+04" > 1.1393e-01 </func>
<func name="MPI_Allreduce" count="21951" bytes="9.2380e+05" > 1.8614e+01 </func>
</region>
</regions>
<internal rank="767" log_i="1723712895.686495" log_t="1.7237e+09" report_delta="-1.0000e+00" fname="./apac4.1723712829.548146.ipm.xml" logrank="0" ></internal>
</task>
</ipm_job_profile>
